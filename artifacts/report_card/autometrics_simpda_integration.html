
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Fluency AutoMetric Report Card</title>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://cdn.datatables.net/2.0.8/css/dataTables.dataTables.min.css">
  <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  <script src="https://cdn.plot.ly/plotly-2.27.0.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>
  
  <style>
    body.dark-mode { background-color: #121212; color: #e0e0e0; }
    body.dark-mode .card { background-color: #1e1e1e; border-color: #333; color: #e0e0e0; }
    body.dark-mode .table, body-dark-mode .table td { background-color: #1e1e1e; color: #e0e0e0; border-color: #333; }
  </style>
  <script>const RC_CORR = {"metrics": [{"name": "Clarity_and_Coherence_of_Simplified_Meaning_Qwen3-32B", "r": 0.8249131252384262, "tau": 0.781126577552403, "x": [4.0, 2.0, 2.0, 3.0, 2.0, 3.0, 3.0, 2.0, 4.0, 1.0], "x_norm": [0.900200712, -0.2934252966666666, -0.2934252966666666, 0.3033877076666667, -0.2934252966666666, 0.3033877076666667, 0.3033877076666667, -0.2934252966666666, 0.900200712, -0.8902383009999999], "y": [0.4150416879999999, -0.5298062060000001, 0.116173038, 0.285011485, -0.8902383009999999, 0.459241533, 0.3930518129999999, 0.147488346, 0.9002007120000001, -0.748478352], "ids": ["1", "2", "3", "4", "5", "6", "7", "8", "9", "10"], "y_min": -0.8902383009999999, "y_max": 0.9002007120000001}, {"name": "fluency_Qwen3-32B_examples", "r": 0.8246953279093472, "tau": 0.8148353450645078, "x": [0.85, -0.5298062060000001, -0.5298062060000001, 0.3930518129999999, -0.748478352, 0.75, 0.45, -0.748478352, 0.9002007120000001, -0.748478352], "x_norm": [0.8456835441607307, -0.6527638587001541, -0.6527638587001541, 0.349445144105194, -0.8902383009999999, 0.7370851485059655, 0.41128996154166986, -0.8902383009999999, 0.900200712, -0.8902383009999999], "y": [0.4150416879999999, -0.5298062060000001, 0.116173038, 0.285011485, -0.8902383009999999, 0.459241533, 0.3930518129999999, 0.147488346, 0.9002007120000001, -0.748478352], "ids": ["1", "2", "3", "4", "5", "6", "7", "8", "9", "10"], "y_min": -0.8902383009999999, "y_max": 0.9002007120000001}, {"name": "Grammatical_Correctness_and_Sentence_Structure_Qwen3-32B", "r": 0.7726556722463451, "tau": 0.6956655929999346, "x": [5.0, 2.0, 3.0, 3.0, 2.0, 3.0, 3.0, 2.0, 4.0, 1.0], "x_norm": [0.900200712, -0.44262854774999993, 0.00498120550000003, 0.00498120550000003, -0.44262854774999993, 0.00498120550000003, 0.00498120550000003, -0.44262854774999993, 0.4525909587499999, -0.8902383009999999], "y": [0.4150416879999999, -0.5298062060000001, 0.116173038, 0.285011485, -0.8902383009999999, 0.459241533, 0.3930518129999999, 0.147488346, 0.9002007120000001, -0.748478352], "ids": ["1", "2", "3", "4", "5", "6", "7", "8", "9", "10"], "y_min": -0.8902383009999999, "y_max": 0.9002007120000001}, {"name": "ROUGE-1-p", "r": -0.14874740066850642, "tau": -0.06819943394704735, "x": [1.0, 0.7777777777777778, 0.5, 0.7777777777777778, 1.0, 0.7037037037037037, 0.6785714285714286, 0.9285714285714286, 0.9375, 0.8571428571428571], "x_norm": [0.900200712, 0.10445003955555565, -0.8902383009999999, 0.10445003955555565, 0.900200712, -0.1608001845925925, -0.25079579635714266, 0.6444237101428572, 0.6763958353749999, 0.3886467082857141], "y": [0.4150416879999999, -0.5298062060000001, 0.116173038, 0.285011485, -0.8902383009999999, 0.459241533, 0.3930518129999999, 0.147488346, 0.9002007120000001, -0.748478352], "ids": ["1", "2", "3", "4", "5", "6", "7", "8", "9", "10"], "y_min": -0.8902383009999999, "y_max": 0.9002007120000001}, {"name": "ROUGE-2-p", "r": 0.5313365677095196, "tau": 0.4140393356054125, "x": [1.0, 0.875, 0.75, 0.6111111111111112, 0.35714285714285715, 0.6129032258064516, 0.631578947368421, 1.0, 1.0, 0.6071428571428571], "x_norm": [0.900200712, 0.5520597928055555, 0.20391887361111127, -0.18290436993827142, -0.8902383009999999, -0.1779131022795698, -0.1258988393099415, 0.900200712, 0.900200712, -0.1939564626111111], "y": [0.4150416879999999, -0.5298062060000001, 0.116173038, 0.285011485, -0.8902383009999999, 0.459241533, 0.3930518129999999, 0.147488346, 0.9002007120000001, -0.748478352], "ids": ["1", "2", "3", "4", "5", "6", "7", "8", "9", "10"], "y_min": -0.8902383009999999, "y_max": 0.9002007120000001}, {"name": "ROUGE-L-p", "r": 0.427792625512383, "tau": 0.28888888888888886, "x": [1.0, 0.823529411764706, 0.6, 0.65625, 0.4761904761904761, 0.6551724137931035, 0.6440677966101694, 0.9454545454545454, 0.8823529411764706, 0.6938775510204083], "x_norm": [0.900200712, 0.2970046808823532, -0.46704362519999987, -0.2747748902812498, -0.8902383009999999, -0.27845819937930993, -0.316415011779661, 0.7137583023818179, 0.498070024588235, -0.14615975014285665], "y": [0.4150416879999999, -0.5298062060000001, 0.116173038, 0.285011485, -0.8902383009999999, 0.459241533, 0.3930518129999999, 0.147488346, 0.9002007120000001, -0.748478352], "ids": ["1", "2", "3", "4", "5", "6", "7", "8", "9", "10"], "y_min": -0.8902383009999999, "y_max": 0.9002007120000001}, {"name": "ROUGE-Lsum-p", "r": 0.38214764262965817, "tau": 0.28888888888888886, "x": [1.0, 0.625, 0.18181818181818182, 0.5384615384615384, 0.3, 0.46153846153846156, 0.4444444444444444, 0.7777777777777778, 0.8, 0.55], "x_norm": [0.900200712, 0.07958283104166686, -0.8902383009999999, -0.10979052610256401, -0.6316193324555555, -0.27812239911965797, -0.31552948201234565, 0.413908634395062, 0.4625378421555557, -0.08454074514999987], "y": [0.4150416879999999, -0.5298062060000001, 0.116173038, 0.285011485, -0.8902383009999999, 0.459241533, 0.3930518129999999, 0.147488346, 0.9002007120000001, -0.748478352], "ids": ["1", "2", "3", "4", "5", "6", "7", "8", "9", "10"], "y_min": -0.8902383009999999, "y_max": 0.9002007120000001}, {"name": "ROUGE-1-r", "r": 0.4175487814266966, "tau": 0.3146266024828463, "x": [1.0, 0.625, 0.2857142857142857, 0.4, 0.15384615384615385, 0.4, 0.36666666666666664, 0.9473684210526315, 0.7272727272727273, 0.4074074074074074], "x_norm": [0.900200712, 0.10671069487499996, -0.6112088444285715, -0.3693833153999999, -0.8902383009999999, -0.3693833153999999, -0.43991576136666666, 0.7888336920526313, 0.3231170631818183, -0.35370943851851855], "y": [0.4150416879999999, -0.5298062060000001, 0.116173038, 0.285011485, -0.8902383009999999, 0.459241533, 0.3930518129999999, 0.147488346, 0.9002007120000001, -0.748478352], "ids": ["1", "2", "3", "4", "5", "6", "7", "8", "9", "10"], "y_min": -0.8902383009999999, "y_max": 0.9002007120000001}, {"name": "ROUGE-2-r", "r": 0.42674305404361434, "tau": 0.28888888888888886, "x": [1.0, 0.625, 0.2222222222222222, 0.45161290322580644, 0.17391304347826086, 0.42857142857142855, 0.38596491228070173, 0.7924528301886792, 0.7500000000000001, 0.46808510638297873], "x_norm": [0.900200712, 0.08743563373026308, -0.7855342651520467, -0.28835897235144303, -0.8902383009999999, -0.338298454887218, -0.4306426910147737, 0.4503684674230386, 0.3583573264868425, -0.252657555049272], "y": [0.4150416879999999, -0.5298062060000001, 0.116173038, 0.285011485, -0.8902383009999999, 0.459241533, 0.3930518129999999, 0.147488346, 0.9002007120000001, -0.748478352], "ids": ["1", "2", "3", "4", "5", "6", "7", "8", "9", "10"], "y_min": -0.8902383009999999, "y_max": 0.9002007120000001}, {"name": "ROUGE-L-r", "r": 0.05863898517294575, "tau": 0.022222222222222223, "x": [1.0, 0.7777777777777778, 0.5, 0.7407407407407407, 0.8181818181818182, 0.6666666666666666, 0.6428571428571429, 0.8928571428571429, 0.9375, 0.8095238095238095], "x_norm": [0.900200712, 0.10445003955555565, -0.8902383009999999, -0.028175072518518585, 0.24913198000000025, -0.2934252966666667, -0.37868429728571407, 0.5165352092142859, 0.6763958353749999, 0.218128707047619], "y": [0.4150416879999999, -0.5298062060000001, 0.116173038, 0.285011485, -0.8902383009999999, 0.459241533, 0.3930518129999999, 0.147488346, 0.9002007120000001, -0.748478352], "ids": ["1", "2", "3", "4", "5", "6", "7", "8", "9", "10"], "y_min": -0.8902383009999999, "y_max": 0.9002007120000001}, {"name": "ROUGE-Lsum-r", "r": 0.44540290119781795, "tau": 0.34099716973523675, "x": [1.0, 0.875, 0.75, 0.5483870967741935, 0.35714285714285715, 0.5806451612903226, 0.5806451612903226, 1.0, 0.9090909090909091, 0.5714285714285714], "x_norm": [0.900200712, 0.5520597928055555, 0.20391887361111127, -0.3575987379928315, -0.8902383009999999, -0.26775592013620053, -0.26775592013620053, 0.900200712, 0.6470073162222223, -0.2934252966666666], "y": [0.4150416879999999, -0.5298062060000001, 0.116173038, 0.285011485, -0.8902383009999999, 0.459241533, 0.3930518129999999, 0.147488346, 0.9002007120000001, -0.748478352], "ids": ["1", "2", "3", "4", "5", "6", "7", "8", "9", "10"], "y_min": -0.8902383009999999, "y_max": 0.9002007120000001}, {"name": "ROUGE-1-f1", "r": 0.47832541244258164, "tau": 0.28888888888888886, "x": [1.0, 0.823529411764706, 0.6, 0.625, 0.4, 0.6206896551724138, 0.6101694915254238, 0.9090909090909091, 0.8823529411764706, 0.6296296296296297], "x_norm": [0.900200712, 0.37360100229411786, -0.2934252966666667, -0.218823671125, -0.8902383009999999, -0.2316860203563218, -0.26307887271751396, 0.6289220736666665, 0.5491342388627449, -0.20500855528395046], "y": [0.4150416879999999, -0.5298062060000001, 0.116173038, 0.285011485, -0.8902383009999999, 0.459241533, 0.3930518129999999, 0.147488346, 0.9002007120000001, -0.748478352], "ids": ["1", "2", "3", "4", "5", "6", "7", "8", "9", "10"], "y_min": -0.8902383009999999, "y_max": 0.9002007120000001}, {"name": "ROUGE-2-f1", "r": 0.05863898517294575, "tau": 0.022222222222222223, "x": [1.0, 0.7777777777777778, 0.5, 0.7407407407407407, 0.8181818181818182, 0.6666666666666666, 0.6428571428571429, 0.8928571428571429, 0.9375, 0.8095238095238095], "x_norm": [0.900200712, 0.10445003955555565, -0.8902383009999999, -0.028175072518518585, 0.24913198000000025, -0.2934252966666667, -0.37868429728571407, 0.5165352092142859, 0.6763958353749999, 0.218128707047619], "y": [0.4150416879999999, -0.5298062060000001, 0.116173038, 0.285011485, -0.8902383009999999, 0.459241533, 0.3930518129999999, 0.147488346, 0.9002007120000001, -0.748478352], "ids": ["1", "2", "3", "4", "5", "6", "7", "8", "9", "10"], "y_min": -0.8902383009999999, "y_max": 0.9002007120000001}, {"name": "ROUGE-L-f1", "r": 0.44540290119781795, "tau": 0.34099716973523675, "x": [1.0, 0.875, 0.75, 0.5483870967741935, 0.35714285714285715, 0.5806451612903226, 0.5806451612903226, 1.0, 0.9090909090909091, 0.5714285714285714], "x_norm": [0.900200712, 0.5520597928055555, 0.20391887361111127, -0.3575987379928315, -0.8902383009999999, -0.26775592013620053, -0.26775592013620053, 0.900200712, 0.6470073162222223, -0.2934252966666666], "y": [0.4150416879999999, -0.5298062060000001, 0.116173038, 0.285011485, -0.8902383009999999, 0.459241533, 0.3930518129999999, 0.147488346, 0.9002007120000001, -0.748478352], "ids": ["1", "2", "3", "4", "5", "6", "7", "8", "9", "10"], "y_min": -0.8902383009999999, "y_max": 0.9002007120000001}, {"name": "ROUGE-Lsum-f1", "r": 0.47832541244258164, "tau": 0.28888888888888886, "x": [1.0, 0.823529411764706, 0.6, 0.625, 0.4, 0.6206896551724138, 0.6101694915254238, 0.9090909090909091, 0.8823529411764706, 0.6296296296296297], "x_norm": [0.900200712, 0.37360100229411786, -0.2934252966666667, -0.218823671125, -0.8902383009999999, -0.2316860203563218, -0.26307887271751396, 0.6289220736666665, 0.5491342388627449, -0.20500855528395046], "y": [0.4150416879999999, -0.5298062060000001, 0.116173038, 0.285011485, -0.8902383009999999, 0.459241533, 0.3930518129999999, 0.147488346, 0.9002007120000001, -0.748478352], "ids": ["1", "2", "3", "4", "5", "6", "7", "8", "9", "10"], "y_min": -0.8902383009999999, "y_max": 0.9002007120000001}], "regression": {"name": "Autometrics_Regression_fluency", "r": 0.7125216737462297, "tau": 0.5555555555555555, "x": [0.8126141690607317, 0.09478251126734392, -0.19116471534365181, -0.030426967265826235, -0.5900955022002614, -0.0016965524909065555, -0.04227134202160927, 0.27020747963399155, 0.5777659870058702, -0.35202931164568185], "x_norm": [0.8126141690607317, 0.09478251126734392, -0.19116471534365181, -0.030426967265826235, -0.5900955022002614, -0.0016965524909065555, -0.04227134202160927, 0.27020747963399155, 0.5777659870058702, -0.35202931164568185], "y": [0.4150416879999999, -0.5298062060000001, 0.116173038, 0.285011485, -0.8902383009999999, 0.459241533, 0.3930518129999999, 0.147488346, 0.9002007120000001, -0.748478352], "ids": ["1", "2", "3", "4", "5", "6", "7", "8", "9", "10"], "y_min": -0.8902383009999999, "y_max": 0.9002007120000001}}; const RC_RUNTIME = {"per_metric": {"Clarity_and_Coherence_of_Simplified_Meaning_Qwen3-32B": [0.05559396743774414, 0.0590662956237793, 0.060041189193725586, 0.0615849494934082, 0.0639030933380127, 0.06392288208007812], "fluency_Qwen3-32B_examples": [0.06059527397155762, 0.06264090538024902, 0.06522369384765625, 0.06537103652954102, 0.0664823055267334, 0.06760168075561523], "Grammatical_Correctness_and_Sentence_Structure_Qwen3-32B": [0.04095911979675293, 0.04159379005432129, 0.04295206069946289, 0.04385781288146973, 0.04470419883728027, 0.04873347282409668], "ROUGE": [0.008502483367919922, 0.008578300476074219, 0.00890493392944336, 0.00911402702331543, 0.009399175643920898, 0.009461164474487305]}, "aggregate": {"sequence_times": [0.19852757453918457, 0.20495963096618652, 0.17642784118652344, 0.1678147315979004, 0.17797350883483887, 0.18248462677001953, 0.16653084754943848, 0.18953180313110352, 0.17459464073181152, 0.17122173309326172], "parallel_times": [0.07568073272705078, 0.08353447914123535, 0.06392288208007812, 0.06760168075561523, 0.06537103652954102, 0.06522369384765625, 0.057119131088256836, 0.07061338424682617, 0.0639030933380127, 0.06839561462402344], "sequence_mean": 0.18100669384002685, "parallel_mean": 0.06813657283782959, "sequence_ci": 0.008451882915937455, "parallel_ci": 0.004739818590478489}, "sample_size": 10}; const RC_ROB = {"available": true, "scores": {"Clarity_and_Coherence_of_Simplified_Meaning_Qwen3-32B": {"sensitivity": -0.027777777777777735, "stability": 0.25}, "fluency_Qwen3-32B_examples": {"sensitivity": 0.3023558039526917, "stability": 0.6355379176165102}, "Grammatical_Correctness_and_Sentence_Structure_Qwen3-32B": {"sensitivity": -0.041666666666666685, "stability": 0.4375}, "ROUGE-1-p": {"sensitivity": 0.4593623000571473, "stability": 0.5754555873275236}, "ROUGE-2-p": {"sensitivity": -0.1290696610672687, "stability": 0.8188511050353157}, "ROUGE-L-p": {"sensitivity": 0.29068073963296714, "stability": 0.7197476792597509}, "ROUGE-Lsum-p": {"sensitivity": 0.39627645041736875, "stability": 0.5709407291303843}, "ROUGE-1-r": {"sensitivity": 0.04972181462460257, "stability": 0.9436502659574468}, "ROUGE-2-r": {"sensitivity": 0.23254960310543676, "stability": 0.7167018407473963}, "ROUGE-L-r": {"sensitivity": 0.5427378653889197, "stability": 0.5251754253472976}, "ROUGE-Lsum-r": {"sensitivity": -0.015434778592673393, "stability": 0.9212752275425407}, "ROUGE-1-f1": {"sensitivity": 0.3911306506266781, "stability": 0.6482989115159793}, "ROUGE-2-f1": {"sensitivity": 0.5427378653889197, "stability": 0.5251754253472976}, "ROUGE-L-f1": {"sensitivity": -0.015434778592673393, "stability": 0.9212752275425407}, "ROUGE-Lsum-f1": {"sensitivity": 0.3911306506266781, "stability": 0.6482989115159793}, "Autometrics_Regression_fluency": {"sensitivity": 0.1524445306191921, "stability": 0.6847728266531321}}}; const RC_DOCS = {"Clarity_and_Coherence_of_Simplified_Meaning_Qwen3-32B": "---\n# Metric Card for Clarity_and_Coherence_of_Simplified_Meaning_Qwen3-32B\n\n**Clarity and Coherence of Simplified Meaning** The simplified sentence must clearly convey the original intent without ambiguity or omissions. Bad examples sometimes drop critical details or use vague terms (e.g., \"animals have a response... (ermine snowshoe, or hare)\").\n\n## Metric Details\n\n**Clarity_and_Coherence_of_Simplified_Meaning_Qwen3-32B** is a **reference-based** LLM-as-a-Judge metric that prompts an LLM to rate a system output along a single, run-time-specified evaluation axis.\nIn this case the axis is `**Clarity and Coherence of Simplified Meaning** The simplified sentence must clearly convey the original intent without ambiguity or omissions. Bad examples sometimes drop critical details or use vague terms (e.g., \"animals have a response... (ermine snowshoe, or hare)\").`.\n\nThe prompt supplies:\n\n1. **Task description** *d*\n2. **Axis rubric** `**Clarity and Coherence of Simplified Meaning** The simplified sentence must clearly convey the original intent without ambiguity or omissions. Bad examples sometimes drop critical details or use vague terms (e.g., \"animals have a response... (ermine snowshoe, or hare)\").`\n3. **Input text** *x*\n4. **Reference text** *r*\n5. **Output text** *y*\n\nGreedy decoding (temperature = 0) yields an integer score $\\hat{s}\\!\\in\\!\\{1,2,3,4,5\\}$; higher = better adherence to the axis.\n\n- **Metric Type:** LLM as a Judge\n- **Range:** 1-5 (1 = worst, 5 = best)\n- **Higher is Better?:** Yes\n- **Reference-Based?:** Yes\n- **Input-Required?:** Yes (plus reference)\n\n### Formal Definition\n\nLet $f _{\\theta}$ be the LLM and\n$\\pi _{\text{RB}}(d,\\{axis\\},x,r,y)$ construct the textual prompt.\n\n$$\n\\hat{s} \\;=\\; \\operatorname*{arg\\,max}\\limits_{s \\in \\{1,\\dots,5\\}} f _{\theta}\\!\bigl(s \\,\bigl|\\, \\pi _{\text{RB}}(d,\\{axis\\},x,r,y)\bigr)\n$$\n\nThe metric value is $\\operatorname{LJ}^{\text{RB}}_{\\{axis\\}}(d,x,r,y)=\\hat{s}$.\n\n### Inputs and Outputs\n- **Inputs:**\n  - **Task description** *d*\n  - **Axis rubric** `**Clarity and Coherence of Simplified Meaning** The simplified sentence must clearly convey the original intent without ambiguity or omissions. Bad examples sometimes drop critical details or use vague terms (e.g., \"animals have a response... (ermine snowshoe, or hare)\").`\n  - **Input text** *x*\n  - **Reference text** *r*\n  - **Output text** *y*\n- **Outputs:**\n  - Scalar score $\\hat{s} \\in \\{1,2,3,4,5\\}$\n\n## Intended Use\n\n- **Domain:** Text Simplification\n- **Tasks:** \n  - Educational Content Adaptation\n  - Technical Writing for Non-Experts\n  - Public Communication Simplification\n  - Accessibility Improvements for Reading Difficulties\n  - General Text Simplification for Broad Audiences\n- **Best Suited For:** \n  - When the original text contains complex terminology requiring precise simplification without losing key details\n  - When the audience requires unambiguous, straightforward language (e.g., students, non-native speakers)\n  - When the task emphasizes strict adherence to the original meaning while improving readability\n  - When the simplified text must avoid vague or placeholder terms (e.g., 'response...' in the bad example)\n- **Not Recommended For:** \n  - When the original text is inherently ambiguous or relies on context not provided in the input\n  - When the task allows for creative reinterpretation rather than strict fidelity to the original meaning\n  - When domain-specific knowledge (e.g., technical jargon in niche fields) is required to evaluate simplification quality\n  - When the simplified text is intentionally concise to the point of omitting non-essential details (if the axis prioritizes completeness)\n\n## Metric Implementation\n\n### Reference Implementations\n\n- **Libraries/Packages:**\n  - [AutoMetrics LLM as a Judge (reference-based)](https://github.com/XenonMolecule/autometrics/blob/main/autometrics/metrics/generated/GeneratedLLMJudgeMetric.py)\n\n### Computational Complexity\n\n- **Efficiency:**\n  - Requires a single LLM call per input-output pair.\n  - AutoMetrics does parallel calls on batched inputs.\n\n- **Scalability:**\n  - Performance is linear in the number of input-output pairs.\n  - Performance depends on the underlying LLM model and the dataset size.  Additional consideration would include whether or not the LLM is a reasoning model.\n\n## Known Limitations\n\n- **Biases:** \n  - The model may prioritize brevity over completeness, omitting critical details to make the sentence shorter.\n  - It might favor generic terms (e.g., 'animals' instead of 'ermine') to avoid specificity, risking ambiguity.\n  - The model could assume all audiences lack domain knowledge, over-simplifying technical terms unnecessarily.\n  - It may conflate grammatical simplicity with conceptual clarity, failing to address true complexity.\n- **Task Misalignment Risks:** \n  - Focusing on grammatical structure rather than ensuring the simplified meaning aligns with the original intent.\n  - Treating simplification as paraphrasing instead of targeted accessibility improvement for non-experts.\n  - Overemphasizing vocabulary simplicity at the cost of logical coherence or contextual accuracy.\n  - Failing to distinguish between simplification and trivialization, which could distort the original message.\n- **Failure Cases:** \n  - Dropping essential qualifiers (e.g., 'statistically significant' becomes 'important') that alter the original meaning.\n  - Replacing specific terms with vague placeholders (e.g., 'X species' instead of 'ermine') to avoid complexity.\n  - Producing a grammatically correct but logically incoherent sentence due to over-simplification.\n  - Retaining complex jargon under the guise of 'clarity' while failing to explain it for broader audiences.\n\n## Related Metrics\n\n- **Related Metrics:**\n  - **LevenshteinDistance:** Levenshtein Distance measures the minimum number of single-character edits\u2014insertions, deletions, or substitutions\u2014required to transform one sequence into another.\n  - **BARTScore:** BARTScore is a reference-based evaluation metric for text generation that formulates evaluation as a text generation task.\n  - **PseudoPARENT:** **PseudoPARENT** is a *custom adaptation* of the PARENT metric for evaluating text generation from structured inputs.\n\n## Further Reading\n\n- **Papers:**\n  - [Autometrics](https://github.com/XenonMolecule/autometrics)\n  - [Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena](https://openreview.net/pdf?id=uccHPGDlao)\n\n## Citation\n\n```\n@software{Ryan_Autometrics_2025,\n    author = {Ryan, Michael J. and Zhang, Yanzhe and Salunkhe, Amol and Chu, Yi and Rahman, Emily and Xu, Di and Yang, Diyi},\n    license = {MIT},\n    title = {{Autometrics}},\n    url = {https://github.com/XenonMolecule/autometrics},\n    version = {1.0.0},\n    year = {2025}\n}\n```\n\n## Metric Card Authors\n\n- **Authors:** This metric card was automatically generated by Qwen3-32B.\n- **Acknowledgement of AI Assistance:** This metric card was entirely automatically generated by Qwen3-32B using the Autometrics library. No human intervention was involved. User discretion is advised.\n- **Contact:** For questions about the autometrics library, please contact [Michael J Ryan](mailto:mryan0@stanford.edu).", "fluency_Qwen3-32B_examples": "---\n# Metric Card for fluency_Qwen3-32B_examples\n\nExample-based llm as a judge metric for 'fluency'.  The original task description for the task we are evaluating is: Given a complicated original sentence, simplify it in a way such that a broader audience could easily understand it.\n\n## Metric Details\n\n**fluency_Qwen3-32B_examples** is a **reference-based** example-based LLM-as-a-Judge metric that uses optimized few-shot examples to evaluate system outputs.\nThe evaluation axis is: `Score for fluency`.\n\nExample-based LLM judging differs from standard LLM-as-a-Judge by:\n\n1. **Example Selection**: Uses quintile-based bucketing to select diverse examples across score ranges\n2. **Few-Shot Optimization**: Optimizes example selection through multiple attempts and evaluation\n3. **Consistent Scoring**: Examples provide concrete scoring patterns for the LLM to follow\n\nThis metric was optimized using 5 attempts with 2 examples per score range.\n\n### Optimized Examples\n\nThe final optimized prompt includes 9 carefully selected examples.\n\n| Input Text | Score |\n|------------|-------|\n| \u00abInput (original): \u00aba town may be correctly described as a market town or as having market rights even if it no longer holds a market, provided the right to do so still exists.\u00bb Reference 1 (ref1): \u00aba town can be correctly described as a market town or as having rights to trade even if it doesn't have a market anymore because the right to do it is still exists.\u00bb Reference 2 (ref2): \u00aba town can ... | -0.748478352 |\n| \u00abInput (original): \u00aba few animals have chromatic response, changing color in changing environments, either seasonally (ermine, snowshoe hare) or far more rapidly with chromatophores in their integument (the cephalopod family).\u00bb Reference 1 (ref1): \u00absome animals change color in different environments. sometimes this is because of the season.\u00bb Reference 2 (ref2): \u00aba few animals have a chromatic r... | -0.8902383009999999 |\n| \u00abInput (original): \u00aba bastion on the eastern approaches was built later.\u00bb Reference 1 (ref1): \u00aba fort on the eastern access road was built later.\u00bb Reference 2 (ref2): \u00aba fortification on the eastern approaches was built later.\u00bb Reference 3 (ref3): \u00aba support on the east was built later.\u00bb Reference 4 (ref4): \u00aban extension on the eastern side was built later.\u00bb Reference 5 (ref5): \u00aba fort on the e... | 0.116173038 |\n\n*Showing 3 of 9 total examples.*\n\n### Evaluation Process\n\nThe evaluation follows this process:\n\n1. **Task description** *d*\n2. **Evaluation axis** `Score for fluency`\n3. **Optimized examples** showing score patterns\n4. **Input text** *x*\n5. **Reference text** *r*\n6. **Output text** *y*\n\nThe LLM follows the example patterns to assign scores $\\hat{s}\\!\\in\\!\\{1,2,3,4,5\\}$ within the suggested range; higher = better adherence to the axis.\n\n- **Metric Type:** Example-based LLM as a Judge\n- **Range:** Variable (depends on suggested range, typically 1-5)\n- **Higher is Better?:** Yes\n- **Reference-Based?:** Yes\n- **Input-Required?:** Yes (plus reference)\n\n### Example Optimization Details\n\n- **Optimization Attempts**: 5\n- **Examples per Score Range**: 2\n- **Evaluation Function**: inverse_distance\n- **Score Range**: -0.8902383009999999 to 0.9002007120000001\n- **Random Seed**: 43 (for reproducible example selection)\n\n### Inputs and Outputs\n- **Inputs:**\n  - **Task description** *d*\n  - **Evaluation axis** `Score for fluency`\n  - **Optimized examples** (embedded in prompt)\n  - **Input text** *x*\n  - **Reference text** *r*\n  - **Output text** *y*\n- **Outputs:**\n  - Scalar score within range -0.8902383009999999-0.9002007120000001\n\n## Intended Use\n\n- **Domain:** Text Generation\n- **Tasks:** \n  - Sentence simplification\n  - Text paraphrasing for accessibility\n  - Generating clear explanations for technical content\n- **Best Suited For:** \n  - When high-quality example pairs (complex vs. simplified) are available\n  - For tasks requiring natural-sounding output over strict fidelity\n  - When fluency is prioritized over preserving exact terminology\n  - In domains with established simplification patterns (e.g., news summaries)\n- **Not Recommended For:** \n  - When preserving precise technical terms is critical\n  - For highly domain-specific content without available examples\n  - When dealing with culturally nuanced or idiomatic expressions\n  - In cases requiring strict adherence to original sentence structure\n\n## Metric Implementation\n\n### Reference Implementations\n\n- **Libraries/Packages:**\n  - [AutoMetrics Example-based LLM Judge (reference-based)](https://github.com/XenonMolecule/autometrics/blob/main/autometrics/metrics/generated/GeneratedExampleRubric.py)\n  - [DSPy Few-shot Optimization](https://dspy-docs.vercel.app/)\n\n### Computational Complexity\n\n- **Efficiency:**\n  - Requires a single LLM call per input-output pair (same as basic LLM judge).\n  - AutoMetrics does parallel calls on batched inputs.\n  - One-time optimization cost during metric creation.\n\n- **Scalability:**\n  - Performance is linear in the number of input-output pairs.\n  - Performance depends on the underlying LLM model and the dataset size.\n  - Example optimization improves consistency but doesn't affect runtime complexity.\n\n## Known Limitations\n\n- **Biases:** \n  - Preference for complex vocabulary if it aligns with original sentence structure\n  - Overemphasis on grammatical correctness at the expense of clarity\n  - Bias toward preserving syntactic patterns from the original text\n  - Assumption that shorter sentences are inherently more fluent\n  - Cultural or linguistic biases in what constitutes 'natural' phrasing\n- **Task Misalignment Risks:** \n  - High fluency score for a simplification that retains complex terminology\n  - Rewarding literal translations that maintain original structure but lack simplicity\n  - Failing to account for domain-specific jargon in fluency assessment\n  - Prioritizing sentence brevity over semantic completeness\n  - Inconsistent application of fluency criteria across different text genres\n- **Failure Cases:** \n  - Producing a grammatically correct but unnatural simplification (e.g., 'The cat which was black ran' instead of 'The black cat ran')\n  - Creating a fluent sentence that introduces new ambiguities not present in the original\n  - Simplifying to the point of losing critical information while maintaining fluency\n  - Generating fluent but factually inconsistent simplifications\n  - Failing to handle nested clauses or passive voice effectively while preserving fluency\n\n## Related Metrics\n\n- **Related Metrics:**\n  - **LevenshteinDistance:** Levenshtein Distance measures the minimum number of single-character edits\u2014insertions, deletions, or substitutions\u2014required to transform one sequence into another.\n  - **BARTScore:** BARTScore is a reference-based evaluation metric for text generation that formulates evaluation as a text generation task.\n  - **PseudoPARENT:** **PseudoPARENT** is a *custom adaptation* of the PARENT metric for evaluating text generation from structured inputs.\n\n## Further Reading\n\n- **Papers:**\n  - [Autometrics](https://github.com/XenonMolecule/autometrics)\n  - [Few-Shot Learning with DSPy](https://dspy-docs.vercel.app/docs/building-blocks/optimizers)\n  - [Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena](https://openreview.net/pdf?id=uccHPGDlao)\n\n## Citation\n\n```\n@software{Ryan_Autometrics_2025,\n    author = {Ryan, Michael J. and Zhang, Yanzhe and Salunkhe, Amol and Chu, Yi and Rahman, Emily and Xu, Di and Yang, Diyi},\n    license = {MIT},\n    title = {{Autometrics}},\n    url = {https://github.com/XenonMolecule/autometrics},\n    version = {1.0.0},\n    year = {2025}\n}\n```\n\n## Metric Card Authors\n\n- **Authors:** This metric card was automatically generated by Qwen3-32B.\n- **Acknowledgement of AI Assistance:** This metric card was entirely automatically generated by Qwen3-32B using the Autometrics library. No human intervention was involved. User discretion is advised.\n- **Contact:** For questions about the autometrics library, please contact [Michael J Ryan](mailto:mryan0@stanford.edu).", "Grammatical_Correctness_and_Sentence_Structure_Qwen3-32B": "---\n# Metric Card for Grammatical_Correctness_and_Sentence_Structure_Qwen3-32B\n\n**Grammatical Correctness and Sentence Structure** Proper grammar, complete sentences, and logical phrasing ensure fluency. Bad examples often have missing words, incorrect verb forms, or awkward syntax (e.g., \"provided the right to do exists\").\n\n## Metric Details\n\n**Grammatical_Correctness_and_Sentence_Structure_Qwen3-32B** is a **reference-based** LLM-as-a-Judge metric that prompts an LLM to rate a system output along a single, run-time-specified evaluation axis.\nIn this case the axis is `**Grammatical Correctness and Sentence Structure** Proper grammar, complete sentences, and logical phrasing ensure fluency. Bad examples often have missing words, incorrect verb forms, or awkward syntax (e.g., \"provided the right to do exists\").`.\n\nThe prompt supplies:\n\n1. **Task description** *d*\n2. **Axis rubric** `**Grammatical Correctness and Sentence Structure** Proper grammar, complete sentences, and logical phrasing ensure fluency. Bad examples often have missing words, incorrect verb forms, or awkward syntax (e.g., \"provided the right to do exists\").`\n3. **Input text** *x*\n4. **Reference text** *r*\n5. **Output text** *y*\n\nGreedy decoding (temperature = 0) yields an integer score $\\hat{s}\\!\\in\\!\\{1,2,3,4,5\\}$; higher = better adherence to the axis.\n\n- **Metric Type:** LLM as a Judge\n- **Range:** 1-5 (1 = worst, 5 = best)\n- **Higher is Better?:** Yes\n- **Reference-Based?:** Yes\n- **Input-Required?:** Yes (plus reference)\n\n### Formal Definition\n\nLet $f _{\\theta}$ be the LLM and\n$\\pi _{\text{RB}}(d,\\{axis\\},x,r,y)$ construct the textual prompt.\n\n$$\n\\hat{s} \\;=\\; \\operatorname*{arg\\,max}\\limits_{s \\in \\{1,\\dots,5\\}} f _{\theta}\\!\bigl(s \\,\bigl|\\, \\pi _{\text{RB}}(d,\\{axis\\},x,r,y)\bigr)\n$$\n\nThe metric value is $\\operatorname{LJ}^{\text{RB}}_{\\{axis\\}}(d,x,r,y)=\\hat{s}$.\n\n### Inputs and Outputs\n- **Inputs:**\n  - **Task description** *d*\n  - **Axis rubric** `**Grammatical Correctness and Sentence Structure** Proper grammar, complete sentences, and logical phrasing ensure fluency. Bad examples often have missing words, incorrect verb forms, or awkward syntax (e.g., \"provided the right to do exists\").`\n  - **Input text** *x*\n  - **Reference text** *r*\n  - **Output text** *y*\n- **Outputs:**\n  - Scalar score $\\hat{s} \\in \\{1,2,3,4,5\\}$\n\n## Intended Use\n\n- **Domain:** Text Generation (specifically sentence simplification)\n- **Tasks:** \n  - Sentence Simplification\n  - Grammar Checking\n  - Text Clarity Evaluation\n  - Syntax Correction\n  - Readability Enhancement\n- **Best Suited For:** \n  - When evaluating simplified sentences for non-native speakers or educational contexts requiring clarity\n  - For tasks where grammatical accuracy is critical (e.g., academic writing, professional communication)\n  - When the original text contains overly complex syntax that risks misinterpretation\n  - In scenarios requiring adherence to formal writing standards (e.g., legal or technical documentation)\n- **Not Recommended For:** \n  - When the task prioritizes creative or stylistic expression over strict grammatical rules\n  - For evaluating domain-specific jargon simplification where technical accuracy is paramount\n  - In cases where the original sentence's complexity is intentional (e.g., literary or poetic devices)\n  - When evaluating multilingual or code-switched text where grammar rules vary dynamically\n\n## Metric Implementation\n\n### Reference Implementations\n\n- **Libraries/Packages:**\n  - [AutoMetrics LLM as a Judge (reference-based)](https://github.com/XenonMolecule/autometrics/blob/main/autometrics/metrics/generated/GeneratedLLMJudgeMetric.py)\n\n### Computational Complexity\n\n- **Efficiency:**\n  - Requires a single LLM call per input-output pair.\n  - AutoMetrics does parallel calls on batched inputs.\n\n- **Scalability:**\n  - Performance is linear in the number of input-output pairs.\n  - Performance depends on the underlying LLM model and the dataset size.  Additional consideration would include whether or not the LLM is a reasoning model.\n\n## Known Limitations\n\n- **Biases:** \n  - The model may favor complex grammatical structures it is familiar with, assuming they are inherently 'better' than simpler alternatives.\n  - Bias toward formal grammar rules might lead to rejecting colloquial or conversational phrasing that is easier for a broad audience to understand.\n  - The model might prioritize syntactic completeness over semantic clarity, producing grammatically correct but semantically ambiguous simplifications.\n- **Task Misalignment Risks:** \n  - Overemphasis on grammatical correctness could result in simplified sentences that retain unnecessary complexity for the target audience.\n  - The model might evaluate simplifications based on structural similarity to the original rather than the effectiveness of the simplification for comprehension.\n  - Judging solely on grammar may ignore critical aspects of simplification, such as vocabulary choice or contextual relevance to the audience.\n- **Failure Cases:** \n  - A grammatically correct simplification might still use jargon or technical terms that are inaccessible to the intended audience.\n  - The model could penalize a simplified sentence for being 'too short' or 'overly casual,' even if it improves understanding for non-experts.\n  - A simplification that splits a complex sentence into multiple short sentences might be deemed 'awkward' due to structural repetition, despite enhancing clarity.\n\n## Related Metrics\n\n- **Related Metrics:**\n  - **LevenshteinDistance:** Levenshtein Distance measures the minimum number of single-character edits\u2014insertions, deletions, or substitutions\u2014required to transform one sequence into another.\n  - **BARTScore:** BARTScore is a reference-based evaluation metric for text generation that formulates evaluation as a text generation task.\n  - **PseudoPARENT:** **PseudoPARENT** is a *custom adaptation* of the PARENT metric for evaluating text generation from structured inputs.\n\n## Further Reading\n\n- **Papers:**\n  - [Autometrics](https://github.com/XenonMolecule/autometrics)\n  - [Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena](https://openreview.net/pdf?id=uccHPGDlao)\n\n## Citation\n\n```\n@software{Ryan_Autometrics_2025,\n    author = {Ryan, Michael J. and Zhang, Yanzhe and Salunkhe, Amol and Chu, Yi and Rahman, Emily and Xu, Di and Yang, Diyi},\n    license = {MIT},\n    title = {{Autometrics}},\n    url = {https://github.com/XenonMolecule/autometrics},\n    version = {1.0.0},\n    year = {2025}\n}\n```\n\n## Metric Card Authors\n\n- **Authors:** This metric card was automatically generated by Qwen3-32B.\n- **Acknowledgement of AI Assistance:** This metric card was entirely automatically generated by Qwen3-32B using the Autometrics library. No human intervention was involved. User discretion is advised.\n- **Contact:** For questions about the autometrics library, please contact [Michael J Ryan](mailto:mryan0@stanford.edu).", "ROUGE": "---\n# Metric Card for ROUGE (ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-LSum)\n\nROUGE (Recall-Oriented Understudy for Gisting Evaluation) is a widely used evaluation metric for text summarization, machine translation, and text generation tasks. It measures the overlap between an automatically generated text and reference texts using various methods such as **n-gram overlap (ROUGE-1, ROUGE-2), longest common subsequence (ROUGE-L), and summary-level longest common subsequence (ROUGE-LSum)**.\n\nThe **rouge-score** Python package provides a native implementation that replicates results from the original Perl-based ROUGE package. It supports **text normalization, Porter stemming, and confidence interval calculation** while omitting stopword removal due to licensing restrictions.\n\n## Metric Details\n\n### Metric Description\n\nROUGE evaluates generated text by comparing it with human-written references. The key variants included in this implementation are:\n\n- **ROUGE-1**: Measures unigram (single-word) overlap between candidate and reference texts.\n- **ROUGE-2**: Measures bigram (two-word sequence) overlap.\n- **ROUGE-L**: Measures the longest common subsequence (LCS) between candidate and reference texts, capturing sentence-level structure similarity.\n- **ROUGE-LSum**: A summary-level variant of ROUGE-L, treating newlines as sentence boundaries and computing LCS across sentence pairs.\n\n- **Metric Type:** Surface-Level Similarity\n- **Range:** 0 to 1\n- **Higher is Better?:** Yes\n- **Reference-Based?:** Yes\n- **Input-Required?:** No\n\n### Formal Definition\n\n#### ROUGE-N (N-gram Overlap)\n\nFor an n-gram of length $n$:\n\n$$\n\text{ROUGE-N} = \frac{\\sum _{S \\in \text{Reference Summaries}} \\sum _{\text{gram} _{n} \\in S} \text{Count} _{\text{match}}(\text{gram} _{n})}\n{\\sum _{S \\in \text{Reference Summaries}} \\sum _{\text{gram} _{n} \\in S} \text{Count}(\text{gram} _{n})}\n$$\n\nwhere $\text{Count} _{\text{match}}(\text{gram} _{n})$ is the number of n-grams appearing in both the candidate and reference summaries.\n\n## **ROUGE-L (Longest Common Subsequence)**\n\nROUGE-L evaluates the longest common subsequence (LCS) between the candidate and reference texts. The LCS captures sentence structure similarity by considering word order while allowing gaps.\n\nGiven a candidate summary $X$ of length $m$ and a reference summary $Y$ of length $n$, let $LCS(X, Y)$ denote the length of their longest common subsequence.\n\n### **Recall ($R_{LCS}$):**\n\n$$\nR_{LCS} = \frac{LCS(X, Y)}{n}\n$$\n\nMeasures the proportion of the reference summary captured by the candidate summary.\n\n### **Precision ($P_{LCS}$):**\n\n$$\nP_{LCS} = \frac{LCS(X, Y)}{m}\n$$\n\nMeasures the proportion of the candidate summary that is part of the LCS.\n\n### **F-measure ($F_{LCS}$):**\n\n$$\nF_{LCS} = \frac{(1 + \beta^2) \\cdot R_{LCS} \\cdot P_{LCS}}{R_{LCS} + \beta^2 \\cdot P_{LCS}}\n$$\n\nWhere $\beta$ determines the relative weight of recall versus precision. A common choice is $\beta = 1$, giving equal weight to both.\n\n#### ROUGE-LSum (Summary-Level LCS)\n\nROUGE-LSum extends ROUGE-L to the summary level by treating newlines as sentence boundaries. Instead of computing a single LCS over the entire text, it:\n\n1. Splits the candidate and reference summaries into sentences.\n2. Computes LCS for each candidate-reference sentence pair.\n3. Aggregates results to produce an overall ROUGE-LSum score.\n\n### Inputs and Outputs\n\n- **Inputs:**  \n  - Generated text (candidate summary)  \n  - Reference text(s) (human-written summary)\n\n- **Outputs:**  \n  - Scalar ROUGE score (range: 0 to 1), providing recall, precision, and F1-score.\n\n## Intended Use\n\n### Domains and Tasks\n\n- **Domain:** Text Generation\n- **Tasks:** Summarization, Machine Translation, Paraphrasing, Data-to-Text Generation\n\n### Applicability and Limitations\n\n- **Best Suited For:**  \n  - Evaluating text generation tasks where lexical similarity is a reliable proxy for quality.\n  - Comparing multiple summarization systems against a reference standard.\n\n- **Not Recommended For:**  \n  - Evaluating abstractiveness, coherence, fluency, or factual consistency.\n  - Tasks where paraphrasing or rewording is expected, as ROUGE penalizes non-exact matches.\n\n## Metric Implementation\n\n### Reference Implementations\n\n- **Libraries/Packages:**\n  - [Google Research ROUGE](https://github.com/google-research/google-research/tree/master/rouge)\n  - [Hugging Face `evaluate`](https://huggingface.co/docs/evaluate)\n  - [Python `rouge_score` package](https://pypi.org/project/rouge-score/)\n\n### Computational Complexity\n\n- **Efficiency:**  \n  - ROUGE-N complexity is $O(n \\cdot m)$ for n-gram counting, where $n$ is the candidate text length and $m$ is the reference text length.\n  - ROUGE-L requires LCS computation, which is $O(n \\cdot m)$ using dynamic programming.\n\n- **Scalability:**  \n  - ROUGE scales well to large datasets but can be computationally intensive when multiple reference texts are used.\n\n## Known Limitations\n\n- **Biases:**  \n  - Prefers texts with high lexical overlap, penalizing valid paraphrases.\n  - Highly sensitive to the number and quality of reference summaries.\n\n- **Task Misalignment Risks:**  \n  - Cannot capture meaning beyond exact n-gram matches.\n  - Does not account for factual correctness or grammaticality.\n\n- **Failure Cases:**  \n  - Overestimates quality for summaries with high recall but poor readability.\n  - Struggles with abstractive summarization, which may use different wording.\n\n## Related Metrics\n\n- **BLEU:** A precision-based alternative used in machine translation.  \n- **METEOR:** Incorporates synonym matching and paraphrase detection.  \n- **BERTScore:** Uses contextual embeddings for semantic similarity.  \n\n## Further Reading\n\n- **Papers:**  \n  - [Lin, 2004: ROUGE: A Package for Automatic Evaluation of Summaries](https://aclanthology.org/W04-1013)  \n  - [Ganesan, 2018: ROUGE 2.0 - Improved Evaluation Measures](https://arxiv.org/abs/1803.01937)  \n\n- **Blogs/Tutorials:**  \n  - [ROUGE How-To](http://kavita-ganesan.com/rouge-howto)  \n  - [ROUGE in Hugging Face](https://huggingface.co/docs/evaluate)  \n\n## Metric Card Authors\n\n## Citation\n\n  ```\n  @inproceedings{lin-2004-rouge,\n      title = \"{ROUGE}: A Package for Automatic Evaluation of Summaries\",\n      author = \"Lin, Chin-Yew\",\n      booktitle = \"Text Summarization Branches Out\",\n      month = jul,\n      year = \"2004\",\n      address = \"Barcelona, Spain\",\n      publisher = \"Association for Computational Linguistics\",\n      url = \"https://aclanthology.org/W04-1013/\",\n      pages = \"74--81\"\n  }\n  ```\n\n- **Authors:** Michael J. Ryan  \n- **Acknowledgment of AI Assistance:**  \n  Portions of this metric card were drafted with assistance from generative AI. All content has been reviewed and curated by the author to ensure accuracy.  \n- **Contact:** mryan0@stanford.edu  "}; const RC_DOCS_MAP = {"ROUGE-1-p": "ROUGE", "ROUGE_ROUGE-1-p": "ROUGE", "ROUGE-2-p": "ROUGE", "ROUGE_ROUGE-2-p": "ROUGE", "ROUGE-L-p": "ROUGE", "ROUGE_ROUGE-L-p": "ROUGE", "ROUGE-Lsum-p": "ROUGE", "ROUGE_ROUGE-Lsum-p": "ROUGE", "ROUGE-1-r": "ROUGE", "ROUGE_ROUGE-1-r": "ROUGE", "ROUGE-2-r": "ROUGE", "ROUGE_ROUGE-2-r": "ROUGE", "ROUGE-L-r": "ROUGE", "ROUGE_ROUGE-L-r": "ROUGE", "ROUGE-Lsum-r": "ROUGE", "ROUGE_ROUGE-Lsum-r": "ROUGE", "ROUGE-1-f1": "ROUGE", "ROUGE_ROUGE-1-f1": "ROUGE", "ROUGE-2-f1": "ROUGE", "ROUGE_ROUGE-2-f1": "ROUGE", "ROUGE-L-f1": "ROUGE", "ROUGE_ROUGE-L-f1": "ROUGE", "ROUGE-Lsum-f1": "ROUGE", "ROUGE_ROUGE-Lsum-f1": "ROUGE"};</script>
</head>
<body>
  <div class="container my-5">
    <div class="d-flex justify-content-between align-items-center mb-4">
      <h1>Fluency AutoMetric Report Card</h1>
      <div class="d-flex align-items-center">
        <div class="form-check form-switch me-3">
          <input class="form-check-input" type="checkbox" id="darkModeToggle">
          <label class="form-check-label" for="darkModeToggle">Dark Mode</label>
        </div>
        <button class="btn btn-primary" onclick="window.print()">Export to PDF</button>
      </div>
    </div>

    <div class="row g-4">
      <div class="col-md-6">
        <div class="card p-3 h-100">
          <h2>Regression Coefficients</h2>
          <table class="table table-striped"><thead><tr><th>Metric</th><th>Coeff.</th></tr></thead>
            <tbody><tr><td><a href="#" class="coeff-link" data-metric="Clarity_and_Coherence_of_Simplified_Meaning_Qwen3-32B">Clarity_and_Coherence_of_Simplified_Meaning_Qwen3-32B</a></td><td>0.0609</td></tr><tr><td><a href="#" class="coeff-link" data-metric="fluency_Qwen3-32B_examples">fluency_Qwen3-32B_examples</a></td><td>0.0609</td></tr><tr><td><a href="#" class="coeff-link" data-metric="Grammatical_Correctness_and_Sentence_Structure_Qwen3-32B">Grammatical_Correctness_and_Sentence_Structure_Qwen3-32B</a></td><td>0.0570</td></tr><tr><td><a href="#" class="coeff-link" data-metric="ROUGE-2-p">ROUGE-2-p</a></td><td>0.0392</td></tr><tr><td><a href="#" class="coeff-link" data-metric="ROUGE-1-f1">ROUGE-1-f1</a></td><td>0.0353</td></tr><tr><td><a href="#" class="coeff-link" data-metric="ROUGE-Lsum-f1">ROUGE-Lsum-f1</a></td><td>0.0353</td></tr><tr><td><a href="#" class="coeff-link" data-metric="ROUGE-Lsum-r">ROUGE-Lsum-r</a></td><td>0.0329</td></tr><tr><td><a href="#" class="coeff-link" data-metric="ROUGE-L-f1">ROUGE-L-f1</a></td><td>0.0329</td></tr><tr><td><a href="#" class="coeff-link" data-metric="ROUGE-L-p">ROUGE-L-p</a></td><td>0.0316</td></tr><tr><td><a href="#" class="coeff-link" data-metric="ROUGE-2-r">ROUGE-2-r</a></td><td>0.0315</td></tr><tr><td><a href="#" class="coeff-link" data-metric="ROUGE-1-r">ROUGE-1-r</a></td><td>0.0308</td></tr><tr><td><a href="#" class="coeff-link" data-metric="ROUGE-Lsum-p">ROUGE-Lsum-p</a></td><td>0.0282</td></tr><tr><td><a href="#" class="coeff-link" data-metric="ROUGE-2-f1">ROUGE-2-f1</a></td><td>0.0043</td></tr><tr><td><a href="#" class="coeff-link" data-metric="ROUGE-L-r">ROUGE-L-r</a></td><td>0.0043</td></tr><tr><td><a href="#" class="coeff-link" data-metric="ROUGE-1-p">ROUGE-1-p</a></td><td>-0.0110</td></tr></tbody>
          </table>
        </div>
      </div>

      <div class="col-md-6">
        <div class="card p-3 h-100">
          <h2>Correlation</h2>
          <div id="correlation-chart" style="height:420px;"></div>
          <div id="correlation-stats" class="mt-2" style="text-align:center; font-size: 1rem; font-weight: 600;"></div>
        </div>
      </div>

      <div class="col-md-6">
        <div class="card p-3 h-100">
          <h2>Robustness <sup><span class="robust-tip text-primary" data-tip-id="robustness-tip-template" style="cursor:pointer; text-decoration: underline; font-size: 0.9rem;">?</span></sup></h2>
          <div id="robustness-sens" style="height:240px;"></div>
          <div id="robustness-stab" style="height:240px;"></div>
        </div>
      </div>

      <div class="col-md-6">
        <div class="card p-3 h-100">
          <h2>Run Time Distribution</h2>
          <div id="runtime-chart" style="height:300px;"></div>
          <p id="runtime-info" class="mt-2"></p>
        </div>
      </div>

      <div class="col-md-6">
        <div class="card p-3 h-100">
          <h2>Metric Details</h2>
          <div class="accordion" id="metricDetails">
            <div class="accordion-item">
              <h2 class="accordion-header" id="descHeader"><button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#descPanel">Descriptions</button></h2>
              <div id="descPanel" class="accordion-collapse collapse"><div class="accordion-body"><ul><li><strong>Clarity_and_Coherence_of_Simplified_Meaning_Qwen3-32B:</strong><div class="mt-2"><pre style="white-space: pre-wrap; background:#f8f9fa; padding:8px; border-radius:6px;">**Clarity and Coherence of Simplified Meaning** The simplified sentence must clearly convey the original intent without ambiguity or omissions. Bad examples sometimes drop critical details or use vague terms (e.g., "animals have a response... (ermine snowshoe, or hare)").</pre></div></li>
<li><strong>fluency_Qwen3-32B_examples:</strong><div class="mt-2"><pre style="white-space: pre-wrap; background:#f8f9fa; padding:8px; border-radius:6px;">Example-based llm as a judge metric for 'fluency'.  The original task description for the task we are evaluating is: Given a complicated original sentence, simplify it in a way such that a broader audience could easily understand it.</pre></div></li>
<li><strong>Grammatical_Correctness_and_Sentence_Structure_Qwen3-32B:</strong><div class="mt-2"><pre style="white-space: pre-wrap; background:#f8f9fa; padding:8px; border-radius:6px;">**Grammatical Correctness and Sentence Structure** Proper grammar, complete sentences, and logical phrasing ensure fluency. Bad examples often have missing words, incorrect verb forms, or awkward syntax (e.g., "provided the right to do exists").</pre></div></li>
<li><strong>ROUGE:</strong><div class="mt-2"><pre style="white-space: pre-wrap; background:#f8f9fa; padding:8px; border-radius:6px;">ROUGE evaluates generated text by comparing it with human-written references. The key variants included in this implementation are:

- **ROUGE-1**: Measures unigram (single-word) overlap between candidate and reference texts.
- **ROUGE-2**: Measures bigram (two-word sequence) overlap.
- **ROUGE-L**: Measures the longest common subsequence (LCS) between candidate and reference texts, capturing sentence-level structure similarity.
- **ROUGE-LSum**: A summary-level variant of ROUGE-L, treating newlines as sentence boundaries and computing LCS across sentence pairs.

- **Metric Type:** Surface-Level Similarity
- **Range:** 0 to 1
- **Higher is Better?:** Yes
- **Reference-Based?:** Yes
- **Input-Required?:** No</pre></div></li></ul></div></div>
            </div>
            <div class="accordion-item">
              <h2 class="accordion-header" id="usageHeader"><button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#usagePanel">Usage</button></h2>
              <div id="usagePanel" class="accordion-collapse collapse"><div class="accordion-body"><ul><li><strong>Clarity_and_Coherence_of_Simplified_Meaning_Qwen3-32B:</strong><div class="mt-2"><pre style="white-space: pre-wrap; background:#f8f9fa; padding:8px; border-radius:6px;">- **Domain:** Text Simplification
- **Tasks:** 
  - Educational Content Adaptation
  - Technical Writing for Non-Experts
  - Public Communication Simplification
  - Accessibility Improvements for Reading Difficulties
  - General Text Simplification for Broad Audiences
- **Best Suited For:** 
  - When the original text contains complex terminology requiring precise simplification without losing key details
  - When the audience requires unambiguous, straightforward language (e.g., students, non-native speakers)
  - When the task emphasizes strict adherence to the original meaning while improving readability
  - When the simplified text must avoid vague or placeholder terms (e.g., 'response...' in the bad example)
- **Not Recommended For:** 
  - When the original text is inherently ambiguous or relies on context not provided in the input
  - When the task allows for creative reinterpretation rather than strict fidelity to the original meaning
  - When domain-specific knowledge (e.g., technical jargon in niche fields) is required to evaluate simplification quality
  - When the simplified text is intentionally concise to the point of omitting non-essential details (if the axis prioritizes completeness)</pre></div></li>
<li><strong>fluency_Qwen3-32B_examples:</strong><div class="mt-2"><pre style="white-space: pre-wrap; background:#f8f9fa; padding:8px; border-radius:6px;">- **Domain:** Text Generation
- **Tasks:** 
  - Sentence simplification
  - Text paraphrasing for accessibility
  - Generating clear explanations for technical content
- **Best Suited For:** 
  - When high-quality example pairs (complex vs. simplified) are available
  - For tasks requiring natural-sounding output over strict fidelity
  - When fluency is prioritized over preserving exact terminology
  - In domains with established simplification patterns (e.g., news summaries)
- **Not Recommended For:** 
  - When preserving precise technical terms is critical
  - For highly domain-specific content without available examples
  - When dealing with culturally nuanced or idiomatic expressions
  - In cases requiring strict adherence to original sentence structure</pre></div></li>
<li><strong>Grammatical_Correctness_and_Sentence_Structure_Qwen3-32B:</strong><div class="mt-2"><pre style="white-space: pre-wrap; background:#f8f9fa; padding:8px; border-radius:6px;">- **Domain:** Text Generation (specifically sentence simplification)
- **Tasks:** 
  - Sentence Simplification
  - Grammar Checking
  - Text Clarity Evaluation
  - Syntax Correction
  - Readability Enhancement
- **Best Suited For:** 
  - When evaluating simplified sentences for non-native speakers or educational contexts requiring clarity
  - For tasks where grammatical accuracy is critical (e.g., academic writing, professional communication)
  - When the original text contains overly complex syntax that risks misinterpretation
  - In scenarios requiring adherence to formal writing standards (e.g., legal or technical documentation)
- **Not Recommended For:** 
  - When the task prioritizes creative or stylistic expression over strict grammatical rules
  - For evaluating domain-specific jargon simplification where technical accuracy is paramount
  - In cases where the original sentence's complexity is intentional (e.g., literary or poetic devices)
  - When evaluating multilingual or code-switched text where grammar rules vary dynamically</pre></div></li>
<li><strong>ROUGE:</strong><div class="mt-2"><pre style="white-space: pre-wrap; background:#f8f9fa; padding:8px; border-radius:6px;">### Domains and Tasks

- **Domain:** Text Generation
- **Tasks:** Summarization, Machine Translation, Paraphrasing, Data-to-Text Generation

### Applicability and Limitations

- **Best Suited For:**  
  - Evaluating text generation tasks where lexical similarity is a reliable proxy for quality.
  - Comparing multiple summarization systems against a reference standard.

- **Not Recommended For:**  
  - Evaluating abstractiveness, coherence, fluency, or factual consistency.
  - Tasks where paraphrasing or rewording is expected, as ROUGE penalizes non-exact matches.</pre></div></li></ul></div></div>
            </div>
            <div class="accordion-item">
              <h2 class="accordion-header" id="limitsHeader"><button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#limitsPanel">Limitations</button></h2>
              <div id="limitsPanel" class="accordion-collapse collapse"><div class="accordion-body"><ul><li><strong>Clarity_and_Coherence_of_Simplified_Meaning_Qwen3-32B:</strong><div class="mt-2"><pre style="white-space: pre-wrap; background:#f8f9fa; padding:8px; border-radius:6px;">- **Biases:** 
  - The model may prioritize brevity over completeness, omitting critical details to make the sentence shorter.
  - It might favor generic terms (e.g., 'animals' instead of 'ermine') to avoid specificity, risking ambiguity.
  - The model could assume all audiences lack domain knowledge, over-simplifying technical terms unnecessarily.
  - It may conflate grammatical simplicity with conceptual clarity, failing to address true complexity.
- **Task Misalignment Risks:** 
  - Focusing on grammatical structure rather than ensuring the simplified meaning aligns with the original intent.
  - Treating simplification as paraphrasing instead of targeted accessibility improvement for non-experts.
  - Overemphasizing vocabulary simplicity at the cost of logical coherence or contextual accuracy.
  - Failing to distinguish between simplification and trivialization, which could distort the original message.
- **Failure Cases:** 
  - Dropping essential qualifiers (e.g., 'statistically significant' becomes 'important') that alter the original meaning.
  - Replacing specific terms with vague placeholders (e.g., 'X species' instead of 'ermine') to avoid complexity.
  - Producing a grammatically correct but logically incoherent sentence due to over-simplification.
  - Retaining complex jargon under the guise of 'clarity' while failing to explain it for broader audiences.</pre></div></li>
<li><strong>fluency_Qwen3-32B_examples:</strong><div class="mt-2"><pre style="white-space: pre-wrap; background:#f8f9fa; padding:8px; border-radius:6px;">- **Biases:** 
  - Preference for complex vocabulary if it aligns with original sentence structure
  - Overemphasis on grammatical correctness at the expense of clarity
  - Bias toward preserving syntactic patterns from the original text
  - Assumption that shorter sentences are inherently more fluent
  - Cultural or linguistic biases in what constitutes 'natural' phrasing
- **Task Misalignment Risks:** 
  - High fluency score for a simplification that retains complex terminology
  - Rewarding literal translations that maintain original structure but lack simplicity
  - Failing to account for domain-specific jargon in fluency assessment
  - Prioritizing sentence brevity over semantic completeness
  - Inconsistent application of fluency criteria across different text genres
- **Failure Cases:** 
  - Producing a grammatically correct but unnatural simplification (e.g., 'The cat which was black ran' instead of 'The black cat ran')
  - Creating a fluent sentence that introduces new ambiguities not present in the original
  - Simplifying to the point of losing critical information while maintaining fluency
  - Generating fluent but factually inconsistent simplifications
  - Failing to handle nested clauses or passive voice effectively while preserving fluency</pre></div></li>
<li><strong>Grammatical_Correctness_and_Sentence_Structure_Qwen3-32B:</strong><div class="mt-2"><pre style="white-space: pre-wrap; background:#f8f9fa; padding:8px; border-radius:6px;">- **Biases:** 
  - The model may favor complex grammatical structures it is familiar with, assuming they are inherently 'better' than simpler alternatives.
  - Bias toward formal grammar rules might lead to rejecting colloquial or conversational phrasing that is easier for a broad audience to understand.
  - The model might prioritize syntactic completeness over semantic clarity, producing grammatically correct but semantically ambiguous simplifications.
- **Task Misalignment Risks:** 
  - Overemphasis on grammatical correctness could result in simplified sentences that retain unnecessary complexity for the target audience.
  - The model might evaluate simplifications based on structural similarity to the original rather than the effectiveness of the simplification for comprehension.
  - Judging solely on grammar may ignore critical aspects of simplification, such as vocabulary choice or contextual relevance to the audience.
- **Failure Cases:** 
  - A grammatically correct simplification might still use jargon or technical terms that are inaccessible to the intended audience.
  - The model could penalize a simplified sentence for being 'too short' or 'overly casual,' even if it improves understanding for non-experts.
  - A simplification that splits a complex sentence into multiple short sentences might be deemed 'awkward' due to structural repetition, despite enhancing clarity.</pre></div></li>
<li><strong>ROUGE:</strong><div class="mt-2"><pre style="white-space: pre-wrap; background:#f8f9fa; padding:8px; border-radius:6px;">- **Biases:**  
  - Prefers texts with high lexical overlap, penalizing valid paraphrases.
  - Highly sensitive to the number and quality of reference summaries.

- **Task Misalignment Risks:**  
  - Cannot capture meaning beyond exact n-gram matches.
  - Does not account for factual correctness or grammaticality.

- **Failure Cases:**  
  - Overestimates quality for summaries with high recall but poor readability.
  - Struggles with abstractive summarization, which may use different wording.</pre></div></li></ul></div></div>
            </div>
          </div>
        </div>
      </div>

      <div class="col-md-6">
        <div class="card p-3 h-100">
          <h2>Compute Requirements</h2>
          <table class="table table-striped"><thead><tr><th>Metric</th><th>GPU RAM (MB)</th><th>CPU RAM (MB)</th></tr></thead>
            <tbody><tr><td>Clarity_and_Coherence_of_Simplified_Meaning_Qwen3-32B</td><td>--</td><td>--</td></tr><tr><td>fluency_Qwen3-32B_examples</td><td>--</td><td>--</td></tr><tr><td>Grammatical_Correctness_and_Sentence_Structure_Qwen3-32B</td><td>--</td><td>--</td></tr><tr><td>ROUGE</td><td>0.0</td><td>726.48828125</td></tr></tbody>
          </table>
        </div>
      </div>
    </div>

    <div class="mt-5 card p-3">
      <h3>Metric Summary</h3>
      <p>The aggregate fluency metric evaluates how well simplified sentences balance clarity, grammatical correctness, and alignment with example-based fluency standards. It prioritizes simplifications that retain original meaning while using accessible language (e.g., replacing "eastern approaches" with "eastern side"), as measured by Clarity_and_Coherence and fluency_Qwen3-32B_examples. Grammatical_Correctness ensures structural validity, penalizing errors like "east organizing" that disrupt readability. ROUGE metrics add nuance, with positive coefficients rewarding lexical overlap with references (e.g., ROUGE-Lsum-f1) and negative coefficients discouraging overfitting to specific phrasing (e.g., ROUGE-1-p). The metric is well-suited for educational and accessibility-focused simplification but risks over-prioritizing syntax over semantic simplicity. Caveats include potential bias toward formal grammar, sensitivity to reference quality, and the challenge of distinguishing between appropriate simplification and trivialization. While effective for broad-audience readability, it requires careful calibration to avoid penalizing concise, contextually appropriate simplifications.</p>
    </div>

    <div class="mt-4 card p-3">
      <div class="d-flex justify-content-between align-items-center mb-2">
        <h3 class="mb-0">Examples</h3>
        <button id="clear-examples-filter" class="btn btn-sm btn-outline-secondary" type="button">Show All</button>
      </div>
      <div style="overflow-x: auto; max-width: 100%;"><table border="1" class="dataframe table table-striped table-sm w-100" id="examples-table">
  <thead>
    <tr style="text-align: right;">
      <th>ID</th>
      <th>original</th>
      <th>simple</th>
      <th>ref1</th>
      <th>ref2</th>
      <th>ref3</th>
      <th>ref4</th>
      <th>ref5</th>
      <th>ref6</th>
      <th>ref7</th>
      <th>ref8</th>
      <th>ref9</th>
      <th>ref10</th>
      <th>fluency</th>
      <th>Clarity_and_Coherence_of_Simplified_Meaning_Qwen3-32B</th>
      <th>fluency_Qwen3-32B_examples</th>
      <th>Grammatical_Correctness_and_Sentence_Structure_Qwen3-32B</th>
      <th>ROUGE-1-p</th>
      <th>ROUGE-2-p</th>
      <th>ROUGE-L-p</th>
      <th>ROUGE-Lsum-p</th>
      <th>ROUGE-1-r</th>
      <th>ROUGE-2-r</th>
      <th>ROUGE-L-r</th>
      <th>ROUGE-Lsum-r</th>
      <th>ROUGE-1-f1</th>
      <th>ROUGE-2-f1</th>
      <th>ROUGE-L-f1</th>
      <th>ROUGE-Lsum-f1</th>
      <th>Autometrics_Regression_fluency</th>
      <th>Autometrics_Regression_fluency__feedback</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>a bastion on the eastern approaches was built later.</td>
      <td>a bastion on the eastern side was built later.</td>
      <td>a fort on the eastern access road was built later.</td>
      <td>a fortification on the eastern approaches was built later.</td>
      <td>a support on the east was built later.</td>
      <td>an extension on the eastern side was built later.</td>
      <td>a fort on the eastern side of the area was built later.</td>
      <td>a wall on the east side was built later.</td>
      <td>later, it was fortified on the easterrn side.</td>
      <td>a  defense on the eastern side was built later.</td>
      <td>a bastion on the eastern side was built later.</td>
      <td>an eastern bastion was built later.</td>
      <td>0.415042</td>
      <td>4.0</td>
      <td>0.850000</td>
      <td>5.0</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>0.812614</td>
      <td>The output text "a bastion on the eastern side was built later" retains the core meaning of the original sentence but simplifies "eastern approaches" to "eastern side," which is less precise than the reference's "eastern access road." While "bastion" is retained (instead of simplifying to "fort" as in the reference), it is still a specific term that may not be as widely understood as "fort." However, the output avoids ambiguity and omissions, maintaining clarity and coherence. The simplification is acceptable but slightly less precise than the reference.<br/>The output sentence "a bastion on the eastern side was built later" is grammatically correct, maintains clarity, and aligns closely with the original meaning. It simplifies "eastern approaches" to "eastern side," which is more accessible to a broader audience while preserving the key information. The sentence structure is straightforward and matches one of the provided references (ref9), indicating strong fluency. No awkward phrasing or errors are present.<br/>The output text "a bastion on the eastern side was built later" is grammatically correct and maintains logical sentence structure. It uses a complete sentence with proper verb form ("was built") and clear phrasing. While "eastern side" is a simplification of "eastern approaches," it avoids awkward syntax and ensures fluency. The output avoids missing words or incorrect verb forms, aligning with the reference's clarity while retaining grammatical accuracy.</td>
    </tr>
    <tr>
      <td>2</td>
      <td>a bastion on the eastern approaches was built later.</td>
      <td>a bastion on the east organizing was built later.</td>
      <td>a fort on the eastern access road was built later.</td>
      <td>a fortification on the eastern approaches was built later.</td>
      <td>a support on the east was built later.</td>
      <td>an extension on the eastern side was built later.</td>
      <td>a fort on the eastern side of the area was built later.</td>
      <td>a wall on the east side was built later.</td>
      <td>later, it was fortified on the easterrn side.</td>
      <td>a  defense on the eastern side was built later.</td>
      <td>a bastion on the eastern side was built later.</td>
      <td>an eastern bastion was built later.</td>
      <td>-0.529806</td>
      <td>2.0</td>
      <td>-0.529806</td>
      <td>2.0</td>
      <td>0.777778</td>
      <td>0.875000</td>
      <td>0.823529</td>
      <td>0.625000</td>
      <td>0.625000</td>
      <td>0.625000</td>
      <td>0.777778</td>
      <td>0.875000</td>
      <td>0.823529</td>
      <td>0.777778</td>
      <td>0.875000</td>
      <td>0.823529</td>
      <td>0.094783</td>
      <td>The output text uses "east organizing," which is a misinterpretation of the original "eastern approaches." The term "organizing" is a verb and does not convey the intended meaning of a geographical direction or access route. The reference text correctly simplifies "eastern approaches" to "eastern access road," maintaining clarity. The output fails to preserve the original intent due to this incorrect substitution, leading to ambiguity and loss of critical detail.<br/>The output sentence "a bastion on the east organizing was built later" contains a critical error in word choice ("organizing" instead of "approaches" or "side"), which disrupts fluency. The term "organizing" is semantically unrelated to the context of geographical positioning or fortifications, making the sentence confusing and ungrammatical. While the structure resembles the original, the incorrect word severely impacts readability and clarity. This aligns with the lower end of the fluency scale due to the jarring error.<br/>The output text contains a grammatical error in the phrase "east organizing," which is not a standard or coherent noun phrase. The reference text correctly uses "eastern access road," a clear and grammatically appropriate noun phrase. The output's use of "organizing" (a verb form) instead of a noun like "approaches" or "access road" creates an awkward and incorrect structure, disrupting fluency and clarity. While the rest of the sentence ("was built later") is grammatically correct, the critical error in the noun phrase significantly impacts the overall grammatical correctness and logical phrasing.</td>
    </tr>
    <tr>
      <td>3</td>
      <td>a bastion on the eastern approaches was built later.</td>
      <td>a bastion in the east nears was built at a later date.</td>
      <td>a fort on the eastern access road was built later.</td>
      <td>a fortification on the eastern approaches was built later.</td>
      <td>a support on the east was built later.</td>
      <td>an extension on the eastern side was built later.</td>
      <td>a fort on the eastern side of the area was built later.</td>
      <td>a wall on the east side was built later.</td>
      <td>later, it was fortified on the easterrn side.</td>
      <td>a  defense on the eastern side was built later.</td>
      <td>a bastion on the eastern side was built later.</td>
      <td>an eastern bastion was built later.</td>
      <td>0.116173</td>
      <td>2.0</td>
      <td>-0.529806</td>
      <td>3.0</td>
      <td>0.500000</td>
      <td>0.750000</td>
      <td>0.600000</td>
      <td>0.181818</td>
      <td>0.285714</td>
      <td>0.222222</td>
      <td>0.500000</td>
      <td>0.750000</td>
      <td>0.600000</td>
      <td>0.500000</td>
      <td>0.750000</td>
      <td>0.600000</td>
      <td>-0.191165</td>
      <td>The output text "a bastion in the east nears was built at a later date" fails to maintain clarity and coherence in the simplified meaning. The phrase "east nears" is ambiguous and likely a typo or incorrect substitution for "eastern approaches/access road" (as in the reference). This introduces confusion about the location. Additionally, "bastion" is retained instead of simplified to "fort" (as in the reference), which may not be as accessible to a broader audience. The phrase "at a later date" is redundant but not critical. The critical omission of clarity in the location term ("nears") undermines the simplified meaning.<br/>The output sentence "a bastion in the east nears was built at a later date" contains minor fluency issues. The phrase "east nears" is awkward and less precise compared to the original "eastern approaches" or references like "eastern side." Additionally, "at a later date" is more verbose than the concise "later," slightly reducing readability. While the sentence remains understandable, the phrasing lacks the natural flow of the references, which use clearer terms like "eastern side" or "east." The score reflects moderate fluency but with noticeable imperfections.<br/>The output text contains grammatical errors and awkward phrasing. The phrase "in the east nears" is incorrect and confusing (likely intending "near the east" or "in the eastern area"). "Nears" is improperly used as a noun here. Additionally, "at a later date" is verbose and less natural than the reference's "later." While the structure is mostly complete, the incorrect word choice ("nears") and awkward syntax reduce clarity and grammatical correctness. The reference simplifies "approaches" to "access road" more effectively and maintains proper grammar.</td>
    </tr>
    <tr>
      <td>4</td>
      <td>a few animals have chromatic response, changing color in changing environments, either seasonally (ermine, snowshoe hare) or far more rapidly with chromatophores in their integument (the cephalopod family).</td>
      <td>a few animals have chromatic response, changing color in changing environments, either seasonally (snowshoe hare) or far more rapidly with chromatophores in their integument (the cephalopod family).</td>
      <td>some animals change color in different environments. sometimes this is because of the season.</td>
      <td>a few animals have a chromatic response, which means they change colors depending on their environment.  they can either do this with the seasons, or far more rapidly with cells in their skin.</td>
      <td>a few animals have chromatic response. they can change color in changing environments. some change seasonally, like the ermine or snowshoe hare. some change quickly, like members of the cephalopod family.</td>
      <td>some animals have chromatic response.  this means that they change colour in changing environments.  the change can be seasonal (e.g. ermine, snowshoe hare) or more quickly (e.g. chromatophores).</td>
      <td>some animals change color in different environments.  they change colors with the seasons or with their skin cells.</td>
      <td>a few animals change colors depending on their environment. this could be a seasonal change or a faster change.</td>
      <td>a couple of animals change colors when their surroundings change either by season like the ermine and snowshoe hare or a lot quicker because they have organs in the skin (the cephalopod family).</td>
      <td>some animals change colors in different environments. some animals like the ermine and snowshoe hare change color when the season changes. other types of animal change faster.</td>
      <td>some animals have 'chromatic responses', they change color depending on what's around them.</td>
      <td>a few animals have chromatic response which means they change colors when their environment changes. this can happen either seasonally (ermine, snowshoe hare) or far more rapidly with pigment  in their outer layer of skin (octopus, squid).</td>
      <td>0.285011</td>
      <td>3.0</td>
      <td>0.393052</td>
      <td>3.0</td>
      <td>0.777778</td>
      <td>0.611111</td>
      <td>0.656250</td>
      <td>0.538462</td>
      <td>0.400000</td>
      <td>0.451613</td>
      <td>0.740741</td>
      <td>0.548387</td>
      <td>0.625000</td>
      <td>0.740741</td>
      <td>0.548387</td>
      <td>0.625000</td>
      <td>-0.030427</td>
      <td>The output text retains the core idea of animals changing color in response to environments, mentioning both seasonal and rapid changes. However, it uses the technical term "chromatic response," which may not be accessible to a broader audience. The reference text simplifies this to "change color in different environments" and avoids jargon. While the output includes specific examples (snowshoe hare, cephalopod family), it omits "ermine" from the original input, though the reference also omits specific examples. The structure of the output is still somewhat complex due to technical terms and parenthetical details, reducing clarity compared to the reference. The simplified meaning is coherent but not as clear or universally accessible as the reference.<br/>The output sentence maintains the original structure and technical terms ("chromatic response," "chromatophores") but removes "ermine" from the seasonal example. While the sentence is grammatically correct and coherent, it does not significantly simplify the original text for broader accessibility. The fluency is preserved, but the lack of substantial simplification (e.g., rephrasing technical terms) limits its effectiveness for a general audience. Compared to references that rephrase terms like "chromatic response" into plain language, this output falls mid-range in fluency due to its balance of clarity and retention of technical jargon.<br/>The output text maintains grammatical correctness with proper sentence structure and logical phrasing. However, it retains technical terms like "chromatic response," "chromatophores," and "integument," which are not simplified as effectively as in the reference text. The reference uses simpler language ("change color," "because of the season") and shorter sentences, making it more accessible. While the output avoids grammatical errors, its structure and terminology remain complex, limiting its effectiveness for a broader audience. The sentence is complete and coherent but lacks the simplicity seen in the reference.</td>
    </tr>
    <tr>
      <td>5</td>
      <td>a few animals have chromatic response, changing color in changing environments, either seasonally (ermine, snowshoe hare) or far more rapidly with chromatophores in their integument (the cephalopod family).</td>
      <td>animals have a response, changing color, either ermine snowshoe, or hare).</td>
      <td>some animals change color in different environments. sometimes this is because of the season.</td>
      <td>a few animals have a chromatic response, which means they change colors depending on their environment.  they can either do this with the seasons, or far more rapidly with cells in their skin.</td>
      <td>a few animals have chromatic response. they can change color in changing environments. some change seasonally, like the ermine or snowshoe hare. some change quickly, like members of the cephalopod family.</td>
      <td>some animals have chromatic response.  this means that they change colour in changing environments.  the change can be seasonal (e.g. ermine, snowshoe hare) or more quickly (e.g. chromatophores).</td>
      <td>some animals change color in different environments.  they change colors with the seasons or with their skin cells.</td>
      <td>a few animals change colors depending on their environment. this could be a seasonal change or a faster change.</td>
      <td>a couple of animals change colors when their surroundings change either by season like the ermine and snowshoe hare or a lot quicker because they have organs in the skin (the cephalopod family).</td>
      <td>some animals change colors in different environments. some animals like the ermine and snowshoe hare change color when the season changes. other types of animal change faster.</td>
      <td>some animals have 'chromatic responses', they change color depending on what's around them.</td>
      <td>a few animals have chromatic response which means they change colors when their environment changes. this can happen either seasonally (ermine, snowshoe hare) or far more rapidly with pigment  in their outer layer of skin (octopus, squid).</td>
      <td>-0.890238</td>
      <td>2.0</td>
      <td>-0.748478</td>
      <td>2.0</td>
      <td>1.000000</td>
      <td>0.357143</td>
      <td>0.476190</td>
      <td>0.300000</td>
      <td>0.153846</td>
      <td>0.173913</td>
      <td>0.818182</td>
      <td>0.357143</td>
      <td>0.400000</td>
      <td>0.818182</td>
      <td>0.357143</td>
      <td>0.400000</td>
      <td>-0.590096</td>
      <td>The output text fails to clearly convey the original intent due to several issues. First, it incorrectly lists "ermine snowshoe" as a single entity instead of specifying "snowshoe hare," which is a known seasonal color-changer. The phrase "either ermine snowshoe, or hare" is ambiguous and grammatically incorrect. Additionally, the output omits critical details from the original input, such as the mention of "chromatophores" and "cephalopod family," which explain rapid color changes. The reference text is concise and accurate, while the output introduces confusion through poor phrasing and missing context. The score reflects these omissions and lack of clarity.<br/>The output sentence "animals have a response, changing color, either ermine snowshoe, or hare)" is grammatically incorrect and lacks clarity. The phrase "either ermine snowshoe, or hare)" is improperly structured, missing the connection to "seasonally" and the explanation of chromatophores. The misplaced parenthesis and fragmented syntax reduce fluency. While the core idea is present, the errors and lack of proper formatting (e.g., missing commas, incorrect grouping of examples) make it harder to read compared to the original and references. This aligns with lower fluency scores in similar examples.<br/>The output text contains several grammatical errors and structural issues. It lacks an opening parenthesis before "ermine snowshoe" and improperly closes with a parenthesis at the end. The phrase "either ermine snowshoe, or hare)" is syntactically incorrect and confusing, as it omits necessary punctuation and fails to clarify the examples. Additionally, the sentence structure is incomplete and awkward compared to the reference text's clear, grammatically correct phrasing. These errors hinder fluency and readability.</td>
    </tr>
    <tr>
      <td>6</td>
      <td>a few animals have chromatic response, changing color in changing environments, either seasonally (ermine, snowshoe hare) or far more rapidly with chromatophores in their integument (the cephalopod family).</td>
      <td>a few animals have chromatic response, changing color in changing environments, either seasonally (stoat, snowshoe hare) or more quickly with chromatophores in their integument (the cephalopod family).</td>
      <td>some animals change color in different environments. sometimes this is because of the season.</td>
      <td>a few animals have a chromatic response, which means they change colors depending on their environment.  they can either do this with the seasons, or far more rapidly with cells in their skin.</td>
      <td>a few animals have chromatic response. they can change color in changing environments. some change seasonally, like the ermine or snowshoe hare. some change quickly, like members of the cephalopod family.</td>
      <td>some animals have chromatic response.  this means that they change colour in changing environments.  the change can be seasonal (e.g. ermine, snowshoe hare) or more quickly (e.g. chromatophores).</td>
      <td>some animals change color in different environments.  they change colors with the seasons or with their skin cells.</td>
      <td>a few animals change colors depending on their environment. this could be a seasonal change or a faster change.</td>
      <td>a couple of animals change colors when their surroundings change either by season like the ermine and snowshoe hare or a lot quicker because they have organs in the skin (the cephalopod family).</td>
      <td>some animals change colors in different environments. some animals like the ermine and snowshoe hare change color when the season changes. other types of animal change faster.</td>
      <td>some animals have 'chromatic responses', they change color depending on what's around them.</td>
      <td>a few animals have chromatic response which means they change colors when their environment changes. this can happen either seasonally (ermine, snowshoe hare) or far more rapidly with pigment  in their outer layer of skin (octopus, squid).</td>
      <td>0.459242</td>
      <td>3.0</td>
      <td>0.750000</td>
      <td>3.0</td>
      <td>0.703704</td>
      <td>0.612903</td>
      <td>0.655172</td>
      <td>0.461538</td>
      <td>0.400000</td>
      <td>0.428571</td>
      <td>0.666667</td>
      <td>0.580645</td>
      <td>0.620690</td>
      <td>0.666667</td>
      <td>0.580645</td>
      <td>0.620690</td>
      <td>-0.001697</td>
      <td>The output text retains the key elements of the original sentence, including the concept of chromatic response, seasonal changes (with specific examples like stoat/snowshoe hare), and rapid color changes via chromatophores in cephalopods. However, it still uses technical terms like "chromatophores" and "integument," which may not be fully simplified for a broad audience. The reference text is more concise and avoids technical jargon entirely, but the output maintains more specific details. While the output is coherent and accurate, it falls short of the reference's simplicity, potentially sacrificing clarity for some readers. The substitution of "stoat" for "ermine" is appropriate and adds clarity. The score reflects a balance between retaining important details and achieving full simplification.<br/>The output sentence is grammatically correct and maintains a clear structure. It uses appropriate transitions ("either...or") and retains the key terms ("chromatic response," "chromatophores") while simplifying phrases like "far more rapidly" to "more quickly." The substitution of "stoat" for "ermine" (synonyms) does not affect fluency. The sentence flows smoothly, with logical connections between clauses and proper use of parentheses for examples. While some references simplify further (e.g., replacing "chromatic response" with "change color"), the output balances technical accuracy with readability. Minor improvements could include simplifying "chromatic response" for broader audiences, but the fluency is strong overall.<br/>The output text is grammatically correct with complete sentences and logical phrasing. It retains the original structure but simplifies "ermine" to "stoat" (a more common name) and "far more rapidly" to "more quickly." However, it still includes technical terms like "chromatic response," "chromatophores," and "integument," which may not be accessible to a broader audience. The reference text avoids such terms entirely, using simpler language ("change color," "season"). While the output is structurally sound, it falls short of the reference's clarity and simplicity.</td>
    </tr>
    <tr>
      <td>7</td>
      <td>a few animals have chromatic response, changing color in changing environments, either seasonally (ermine, snowshoe hare) or far more rapidly with chromatophores in their integument (the cephalopod family).</td>
      <td>a few animals have chromatic response, change color in a changing environment, either seasonally (ermine, snowshoe hare) or much faster with chromatophores in their coat (the cephalopod family).</td>
      <td>some animals change color in different environments. sometimes this is because of the season.</td>
      <td>a few animals have a chromatic response, which means they change colors depending on their environment.  they can either do this with the seasons, or far more rapidly with cells in their skin.</td>
      <td>a few animals have chromatic response. they can change color in changing environments. some change seasonally, like the ermine or snowshoe hare. some change quickly, like members of the cephalopod family.</td>
      <td>some animals have chromatic response.  this means that they change colour in changing environments.  the change can be seasonal (e.g. ermine, snowshoe hare) or more quickly (e.g. chromatophores).</td>
      <td>some animals change color in different environments.  they change colors with the seasons or with their skin cells.</td>
      <td>a few animals change colors depending on their environment. this could be a seasonal change or a faster change.</td>
      <td>a couple of animals change colors when their surroundings change either by season like the ermine and snowshoe hare or a lot quicker because they have organs in the skin (the cephalopod family).</td>
      <td>some animals change colors in different environments. some animals like the ermine and snowshoe hare change color when the season changes. other types of animal change faster.</td>
      <td>some animals have 'chromatic responses', they change color depending on what's around them.</td>
      <td>a few animals have chromatic response which means they change colors when their environment changes. this can happen either seasonally (ermine, snowshoe hare) or far more rapidly with pigment  in their outer layer of skin (octopus, squid).</td>
      <td>0.393052</td>
      <td>3.0</td>
      <td>0.450000</td>
      <td>3.0</td>
      <td>0.678571</td>
      <td>0.631579</td>
      <td>0.644068</td>
      <td>0.444444</td>
      <td>0.366667</td>
      <td>0.385965</td>
      <td>0.642857</td>
      <td>0.580645</td>
      <td>0.610169</td>
      <td>0.642857</td>
      <td>0.580645</td>
      <td>0.610169</td>
      <td>-0.042271</td>
      <td>The output text retains the key elements of the original sentence, including the mention of chromatic response, seasonal examples (ermine, snowshoe hare), and rapid color changes via chromatophores in cephalopods. However, it fails to simplify the language sufficiently for a broader audience. Terms like "chromatic response" and "chromatophores" remain technical, and the phrase "changing environment" is less natural than the reference's "different environments." While the examples are preserved, the output does not achieve the same level of clarity and simplicity as the reference text, which uses plain language ("some animals change color," "because of the season"). The output is coherent but not optimally simplified.<br/>The output sentence is grammatically correct and maintains a clear structure. It simplifies terms like "integument" to "coat," which is more accessible. The sentence flows smoothly, though minor adjustments like "changing environment" (singular) instead of "changing environments" (plural) could slightly affect consistency. However, the overall fluency is strong, with no major disruptions in readability or coherence. The use of parentheses and transitional phrases ("either... or") enhances clarity.<br/>The output text is mostly grammatically correct but has minor issues. The phrase "change color in a changing environment" uses a comma splice (two independent clauses separated by a comma without a conjunction). It should be "changing color in a changing environment" to maintain parallel structure with the preceding clause. Additionally, "much faster" is less precise than the reference's "because of the season," and the sentence structure could be clearer. The use of "chromatic response" retains technical jargon not simplified as effectively as the reference's "change color." However, the overall grammar and syntax are functional, with only minor awkwardness.</td>
    </tr>
    <tr>
      <td>8</td>
      <td>a historian has stated that "it was quinine's efficacy that gave colonists fresh opportunities to swarm into the gold coast, nigeria and other parts of west africa".</td>
      <td>a historian has stated that "it was quinine's efficacy that gave colonists fresh opportunities to swarm into the gold nigeria coast, and other parts of west africa".</td>
      <td>quinine's potency gave colonists opportunities to swarm into the gold coast, nigeria, and other parts of west africa.</td>
      <td>a historian said that "it was quinine's efficacy that gave colonists fresh opportunities to swarm into the gold coast, nigeria and other parts of west africa".</td>
      <td>a historian said that quinine caused colonists to go to west africa.</td>
      <td>a historian has stated that "it was quinine's efficacy that gave colonists opportunities to go into the gold coast, nigeria and other parts of west africa".</td>
      <td>a historian stated that, "it was quinine's effectiveness that allowed colonists to go to the gold coast, nigeria and other parts of west africa".</td>
      <td>a historian has stated that "it was quinine's efficacy that gave colonists fresh opportunities to swarm into...west africa".</td>
      <td>a historian said that quinine helped colonists settle in the gold coast, nigeria, and other parts of west africa.</td>
      <td>historians say that an anti-malaria drug let colonists enter west africa in large numbers.</td>
      <td>a historian has said that "it was quinine's success that gave the colonists new opportunities to flood into the gold coast (nigeria and other parts of west africa)".</td>
      <td>a historian said it was quinine was so good it gave the people settling there new chances to go to the gold coast nigeria and other parts of west africa.</td>
      <td>0.147488</td>
      <td>2.0</td>
      <td>-0.748478</td>
      <td>2.0</td>
      <td>0.928571</td>
      <td>1.000000</td>
      <td>0.945455</td>
      <td>0.777778</td>
      <td>0.947368</td>
      <td>0.792453</td>
      <td>0.892857</td>
      <td>1.000000</td>
      <td>0.909091</td>
      <td>0.892857</td>
      <td>1.000000</td>
      <td>0.909091</td>
      <td>0.270207</td>
      <td>The output text incorrectly merges "gold coast" and "nigeria" into "gold nigeria coast," which is a geographical inaccuracy. This alters the original meaning by conflating distinct regions. Additionally, the list structure is flawed (missing a comma after "gold nigeria coast"), creating ambiguity. While the core idea about quinine's role is preserved, the critical error in place names and punctuation significantly reduces clarity and coherence. The reference text correctly lists the regions without such errors.<br/>The output sentence contains a geographical inaccuracy by incorrectly combining "gold nigeria coast" instead of maintaining the original distinction between "gold coast" (a historical region in West Africa, now Ghana) and "nigeria" as separate entities. This disrupts fluency by introducing factual confusion. While the grammatical structure is otherwise correct, the error in phrasing ("gold nigeria coast") creates a jarring and misleading phrase that would hinder clarity for a broader audience. The references and original text correctly separate these locations, making the output's phrasing a notable flaw in fluency.<br/>The output text contains a grammatical error in the phrase "gold nigeria coast," which incorrectly combines "Gold Coast" and "Nigeria" into a single, nonsensical term. The reference text correctly lists "Gold Coast, Nigeria" as separate entities. This error disrupts clarity and logical phrasing. While the rest of the sentence structure is mostly correct, the critical mistake in the list order and country name placement significantly impacts grammatical correctness. The output also retains "fresh opportunities," which the reference omits, but this is a minor issue compared to the primary error.</td>
    </tr>
    <tr>
      <td>9</td>
      <td>a mutant is a type of fictional character that appears in comic books published by marvel comics.</td>
      <td>a mutant is a type of character that appears in comic books published by marvel comics.</td>
      <td>a pretend character in comic books by marvel comics is called a mutant.</td>
      <td>a mutant is a made up character. they are shown in comic books by marvel comics.</td>
      <td>a mutant is a made-up character in marvel comic books.</td>
      <td>a mutant is a type of made up character that appears in comic books released by marvel comics.</td>
      <td>a mutant is a fictional comic book character.  it appears on comic books published by marvel comics.</td>
      <td>a mutant is a fictional character that appears in marvel comic books.</td>
      <td>a mutant is a marvel comic book character.</td>
      <td>a mutant is a fictional character that shows up in marvel comic books.</td>
      <td>a mutant is an imaginary character in marvel comic books.</td>
      <td>a mutant is a kind of character appearing in marvel comics.</td>
      <td>0.900201</td>
      <td>4.0</td>
      <td>0.900201</td>
      <td>4.0</td>
      <td>0.937500</td>
      <td>1.000000</td>
      <td>0.882353</td>
      <td>0.800000</td>
      <td>0.727273</td>
      <td>0.750000</td>
      <td>0.937500</td>
      <td>0.909091</td>
      <td>0.882353</td>
      <td>0.937500</td>
      <td>0.909091</td>
      <td>0.882353</td>
      <td>0.577766</td>
      <td>The output text maintains the core meaning of the original sentence by correctly identifying that a mutant is a character in Marvel comics. However, it does not simplify the language as effectively as the reference. The phrase "type of character" is less vivid and slightly less clear than the reference's "pretend character," which better conveys the fictional nature of mutants. The output avoids omissions but uses a more generic term ("type of character") that may not be as immediately accessible to a broader audience. The structure is coherent, but the simplification falls short of the reference's clarity and directness.<br/>The output sentence "a mutant is a type of character that appears in comic books published by marvel comics" is grammatically correct, uses simple and clear language, and maintains a natural flow. It avoids complex terms like "fictional" while retaining the core meaning, making it accessible to a broader audience. The structure is straightforward, with no awkward phrasing or redundancy. This aligns well with the task of simplifying the original sentence for clarity and fluency.<br/>The output text is grammatically correct with proper sentence structure. It maintains the original meaning while using standard syntax and complete sentences. However, it does not simplify the sentence as effectively as the reference text. The reference uses a more straightforward structure ("is called a mutant") and simpler vocabulary ("pretend" instead of "fictional"). The output retains the phrase "type of character" and the passive construction "appears in comic books published by," which are grammatically correct but less simplified than the reference's active phrasing. The output avoids errors but falls short of optimal simplification.</td>
    </tr>
    <tr>
      <td>10</td>
      <td>a town may be correctly described as a market town or as having market rights even if it no longer holds a market, provided the right to do so still exists.</td>
      <td>may be described a town or as having market rights if it holds a market, provided the right to do exists.</td>
      <td>a town can be correctly described as a market town or as having rights to trade even if it doesn't have a market anymore because the right to do it is still exists.</td>
      <td>a town can be described as a market town or as having market rights even if it doesn't have a market as long as the right to have a market still exists.</td>
      <td>a town could be described as a market town or having market rights even if it doesn't take care of money if it is able to.</td>
      <td>so long as the right exists, a town me remain classified as a market town.</td>
      <td>a town may still be a market town even if no longer has a market.</td>
      <td>towns may or may not have markets but can still carry the name "market town".</td>
      <td>a town can be described as a market town or as having market rights.  this is so, even if the town no longer has a market as long as it could lawfully have one.</td>
      <td>a town may be called a market town or may have market rights even if it no longer has a market.</td>
      <td>a town can be described as a market town even it if no longer has a market, as long as the right to do so still exists.</td>
      <td>a town can be called a market town or as having market rights even if it no longer has one, if the right to do so still exists.</td>
      <td>-0.748478</td>
      <td>1.0</td>
      <td>-0.748478</td>
      <td>1.0</td>
      <td>0.857143</td>
      <td>0.607143</td>
      <td>0.693878</td>
      <td>0.550000</td>
      <td>0.407407</td>
      <td>0.468085</td>
      <td>0.809524</td>
      <td>0.571429</td>
      <td>0.629630</td>
      <td>0.809524</td>
      <td>0.571429</td>
      <td>0.629630</td>
      <td>-0.352029</td>
      <td>The output text fails to maintain clarity and coherence in conveying the original intent. It incorrectly states "if it holds a market," which contradicts the original sentence's key point that the town may no longer hold a market. The phrase "may be described a town" is grammatically incorrect and ambiguous. Additionally, the output omits the critical condition that the right to hold a market must still exist, even if the market is no longer active. The reference text clearly preserves the original meaning with proper structure, while the output introduces confusion and inaccuracies.<br/>The output sentence has several fluency issues. It incorrectly states "may be described a town" (missing "as"), reverses the original condition ("if it holds a market" instead of "even if it no longer holds a market"), and contains a grammatical error in "provided the right to do exists" (missing "so"). These errors disrupt readability and coherence compared to the original and references, which maintain clearer structure and logical flow. The sentence is simplified but loses grammatical correctness and clarity, lowering its fluency score.<br/>The output text contains multiple grammatical errors and structural issues. The phrase "may be described a town" is incorrectly worded (should be "a town may be described"). The sentence structure is awkward, with missing prepositions ("the right to do exists" instead of "the right to do so exists"). Additionally, the output omits key elements from the original input, such as "even if it no longer holds a market," altering the intended meaning. These errors significantly reduce fluency and clarity, failing to meet the axis requirements for proper grammar and logical phrasing.</td>
    </tr>
  </tbody>
</table></div>
    </div>
  </div>

  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/js/bootstrap.bundle.min.js"></script>
  <script src="https://cdn.datatables.net/2.0.8/js/dataTables.min.js"></script>
  <script>
    function getThemeLayout() {
      const color = getComputedStyle(document.body).color;
      return { paper_bgcolor: 'rgba(0,0,0,0)', plot_bgcolor: 'rgba(0,0,0,0)', font: { color } };
    }
    document.getElementById('darkModeToggle').addEventListener('change',e=>{document.body.classList.toggle('dark-mode',e.target.checked); drawAll();});
    // Enable tooltips
    const tooltipTriggerList = [].slice.call(document.querySelectorAll('[data-bs-toggle="tooltip"]'));
    tooltipTriggerList.map(function (el) {
      const tip = new bootstrap.Tooltip(el, {trigger: 'hover focus', delay: {show: 0, hide: 50}, placement: 'right'});
      el.addEventListener('shown.bs.tooltip', function () {
        try { if (window.MathJax && MathJax.typesetPromise) { MathJax.typesetPromise(); } } catch(_) {}
      });
      return tip;
    });

    // Initialize tooltips; use template content for robustness
    document.addEventListener('DOMContentLoaded', function () {
      document.querySelectorAll('.robust-tip').forEach(function (el) {
        const id = el.getAttribute('data-tip-id');
        let titleHtml = '';
        if (id) {
          const tpl = document.getElementById(id);
          if (tpl) titleHtml = tpl.innerHTML;
        }
        if (!titleHtml) {
          titleHtml = '<div style="max-width: 320px">Robustness tooltip unavailable.</div>';
        }
        const tip = new bootstrap.Tooltip(el, {
          trigger: 'hover focus',
          delay: {show: 0, hide: 50},
          placement: 'right',
          html: true,
          title: titleHtml
        });
      });
    });

    function drawCorrelation() {
      const layout = Object.assign({xaxis:{title:'Metric Score (normalized to target scale)'}, yaxis:{title:'Ground Truth'}}, getThemeLayout());
      layout.legend = layout.legend || {}; layout.legend.font = { size: 9 }; layout.margin = {l:40,r:10,t:30,b:40};
      const traces = [];
      if (RC_CORR.metrics) {
        // Determine top 3 metrics by absolute coefficient if available
        let topNames = [];
        try {
          const coeffPairs = ([["Clarity_and_Coherence_of_Simplified_Meaning_Qwen3-32B", 0.06090341858921349], ["fluency_Qwen3-32B_examples", 0.06088733859061157], ["Grammatical_Correctness_and_Sentence_Structure_Qwen3-32B", 0.05704524560516381], ["ROUGE-2-p", 0.039228631967294376], ["ROUGE-1-f1", 0.03531481306886559], ["ROUGE-Lsum-f1", 0.03531481306886559], ["ROUGE-Lsum-r", 0.03288414076895716], ["ROUGE-L-f1", 0.03288414076895716], ["ROUGE-L-p", 0.03158397235276], ["ROUGE-2-r", 0.03150648238618535], ["ROUGE-1-r", 0.03082766831876505], ["ROUGE-Lsum-p", 0.028213998698624503], ["ROUGE-2-f1", 0.004329322143592244], ["ROUGE-L-r", 0.004329322143592242], ["ROUGE-1-p", -0.010982035477194148], ["(intercept)", 0.054768575599999994]]);
          const sorted = coeffPairs.filter(p=>p[0] !== '(intercept)').sort((a,b)=>Math.abs(b[1]) - Math.abs(a[1]));
          topNames = sorted.slice(0,3).map(p=>p[0]);
        } catch (e) { topNames = []; }
        for (const m of RC_CORR.metrics) {
          const rlab = (m.r!=null ? (m.r.toFixed ? m.r.toFixed(2) : m.r) : 'NA');
          const tlab = (m.tau!=null ? (m.tau.toFixed ? m.tau.toFixed(2) : m.tau) : 'NA');
          const visible = (topNames.includes(m.name)) ? true : 'legendonly';
          const ids = m.ids || [];
          const text = ids.map(id => 'ID: ' + id);
          traces.push({ x: m.x_norm || m.x || [], y: m.y || [], mode: 'markers', name: (m.name || '') + ' (r=' + rlab + ', =' + tlab + ')', visible, text: text, hovertemplate: '%{text}<br>x=%{x:.3f}<br>y=%{y:.3f}<extra></extra>' });
        }
      }
      if (RC_CORR.regression) {
        const rlab = (RC_CORR.regression.r!=null ? (RC_CORR.regression.r.toFixed ? RC_CORR.regression.r.toFixed(2) : RC_CORR.regression.r) : 'NA');
        const tlab = (RC_CORR.regression.tau!=null ? (RC_CORR.regression.tau.toFixed ? RC_CORR.regression.tau.toFixed(2) : RC_CORR.regression.tau) : 'NA');
        const ids = RC_CORR.regression.ids || [];
        const text = ids.map(id => 'ID: ' + id);
        traces.push({ x: RC_CORR.regression.x_norm || RC_CORR.regression.x || [], y: RC_CORR.regression.y || [], mode: 'markers', name: (RC_CORR.regression.name || 'Aggregate') + ' (r=' + rlab + ', =' + tlab + ')', marker: { size: 8, color: 'black' }, text: text, hovertemplate: '%{text}<br>x=%{x:.3f}<br>y=%{y:.3f}<extra></extra>' });
        document.getElementById('correlation-stats').innerText = 'Aggregate metric: r=' + rlab + ', =' + tlab;
      }
      Plotly.newPlot('correlation-chart', traces, layout, {displayModeBar: false});
      // Click-to-jump: when a point is clicked, locate its ID in the examples table and jump to it
      const chart = document.getElementById('correlation-chart');
      chart.on('plotly_click', function(data) {
        try {
          if (!data || !data.points || data.points.length === 0) return;
          const pt = data.points[0];
          const idText = (pt.text || '').toString(); // format: 'ID: <val>'
          const id = idText.startsWith('ID: ') ? idText.slice(4) : idText;
          const tblEl = document.getElementById('examples-table');
          if (!tblEl) return;
          // Try DataTables jQuery API first
          if (window.jQuery && jQuery.fn && jQuery.fn.dataTable) {
            const dt = jQuery(tblEl).DataTable();
            // Search by exact match in first column (ID)
            dt.search('');
            dt.columns(0).search('^' + id.replace(/[.*+?^${}()|[\]\\]/g, '\\$&') + '$', true, false).draw();
            // Scroll into view first visible row after draw completes
            setTimeout(function(){
              let rowNode = null;
              try {
                const idxs = dt.rows({ search: 'applied' }).indexes();
                if (idxs && idxs.length) rowNode = dt.row(idxs[0]).node();
              } catch(_){ }
              if (!rowNode) {
                try { rowNode = dt.row(0).node(); } catch(_) {}
              }
              if (rowNode && rowNode.scrollIntoView) {
                rowNode.scrollIntoView({behavior:'smooth', block:'center'});
                try { rowNode.classList.add('table-active'); setTimeout(()=>rowNode.classList.remove('table-active'), 1200); } catch(_) {}
              }
            }, 60);
          } else if (typeof DataTable !== 'undefined') {
            // Vanilla DataTables 2 API
            const dt = DataTable.get(tblEl) || new DataTable(tblEl);
            dt.search('');
            // Filter to rows whose first cell (ID) matches
            dt.columns().every(function(idx) {
              if (idx === 0) {
                this.search('^' + id.replace(/[.*+?^${}()|[\]\\]/g, '\\$&') + '$', true, false);
              } else {
                this.search('');
              }
            });
            dt.draw();
            setTimeout(function(){
              let firstRow = null;
              try {
                const nodes = dt.rows({ search: 'applied' }).nodes();
                if (nodes && nodes.length) firstRow = nodes[0];
              } catch(_) {}
              if (!firstRow) {
                const body = tblEl.tBodies && tblEl.tBodies[0];
                firstRow = body && body.rows && body.rows[0];
              }
              if (!firstRow) {
                try {
                  const rows = Array.from(tblEl.tBodies[0].rows || []);
                  firstRow = rows.find(r => (r.cells && r.cells[0] && (r.cells[0].textContent||'').trim() === id));
                } catch(_) {}
              }
              if (firstRow && firstRow.scrollIntoView) {
                firstRow.scrollIntoView({behavior:'smooth', block:'center'});
                try { firstRow.classList.add('table-active'); setTimeout(()=>firstRow.classList.remove('table-active'), 1200); } catch(_) {}
              }
            }, 60);
          }
        } catch(e) { try { console.error('[ReportCard] click-jump failed', e); } catch(_){} }
      });
    }

    function drawRuntime() {
      const layout = Object.assign({yaxis:{title:'Time per Sample (s)'}}, getThemeLayout());
      const boxes = [];
      if (RC_RUNTIME.per_metric) {
        for (const [name, arr] of Object.entries(RC_RUNTIME.per_metric)) {
          boxes.push({ y: arr, type: 'box', name });
        }
      }
      Plotly.newPlot('runtime-chart', boxes, layout);
      if (RC_RUNTIME.aggregate) {
        const agg = RC_RUNTIME.aggregate;
        var seq = (agg.sequence_mean||0);
        if (typeof seq === 'number' && seq.toFixed) { seq = seq.toFixed(2); }
        var par = (agg.parallel_mean||0);
        if (typeof par === 'number' && par.toFixed) { par = par.toFixed(2); }
        var seqCI = (agg.sequence_ci||0);
        if (typeof seqCI === 'number' && seqCI.toFixed) { seqCI = seqCI.toFixed(2); }
        var parCI = (agg.parallel_ci||0);
        if (typeof parCI === 'number' && parCI.toFixed) { parCI = parCI.toFixed(2); }
        document.getElementById('runtime-info').innerHTML = 'Avg time/sample (sequence): ' + seq + 's  ' + seqCI + 's' + '<br/>' + 'Avg time/sample (parallel): ' + par + 's  ' + parCI + 's (95% CI)';
      }
    }

    function drawRobustness() {
      if (!RC_ROB.available || !RC_ROB.scores) {
        document.getElementById('robustness-sens').innerHTML = '<em>Robustness not available.</em>';
        document.getElementById('robustness-stab').innerHTML = '';
        return;
      }
      const names = Object.keys(RC_ROB.scores);
      const sens = names.map(n => (RC_ROB.scores[n] && RC_ROB.scores[n].sensitivity) || 0);
      const stab = names.map(n => (RC_ROB.scores[n] && RC_ROB.scores[n].stability) || 0);
      Plotly.newPlot('robustness-sens', [{x: names, y: sens, type:'bar', name:'Sensitivity'}], Object.assign({yaxis:{title:'Sensitivity'}}, getThemeLayout()));
      Plotly.newPlot('robustness-stab', [{x: names, y: stab, type:'bar', name:'Stability'}], Object.assign({yaxis:{title:'Stability'}}, getThemeLayout()));
    }

    function drawAll() { drawCorrelation(); drawRuntime(); drawRobustness(); }
    drawAll();
  </script>
  <!-- Modal for Metric Card -->
  <div class="modal fade" id="metricDocModal" tabindex="-1" aria-hidden="true">
    <div class="modal-dialog modal-xl modal-dialog-scrollable">
      <div class="modal-content">
        <div class="modal-header">
          <h5 class="modal-title" id="metricDocTitle"></h5>
          <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
        </div>
        <div class="modal-body">
          <div id="metricDocBody" style="white-space: normal;"></div>
        </div>
      </div>
    </div>
  </div>
  <script>
    (function() {
      const tbl = document.getElementById('examples-table');
      if (!tbl) return;
      const clearBtn = document.getElementById('clear-examples-filter');
      try {
        if (window.jQuery && jQuery.fn && typeof jQuery.fn.dataTable !== 'undefined') {
          jQuery(tbl).DataTable({
            paging: true,
            pageLength: 5,
            ordering: true,
            searching: true,
            scrollX: true
          });
          if (clearBtn) {
            clearBtn.addEventListener('click', function(){
              try {
                const dt = jQuery(tbl).DataTable();
                dt.search('');
                dt.columns().every(function(){ this.search(''); });
                dt.draw();
              } catch(_) {}
            });
          }
        } else if (typeof DataTable !== 'undefined') {
          new DataTable(tbl, {
            paging: true,
            pageLength: 5,
            ordering: true,
            searching: true,
            scrollX: true
          });
          if (clearBtn) {
            clearBtn.addEventListener('click', function(){
              try {
                const dt = DataTable.get(tbl);
                dt.search('');
                dt.columns().every(function(){ this.search(''); });
                dt.draw();
              } catch(_) {}
            });
          }
        }
      } catch (e) { try { console.error('[ReportCard] DataTables init error:', e); } catch(_){} }
    })();
  </script>
  <script>
    // Click handlers for regression coefficient metric links -> open modal with metric card
    document.addEventListener('click', function(e) {
      const a = e.target.closest && e.target.closest('a.coeff-link');
      if (!a) return;
      e.preventDefault();
      try {
        let metric = a.getAttribute('data-metric');
        // Resolve submetric to parent metric if available
        if (RC_DOCS && !(metric in RC_DOCS) && RC_DOCS_MAP && RC_DOCS_MAP[metric]) {
          metric = RC_DOCS_MAP[metric];
        }
        const doc = (RC_DOCS && RC_DOCS[metric]) ? RC_DOCS[metric] : 'No metric card available.';
        const titleNode = document.getElementById('metricDocTitle');
        const bodyNode = document.getElementById('metricDocBody');
        if (titleNode) titleNode.textContent = metric + '  Metric Card';
        if (bodyNode) {
          try {
            bodyNode.innerHTML = marked.parse(doc);
          } catch(_) {
            bodyNode.textContent = doc;
          }
        }
        const modalEl = document.getElementById('metricDocModal');
        if (modalEl && bootstrap && bootstrap.Modal) {
          const modal = bootstrap.Modal.getOrCreateInstance(modalEl, {backdrop: true});
          modal.show();
        }
      } catch(_) {}
    });
  </script>
  <div id="robustness-tip-template" class="d-none">
    <div style="max-width: 360px">
      <strong>Sensitivity</strong> (worse_obvious): how much the metric tends to drop when the output is intentionally degraded. For each example, we measure the relative drop from the original to the average worse_obvious score, clip negative values to 0 (no drop), and then average across examples.
      <br/><br/>
      <strong>Stability</strong> (same_obvious): how consistent the metric stays under neutral edits that should not change meaning. For each example, we measure how close the original is to the average same_obvious score (scaled by the original magnitude), clip below 0, and then average across examples. Higher means more stable.
    </div>
  </div>
</body>
</html>
