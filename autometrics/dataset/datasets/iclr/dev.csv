id,full_text,abstract,conference,accepted,format,review_comments,appropriateness,clarity,impact,meaningful_comparison,originality,recommendation,recommendation_unofficial,replicability,reviewer_confidence,soundness_correctness,substance
316,"SEMI-SUPERVISED KNOWLEDGE TRANSFER FOR DEEP LEARNING FROM PRIVATE TRAINING DATA
Authors: Nicolas Papernot
Source file: 316.pdf

ABSTRACT
Some machine learning applications involve training data that is sensitive, such as the medical histories of patients in a clinical trial. A model may inadvertently and implicitly store some of its training data; careful analysis of the model may therefore reveal sensitive information. To address this problem, we demonstrate a generally applicable approach to providing strong privacy guarantees for training data: Private Aggregation of Teacher Ensembles (PATE). The approach combines, in a black-box fashion, multiple models trained with disjoint datasets, such as records from different subsets of users. Because they rely directly on sensitive data, these models are not published, but instead used as “teachers” for a “student” model. The student learns to predict an output chosen by noisy voting among all of the teachers, and cannot directly access an individual teacher or the underlying data or parameters. The student’s privacy properties can be understood both intuitively (since no single teacher and thus no single dataset dictates the student’s training) and formally, in terms of differential privacy. These properties hold even if an adversary can not only query the student but also inspect its internal workings. Compared with previous work, the approach imposes only weak assumptions on how teachers are trained: it applies to any model, including non-convex models like DNNs. We achieve state-of-the-art privacy/utility trade-offs on MNIST and SVHN thanks to an improved privacy analysis and semi-supervised learning.

1 INTRODUCTION
Some machine learning applications with great benefits are enabled only through the analysis of sensitive data, such as users’ personal contacts, private photographs or correspondence, or even medical records or genetic sequences (Alipanahi et al., 2015; Kannan et al., 2016; Kononenko, 2001; Sweeney, 1997). Ideally, in those cases, the learning algorithms would protect the privacy of users’ training data, e.g., by guaranteeing that the output model generalizes away from the specifics of any individual user. Unfortunately, established machine learning algorithms make no such guarantee; indeed, though state-of-the-art algorithms generalize well to the test set, they continue to overfit on specific training examples in the sense that some of these examples are implicitly memorized.
Recent attacks exploiting this implicit memorization in machine learning have demonstrated that private, sensitive training data can be recovered from models. Such attacks can proceed directly, by analyzing internal model parameters, but also indirectly, by repeatedly querying opaque models to gather data for the attack’s analysis. For example, Fredrikson et al. (2015) used hill-climbing on the output probabilities of a computer-vision classifier to reveal individual faces from the training data.
∗Work done while the author was at Google. †Work done both at Google Brain and at OpenAI.
Because of those demonstrations—and because privacy guarantees must apply to worst-case outliers, not only the average—any strategy for protecting the privacy of training data should prudently assume that attackers have unfettered access to internal model parameters.
To protect the privacy of training data, this paper improves upon a specific, structured application of the techniques of knowledge aggregation and transfer (Breiman, 1994), previously explored by Nissim et al. (2007), Pathak et al. (2010), and particularly Hamm et al. (2016). In this strategy, first, an ensemble (Dietterich, 2000) of teacher models is trained on disjoint subsets of the sensitive data. Then, using auxiliary, unlabeled non-sensitive data, a student model is trained on the aggregate output of the ensemble, such that the student learns to accurately mimic the ensemble. Intuitively, this strategy ensures that the student does not depend on the details of any single sensitive training data point (e.g., of any single user), and, thereby, the privacy of the training data is protected even if attackers can observe the student’s internal model parameters.
This paper shows how this strategy’s privacy guarantees can be strengthened by restricting student training to a limited number of teacher votes, and by revealing only the topmost vote after carefully adding random noise. We call this strengthened strategy PATE, for Private Aggregation of Teacher Ensembles. Furthermore, we introduce an improved privacy analysis that makes the strategy generally applicable to machine learning algorithms with high utility and meaningful privacy guarantees—in particular, when combined with semi-supervised learning.
To establish strong privacy guarantees, it is important to limit the student’s access to its teachers, so that the student’s exposure to teachers’ knowledge can be meaningfully quantified and bounded. Fortunately, there are many techniques for speeding up knowledge transfer that can reduce the rate of student/teacher consultation during learning. We describe several techniques in this paper, the most effective of which makes use of generative adversarial networks (GANs) (Goodfellow et al., 2014) applied to semi-supervised learning, using the implementation proposed by Salimans et al. (2016). For clarity, we use the term PATE-G when our approach is combined with generative, semisupervised methods. Like all semi-supervised learning methods, PATE-G assumes the student has access to additional, unlabeled data, which, in this context, must be public or non-sensitive. This assumption should not greatly restrict our method’s applicability: even when learning on sensitive data, a non-overlapping, unlabeled set of data often exists, from which semi-supervised methods can extract distribution priors. For instance, public datasets exist for text and images, and for medical data.
It seems intuitive, or even obvious, that a student machine learning model will provide good privacy when trained without access to sensitive training data, apart from a few, noisy votes from a teacher quorum. However, intuition is not sufficient because privacy properties can be surprisingly hard to reason about; for example, even a single data item can greatly impact machine learning models trained on a large corpus (Chaudhuri et al., 2011). Therefore, to limit the effect of any single sensitive data item on the student’s learning, precisely and formally, we apply the well-established, rigorous standard of differential privacy (Dwork & Roth, 2014). Like all differentially private algorithms, our learning strategy carefully adds noise, so that the privacy impact of each data item can be analyzed and bounded. In particular, we dynamically analyze the sensitivity of the teachers’ noisy votes; for this purpose, we use the state-of-the-art moments accountant technique from Abadi et al. (2016), which tightens the privacy bound when the topmost vote has a large quorum. As a result, for MNIST and similar benchmark learning tasks, our methods allow students to provide excellent utility, while our analysis provides meaningful worst-case guarantees. In particular, we can bound the metric for privacy loss (the differential-privacy ε) to a range similar to that of existing, real-world privacyprotection mechanisms, such as Google’s RAPPOR (Erlingsson et al., 2014).
Finally, it is an important advantage that our learning strategy and our privacy analysis do not depend on the details of the machine learning techniques used to train either the teachers or their student. Therefore, the techniques in this paper apply equally well for deep learning methods, or any such learning methods with large numbers of parameters, as they do for shallow, simple techniques. In comparison, Hamm et al. (2016) guarantee privacy only conditionally, for a restricted class of student classifiers—in effect, limiting applicability to logistic regression with convex loss. Also, unlike the methods of Abadi et al. (2016), which represent the state-of-the-art in differentiallyprivate deep learning, our techniques make no assumptions about details such as batch selection, the loss function, or the choice of the optimization algorithm. Even so, as we show in experiments on
MNIST and SVHN, our techniques provide a privacy/utility tradeoff that equals or improves upon bespoke learning methods such as those of Abadi et al. (2016).
Section 5 further discusses the related work. Building on this related work, our contributions are as follows:
• We demonstrate a general machine learning strategy, the PATE approach, that provides differential privacy for training data in a “black-box” manner, i.e., independent of the learning algorithm, as demonstrated by Section 4 and Appendix C.
• We improve upon the strategy outlined in Hamm et al. (2016) for learning machine models that protect training data privacy. In particular, our student only accesses the teachers’ top vote and the model does not need to be trained with a restricted class of convex losses.
• We explore four different approaches for reducing the student’s dependence on its teachers, and show how the application of GANs to semi-supervised learning of Salimans et al. (2016) can greatly reduce the privacy loss by radically reducing the need for supervision.
• We present a new application of the moments accountant technique from Abadi et al. (2016) for improving the differential-privacy analysis of knowledge transfer, which allows the training of students with meaningful privacy bounds.
• We evaluate our framework on MNIST and SVHN, allowing for a comparison of our results with previous differentially private machine learning methods. Our classifiers achieve an (ε, δ) differential-privacy bound of (2.04, 10−5) for MNIST and (8.19, 10−6) for SVHN, respectively with accuracy of 98.00% and 90.66%. In comparison, for MNIST, Abadi et al. (2016) obtain a looser (8, 10−5) privacy bound and 97% accuracy. For SVHN, Shokri & Shmatikov (2015) report approx. 92% accuracy with ε > 2 per each of 300,000 model parameters, naively making the total ε > 600,000, which guarantees no meaningful privacy.
• Finally, we show that the PATE approach can be successfully applied to other model structures and to datasets with different characteristics. In particular, in Appendix C PATE protects the privacy of medical data used to train a model based on random forests.
Our results are encouraging, and highlight the benefits of combining a learning strategy based on semi-supervised knowledge transfer with a precise, data-dependent privacy analysis. However, the most appealing aspect of this work is probably that its guarantees can be compelling to both an expert and a non-expert audience. In combination, our techniques simultaneously provide both an intuitive and a rigorous guarantee of training data privacy, without sacrificing the utility of the targeted model. This gives hope that users will increasingly be able to confidently and safely benefit from machine learning models built from their sensitive data.

2 PRIVATE LEARNING WITH ENSEMBLES OF TEACHERS
In this section, we introduce the specifics of the PATE approach, which is illustrated in Figure 1. We describe how the data is partitioned to train an ensemble of teachers, and how the predictions made by this ensemble are noisily aggregated. In addition, we discuss how GANs can be used in training the student, and distinguish PATE-G variants that improve our approach using generative, semi-supervised methods.

2.1 TRAINING THE ENSEMBLE OF TEACHERS
Data partitioning and teachers: Instead of training a single model to solve the task associated with dataset (X,Y ), where X denotes the set of inputs, and Y the set of labels, we partition the data in n disjoint sets (Xn, Yn) and train a model separately on each set. As evaluated in Section 4.1, assuming that n is not too large with respect to the dataset size and task complexity, we obtain n classifiers fi called teachers. We then deploy them as an ensemble making predictions on unseen inputs x by querying each teacher for a prediction fi(x) and aggregating these into a single prediction.
Aggregation: The privacy guarantees of this teacher ensemble stems from its aggregation. Let m be the number of classes in our task. The label count for a given class j ∈ [m] and an input ~x is the number of teachers that assigned class j to input ~x: nj(~x) = |{i : i ∈ [n], fi(~x) = j}|. If we simply apply plurality—use the label with the largest count—the ensemble’s decision may depend on a single teacher’s vote. Indeed, when two labels have a vote count differing by at most one, there is a tie: the aggregated output changes if one teacher makes a different prediction. We add random noise to the vote counts nj to introduce ambiguity:
f(x) = argmax j
{ nj(~x) + Lap ( 1
γ
)} (1)
In this equation, γ is a privacy parameter and Lap(b) the Laplacian distribution with location 0 and scale b. The parameter γ influences the privacy guarantee we can prove. Intuitively, a large γ leads to a strong privacy guarantee, but can degrade the accuracy of the labels, as the noisy maximum f above can differ from the true plurality.
While we could use an f such as above to make predictions, the noise required would increase as we make more predictions, making the model useless after a bounded number of queries. Furthermore, privacy guarantees do not hold when an adversary has access to the model parameters. Indeed, as each teacher fi was trained without taking into account privacy, it is conceivable that they have sufficient capacity to retain details of the training data. To address these limitations, we train another model, the student, using a fixed number of labels predicted by the teacher ensemble.

2.2 SEMI-SUPERVISED TRANSFER OF THE KNOWLEDGE FROM AN ENSEMBLE TO A STUDENT
We train a student on nonsensitive and unlabeled data, some of which we label using the aggregation mechanism. This student model is the one deployed, in lieu of the teacher ensemble, so as to fix the privacy loss to a value that does not grow with the number of user queries made to the student model. Indeed, the privacy loss is now determined by the number of queries made to the teacher ensemble during student training and does not increase as end-users query the deployed student model. Thus, the privacy of users who contributed to the original training dataset is preserved even if the student’s architecture and parameters are public or reverse-engineered by an adversary.
We considered several techniques to trade-off the student model’s quality with the number of labels it needs to access: distillation, active learning, semi-supervised learning (see Appendix B). Here, we only describe the most successful one, used in PATE-G: semi-supervised learning with GANs.
Training the student with GANs: The GAN framework involves two machine learning models, a generator and a discriminator. They are trained in a competing fashion, in what can be viewed as a two-player game (Goodfellow et al., 2014). The generator produces samples from the data distribution by transforming vectors sampled from a Gaussian distribution. The discriminator is trained to distinguish samples artificially produced by the generator from samples part of the real data distribution. Models are trained via simultaneous gradient descent steps on both players’ costs. In practice, these dynamics are often difficult to control when the strategy set is non-convex (e.g., a DNN). In their application of GANs to semi-supervised learning, Salimans et al. (2016) made the following modifications. The discriminator is extended from a binary classifier (data vs. generator sample) to a multi-class classifier (one of k classes of data samples, plus a class for generated samples). This classifier is then trained to classify labeled real samples in the correct class, unlabeled real samples in any of the k classes, and the generated samples in the additional class.
Although no formal results currently explain why yet, the technique was empirically demonstrated to greatly improve semi-supervised learning of classifiers on several datasets, especially when the classifier is trained with feature matching loss (Salimans et al., 2016).
Training the student in a semi-supervised fashion makes better use of the entire data available to the student, while still only labeling a subset of it. Unlabeled inputs are used in unsupervised learning to estimate a good prior for the distribution. Labeled inputs are then used for supervised learning.

3 PRIVACY ANALYSIS OF THE APPROACH
We now analyze the differential privacy guarantees of our PATE approach. Namely, we keep track of the privacy budget throughout the student’s training using the moments accountant (Abadi et al., 2016). When teachers reach a strong quorum, this allows us to bound privacy costs more strictly.

3.1 DIFFERENTIAL PRIVACY PRELIMINARIES AND A SIMPLE ANALYSIS OF PATE
Differential privacy (Dwork et al., 2006b; Dwork, 2011) has established itself as a strong standard. It provides privacy guarantees for algorithms analyzing databases, which in our case is a machine learning training algorithm processing a training dataset. Differential privacy is defined using pairs of adjacent databases: in the present work, these are datasets that only differ by one training example. Recall the following variant of differential privacy introduced in Dwork et al. (2006a). Definition 1. A randomized mechanismM with domain D and rangeR satisfies (ε, δ)-differential privacy if for any two adjacent inputs d, d′ ∈ D and for any subset of outputs S ⊆ R it holds that:
Pr[M(d) ∈ S] ≤ eε Pr[M(d′) ∈ S] + δ. (2)
It will be useful to define the privacy loss and the privacy loss random variable. They capture the differences in the probability distribution resulting from runningM on d and d′. Definition 2. LetM : D → R be a randomized mechanism and d, d′ a pair of adjacent databases. Let aux denote an auxiliary input. For an outcome o ∈ R, the privacy loss at o is defined as:
c(o;M,aux, d, d′) ∆= log Pr[M(aux, d) = o] Pr[M(aux, d′) = o] . (3)
The privacy loss random variable C(M,aux, d, d′) is defined as c(M(d);M,aux, d, d′), i.e. the random variable defined by evaluating the privacy loss at an outcome sampled fromM(d).
A natural way to bound our approach’s privacy loss is to first bound the privacy cost of each label queried by the student, and then use the strong composition theorem (Dwork et al., 2010) to derive the total cost of training the student. For neighboring databases d, d′, each teacher gets the same training data partition (that is, the same for the teacher with d and with d′, not the same across teachers), with the exception of one teacher whose corresponding training data partition differs. Therefore, the label counts nj(~x) for any example ~x, on d and d′ differ by at most 1 in at most two locations. In the next subsection, we show that this yields loose guarantees.

3.2 THE MOMENTS ACCOUNTANT: A BUILDING BLOCK FOR BETTER ANALYSIS
To better keep track of the privacy cost, we use recent advances in privacy cost accounting. The moments accountant was introduced by Abadi et al. (2016), building on previous work (Bun & Steinke, 2016; Dwork & Rothblum, 2016; Mironov, 2016). Definition 3. LetM : D → R be a randomized mechanism and d, d′ a pair of adjacent databases. Let aux denote an auxiliary input. The moments accountant is defined as:
αM(λ) ∆ = max aux,d,d′ αM(λ;aux, d, d′) (4)
where αM(λ;aux, d, d′) ∆ = logE[exp(λC(M,aux, d, d′))] is the moment generating function of the privacy loss random variable.
The following properties of the moments accountant are proved in Abadi et al. (2016).
Theorem 1. 1. [Composability] Suppose that a mechanism M consists of a sequence of adaptive mechanismsM1, . . . ,Mk whereMi : ∏i−1 j=1Rj × D → Ri. Then, for any output sequence o1, . . . , ok−1 and any λ
αM(λ; d, d ′) = k∑ i=1 αMi(λ; o1, . . . , oi−1, d, d ′) ,
where αM is conditioned onMi’s output being oi for i < k. 2. [Tail bound] For any ε > 0, the mechanismM is (ε, δ)-differentially private for
δ = min λ
exp(αM(λ)− λε) .
We write down two important properties of the aggregation mechanism from Section 2. The first property is proved in Dwork & Roth (2014), and the second follows from Bun & Steinke (2016).
Theorem 2. Suppose that on neighboring databases d, d′, the label counts nj differ by at most 1 in each coordinate. Let M be the mechanism that reports argmaxj { nj + Lap( 1 γ ) }
. Then M satisfies (2γ, 0)-differential privacy. Moreover, for any l, aux, d and d′,
α(l;aux, d, d′) ≤ 2γ2l(l + 1) (5)
At each step, we use the aggregation mechanism with noise Lap( 1γ ) which is (2γ, 0)-DP. Thus over T steps, we get (4Tγ2 + 2γ √ 2T ln 1δ , δ)-differential privacy. This can be rather large: plugging in values that correspond to our SVHN result, γ = 0.05, T = 1000, δ = 1e−6 gives us ε ≈ 26 or alternatively plugging in values that correspond to our MNIST result, γ = 0.05, T = 100, δ = 1e−5 gives us ε ≈ 5.80.

3.3 A PRECISE, DATA-DEPENDENT PRIVACY ANALYSIS OF PATE
Our data-dependent privacy analysis takes advantage of the fact that when the quorum among the teachers is very strong, the majority outcome has overwhelming likelihood, in which case the privacy cost is small whenever this outcome occurs. The moments accountant allows us analyze the composition of such mechanisms in a unified framework.
The following theorem, proved in Appendix A, provides a data-dependent bound on the moments of any differentially private mechanism where some specific outcome is very likely.
Theorem 3. LetM be (2γ, 0)-differentially private and q ≥ Pr[M(d) 6= o∗] for some outcome o∗. Let l, γ ≥ 0 and q < e
2γ−1 e4γ−1 . Then for any aux and any neighbor d ′ of d,M satisfies
α(l;aux, d, d′) ≤ log((1− q) ( 1− q 1− e2γq )l + q exp(2γl)).
To upper bound q for our aggregation mechanism, we use the following simple lemma, also proved in Appendix A.
Lemma 4. Let n be the label score vector for a database d with nj∗ ≥ nj for all j. Then
Pr[M(d) 6= j∗] ≤ ∑ j 6=j∗ 2 + γ(nj∗ − nj) 4 exp(γ(nj∗ − nj))
This allows us to upper bound q for a specific score vector n, and hence bound specific moments. We take the smaller of the bounds we get from Theorems 2 and 3. We compute these moments for a few values of λ (integers up to 8). Theorem 1 allows us to add these bounds over successive steps, and derive an (ε, δ) guarantee from the final α. Interested readers are referred to the script that we used to empirically compute these bounds, which is released along with our code: https://github. com/tensorflow/models/tree/master/differential_privacy/multiple_teachers
Since the privacy moments are themselves now data dependent, the final ε is itself data-dependent and should not be revealed. To get around this, we bound the smooth sensitivity (Nissim et al., 2007) of the moments and add noise proportional to it to the moments themselves. This gives us a differentially private estimate of the privacy cost. Our evaluation in Section 4 ignores this overhead and reports the un-noised values of ε. Indeed, in our experiments on MNIST and SVHN, the scale of the noise one needs to add to the released ε is smaller than 0.5 and 1.0 respectively.
How does the number of teachers affect the privacy cost? Recall that the student uses a noisy label computed in (1) which has a parameter γ. To ensure that the noisy label is likely to be the correct one, the noise scale 1γ should be small compared to the the additive gap between the two largest vales of nj . While the exact dependence of γ on the privacy cost in Theorem 3 is subtle, as a general principle, a smaller γ leads to a smaller privacy cost. Thus, a larger gap translates to a smaller privacy cost. Since the gap itself increases with the number of teachers, having more teachers would lower the privacy cost. This is true up to a point. With n teachers, each teacher only trains on a 1n fraction of the training data. For large enough n, each teachers will have too little training data to be accurate.
To conclude, we note that our analysis is rather conservative in that it pessimistically assumes that, even if just one example in the training set for one teacher changes, the classifier produced by that teacher may change arbitrarily. One advantage of our approach, which enables its wide applicability, is that our analysis does not require any assumptions about the workings of the teachers. Nevertheless, we expect that stronger privacy guarantees may perhaps be established in specific settings—when assumptions can be made on the learning algorithm used to train the teachers.

4 EVALUATION
In our evaluation of PATE and its generative variant PATE-G, we first train a teacher ensemble for each dataset. The trade-off between the accuracy and privacy of labels predicted by the ensemble is greatly dependent on the number of teachers in the ensemble: being able to train a large set of teachers is essential to support the injection of noise yielding strong privacy guarantees while having a limited impact on accuracy. Second, we minimize the privacy budget spent on learning the student by training it with as few queries to the ensemble as possible.
Our experiments use MNIST and the extended SVHN datasets. Our MNIST model stacks two convolutional layers with max-pooling and one fully connected layer with ReLUs. When trained on the entire dataset, the non-private model has a 99.18% test accuracy. For SVHN, we add two hidden layers.1 The non-private model achieves a 92.8% test accuracy, which is shy of the state-of-the-art. However, we are primarily interested in comparing the private student’s accuracy with the one of a non-private model trained on the entire dataset, for different privacy guarantees. The source code for reproducing the results in this section is available on GitHub.2

4.1 TRAINING AN ENSEMBLE OF TEACHERS PRODUCING PRIVATE LABELS
As mentioned above, compensating the noise introduced by the Laplacian mechanism presented in Equation 1 requires large ensembles. We evaluate the extent to which the two datasets considered can be partitioned with a reasonable impact on the performance of individual teachers. Specifically, we show that for MNIST and SVHN, we are able to train ensembles of 250 teachers. Their aggregated predictions are accurate despite the injection of large amounts of random noise to ensure privacy. The aggregation mechanism output has an accuracy of 93.18% for MNIST and 87.79% for SVHN, when evaluated on their respective test sets, while each query has a low privacy budget of ε = 0.05.
Prediction accuracy: All other things being equal, the number n of teachers is limited by a tradeoff between the classification task’s complexity and the available data. We train n teachers by partitioning the training data n-way. Larger values of n lead to larger absolute gaps, hence potentially allowing for a larger noise level and stronger privacy guarantees. At the same time, a larger n implies a smaller training dataset for each teacher, potentially reducing the teacher accuracy. We empirically find appropriate values of n for the MNIST and SVHN datasets by measuring the test
1The model is adapted from https://www.tensorflow.org/tutorials/deep_cnn 2 https://github.com/tensorflow/models/tree/master/differential_privacy/multiple_teachers
set accuracy of each teacher trained on one of the n partitions of the training data. We find that even for n = 250, the average test accuracy of individual teachers is 83.86% for MNIST and 83.18% for SVHN. The larger size of SVHN compensates its increased task complexity.
Prediction confidence: As outlined in Section 3, the privacy of predictions made by an ensemble of teachers intuitively requires that a quorum of teachers generalizing well agree on identical labels. This observation is reflected by our data-dependent privacy analysis, which provides stricter privacy bounds when the quorum is strong. We study the disparity of labels assigned by teachers. In other words, we count the number of votes for each possible label, and measure the difference in votes between the most popular label and the second most popular label, i.e., the gap. If the gap is small, introducing noise during aggregation might change the label assigned from the first to the second. Figure 3 shows the gap normalized by the total number of teachers n. As n increases, the gap remains larger than 60% of the teachers, allowing for aggregation mechanisms to output the correct label in the presence of noise.
Noisy aggregation: For MNIST and SVHN, we consider three ensembles of teachers with varying number of teachers n ∈ {10, 100, 250}. For each of them, we perturb the vote counts with Laplacian noise of inversed scale γ ranging between 0.01 and 1. This choice is justified below in Section 4.2. We report in Figure 2 the accuracy of test set labels inferred by the noisy aggregation mechanism for these values of ε. Notice that the number of teachers needs to be large to compensate for the impact of noise injection on the accuracy.

4.2 SEMI-SUPERVISED TRAINING OF THE STUDENT WITH PRIVACY
The noisy aggregation mechanism labels the student’s unlabeled training set in a privacy-preserving fashion. To reduce the privacy budget spent on student training, we are interested in making as few label queries to the teachers as possible. We therefore use the semi-supervised training approach described previously. Our MNIST and SVHN students with (ε, δ) differential privacy of (2.04, 10−5) and (8.19, 10−6) achieve accuracies of 98.00% and 90.66%. These results improve the differential privacy state-of-the-art for these datasets. Abadi et al. (2016) previously obtained 97% accuracy with a (8, 10−5) bound on MNIST, starting from an inferior baseline model without privacy. Shokri & Shmatikov (2015) reported about 92% accuracy on SVHN with ε > 2 per model parameter and a model with over 300,000 parameters. Naively, this corresponds to a total ε > 600,000.
We apply semi-supervised learning with GANs to our problem using the following setup for each dataset. In the case of MNIST, the student has access to 9,000 samples, among which a subset of either 100, 500, or 1,000 samples are labeled using the noisy aggregation mechanism discussed in Section 2.1. Its performance is evaluated on the 1,000 remaining samples of the test set. Note that this may increase the variance of our test set accuracy measurements, when compared to those computed over the entire test data. For the MNIST dataset, we randomly shuffle the test set to ensure that the different classes are balanced when selecting the (small) subset labeled to train the student. For SVHN, the student has access to 10,000 training inputs, among which it labels 500 or 1,000 samples using the noisy aggregation mechanism. Its performance is evaluated on the remaining 16,032 samples. For both datasets, the ensemble is made up of 250 teachers. We use Laplacian scale of 20 to guarantee an individual query privacy bound of ε = 0.05. These parameter choices are motivated by the results from Section 4.1.
In Figure 4, we report the values of the (ε, δ) differential privacy guarantees provided and the corresponding student accuracy, as well as the number of queries made by each student. The MNIST student is able to learn a 98% accurate model, which is shy of 1% when compared to the accuracy of a model learned with the entire training set, with only 100 label queries. This results in a strict differentially private bound of ε = 2.04 for a failure probability fixed at 10−5. The SVHN student achieves 90.66% accuracy, which is also comparable to the 92.80% accuracy of one teacher learned with the entire training set. The corresponding privacy bound is ε = 8.19, which is higher than for the MNIST dataset, likely because of the larger number of queries made to the aggregation mechanism.
We observe that our private student outperforms the aggregation’s output in terms of accuracy, with or without the injection of Laplacian noise. While this shows the power of semi-supervised learning, the student may not learn as well on different kinds of data (e.g., medical data), where categories are not explicitly designed by humans to be salient in the input space. Encouragingly, as Appendix C illustrates, the PATE approach can be successfully applied to at least some examples of such data.

5 DISCUSSION AND RELATED WORK
Several privacy definitions are found in the literature. For instance, k-anonymity requires information about an individual to be indistinguishable from at least k − 1 other individuals in the dataset (L. Sweeney, 2002). However, its lack of randomization gives rise to caveats (Dwork & Roth, 2014), and attackers can infer properties of the dataset (Aggarwal, 2005). An alternative definition, differential privacy, established itself as a rigorous standard for providing privacy guarantees (Dwork et al., 2006b). In contrast to k-anonymity, differential privacy is a property of the randomized algorithm and not the dataset itself.
A variety of approaches and mechanisms can guarantee differential privacy. Erlingsson et al. (2014) showed that randomized response, introduced by Warner (1965), can protect crowd-sourced data collected from software users to compute statistics about user behaviors. Attempts to provide differential privacy for machine learning models led to a series of efforts on shallow machine learning models, including work by Bassily et al. (2014); Chaudhuri & Monteleoni (2009); Pathak et al. (2011); Song et al. (2013), and Wainwright et al. (2012).
A privacy-preserving distributed SGD algorithm was introduced by Shokri & Shmatikov (2015). It applies to non-convex models. However, its privacy bounds are given per-parameter, and the large number of parameters prevents the technique from providing a meaningful privacy guarantee. Abadi et al. (2016) provided stricter bounds on the privacy loss induced by a noisy SGD by introducing the moments accountant. In comparison with these efforts, our work increases the accuracy of a private MNIST model from 97% to 98% while improving the privacy bound ε from 8 to 1.9. Furthermore, the PATE approach is independent of the learning algorithm, unlike this previous work. Support for a wide range of architecture and training algorithms allows us to obtain good privacy bounds on an accurate and private SVHN model. However, this comes at the cost of assuming that nonprivate unlabeled data is available, an assumption that is not shared by (Abadi et al., 2016; Shokri & Shmatikov, 2015).
Pathak et al. (2010) first discussed secure multi-party aggregation of locally trained classifiers for a global classifier hosted by a trusted third-party. Hamm et al. (2016) proposed the use of knowledge transfer between a collection of models trained on individual devices into a single model guaranteeing differential privacy. Their work studied linear student models with convex and continuously differentiable losses, bounded and c-Lipschitz derivatives, and bounded features. The PATE approach of this paper is not constrained to such applications, but is more generally applicable.
Previous work also studied semi-supervised knowledge transfer from private models. For instance, Jagannathan et al. (2013) learned privacy-preserving random forests. A key difference is that their approach is tailored to decision trees. PATE works well for the specific case of decision trees, as demonstrated in Appendix C, and is also applicable to other machine learning algorithms, including more complex ones. Another key difference is that Jagannathan et al. (2013) modified the classic model of a decision tree to include the Laplacian mechanism. Thus, the privacy guarantee does not come from the disjoint sets of training data analyzed by different decision trees in the random forest, but rather from the modified architecture. In contrast, partitioning is essential to the privacy guarantees of the PATE approach.

6 CONCLUSIONS
To protect the privacy of sensitive training data, this paper has advanced a learning strategy and a corresponding privacy analysis. The PATE approach is based on knowledge aggregation and transfer from “teacher” models, trained on disjoint data, to a “student” model whose attributes may be made public. In combination, the paper’s techniques demonstrably achieve excellent utility on the MNIST and SVHN benchmark tasks, while simultaneously providing a formal, state-of-the-art bound on users’ privacy loss. While our results are not without limits—e.g., they require disjoint training data for a large number of teachers (whose number is likely to increase for tasks with many output classes)—they are encouraging, and highlight the advantages of combining semi-supervised learning with precise, data-dependent privacy analysis, which will hopefully trigger further work. In particular, such future work may further investigate whether or not our semi-supervised approach will also reduce teacher queries for tasks other than MNIST and SVHN, for example when the discrete output categories are not as distinctly defined by the salient input space features.
A key advantage is that this paper’s techniques establish a precise guarantee of training data privacy in a manner that is both intuitive and rigorous. Therefore, they can be appealing, and easily explained, to both an expert and non-expert audience. However, perhaps equally compelling are the techniques’ wide applicability. Both our learning approach and our analysis methods are “blackbox,” i.e., independent of the learning algorithm for either teachers or students, and therefore apply, in general, to non-convex, deep learning, and other learning methods. Also, because our techniques do not constrain the selection or partitioning of training data, they apply when training data is naturally and non-randomly partitioned—e.g., because of privacy, regulatory, or competitive concerns— or when each teacher is trained in isolation, with a different method. We look forward to such further applications, for example on RNNs and other sequence-based models.

ACKNOWLEDGMENTS
Nicolas Papernot is supported by a Google PhD Fellowship in Security. The authors would like to thank Ilya Mironov and Li Zhang for insightful discussions about early drafts of this document.

A MISSING DETAILS ON THE ANALYSIS
We provide missing proofs from Section 3. Theorem 3. LetM be (2γ, 0)-differentially private and q ≥ Pr[M(d) 6= o∗] for some outcome o∗. Let l, γ ≥ 0 and q < e
2γ−1 e4γ−1 . Then for any aux and any neighbor d ′ of d,M satisfies
α(l;aux, d, d′) ≤ log((1− q) ( 1− q 1− e2γq )l + q exp(2γl)).
Proof. Since M is 2γ-differentially private, for every outcome o, Pr[M(d)=o]Pr[M(d′)=o] ≤ exp(2γ). Let q′ = Pr[M(d) 6= o∗]. Then Pr[M(d′) 6= o∗] ≤ exp(2γ)q′. Thus exp(α(l;aux, d, d′)) = ∑ o Pr[M(d) = o] ( Pr[M(d) = o] Pr[M(d′) = o]
)l = Pr[M(d) = o∗] ( Pr[M(d) = o∗] Pr[M(d′) = o∗] )l + ∑ o6=o∗ Pr[M(d) = o] ( Pr[M(d) = o] Pr[M(d′) = o]
)l ≤ (1− q′) ( 1− q′ 1− e2γq′ )l + ∑ o6=o∗ Pr[M(d) = o](e2γ)l
≤ (1− q′) ( 1− q′ 1− e2γq′ )l + q′e2γl.
Now consider the function
f(z) = (1− z) ( 1− z 1− e2γz )l + ze2γl.
We next argue that this function is non-decreasing in (0, e 2γ−1 e4γ−1 ) under the conditions of the lemma. Towards this goal, define
g(z, w) = (1− z) ( 1− w 1− e2γw )l + ze2γl,
and observe that f(z) = g(z, z). We can easily verify by differentiation that g(z, w) is increasing individually in z and in w in the range of interest. This implies that f(q′) ≤ f(q) completing the proof.
Lemma 4. Let n be the label score vector for a database d with nj∗ ≥ nj for all j. Then
Pr[M(d) 6= j∗] ≤ ∑ j 6=j∗ 2 + γ(nj∗ − nj) 4 exp(γ(nj∗ − nj))
Proof. The probability that nj∗ + Lap( 1γ ) < nj + Lap( 1 γ ) is equal to the probability that the sum of two independent Lap(1) random variables exceeds γ(nj∗ − nj). The sum of two independent Lap(1) variables has the same distribution as the difference of twoGamma(2, 1) random variables. Recalling that theGamma(2, 1) distribution has pdf xe−x, we can compute the pdf of the difference via convolution as∫ ∞
y=0
(y + |x|)e−y−|x|ye−y dy = 1 e|x| ∫ ∞ y=0 (y2 + y|x|)e−2y dy = 1 + |x| 4e|x| .
The probability mass in the tail can then be computed by integration as 2+γ(nj∗−nj)4 exp(γ(nj∗−nj) . Taking a union bound over the various candidate j’s gives the claimed bound.

B APPENDIX: TRAINING THE STUDENT WITH MINIMAL TEACHER QUERIES
In this appendix, we describe approaches that were considered to reduce the number of queries made to the teacher ensemble by the student during its training. As pointed out in Sections 3 and 4, this effort is motivated by the direct impact of querying on the total privacy cost associated with student training. The first approach is based on distillation, a technique used for knowledge transfer and model compression (Hinton et al., 2015). The three other techniques considered were proposed in the context of active learning, with the intent of identifying training examples most useful for learning. In Sections 2 and 4, we described semi-supervised learning, which yielded the best results. The student models in this appendix differ from those in Sections 2 and 4, which were trained using GANs. In contrast, all students in this appendix were learned in a fully supervised fashion from a subset of public, labeled examples. Thus, the learning goal was to identify the subset of labels yielding the best learning performance.
B.1 TRAINING STUDENTS USING DISTILLATION
Distillation is a knowledge transfer technique introduced as a means of compressing large models into smaller ones, while retaining their accuracy (Bucilua et al., 2006; Hinton et al., 2015). This is for instance useful to train models in data centers before deploying compressed variants in phones. The transfer is accomplished by training the smaller model on data that is labeled with probability vectors produced by the first model, which encode the knowledge extracted from training data. Distillation is parameterized by a temperature parameter T , which controls the smoothness of probabilities output by the larger model: when produced at small temperatures, the vectors are discrete, whereas at high temperature, all classes are assigned non-negligible values. Distillation is a natural candidate to compress the knowledge acquired by the ensemble of teachers, acting as the large model, into a student, which is much smaller with n times less trainable parameters compared to the n teachers.
To evaluate the applicability of distillation, we consider the ensemble of n = 50 teachers for SVHN. In this experiment, we do not add noise to the vote counts when aggregating the teacher predictions. We compare the accuracy of three student models: the first is a baseline trained with labels obtained by plurality, the second and third are trained with distillation at T ∈ {1, 5}. We use the first 10,000 samples from the test set as unlabeled data. Figure 5 reports the accuracy of the student model on the last 16,032 samples from the test set, which were not accessible to the model during training. It is plotted with respect to the number of samples used to train the student (and hence the number of queries made to the teacher ensemble). Although applying distillation yields classifiers that perform more accurately, the increase in accuracy is too limited to justify the increased privacy cost of revealing the entire probability vector output by the ensemble instead of simply the class assigned the largest number of votes. Thus, we turn to an investigation of active learning.
B.2 ACTIVE LEARNING OF THE STUDENT
Active learning is a class of techniques that aims to identify and prioritize points in the student’s training set that have a high potential to contribute to learning (Angluin, 1988; Baum, 1991). If the label of an input in the student’s training set can be predicted confidently from what we have learned so far by querying the teachers, it is intuitive that querying it is not worth the privacy budget spent. In our experiments, we made several attempts before converging to a simpler final formulation.
Siamese networks: Our first attempt was to train a pair of siamese networks, introduced by Bromley et al. (1993) in the context of one-shot learning and later improved by Koch (2015). The siamese networks take two images as input and return 1 if the images are equal and 0 otherwise. They are two identical networks trained with shared parameters to force them to produce similar representations of the inputs, which are then compared using a distance metric to determine if the images are identical or not. Once the siamese models are trained, we feed them a pair of images where the first is unlabeled and the second labeled. If the unlabeled image is confidently matched with a known labeled image, we can infer the class of the unknown image from the labeled image. In our experiments, the siamese networks were able to say whether two images are identical or not, but did not generalize well: two images of the same class did not receive sufficiently confident matches. We also tried a variant of this approach where we trained the siamese networks to output 1 when the two
images are of the same class and 0 otherwise, but the learning task proved too complicated to be an effective means for reducing the number of queries made to teachers.
Collection of binary experts: Our second attempt was to train a collection of binary experts, one per class. An expert for class j is trained to output 1 if the sample is in class j and 0 otherwise. We first trained the binary experts by making an initial batch of queries to the teachers. Using the experts, we then selected available unlabeled student training points that had a candidate label score below 0.9 and at least 4 other experts assigning a score above 0.1. This gave us about 500 unconfident points for 1700 initial label queries. After labeling these unconfident points using the ensemble of teachers, we trained the student. Using binary experts improved the student’s accuracy when compared to the student trained on arbitrary data with the same number of teacher queries. The absolute increases in accuracy were however too limited—between 1.5% and 2.5%.
Identifying unconfident points using the student: This last attempt was the simplest yet the most effective. Instead of using binary experts to identify student training points that should be labeled by the teachers, we used the student itself. We asked the student to make predictions on each unlabeled training point available. We then sorted these samples by increasing values of the maximum probability assigned to a class for each sample. We queried the teachers to label these unconfident inputs first and trained the student again on this larger labeled training set. This improved the accuracy of the student when compared to the student trained on arbitrary data. For the same number of teacher queries, the absolute increases in accuracy of the student trained on unconfident inputs first when compared to the student trained on arbitrary data were in the order of 4%− 10%.

C APPENDIX: ADDITIONAL EXPERIMENTS ON THE UCI ADULT AND DIABETES DATASETS
In order to further demonstrate the general applicability of our approach, we performed experiments on two additional datasets. While our experiments on MNIST and SVHN in Section 4 used convolutional neural networks and GANs, here we use random forests to train our teacher and student models for both of the datasets. Our new results on these datasets show that, despite the differing data types and architectures, we are able to provide meaningful privacy guarantees.
UCI Adult dataset: The UCI Adult dataset is made up of census data, and the task is to predict when individuals make over $50k per year. Each input consists of 13 features (which include the age, workplace, education, occupation—see the UCI website for a full list3). The only pre-processing we apply to these features is to map all categorical features to numerical values by assigning an integer value to each possible category. The model is a random forest provided by the scikit-learn Python package. When training both our teachers and student, we keep all the default parameter values, except for the number of estimators, which we set to 100. The data is split between a training set of 32,562 examples, and a test set of 16,282 inputs.
UCI Diabetes dataset: The UCI Diabetes dataset includes de-identified records of diabetic patients and corresponding hospital outcomes, which we use to predict whether diabetic patients were readmitted less than 30 days after their hospital release. To the best of our knowledge, no particular classification task is considered to be a standard benchmark for this dataset. Even so, it is valuable to consider whether our approach is applicable to the likely classification tasks, such as readmission, since this dataset is collected in a medical environment—a setting where privacy concerns arise frequently. We select a subset of 18 input features from the 55 available in the dataset (to avoid features with missing values) and form a dataset balanced between the two output classes (see the UCI website for more details4). In class 0, we include all patients that were readmitted in a 30-day window, while class 1 includes all patients that were readmitted after 30 days or never readmitted at all. Our balanced dataset contains 34,104 training samples and 12,702 evaluation samples. We use a random forest model identical to the one described above in the presentation of the Adult dataset.
Experimental results: We apply our approach described in Section 2. For both datasets, we train ensembles of n = 250 random forests on partitions of the training data. We then use the noisy aggregation mechanism, where vote counts are perturbed with Laplacian noise of scale 0.05 to privately label the first 500 test set inputs. We train the student random forest on these 500 test set inputs and evaluate it on the last 11,282 test set inputs for the Adult dataset, and 6,352 test set inputs for the Diabetes dataset. These numbers deliberately leave out some of the test set, which allowed us to observe how the student performance-privacy trade-off was impacted by varying the number of private labels, as well as the Laplacian scale used when computing these labels.
For the Adult dataset, we find that our student model achieves an 83% accuracy for an (ε, δ) = (2.66, 10−5) differential privacy bound. Our non-private model on the dataset achieves 85% accuracy, which is comparable to the state-of-the-art accuracy of 86% on this dataset (Poulos & Valle, 2016). For the Diabetes dataset, we find that our privacy-preserving student model achieves a 93.94% accuracy for a (ε, δ) = (1.44, 10−5) differential privacy bound. Our non-private model on the dataset achieves 93.81% accuracy.
3 https://archive.ics.uci.edu/ml/datasets/Adult 4 https://archive.ics.uci.edu/ml/datasets/Diabetes+130-US+hospitals+for+years+1999-2008
","Some machine learning applications involve training data that is sensitive, such as the medical histories of patients in a clinical trial. A model may inadvertently and implicitly store some of its training data; careful analysis of the model may therefore reveal sensitive information. To address this problem, we demonstrate a generally applicable approach to providing strong privacy guarantees for training data: Private Aggregation of Teacher Ensembles (PATE). The approach combines, in a black-box fashion, multiple models trained with disjoint datasets, such as records from different subsets of users. Because they rely directly on sensitive data, these models are not published, but instead used as “teachers” for a “student” model. The student learns to predict an output chosen by noisy voting among all of the teachers, and cannot directly access an individual teacher or the underlying data or parameters. The student’s privacy properties can be understood both intuitively (since no single teacher and thus no single dataset dictates the student’s training) and formally, in terms of differential privacy. These properties hold even if an adversary can not only query the student but also inspect its internal workings. Compared with previous work, the approach imposes only weak assumptions on how teachers are trained: it applies to any model, including non-convex models like DNNs. We achieve state-of-the-art privacy/utility trade-offs on MNIST and SVHN thanks to an improved privacy analysis and semi-supervised learning.",ICLR 2017 conference submission,True,,"Altogether a very good paper, a nice read, and interesting. The work advances the state of the art on differentially-private deep learning, is quite well-written, and relatively thorough.

One caveat is that although the approach is intended to be general, no theoretical guarantees are provided about the learning performance. Privacy-preserving machine learning papers often analyze both the privacy (in the worst case, DP setting) and the learning performance (often under different assumptions). Since the learning performance might depend on the choice of architecture; future experimentation is encouraged, even using the same data sets, with different architectures. If this will not be added, then please justify the choice of architecture used, and/or clarify what can be generalized about the observed learning performance.

Another caveat is that the reported epsilons are not those that can be privately released; the authors note that their technique for doing so would change the resulting epsilon. However this would need to be resolved in order to have a meaningful comparison to the epsilon-delta values reported in related work.

Finally, as has been acknowledged in the paper, the present approach may not work on other natural data types. Experiments on other data sets is strongly encouraged. Also, please cite the data sets used.

Other comments:

Discussion of certain parts of the related work are thorough. However, please add some survey/discussion of the related work on differentially-private semi-supervised learning. For example, in the context of random forests, the following paper also proposed differentially-private semi-supervised learning via a teacher-learner approach (although not denoted as “teacher-learner”). The only time the private labeled data is used is when learning the “primary ensemble.”  A ""secondary ensemble"" is then learned only from the unlabeled (non-private) data, with pseudo-labels generated by the primary ensemble.

G. Jagannathan, C. Monteleoni, and K. Pillaipakkamnatt: A Semi-Supervised Learning Approach to Differential Privacy. Proc. 2013 IEEE International Conference on Data Mining Workshops, IEEE Workshop on Privacy Aspects of Data Mining (PADM), 2013.

Section C. does a nice comparison of approaches. Please make sure the quantitative results here constitute an apples-to-apples comparison with the GAN results. 

The paper is extremely well-written, for the most part. Some places needing clarification include:
- Last paragraph of 3.1. “all teachers….get the same training data….” This should be rephrased to make it clear that it is not the same w.r.t. all the teachers, but w.r.t. the same teacher on the neighboring database.
- 4.1: The authors state: “The number n of teachers is limited by a trade-off between the classification task’s complexity and the available data.” However, since this tradeoff is not formalized, the statement is imprecise. In particular, if the analysis is done in the i.i.d. setting, the tradeoff would also likely depend on the relation of the target hypothesis to the data distribution.
- Discussion of figure 3 was rather unclear in the text and caption and should be revised for clarity. In the text section, at first the explanation seems to imply that a larger gap is better (as is also indicated in the caption). However later it is stated that the gap stays under 20%. These sentences seem contradictory, which is likely not what was intended.

---

The paper presents a general teacher-student approach for differentially-private learning in which the student learns to predict a noise vote among a set of teachers. The noise allows the student to be differentially private, whilst maintaining good classification accuracies on MNIST and SVHN. The paper is well-written.

---

I think this paper has great impact.

My question is what is the ""auxiliary input"" in Definition 2.

Could you explain this term in theoretical view and what is that in your paper?

---

Thank you for providing an interesting paper.

In the paper, the student model is trained by semi-supervised fashion as suggested in (Salimans et al., 2016).

As far as I understand, teacher's ensembles are using for supervised learning and nonsensitive data is for unsupervised learning.

So, my question is ""Where is the generator ?"".

The aggregation of teacher network is treated as the generator in GAN framework?

---

Hi,

I have few questions about the paper.

1- What attacker's goal did you consider in your paper? Is it recovering the training data, or checking whether a specific sample has been in the training data? 

2- If the attacker's goal is to recover the training data, does the attacker want to recover the exact data or an approximation would be OK?

3- Talking about neural networks:
- Do you think there is any attack method to recover an exact training data from the learning model?
- Do you think there is any defense method to prevent an attacker from recovering even an approximate training data?

4- How can we quantify the strength of a learning model (specifically neural networks) without any defensive mechanism?

5- How can we quantify the strength of a learning model which has not been trained on exact training data? For example, some forms of adversarial training methods never train the model on the clean data; instead, at each epoch, the model is trained on different adversarial data derived from the real data. 
- How can the model ""memorize"" the training data, when 1) it has never seen the real data, 2) it has been trained on different data in different epochs?

6- How do you compare the performance of your method with adversarial training?

Thanks.

---

This paper addresses the problem of achieving differential privacy in a very general scenario where a set of teachers is trained on disjoint subsets of sensitive data and the student performs prediction based on public data labeled by teachers through noisy voting. I found the approach altogether plausible and very clearly explained by the authors. Adding more discussion of the bound (and its tightness) from Theorem 1 itself would be appreciated. A simple idea of adding perturbation error to the counts, known from differentially-private literature, is nicely re-used by the authors and elegantly applied in a much broader (non-convex setting) and practical context than in a number of differentially-private and other related papers. The generality of the approach, clear improvement over predecessors, and clarity of the writing makes the method worth publishing.

---

This paper discusses how to guarantee privacy for training data. In the proposed approach multiple models trained with disjoint datasets are used as ``teachers'' model, which will train a ``student'' model to predict an output chosen by noisy voting among all of the teachers. 

The theoretical results are nice but also intuitive. Since teachers' result are provided via noisy voting, the student model may not duplicate the teacher's behavior. However, the probabilistic bound has quite a number of  empirical parameters, which makes me difficult to decide whether the security is 100% guaranteed or not.

The experiments on MNIST and SVHN are good. However, as the paper claims, the proposed approach may be mostly useful for sensitive data like medical histories, it will be nice to conduct one or two experiments on such applications.

---

Altogether a very good paper, a nice read, and interesting. The work advances the state of the art on differentially-private deep learning, is quite well-written, and relatively thorough.

One caveat is that although the approach is intended to be general, no theoretical guarantees are provided about the learning performance. Privacy-preserving machine learning papers often analyze both the privacy (in the worst case, DP setting) and the learning performance (often under different assumptions). Since the learning performance might depend on the choice of architecture; future experimentation is encouraged, even using the same data sets, with different architectures. If this will not be added, then please justify the choice of architecture used, and/or clarify what can be generalized about the observed learning performance.

Another caveat is that the reported epsilons are not those that can be privately released; the authors note that their technique for doing so would change the resulting epsilon. However this would need to be resolved in order to have a meaningful comparison to the epsilon-delta values reported in related work.

Finally, as has been acknowledged in the paper, the present approach may not work on other natural data types. Experiments on other data sets is strongly encouraged. Also, please cite the data sets used.

Other comments:

Discussion of certain parts of the related work are thorough. However, please add some survey/discussion of the related work on differentially-private semi-supervised learning. For example, in the context of random forests, the following paper also proposed differentially-private semi-supervised learning via a teacher-learner approach (although not denoted as “teacher-learner”). The only time the private labeled data is used is when learning the “primary ensemble.”  A ""secondary ensemble"" is then learned only from the unlabeled (non-private) data, with pseudo-labels generated by the primary ensemble.

G. Jagannathan, C. Monteleoni, and K. Pillaipakkamnatt: A Semi-Supervised Learning Approach to Differential Privacy. Proc. 2013 IEEE International Conference on Data Mining Workshops, IEEE Workshop on Privacy Aspects of Data Mining (PADM), 2013.

Section C. does a nice comparison of approaches. Please make sure the quantitative results here constitute an apples-to-apples comparison with the GAN results. 

The paper is extremely well-written, for the most part. Some places needing clarification include:
- Last paragraph of 3.1. “all teachers….get the same training data….” This should be rephrased to make it clear that it is not the same w.r.t. all the teachers, but w.r.t. the same teacher on the neighboring database.
- 4.1: The authors state: “The number n of teachers is limited by a trade-off between the classification task’s complexity and the available data.” However, since this tradeoff is not formalized, the statement is imprecise. In particular, if the analysis is done in the i.i.d. setting, the tradeoff would also likely depend on the relation of the target hypothesis to the data distribution.
- Discussion of figure 3 was rather unclear in the text and caption and should be revised for clarity. In the text section, at first the explanation seems to imply that a larger gap is better (as is also indicated in the caption). However later it is stated that the gap stays under 20%. These sentences seem contradictory, which is likely not what was intended.

---

Thanks for interesting and well-organized papers. I have a question about teacher-student model. 

Teachers are trained on sensitive data, and students are trained on non-sensitive data.
I wonder how students work on the outputs of teachers.
Sensitive and non-sensitive are different attributes, so I think there are no correlation between teachers and students. 

Please give me some more details. Thanks.

---

Altogether a very good paper, a nice read, and interesting. The work advances the state of the art on differentially-private deep learning, is quite well-written, and relatively thorough.

One caveat is that although the approach is intended to be general, no theoretical guarantees are provided about the learning performance. Privacy-preserving machine learning papers often analyze both the privacy (in the worst case, DP setting) and the learning performance (often under different assumptions). Since the learning performance might depend on the choice of architecture; future experimentation is encouraged, even using the same data sets, with different architectures. If this will not be added, then please justify the choice of architecture used, and/or clarify what can be generalized about the observed learning performance.

Another caveat is that the reported epsilons are not those that can be privately released; the authors note that their technique for doing so would change the resulting epsilon. However this would need to be resolved in order to have a meaningful comparison to the epsilon-delta values reported in related work.

Finally, as has been acknowledged in the paper, the present approach may not work on other natural data types. Experiments on other data sets is strongly encouraged. Also, please cite the data sets used.

Other comments:

Discussion of certain parts of the related work are thorough. However, please add some survey/discussion of the related work on differentially-private semi-supervised learning. For example, in the context of random forests, the following paper also proposed differentially-private semi-supervised learning via a teacher-learner approach (although not denoted as “teacher-learner”). The only time the private labeled data is used is when learning the “primary ensemble.”  A ""secondary ensemble"" is then learned only from the unlabeled (non-private) data, with pseudo-labels generated by the primary ensemble.

G. Jagannathan, C. Monteleoni, and K. Pillaipakkamnatt: A Semi-Supervised Learning Approach to Differential Privacy. Proc. 2013 IEEE International Conference on Data Mining Workshops, IEEE Workshop on Privacy Aspects of Data Mining (PADM), 2013.

Section C. does a nice comparison of approaches. Please make sure the quantitative results here constitute an apples-to-apples comparison with the GAN results. 

The paper is extremely well-written, for the most part. Some places needing clarification include:
- Last paragraph of 3.1. “all teachers….get the same training data….” This should be rephrased to make it clear that it is not the same w.r.t. all the teachers, but w.r.t. the same teacher on the neighboring database.
- 4.1: The authors state: “The number n of teachers is limited by a trade-off between the classification task’s complexity and the available data.” However, since this tradeoff is not formalized, the statement is imprecise. In particular, if the analysis is done in the i.i.d. setting, the tradeoff would also likely depend on the relation of the target hypothesis to the data distribution.
- Discussion of figure 3 was rather unclear in the text and caption and should be revised for clarity. In the text section, at first the explanation seems to imply that a larger gap is better (as is also indicated in the caption). However later it is stated that the gap stays under 20%. These sentences seem contradictory, which is likely not what was intended.

---

The paper presents a general teacher-student approach for differentially-private learning in which the student learns to predict a noise vote among a set of teachers. The noise allows the student to be differentially private, whilst maintaining good classification accuracies on MNIST and SVHN. The paper is well-written.

---

I think this paper has great impact.

My question is what is the ""auxiliary input"" in Definition 2.

Could you explain this term in theoretical view and what is that in your paper?

---

Thank you for providing an interesting paper.

In the paper, the student model is trained by semi-supervised fashion as suggested in (Salimans et al., 2016).

As far as I understand, teacher's ensembles are using for supervised learning and nonsensitive data is for unsupervised learning.

So, my question is ""Where is the generator ?"".

The aggregation of teacher network is treated as the generator in GAN framework?

---

Hi,

I have few questions about the paper.

1- What attacker's goal did you consider in your paper? Is it recovering the training data, or checking whether a specific sample has been in the training data? 

2- If the attacker's goal is to recover the training data, does the attacker want to recover the exact data or an approximation would be OK?

3- Talking about neural networks:
- Do you think there is any attack method to recover an exact training data from the learning model?
- Do you think there is any defense method to prevent an attacker from recovering even an approximate training data?

4- How can we quantify the strength of a learning model (specifically neural networks) without any defensive mechanism?

5- How can we quantify the strength of a learning model which has not been trained on exact training data? For example, some forms of adversarial training methods never train the model on the clean data; instead, at each epoch, the model is trained on different adversarial data derived from the real data. 
- How can the model ""memorize"" the training data, when 1) it has never seen the real data, 2) it has been trained on different data in different epochs?

6- How do you compare the performance of your method with adversarial training?

Thanks.

---

This paper addresses the problem of achieving differential privacy in a very general scenario where a set of teachers is trained on disjoint subsets of sensitive data and the student performs prediction based on public data labeled by teachers through noisy voting. I found the approach altogether plausible and very clearly explained by the authors. Adding more discussion of the bound (and its tightness) from Theorem 1 itself would be appreciated. A simple idea of adding perturbation error to the counts, known from differentially-private literature, is nicely re-used by the authors and elegantly applied in a much broader (non-convex setting) and practical context than in a number of differentially-private and other related papers. The generality of the approach, clear improvement over predecessors, and clarity of the writing makes the method worth publishing.

---

This paper discusses how to guarantee privacy for training data. In the proposed approach multiple models trained with disjoint datasets are used as ``teachers'' model, which will train a ``student'' model to predict an output chosen by noisy voting among all of the teachers. 

The theoretical results are nice but also intuitive. Since teachers' result are provided via noisy voting, the student model may not duplicate the teacher's behavior. However, the probabilistic bound has quite a number of  empirical parameters, which makes me difficult to decide whether the security is 100% guaranteed or not.

The experiments on MNIST and SVHN are good. However, as the paper claims, the proposed approach may be mostly useful for sensitive data like medical histories, it will be nice to conduct one or two experiments on such applications.

---

Altogether a very good paper, a nice read, and interesting. The work advances the state of the art on differentially-private deep learning, is quite well-written, and relatively thorough.

One caveat is that although the approach is intended to be general, no theoretical guarantees are provided about the learning performance. Privacy-preserving machine learning papers often analyze both the privacy (in the worst case, DP setting) and the learning performance (often under different assumptions). Since the learning performance might depend on the choice of architecture; future experimentation is encouraged, even using the same data sets, with different architectures. If this will not be added, then please justify the choice of architecture used, and/or clarify what can be generalized about the observed learning performance.

Another caveat is that the reported epsilons are not those that can be privately released; the authors note that their technique for doing so would change the resulting epsilon. However this would need to be resolved in order to have a meaningful comparison to the epsilon-delta values reported in related work.

Finally, as has been acknowledged in the paper, the present approach may not work on other natural data types. Experiments on other data sets is strongly encouraged. Also, please cite the data sets used.

Other comments:

Discussion of certain parts of the related work are thorough. However, please add some survey/discussion of the related work on differentially-private semi-supervised learning. For example, in the context of random forests, the following paper also proposed differentially-private semi-supervised learning via a teacher-learner approach (although not denoted as “teacher-learner”). The only time the private labeled data is used is when learning the “primary ensemble.”  A ""secondary ensemble"" is then learned only from the unlabeled (non-private) data, with pseudo-labels generated by the primary ensemble.

G. Jagannathan, C. Monteleoni, and K. Pillaipakkamnatt: A Semi-Supervised Learning Approach to Differential Privacy. Proc. 2013 IEEE International Conference on Data Mining Workshops, IEEE Workshop on Privacy Aspects of Data Mining (PADM), 2013.

Section C. does a nice comparison of approaches. Please make sure the quantitative results here constitute an apples-to-apples comparison with the GAN results. 

The paper is extremely well-written, for the most part. Some places needing clarification include:
- Last paragraph of 3.1. “all teachers….get the same training data….” This should be rephrased to make it clear that it is not the same w.r.t. all the teachers, but w.r.t. the same teacher on the neighboring database.
- 4.1: The authors state: “The number n of teachers is limited by a trade-off between the classification task’s complexity and the available data.” However, since this tradeoff is not formalized, the statement is imprecise. In particular, if the analysis is done in the i.i.d. setting, the tradeoff would also likely depend on the relation of the target hypothesis to the data distribution.
- Discussion of figure 3 was rather unclear in the text and caption and should be revised for clarity. In the text section, at first the explanation seems to imply that a larger gap is better (as is also indicated in the caption). However later it is stated that the gap stays under 20%. These sentences seem contradictory, which is likely not what was intended.

---

Thanks for interesting and well-organized papers. I have a question about teacher-student model. 

Teachers are trained on sensitive data, and students are trained on non-sensitive data.
I wonder how students work on the outputs of teachers.
Sensitive and non-sensitive are different attributes, so I think there are no correlation between teachers and students. 

Please give me some more details. Thanks.",5.0,5.0,4.5,,5.0,8.333333333333334,5.0,,3.6666666666666665,,
325,"Authors: INFUSION TRAINING, Florian Bordes, Sina Honari, Pascal Vincent
Source file: 325.pdf

ABSTRACT
In this work, we investigate a novel training procedure to learn a generative model as the transition operator of a Markov chain, such that, when applied repeatedly on an unstructured random noise sample, it will denoise it into a sample that matches the target distribution from the training set. The novel training procedure to learn this progressive denoising operation involves sampling from a slightly different chain than the model chain used for generation in the absence of a denoising target. In the training chain we infuse information from the training target example that we would like the chains to reach with a high probability. The thus learned transition operator is able to produce quality and varied samples in a small number of steps. Experiments show competitive results compared to the samples generated with a basic Generative Adversarial Net.

1 INTRODUCTION AND MOTIVATION
To go beyond the relatively simpler tasks of classification and regression, advancing our ability to learn good generative models of high-dimensional data appears essential. There are many scenarios where one needs to efficiently produce good high-dimensional outputs where output dimensions have unknown intricate statistical dependencies: from generating realistic images, segmentations, text, speech, keypoint or joint positions, etc..., possibly as an answer to the same, other, or multiple input modalities. These are typically cases where there is not just one right answer but a variety of equally valid ones following a non-trivial and unknown distribution. A fundamental ingredient for such scenarios is thus the ability to learn a good generative model from data, one from which we can subsequently efficiently generate varied samples of high quality.
Many approaches for learning to generate high dimensional samples have been and are still actively being investigated. These approaches can be roughly classified under the following broad categories:
• Ordered visible dimension sampling (van den Oord et al., 2016; Larochelle & Murray, 2011). In this type of auto-regressive approach, output dimensions (or groups of conditionally independent dimensions) are given an arbitrary fixed ordering, and each is sampled conditionally on the previous sampled ones. This strategy is often implemented using a recurrent network (LSTM or GRU). Desirable properties of this type of strategy are that the exact log likelihood can usually be computed tractably, and sampling is exact. Undesirable properties follow from the forced ordering, whose arbitrariness feels unsatisfactory especially for domains that do not have a natural ordering (e.g. images), and imposes for high-dimensional output a long sequential generation that can be slow. • Undirected graphical models with multiple layers of latent variables. These make inference, and thus learning, particularly hard and tend to be costly to sample from (Salakhutdinov & Hinton, 2009). • Directed graphical models trained as variational autoencoders (VAE) (Kingma & Welling, 2014; Rezende et al., 2014)
∗Associate Fellow, Canadian Institute For Advanced Research (CIFAR)
• Adversarially-trained generative networks. (GAN)(Goodfellow et al., 2014) • Stochastic neural networks, i.e. networks with stochastic neurons, trained by an adapted
form of stochastic backpropagation • Generative uses of denoising autoencoders (Vincent et al., 2010) and their generalization
as Generative Stochastic Networks (Alain et al., 2016) • Inverting a non-equilibrium thermodynamic slow diffusion process (Sohl-Dickstein et al.,
2015) • Continuous transformation of a distribution by invertible functions (Dinh et al. (2014), also
used for variational inference in Rezende & Mohamed (2015))
Several of these approaches are based on maximizing an explicit or implicit model log-likelihood or a lower bound of its log-likelihood, but some successful ones are not e.g. GANs. The approach we propose here is based on the notion of “denoising” and thus takes its root in denoising autoencoders and the GSN type of approaches. It is also highly related to the non-equilibrium thermodynamics inverse diffusion approach of Sohl-Dickstein et al. (2015). One key aspect that distinguishes these types of methods from others listed above is that sample generation is achieved thanks to a learned stochastic mapping from input space to input space, rather than from a latent-space to input-space.
Specifically, in the present work, we propose to learn to generate high quality samples through a process of progressive, stochastic, denoising, starting from a simple initial “noise” sample generated in input space from a simple factorial distribution i.e. one that does not take into account any dependency or structure between dimensions. This, in effect, amounts to learning the transition operator of a Markov chain operating on input space. Starting from such an initial “noise” input, and repeatedly applying the operator for a small fixed number T of steps, we aim to obtain a high quality resulting sample, effectively modeling the training data distribution. Our training procedure uses a novel “target-infusion” technique, designed to slightly bias model sampling to move towards a specific data point during training, and thus provide inputs to denoise which are likely under the model’s sample generation paths. By contrast with Sohl-Dickstein et al. (2015) which consists in inverting a slow and fixed diffusion process, our infusion chains make a few large jumps and follow the model distribution as the learning progresses.
The rest of this paper is structured as follows: Section 2 formally defines the model and training procedure. Section 3 discusses and contrasts our approach with the most related methods from the literature. Section 4 presents experiments that validate the approach. Section 5 concludes and proposes future work directions.

2 PROPOSED APPROACH

2.1 SETUP
We are given a finite data set D containing n points in Rd, supposed drawn i.i.d from an unknown distribution q∗. The data set D is supposed split into training, validation and test subsets Dtrain, Dvalid, Dtest. We will denote q∗train the empirical distribution associated to the training set, and use x to denote observed samples from the data set. We are interested in learning the parameters of a generative model p conceived as a Markov Chain from which we can efficiently sample. Note that we are interested in learning an operator that will display fast “burn-in” from the initial factorial “noise” distribution, but beyond the initial T steps we are not concerned about potential slow mixing or being stuck. We will first describe the sampling procedure used to sample from a trained model, before explaining our training procedure.

2.2 GENERATIVE MODEL SAMPLING PROCEDURE
The generative model p is defined as the following sampling procedure:
• Using a simple factorial distribution p(0)(z(0)), draw an initial sample z(0) ∼ p(0), where z(0) ∈ Rd. Since p(0) is factorial, the d components of z(0) are independent: p0 cannot model any dependency structure. z(0) can be pictured as essentially unstructured random noise. • Repeatedly apply T times a stochastic transition operator p(t)(z(t)|z(t−1)), yielding a more “denoised” sample z(t) ∼ p(t)(z(t)|z(t−1)), where all z(t) ∈ Rd.
• Output z(T ) as the final generated sample. Our generative model distribution is thus p(z(T )), the marginal associated to joint p(z(0), . . . , z(T )) = p(0)(z(0)) (∏T t=1 p (t)(z(t)|z(t−1)) ) .
In summary, samples from model p are generated, starting with an initial sample from a simple distribution p(0), by taking the T thsample along Markov chain z(0) → z(1) → z(2) → . . . → z(T ) whose transition operator is p(t)(z(t)|z(t−1)). We will call this chain the model sampling chain. Figure 1 illustrates this sampling procedure using a model (i.e. transition operator) that was trained on MNIST. Note that we impose no formal requirement that the chain converges to a stationary distribution, as we simply read-out z(T ) as the samples from our model p. The chain also needs not be time-homogeneous, as highlighted by notation p(t) for the transitions.
The set of parameters θ of model p comprise the parameters of p(0) and the parameters of transition operator p(t)(z(t)|z(t−1)). For tractability, learnability, and efficient sampling, these distributions will be chosen factorial, i.e. p(0)(z(0)) = ∏d i=1 p (0) i (z (0) i ) and p
(t)(z(t)|z(t−1)) =∏d i=1 p (t) i (z (t) i |z(t−1)). Note that the conditional distribution of an individual component i, p (t) i (z (t) i |z(t−1)) may however be multimodal, e.g. a mixture in which case p(t)(z(t)|z(t−1)) would be a product of independent mixtures (conditioned on z(t−1)), one per dimension. In our experiments, we will take the p(t)(z(t)|z(t−1)) to be simple diagonal Gaussian yielding a Deep Latent Gaussian Model (DLGM) as in Rezende et al. (2014).

2.3 INFUSION TRAINING PROCEDURE
We want to train the parameters of model p such that samples from Dtrain are likely of being generated under the model sampling chain. Let θ(0) be the parameters of p(0) and let θ(t) be the parameters of p(t)(z(t)|z(t−1)). Note that parameters θ(t) for t > 0 can straightforwardly be shared across time steps, which we will be doing in practice. Having committed to using (conditionally) factorial distributions for our p(0)(z(0)) and p(t)(z(t)|z(t−1)), that are both easy to learn and cheap to sample from, let us first consider the following greedy stagewise procedure. We can easily learn p(0)i (z
(0)) to model the marginal distribution of each component xi of the input, by training it by gradient descent on a maximum likelihood objective, i.e.
θ(0) = argmax θ Ex∼q∗train [ log p(0)(x; θ) ] (1)
This gives us a first, very crude unstructured (factorial) model of q∗.
Having learned this p(0), we might be tempted to then greedily learn the next stage p(1) of the chain in a similar fashion, after drawing samples z(0) ∼ p(0) in an attempt to learn to “denoise” the sampled z(0) into x. Yet the corresponding following training objective θ(1) = argmaxθ Ex∼q∗train,z(0)∼p(0) [ log p(1)(x|z(0); θ) ] makes no sense: x and z(0) are sampled independently of each other so z(0) contains no information about x, hence p(1)(x|z(0)) = p(1)(x). So maximizing this second objective becomes essentially the same as what we did when learning p(0). We would learn nothing more. It is essential, if we hope to learn a useful conditional distribution p(1)(x|z(0)) that it be trained on particular z(0) containing some information about x. In other words, we should not take our training inputs to be samples from p(0) but from a slightly different distribution, biased towards containing some information about x. Let us call it q(0)(z(0)|x). A natural choice for it, if it were possible, would be to take q(0)(z(0)|x) = p(z(0)|z(T ) = x) but this is an intractable inference, as all intermediate z(t) between z(0) and z(T ) are effectively latent states that we would need to marginalize over. Using a workaround such as a variational or MCMC approach would be a usual fallback. Instead, let us focus on our initial intent of guiding a progressive stochastic denoising, and think if we can come up with a different way to construct q(0)(z(0)|x) and similarly for the next steps q(t)i (z̃ (t) i |z̃(t−1),x).
Eventually, we expect a sequence of samples from Markov chain p to move from initial “noise” towards a specific example x from the training set rather than another one, primarily if a sample along the chain “resembles” x to some degree. This means that the transition operator should learn to pick up a minor resemblance with an x in order to transition to something likely to be even more similar to x. In other words, we expect samples along a chain leading to x to both have high probability under the transition operator of the chain p(t)(z(t)|z(t−1)), and to have some form of at least partial “resemblance” with x likely to increase as we progress along the chain. One highly inefficient way to emulate such a chain of samples would be, for teach step t, to sample many candidate samples from the transition operator (a conditionally factorial distribution) until we generate one that has some minimal “resemblance” to x (e.g. for a discrete space, this resemblance measure could be based on their Hamming distance). A qualitatively similar result can be obtained at a negligible cost by sampling from a factorial distribution that is very close to the one given by the transition operator, but very slightly biased towards producing something closer to x. Specifically, we can “infuse” a little of x into our sample by choosing for each input dimension, whether we sample it from the distribution given for that dimension by the transition operator, or whether, with a small probability, we take the value of that dimension from x. Samples from this biased chain, in which we slightly “infuse” x, will provide us with the inputs of our input-target training pairs for the transition operator. The target part of the training pairs is simply x.

2.3.1 THE INFUSION CHAIN
Formally we define an infusion chain z̃(0) → z̃(1) → . . . → z̃(T−1) whose distribution q(z̃(0), . . . , z̃(T−1)|x) will be “close” to the sampling chain z(0) → z(1) → z(2) → . . . → z(T−1) of model p in the sense that q(t)(z̃(t)|z̃(t−1),x) will be close to p(t)(z(t)|z(t−1)), but will at every step be slightly biased towards generating samples closer to target x, i.e. x gets progressively “infused” into the chain. This is achieved by defining q(0)i (z̃ (0) i |x) as a mixture between p (0) i (with a large mixture weight) and δxi , a concentrated unimodal distribution around xi, such as a Gaussian with small variance (with a small mixture weight)1. Formally q(0)i (z̃ (0) i |x) = (1 − α(t))p(0)i (z̃ (0) i ) + α (t)δxi(z̃ (0) i ), where 1 − α(t) and α(t) are the mixture weights 2. In other words, when sampling a value for z̃(0)i from q (0) i there will be a small probability α (0) to pick value close to xi (as sampled from δxi ) rather than sampling the value from p (0) i . We call α(t) the infusion rate. We define the transition operator of the infusion chain similarly as: q (t) i (z̃ (t) i |z̃(t−1),x) = (1− α(t))p (t) i (z̃ (t) i |z̃(t−1)) + α(t)δxi(z̃ (t) i ).
1Note that δxi does not denote a Dirac-Delta but a Gaussian with small sigma. 2In all experiments, we use an increasing schedule α(t) = α (t−1) +ω with α (0)
and ω constant. This allows to build our chain such that in the first steps, we give little information about the target and in the last steps we give more informations about the target. This forces the network to have less confidence (greater incertitude) at the beginning of the chain and more confidence on the convergence point at the end of the chain.

2.3.2 DENOISING-BASED INFUSION TRAINING PROCEDURE
For all x ∈ Dtrain: • Sample from the infusion chain z̃ = (z̃(0), . . . , z̃(T−1)) ∼ q(z̃(0), . . . , z̃(T−1)|x).
precisely so: z̃0 ∼ q(0)(z̃(0)|x) . . . z̃(t) ∼ q(t)(z̃(t)|z̃(t−1),x) . . . • Perform a gradient step so that p learns to “denoise” every z̃(t) into x.
θ(t) ← θ(t) + η(t) ∂ log p (t)(x|z̃(t−1); θ(t)) ∂θ(t)
where η(t) is a scalar learning rate. 3
As illustrated in Figure 2, the distribution of samples from the infusion chain evolves as training progresses, since this chain remains close to the model sampling chain.

2.4 STOCHASTIC LOG LIKELIHOOD ESTIMATION
The exact log-likelihood of the generative model implied by our model p is intractable. The logprobability of an example x can however be expressed using proposal distribution q as:
log p(x) = logEq(z̃|x) [ p(z̃,x)
q(z̃|x)
] (2)
Using Jensen’s inequality we can thus derive the following lower bound:
log p(x) ≥ Eq(z̃|x) [log p(z̃,x)− log q(z̃|x)] (3)
where log p(z̃,x) = log p(0)(z̃(0)) + (∑T−1
t=1 log p (t)(z̃(t)|z̃(t−1))
) + log p(T )(x|z̃(T−1)) and
log q(z̃|x) = log q(0)(z̃(0)|x) + ∑T−1 t=1 log q
(t)(z̃(t)|z̃(t−1),x). 3Since we will be sharing parameters between the p(t), in order for the expected larger error gradients on the earlier transitions not to dominate the parameter updates over the later transitions we used an increasing schedule η(t) = η0 tT for t ∈ {1, . . . , T}.
A stochastic estimation can easily be obtained by replacing the expectation by an average using a few samples from q(z̃|x). We can thus compute a lower bound estimate of the average log likelihood over training, validation and test data.
Similarly in addition to the lower-bound based on Eq.3 we can use the same few samples from q(z̃|x) to get an importance-sampling estimate of the likelihood based on Eq. 24.

2.4.1 LOWER-BOUND-BASED INFUSION TRAINING PROCEDURE
Since we have derived a lower bound on the likelihood, we can alternatively choose to optimize this stochastic lower-bound directly during training. This alternative lower-bound based infusion training procedure differs only slightly from the denoising-based infusion training procedure by using z̃(t) as a training target at step t (performing a gradient step to increase log p(t)(z̃(t)|z̃(t−1); θ(t))) whereas denoising training always uses x as its target (performing a gradient step to increase log p(t)(x|z̃(t−1); θ(t))). Note that the same reparametrization trick as used in Variational Autoencoders (Kingma & Welling, 2014) can be used here to backpropagate through the chain’s Gaussian sampling.

3 RELATIONSHIP TO PREVIOUSLY PROPOSED APPROACHES

3.1 MARKOV CHAIN MONTE CARLO FOR ENERGY-BASED MODELS
Generating samples as a repeated application of a Markov transition operator that operates on input space is at the heart of Markov Chain Monte Carlo (MCMC) methods. They allow sampling from an energy-model, where one can efficiently compute the energy or unnormalized negated log probability (or density) at any point. The transition operator is then derived from an explicit energy function such that the Markov chain prescribed by a specific MCMC method is guaranteed to converge to the distribution defined by that energy function, as the equilibrium distribution of the chain. MCMC techniques have thus been used to obtain samples from the energy model, in the process of learning to adjust its parameters.
By contrast here we do not learn an explicit energy function, but rather learn directly a parameterized transition operator, and define an implicit model distribution based on the result of running the Markov chain.

3.2 VARIATIONAL AUTO-ENCODERS
Variational auto-encoders (VAE) (Kingma & Welling, 2014; Rezende et al., 2014) also start from an unstructured (independent) noise sample and non-linearly transform this into a distribution that matches the training data. One difference with our approach is that the VAE typically maps from a lower-dimensional space to the observation space. By contrast we learn a stochastic transition operator from input space to input space that we repeat for T steps. Another key difference, is that the VAE learns a complex heavily parameterized approximate posterior proposal q whereas our infusion based q can be understood as a simple heuristic proposal distribution based on p. Importantly the specific heuristic we use to infuse x into q makes sense precisely because our operator is a map from input space to input space, and couldn’t be readily applied otherwise. The generative network in Rezende et al. (2014) is a Deep Latent Gaussian Model (DLGM) just as ours. But their approximate posterior q is taken to be factorial, including across all layers of the DLGM, whereas our infusion based q involves an ordered sampling of the layers, as we sample from q(t)(z̃(t)|z̃(t−1),x). More recent proposals involve sophisticated approaches to sample from better approximate posteriors, as the work of Salimans et al. (2015) in which Hamiltonian Monte Carlo is combined with variational inference, which looks very promising, though computationally expensive, and Rezende & Mohamed (2015) that generalizes the use of normalizing flows to obtain a better approximate posterior.
4Specifically, the two estimates (lower-bound and IS) start by collecting k samples from q(z̃|x) and computing for each the corresponding ` = log p(z̃,x) − log q(z̃|x). The lower-bound estimate is then obtained by averaging the resulting `1, . . . `k, whereas the IS estimate is obtained by taking the log of the averaged e`1 , . . . , e`k (in a numerical stable manner as logsumexp(`1, . . . , `k)− log k).

3.3 SAMPLING FROM AUTOENCODERS AND GENERATIVE STOCHASTIC NETWORKS
Earlier works that propose to directly learn a transition operator resulted from research to turn autoencoder variants that have a stochastic component, in particular denoising autoencoders (Vincent et al., 2010), into generative models that one can sample from. This development is natural, since a stochastic auto-encoder is a stochastic transition operator form input space to input space. Generative Stochastic Networks (GSN) (Alain et al., 2016) generalized insights from earlier stochastic autoencoder sampling heuristics (Rifai et al., 2012) into a more formal and general framework. These previous works on generative uses of autoencoders and GSNs attempt to learn a chain whose equilibrium distribution will fit the training data. Because autoencoders and the chain are typically started from or very close to training data points, they are concerned with the chain mixing quickly between modes. By contrast our model chain is always restarted from unstructured noise, and is not required to reach or even have an equilibrium distribution. Our concern is only what happens during the T “burn-in” initial steps, and to make sure that it transforms the initial factorial noise distribution into something that best fits the training data distribution. There are no mixing concerns beyond those T initial steps.
A related aspect and limitation of previous denoising autoencoder and GSN approaches is that these were mainly “local” around training samples: the stochastic operator explored space starting from and primarily centered around training examples, and learned based on inputs in these parts of space only. Spurious modes in the generated samples might result from large unexplored parts of space that one might encounter while running a long chain.

3.4 REVERSING A DIFFUSION PROCESS IN NON-EQUILIBRIUM THERMODYNAMICS
The approach of Sohl-Dickstein et al. (2015) is probably the closest to the approach we develop here. Both share a similar model sampling chain that starts from unstructured factorial noise. Neither are concerned about an equilibrium distribution. They are however quite different in several key aspects: Sohl-Dickstein et al. (2015) proceed to invert an explicit diffusion process that starts from a training set example and very slowly destroys its structure to become this random noise, they then learn to reverse this process i.e. an inverse diffusion. To maintain the theoretical argument that the exact reverse process has the same distributional form (e.g. p(x(t−1)|x(t)) and p(x(t)|x(t−1)) both factorial Gaussians), the diffusion has to be infinitesimal by construction, hence the proposed approaches uses chains with thousands of tiny steps. Instead, our aim is to learn an operator that can yield a high quality sample efficiently using only a small number T of larger steps. Also our infusion training does not posit a fixed a priori diffusion process that we would learn to reverse. And while the distribution of diffusion chain samples of Sohl-Dickstein et al. (2015) is fixed and remains the same all along the training, the distribution of our infusion chain samples closely follow the model chain as our model learns. Our proposed infusion sampling technique thus adapts to the changing generative model distribution as the learning progresses.
Drawing on both Sohl-Dickstein et al. (2015) and the walkback procedure introduced for GSN in Alain et al. (2016), a variational variant of the walkback algorithm was investigated by Goyal et al. (2017) at the same time as our work. It can be understood as a different approach to learning a Markov transition operator, in which a “heating” diffusion operator is seen as a variational approximate posterior to the forward “cooling” sampling operator with the exact same form and parameters, except for a different temperature.

4 EXPERIMENTS
We trained models on several datasets with real-valued examples. We used as prior distribution p(0) a factorial Gaussian whose parameters were set to be the mean and variance for each pixel through the training set. Similarly, our models for the transition operators are factorial Gaussians. Their mean and elementwise variance is produced as the output of a neural network that receives the previous z(t−1) as its input, i.e. p(t)(z(t)i |z(t−1)) = N (µi(z(t−1)), σ2i (z(t−1))) where µ and σ2 are computed as output vectors of a neural network. We trained such a model using our infusion training procedure on MNIST (LeCun & Cortes, 1998), Toronto Face Database (Susskind et al., 2010), CIFAR-10 (Krizhevsky & Hinton, 2009), and CelebA (Liu et al., 2015). For all datasets, the only preprocessing we did was to scale the integer pixel values down to range [0,1]. The network
trained on MNIST and TFD is a MLP composed of two fully connected layers with 1200 units using batch-normalization (Ioffe & Szegedy, 2015) 5. The network trained on CIFAR-10 is based on the same generator as the GANs of Salimans et al. (2016), i.e. one fully connected layer followed by three transposed convolutions. CelebA was trained with the previous network where we added another transposed convolution. We use rectifier linear units (Glorot et al., 2011) on each layer inside the networks. Each of those networks have two distinct final layers with a number of units corresponding to the image size. They use sigmoid outputs, one that predict the mean and the second that predict a variance scaled by a scalar β (In our case we chose β = 0.1) and we add an epsilon = 1e − 4 to avoid an excessively small variance. For each experiment, we trained the network on 15 steps of denoising with an increasing infusion rate of 1% (ω = 0.01, α (0)
= 0), except on CIFAR-10 where we use an increasing infusion rate of 2% (ω = 0.02, α (0) = 0) on 20 steps.

4.1 NUMERICAL RESULTS
Since we can’t compute the exact log-likelihood, the evaluation of our model is not straightforward. However we use the lower bound estimator derived in Section 2.4 to evaluate our model during training and prevent overfitting (see Figure 3). Since most previous published results on non-likelihood based models (such as GANs) used a Parzen-window-based estimator (Breuleux et al., 2011), we use it as our first comparison tool, even if it can be misleading (Lucas Theis & Bethge, 2016). Results are shown in Table 1, we use 10 000 generated samples and σ = 0.17 . To get a better estimate of the log-likelihood, we then computed both the stochastic lower bound and the importance sampling estimate (IS) given in Section 2.4. For the IS estimate in our MNIST-trained model, we used 20 000 intermediates samples. In Table 2 we compare our model with the recent Annealed Importance Sampling results (Wu et al., 2016). Note that following their procedure we add an uniform noise of 1/256 to the (scaled) test point before evaluation to avoid overevaluating models that might have overfitted on the 8 bit quantization of pixel values. Another comparison tool that we used is the Inception score as in Salimans et al. (2016) which was developed for natural images and is thus most relevant for CIFAR-10. Since Salimans et al. (2016) used a GAN trained in a semi-supervised way with some tricks, the comparison with our unsupervised trained model isn’t straightforward. However, we can see in Table 3 that our model outperforms the traditional GAN trained without labeled data.

4.2 SAMPLE GENERATION
Another common qualitative way to evaluate generative models is to look at the quality of the samples generated by the model. In Figure 4 we show various samples on each of the datasets we used. In order to get sharper images, we use at sampling time more denoising steps than in the training time (In the MNIST case we use 30 denoising steps for sampling with a model trained on 15 denoising steps). To make sure that our network didn’t learn to copy the training set, we show in the last column the nearest training-set neighbor to the samples in the next-to last column. We can see that our training method allow to generate very sharp and accurate samples on various dataset.
5We don’t share batch norm parameters across the network, i.e for each time step we have different parameters and independent batch statistics.

4.3 INPAINTING
Another method to evaluate a generative model is inpainting. It consists of providing only a partial image from the test set and letting the model generate the missing part. In one experiment, we provide only the top half of CelebA test set images and clamp that top half throughout the sampling chain. We restart sampling from our model several times, to see the variety in the distribution of the bottom part it generates. Figure 5 shows that the model is able to generate a varied set of bottom halves, all consistent with the same top half, displaying different type of smiles and expression. We also see that the generated bottom halves transfer some information about the provided top half of the images (such as pose and more or less coherent hair cut).

5 CONCLUSION AND FUTURE WORK
We presented a new training procedure that allows a neural network to learn a transition operator of a Markov chain. Compared to the previously proposed method of Sohl-Dickstein et al. (2015) based on inverting a slow diffusion process, we showed empirically that infusion training requires far fewer denoising steps, and appears to provide more accurate models. Currently, many successful generative models, judged on sample quality, are based on GAN architectures. However these require to use two different networks, a generator and a discriminator, whose balance is reputed delicate to adjust, which can be source of instability during training. Our method avoids this problem by using only a single network and a simpler training objective.
Denoising-based infusion training optimizes a heuristic surrogate loss for which we cannot (yet) provide theoretical guarantees, but we empirically verified that it results in increasing log-likelihood estimates. On the other hand the lower-bound-based infusion training procedure does maximize an explicit variational lower-bound on the log-likelihood. While we have run most of our experiments with the former, we obtained similar results on the few problems we tried with lower-bound-based infusion training.
Future work shall further investigate the relationship and quantify the compromises achieved with respect to other Markov Chain methods including Sohl-Dickstein et al. (2015); Salimans et al. (2015)
and also to powerful inference methods such as Rezende & Mohamed (2015). As future work, we also plan to investigate the use of more sophisticated neural net generators, similar to DCGAN’s (Radford et al., 2016) and to extend the approach to a conditional generator applicable to structured output problems.

ACKNOWLEDGMENTS
We would like to thank the developers of Theano (Theano Development Team, 2016) for making this library available to build on, Compute Canada and Nvidia for their computation resources, NSERC and Ubisoft for their financial support, and three ICLR anonymous reviewers for helping us improve our paper.

A DETAILS ON THE EXPERIMENTS
A.1 MNIST EXPERIMENTS
We show the impact of the infusion rate α(t) = α (t−1)
+ ω for different numbers of training steps on the lower bound estimate of log-likelihood on the Validation set of MNIST in Figure 6. We also show the quality of generated samples and the lower bound evaluated on the test set in Table 4. Each experiment in Table 4 uses the corresponding models of Figure 6 that obtained the best lower bound value on the validation set. We use the same network architecture as described in Section 4, i.e two fully connected layers with Relu activations composed of 1200 units followed by two distinct fully connected layers composed of 784 units, one that predicts the means, the other one that predicts the variances. Each mean and variance is associated with one pixel. All of the the parameters of the model are shared across different steps except for the batch norm parameters. During training, we use the batch statistics of the current mini-batch in order to evaluate our model on the train and validation sets. At test time (Table 4), we first compute the batch statistics over the entire train set for each step and then use the computed statistics to evaluate our model on the test test.
We did some experiments to evaluate the impact of α or ω in α(t) = α (t−1)
+ ω. Figure 6 shows that as the number of steps increases, the optimal value for infusion rate decreases. Therefore, if we want to use many steps, we should have a small infusion rate. These conclusions are valid for both increasing and constant infusion rate. For example, the optimal α for a constant infusion rate, in Figure 6e with 10 steps is 0.08 and in Figure 6f with 15 steps is 0.06. If the number of steps is not enough or the infusion rate is too small, the network will not be able to learn the target distribution as shown in the first rows of all subsection in Table 4.
In order to show the impact of having a constant versus an increasing infusion rate, we show in Figure 7 the samples created by infused and sampling chains. We observe that having a small infusion rate over many steps ensures a slow blending of the model distribution into the target distribution.
In Table 4, we can see high lower bound values on the test set with few steps even if the model can’t generate samples that are qualitatively satisfying. These results indicate that we can’t rely on the lower bound as the only evaluation metric and this metric alone does not necessarily indicate the suitability of our model to generated good samples. However, it is still a useful tool to prevent overfitting (the networks in Figure 6e and 6f overfit when the infusion rate becomes too high). Concerning the samples quality, we observe that having a small infusion rate over an adequate number of steps leads to better samples.
A.2 INFUSION AND MODEL SAMPLING CHAINS ON NATURAL IMAGES DATASETS
In order to show the behavior of our model trained by Infusion on more complex datasets, we show in Figure 8 chains on CIFAR-10 dataset and in Figure 9 chains on CelebA dataset. In each Figure, the first sub-figure shows the chains infused by some test examples and the second subfigure shows the model sampling chains. In the experiment on CIFAR-10, we use an increasing schedule α(t) = α (t−1) + 0.02 with α(0) = 0 and 20 infusion steps (this corresponds to the training parameters). In the experiment on CelebA, we use an increasing schedule α(t) = α (t−1)
+0.01 with α(0) = 0 and 15 infusion steps.
","In this work, we investigate a novel training procedure to learn a generative model as the transition operator of a Markov chain, such that, when applied repeatedly on an unstructured random noise sample, it will denoise it into a sample that matches the target distribution from the training set. The novel training procedure to learn this progressive denoising operation involves sampling from a slightly different chain than the model chain used for generation in the absence of a denoising target. In the training chain we infuse information from the training target example that we would like the chains to reach with a high probability. The thus learned transition operator is able to produce quality and varied samples in a small number of steps. Experiments show competitive results compared to the samples generated with a basic Generative Adversarial Net.",ICLR 2017 conference submission,True,,"Summary:
This paper introduces a heuristic approach for training a deep directed generative model, where similar to the transition operator of a Markov chain each layer samples from the same conditional distribution. Similar to optimizing a variational lower bound, the approach is to approximate the gradient by replacing the posterior over latents with an alternative distribution. However, the approximating distribution is not updated to improve the lower bound but heuristically constructed in each step. A further difference to variational optimization is that the conditional distributions are optimized greedily rather than following the gradient of the joint log-likelihood.

Review:
The proposed approach is interesting and to me seems worth exploring more. Given that there are approaches for training the same class of models which are 1) theoretically more sound, 2) of similar computational complexity, and 3) work well in practice (e.g. Rezende & Mohamed, 2015), I am nevertheless not sure of its potential to generate impact. My bigger concern, however, is that the empirical evaluation is still quite limited.

I appreciate the authors included proper estimates of the log-likelihood. This will enable and encourage future comparisons with this method on continuous MNIST. However, the authors should point out that the numbers taken from Wu et al. (2016) are not representative of the performance of a VAE. (From the paper: “Therefore, the log-likelihood values we report should not be compared directly against networks which have a more flexible observation model.” “Such observation models can easily achieve much higher log-likelihood scores, […].”)

Comparisons with inpainting results using other methods would have been nice. How practical is the proposed approach compared to other approaches? Similar to the diffusion approach by Sohl-Dickstein et al. (2015), the proposed approach seems to be both efficient and effective for inpainting. Not making this a bigger point and performing the proper evaluations seems like a missed opportunity.

Minor:
– I am missing citations for “ordered visible dimension sampling”
– Typos and frequent incorrect use of \citet and \citep

---

Infusion training is a new, somewhat heuristic, procedure for training deep generative models. It's an interesting novel idea and a good paper, which has also been improved after the authors have been responsive to reviewer feedback.

---

We updated the paper, the main changes are:

Added better log-likelihood estimates (one stochastic lower bound and one based on importance sampling)

Added curves showing that log-likelihood bound improves as infusion training progresses 

Added references to related and relevant works: Rezende & Mohamed, 2015;  Salimans et al. 2015; Dinh et al. 2014; Wu et al. 2016.

Added results tables for likelihood comparison with models from the literature (Parzen estimates by Sohl-Dickstein et al. 2015 and AIS estimates by Wu et al. 2016)

Added further experimental details.

Added an Appendix containing details regarding the infusion rate schedule as well as examples of infused and sampled chains on cifar10 and celebA

Corrected the typos mentioned by the reviewers

---

This paper trains a generative model which transforms noise into model samples by a gradual denoising process. It is similar to a generative model based on diffusion. Unlike the diffusion approach:
- It uses only a small number of denoising steps, and is thus far more computationally efficient.
- Rather than consisting of a reverse trajectory, the conditional chain for the approximate posterior jumps to q(z(0) | x), and then runs in the same direction as the generative model. This allows the inference chain to behave like a perturbation around the generative model, that pulls it towards the data. (This also seems somewhat related to ladder networks.)
- There is no tractable variational bound on the log likelihood.

I liked the idea, and found the visual sample quality given a short chain impressive. The inpainting results were particularly nice, since one shot inpainting is not possible under most generative modeling frameworks. It would be much more convincing to have a log likelihood comparison that doesn't depend on Parzen likelihoods.

Detailed comments follow:

Sec. 2:
""theta(0) the"" -> ""theta(0) be the""
""theta(t) the"" -> ""theta(t) be the""
""what we will be using"" -> ""which we will be doing""
I like that you infer q(z^0|x), and then run inference in the same order as the generative chain. This reminds me slightly of ladder networks.
""q*. Having learned"" -> ""q*. [paragraph break] Having learned""
Sec 3.3:
""learn to inverse"" -> ""learn to reverse""
Sec. 4:
""For each experiments"" -> ""For each experiment""
How sensitive are your results to infusion rate?
Sec. 5: ""appears to provide more accurate models"" I don't think you showed this -- there's no direct comparison to the Sohl-Dickstein paper.
Fig 4. -- neat!

---

The paper presents a method for training a generative model via an iterative denoising procedure. The denoising process is initialized with a random sample from a crude approximation to the data distribution and produces a high quality sample via multiple denoising steps. Training is performed by setting-up a Markov chain that slowly blends propositions from the current denoising model with a real example from the data distribution; using this chain the current denoising model is updated towards reproducing the changed, ""better"", samples from the blending process.

This is a clearly written paper that considers an interesting approach for training generative models. I was intrigued by the simplicity of the presented approach and really enjoyed reading the paper.
The proposed method is novel although it has clear ties to other recent work aiming to use denoising models for sampling from distributions such as the work by Sohl-Dickstein and the recent work on using DAEs as generative models.
I think this general direction of research is important. The proposed procedure takes inspiration from the perspective of generating samples by minimizing an energy function via transitions along a Markov chain and, if successful, it can potentially sidestep many problems of current procedures for training directed generative models such as:
- convergence and mode coverage problems as in generative adversarial networks
- problems with modeling multi-modal distributions which can arise when a too restrictive approximate inference model is paired with a powerful generative model

That being said, another method that seems promising for addressing these issues that also has some superficially similarity to the presented work is the idea of combining Hamiltonian Monte Carlo inference with variational inference as in [1]. As such I am not entirely convinced that the method presented here will be able to perform better than the mentioned paper; although it might be simpler to train. Similarly, although I agree that using a MCMC chain to generate samples via a MC-EM like procedure is likely very costly I am not convinced such a procedure won't at least also work reasonably well for the simple MNIST example. In general a more direct comparison between different inference methods using an MCMC chain like procedure would be nice to have but I understand that this is perhaps out of the scope of this paper. One thing that I would have expected, however, is a direct comparison to the procedure from Sohl-Dickstein in terms of sampling steps and generation quality as it is so directly related.

Other major points (good and bad):
- Although in general the method is explained well some training details are missing. Most importantly it is never mentioned how alpha or omega are set (I am assuming omega is 0.01 as that is the increase mentioned in the experimental setup). It is also unclear how alpha affects the capabilities of the generator. While it intuitively seems reasonable to use a small alpha over many steps to ensure slow blending of the two distributions it is not clear how necessary this is or at what point the procedure would break (I assume alpha = 1 won't work as the generator then would have to magically denoise a sample from the relatively uninformative draw from p0 ?). The authors do mention in one of the figure captions that the denoising model does not produce good samples in only 1-2 steps but that might also be an artifact of training the model with small alpha (at least I see no a priori reason for this). More experiments should be carried out here.
- No infusion chains or generating chains are shown for any of the more complicated data distributions, this is unfortunate as I feel these would be interesting to look at.
- The paper does a good job at evaluating the model with respect to several different metrics. The bound on the log-likelihood is nice to have as well!
- Unfortunately the current approach does not come with any theoretical guarantees. It is unclear for what choices of alpha the procedure will work and whether there is some deeper connection to MCMC sampling or energy based models. In my eyes this does not subtract from the value of the paper but would perhaps be worth a short sentence in the conclusion.

Minor points:
- The second reference seems broken
- Figure 3 starts at 100 epochs and, as a result, contains little information. Perhaps it would be more useful to show the complete training procedure and put the x-axis on a log-scale ?
- The explanation regarding the convolutional networks you use makes no sense to me. You write that you use the same structure as in the ""Improved GANs"" paper which, unlike your model, generates samples from a fixed length random input. I thus suppose you don't really use a generator with 1 fully connected network followed by up-convolutions but rather have several stages of convolutions followed by a fully connected layer and then up-convolutions ?
- The choice of parametrizing the variance via a sigmoid output unit is somewhat unusual, was there a specific reason for this choice ?
- footnote 1 contains errors: ""This allow to"" -> ""allows to"",  ""few informations"" -> ""little information"". ""This force the network"" -> ""forces""
- Page 1 error: etc...
- Page 4 error: ""operator should to learn""

[1] Markov Chain Monte Carlo and Variational Inference: Bridging the Gap, Tim Salimans and Diedrik P. Kingma and Max Welling, ICML 2015


>>> Update <<<<
Copied here from my response below: 

I believe the response of the authors clarifies all open issues. I strongly believe the paper should be accepted to the conference. The only remaining issue I have with the paper is that, as the authors acknowledge the architecture of the generator is likely highly sub-optimal and might hamper the performance of the method in the evaluation. This however does not at all subtract from any of the main points of the paper.

I am thus keeping my score as a clear accept. I want to emphasize that I believe the paper should be published (just in case the review process results in some form of cut-off threshold that is high due to overall ""inflated"" review scores).

---

Summary:
This paper introduces a heuristic approach for training a deep directed generative model, where similar to the transition operator of a Markov chain each layer samples from the same conditional distribution. Similar to optimizing a variational lower bound, the approach is to approximate the gradient by replacing the posterior over latents with an alternative distribution. However, the approximating distribution is not updated to improve the lower bound but heuristically constructed in each step. A further difference to variational optimization is that the conditional distributions are optimized greedily rather than following the gradient of the joint log-likelihood.

Review:
The proposed approach is interesting and to me seems worth exploring more. Given that there are approaches for training the same class of models which are 1) theoretically more sound, 2) of similar computational complexity, and 3) work well in practice (e.g. Rezende & Mohamed, 2015), I am nevertheless not sure of its potential to generate impact. My bigger concern, however, is that the empirical evaluation is still quite limited.

I appreciate the authors included proper estimates of the log-likelihood. This will enable and encourage future comparisons with this method on continuous MNIST. However, the authors should point out that the numbers taken from Wu et al. (2016) are not representative of the performance of a VAE. (From the paper: “Therefore, the log-likelihood values we report should not be compared directly against networks which have a more flexible observation model.” “Such observation models can easily achieve much higher log-likelihood scores, […].”)

Comparisons with inpainting results using other methods would have been nice. How practical is the proposed approach compared to other approaches? Similar to the diffusion approach by Sohl-Dickstein et al. (2015), the proposed approach seems to be both efficient and effective for inpainting. Not making this a bigger point and performing the proper evaluations seems like a missed opportunity.

Minor:
– I am missing citations for “ordered visible dimension sampling”
– Typos and frequent incorrect use of \citet and \citep

---

Summary:
This paper introduces a heuristic approach for training a deep directed generative model, where similar to the transition operator of a Markov chain each layer samples from the same conditional distribution. Similar to optimizing a variational lower bound, the approach is to approximate the gradient by replacing the posterior over latents with an alternative distribution. However, the approximating distribution is not updated to improve the lower bound but heuristically constructed in each step. A further difference to variational optimization is that the conditional distributions are optimized greedily rather than following the gradient of the joint log-likelihood.

Review:
The proposed approach is interesting and to me seems worth exploring more. Given that there are approaches for training the same class of models which are 1) theoretically more sound, 2) of similar computational complexity, and 3) work well in practice (e.g. Rezende & Mohamed, 2015), I am nevertheless not sure of its potential to generate impact. My bigger concern, however, is that the empirical evaluation is still quite limited.

I appreciate the authors included proper estimates of the log-likelihood. This will enable and encourage future comparisons with this method on continuous MNIST. However, the authors should point out that the numbers taken from Wu et al. (2016) are not representative of the performance of a VAE. (From the paper: “Therefore, the log-likelihood values we report should not be compared directly against networks which have a more flexible observation model.” “Such observation models can easily achieve much higher log-likelihood scores, […].”)

Comparisons with inpainting results using other methods would have been nice. How practical is the proposed approach compared to other approaches? Similar to the diffusion approach by Sohl-Dickstein et al. (2015), the proposed approach seems to be both efficient and effective for inpainting. Not making this a bigger point and performing the proper evaluations seems like a missed opportunity.

Minor:
– I am missing citations for “ordered visible dimension sampling”
– Typos and frequent incorrect use of \citet and \citep

---

Infusion training is a new, somewhat heuristic, procedure for training deep generative models. It's an interesting novel idea and a good paper, which has also been improved after the authors have been responsive to reviewer feedback.

---

We updated the paper, the main changes are:

Added better log-likelihood estimates (one stochastic lower bound and one based on importance sampling)

Added curves showing that log-likelihood bound improves as infusion training progresses 

Added references to related and relevant works: Rezende & Mohamed, 2015;  Salimans et al. 2015; Dinh et al. 2014; Wu et al. 2016.

Added results tables for likelihood comparison with models from the literature (Parzen estimates by Sohl-Dickstein et al. 2015 and AIS estimates by Wu et al. 2016)

Added further experimental details.

Added an Appendix containing details regarding the infusion rate schedule as well as examples of infused and sampled chains on cifar10 and celebA

Corrected the typos mentioned by the reviewers

---

This paper trains a generative model which transforms noise into model samples by a gradual denoising process. It is similar to a generative model based on diffusion. Unlike the diffusion approach:
- It uses only a small number of denoising steps, and is thus far more computationally efficient.
- Rather than consisting of a reverse trajectory, the conditional chain for the approximate posterior jumps to q(z(0) | x), and then runs in the same direction as the generative model. This allows the inference chain to behave like a perturbation around the generative model, that pulls it towards the data. (This also seems somewhat related to ladder networks.)
- There is no tractable variational bound on the log likelihood.

I liked the idea, and found the visual sample quality given a short chain impressive. The inpainting results were particularly nice, since one shot inpainting is not possible under most generative modeling frameworks. It would be much more convincing to have a log likelihood comparison that doesn't depend on Parzen likelihoods.

Detailed comments follow:

Sec. 2:
""theta(0) the"" -> ""theta(0) be the""
""theta(t) the"" -> ""theta(t) be the""
""what we will be using"" -> ""which we will be doing""
I like that you infer q(z^0|x), and then run inference in the same order as the generative chain. This reminds me slightly of ladder networks.
""q*. Having learned"" -> ""q*. [paragraph break] Having learned""
Sec 3.3:
""learn to inverse"" -> ""learn to reverse""
Sec. 4:
""For each experiments"" -> ""For each experiment""
How sensitive are your results to infusion rate?
Sec. 5: ""appears to provide more accurate models"" I don't think you showed this -- there's no direct comparison to the Sohl-Dickstein paper.
Fig 4. -- neat!

---

The paper presents a method for training a generative model via an iterative denoising procedure. The denoising process is initialized with a random sample from a crude approximation to the data distribution and produces a high quality sample via multiple denoising steps. Training is performed by setting-up a Markov chain that slowly blends propositions from the current denoising model with a real example from the data distribution; using this chain the current denoising model is updated towards reproducing the changed, ""better"", samples from the blending process.

This is a clearly written paper that considers an interesting approach for training generative models. I was intrigued by the simplicity of the presented approach and really enjoyed reading the paper.
The proposed method is novel although it has clear ties to other recent work aiming to use denoising models for sampling from distributions such as the work by Sohl-Dickstein and the recent work on using DAEs as generative models.
I think this general direction of research is important. The proposed procedure takes inspiration from the perspective of generating samples by minimizing an energy function via transitions along a Markov chain and, if successful, it can potentially sidestep many problems of current procedures for training directed generative models such as:
- convergence and mode coverage problems as in generative adversarial networks
- problems with modeling multi-modal distributions which can arise when a too restrictive approximate inference model is paired with a powerful generative model

That being said, another method that seems promising for addressing these issues that also has some superficially similarity to the presented work is the idea of combining Hamiltonian Monte Carlo inference with variational inference as in [1]. As such I am not entirely convinced that the method presented here will be able to perform better than the mentioned paper; although it might be simpler to train. Similarly, although I agree that using a MCMC chain to generate samples via a MC-EM like procedure is likely very costly I am not convinced such a procedure won't at least also work reasonably well for the simple MNIST example. In general a more direct comparison between different inference methods using an MCMC chain like procedure would be nice to have but I understand that this is perhaps out of the scope of this paper. One thing that I would have expected, however, is a direct comparison to the procedure from Sohl-Dickstein in terms of sampling steps and generation quality as it is so directly related.

Other major points (good and bad):
- Although in general the method is explained well some training details are missing. Most importantly it is never mentioned how alpha or omega are set (I am assuming omega is 0.01 as that is the increase mentioned in the experimental setup). It is also unclear how alpha affects the capabilities of the generator. While it intuitively seems reasonable to use a small alpha over many steps to ensure slow blending of the two distributions it is not clear how necessary this is or at what point the procedure would break (I assume alpha = 1 won't work as the generator then would have to magically denoise a sample from the relatively uninformative draw from p0 ?). The authors do mention in one of the figure captions that the denoising model does not produce good samples in only 1-2 steps but that might also be an artifact of training the model with small alpha (at least I see no a priori reason for this). More experiments should be carried out here.
- No infusion chains or generating chains are shown for any of the more complicated data distributions, this is unfortunate as I feel these would be interesting to look at.
- The paper does a good job at evaluating the model with respect to several different metrics. The bound on the log-likelihood is nice to have as well!
- Unfortunately the current approach does not come with any theoretical guarantees. It is unclear for what choices of alpha the procedure will work and whether there is some deeper connection to MCMC sampling or energy based models. In my eyes this does not subtract from the value of the paper but would perhaps be worth a short sentence in the conclusion.

Minor points:
- The second reference seems broken
- Figure 3 starts at 100 epochs and, as a result, contains little information. Perhaps it would be more useful to show the complete training procedure and put the x-axis on a log-scale ?
- The explanation regarding the convolutional networks you use makes no sense to me. You write that you use the same structure as in the ""Improved GANs"" paper which, unlike your model, generates samples from a fixed length random input. I thus suppose you don't really use a generator with 1 fully connected network followed by up-convolutions but rather have several stages of convolutions followed by a fully connected layer and then up-convolutions ?
- The choice of parametrizing the variance via a sigmoid output unit is somewhat unusual, was there a specific reason for this choice ?
- footnote 1 contains errors: ""This allow to"" -> ""allows to"",  ""few informations"" -> ""little information"". ""This force the network"" -> ""forces""
- Page 1 error: etc...
- Page 4 error: ""operator should to learn""

[1] Markov Chain Monte Carlo and Variational Inference: Bridging the Gap, Tim Salimans and Diedrik P. Kingma and Max Welling, ICML 2015


>>> Update <<<<
Copied here from my response below: 

I believe the response of the authors clarifies all open issues. I strongly believe the paper should be accepted to the conference. The only remaining issue I have with the paper is that, as the authors acknowledge the architecture of the generator is likely highly sub-optimal and might hamper the performance of the method in the evaluation. This however does not at all subtract from any of the main points of the paper.

I am thus keeping my score as a clear accept. I want to emphasize that I believe the paper should be published (just in case the review process results in some form of cut-off threshold that is high due to overall ""inflated"" review scores).

---

Summary:
This paper introduces a heuristic approach for training a deep directed generative model, where similar to the transition operator of a Markov chain each layer samples from the same conditional distribution. Similar to optimizing a variational lower bound, the approach is to approximate the gradient by replacing the posterior over latents with an alternative distribution. However, the approximating distribution is not updated to improve the lower bound but heuristically constructed in each step. A further difference to variational optimization is that the conditional distributions are optimized greedily rather than following the gradient of the joint log-likelihood.

Review:
The proposed approach is interesting and to me seems worth exploring more. Given that there are approaches for training the same class of models which are 1) theoretically more sound, 2) of similar computational complexity, and 3) work well in practice (e.g. Rezende & Mohamed, 2015), I am nevertheless not sure of its potential to generate impact. My bigger concern, however, is that the empirical evaluation is still quite limited.

I appreciate the authors included proper estimates of the log-likelihood. This will enable and encourage future comparisons with this method on continuous MNIST. However, the authors should point out that the numbers taken from Wu et al. (2016) are not representative of the performance of a VAE. (From the paper: “Therefore, the log-likelihood values we report should not be compared directly against networks which have a more flexible observation model.” “Such observation models can easily achieve much higher log-likelihood scores, […].”)

Comparisons with inpainting results using other methods would have been nice. How practical is the proposed approach compared to other approaches? Similar to the diffusion approach by Sohl-Dickstein et al. (2015), the proposed approach seems to be both efficient and effective for inpainting. Not making this a bigger point and performing the proper evaluations seems like a missed opportunity.

Minor:
– I am missing citations for “ordered visible dimension sampling”
– Typos and frequent incorrect use of \citet and \citep",3.0,,4.0,,4.0,7.0,,,4.666666666666667,5.0,3.5
328,"MULTILAYER RECURRENT NETWORK MODELS OF PRI- MATE RETINAL GANGLION CELL RESPONSES
Authors: Eleanor Batty, Josh Merel, Alexander Heitman, Liam Paninski
Source file: 328.pdf

ABSTRACT
Developing accurate predictive models of sensory neurons is vital to understanding sensory processing and brain computations. The current standard approach to modeling neurons is to start with simple models and to incrementally add interpretable features. An alternative approach is to start with a more complex model that captures responses accurately, and then probe the fitted model structure to understand the neural computations. Here, we show that a multitask recurrent neural network (RNN) framework provides the flexibility necessary to model complex computations of neurons that cannot be captured by previous methods. Specifically, multilayer recurrent neural networks that share features across neurons outperform generalized linear models (GLMs) in predicting the spiking responses of parasol ganglion cells in the primate retina to natural images. The networks achieve good predictive performance given surprisingly small amounts of experimental training data. Additionally, we present a novel GLM-RNN hybrid model with separate spatial and temporal processing components which provides insights into the aspects of retinal processing better captured by the recurrent neural networks.

1 INTRODUCTION
Our understanding of sensory processing in the brain is most straightforwardly reflected in our ability to model the process by which stimuli presented at the sensory periphery are transformed into the spiking activity of populations of neurons. For decades, researchers have interrogated stimulus-response
∗These authors contributed equally.
neural properties using simplified targeted stimuli, such as bars, spots, or gratings. While these types of stimuli uncovered many interesting aspects of visual computation, they have several limitations (Barlow & Levick, 1965). These stimuli may not fully drive important components of neural response, and modeling efforts have often assumed a quasi-linear mapping from stimulus to firing rate. Subsequent efforts to characterize cells relied on white noise stimulation and building models through reverse correlation (de Boer R & Kuyper, 1968; Marmarelis & Naka, 1972; Chichilnisky, 2001). A standard model used to relate white noise to spiking responses is the linear-nonlinear-Poisson (LN) or generalized linear model (GLM) which consists of a spatiotemporal linear filtering of the stimulus followed by a nonlinearity and probabilistic spike generation (Chichilnisky, 2001; Simoncelli et al., 2004; Schwartz et al., 2006). Although this family of models have advanced our understanding, they do not optimally capture neural responses, especially to natural scenes which can lead to more complex responses than white noise stimuli (David et al., 2004). Even in the retina, early in the visual processing stream, these commonly-used models capture retinal ganglion cell (RGC) responses to natural stimuli less accurately than to white noise (Heitman et al., 2016).
Recently, deep neural networks have been used to dramatically improve performance on a diverse array of machine learning tasks (Krizhevsky et al., 2012; LeCun et al., 2015). Furthermore, these networks bear a loose resemblance to real neural networks, and provide a sufficiently rich model class that can still be roughly constrained to match the biological architecture (Kriegeskorte, 2015). Most previous research at this intersection of neuroscience and artificial neural networks has focused on training networks on a certain task, such as object recognition, and then comparing the computations performed in different layers of the artificial network to those performed by real neurons (Yamins et al., 2014). Here we take a different approach: we fit multilayer models directly to the spiking responses of neurons, an approach that has not been explored in detail (but see (McIntosh et al., 2016) for some recent independent parallel developments).

2 APPROACH
We fit a range of models, detailed below, to spiking responses of primate RGCs. Our baseline comparisons are the GLM architectures that have been widely used to construct previous neural models (Pillow et al., 2008), though here we focus on individual neuronal responses (we leave modeling of correlations between neurons for future work). We focused on RNNs as a flexible framework in which to model more complex temporal and spatial nonlinearities. We also explored a number of network architectures involving features or weights shared across observed neurons. Given the complexity of the network architectures, we reasoned that sharing statistical strength across neurons by learning a shared feature space might improve predictive performance. This is conceptually a form of multitask learning - we are using a shared representation to achieve better generalization (Baxter, 2000). Motivated by previous research showing significant differences in the processing properties of the two cell types examined, ON and OFF parasol retinal ganglion cells, we fit separate models for each of these cell types (Chichilnisky & Kalmar, 2002).

3 METHODS

3.1 DATA COLLECTION
We fit spiking responses of OFF and ON parasol retinal ganglion cells to natural scenes. Recordings were performed on isolated retina using a large-scale multi-electrode recording system (Litke et al., 2004; Frechette et al., 2005; Field et al., 2007). A standard spike sorting algorithm was used to identify spikes from different cells from the voltage signals on each electrode during visual stimulation (Litke et al., 2004). We focus on two separate experiments (the same experimental procedure in two separate retinas) here; analyses of other datasets yielded similar results. Models were fit separately for the two experiments due to animal to animal variability in cell properties, such as receptive field size and firing rate. Almost all spike sorted cells were used for training (exp 1 = 118 OFF cells, 66 ON cells; exp 2 = 142 OFF cells, 103 ON cells): two cells were removed due to data quality issues (see sec 3.3). Performance metrics in this paper are reported for the same subset of cells used in a previous study (Heitman et al., 2016). These cells passed passed a manual screen for spike sorting accuracy, demonstrated stable light responses, and met a convergence criteria in prior linear-nonlinear modeling (exp 1 = 10 OFF cells, 18 ON cells; exp 2 = 65 OFF cells, 14 ON cells). The naturalistic movie
stimulus consisted of images from the Van Hateren database shown for one second each, with spatial jitter based on eye movements during fixation by awake macaque monkeys (Z.M. Hafed and R.J. Krauzlis, personal communication), (van Hateren & van der Schaaf, 1998). An example stimulus can be found at https://youtu.be/sG_18Uz_6OE. 59 distinct natural scenes movies of length one minute (the training data) were interleaved with 59 repetitions of a 30 second movie (the test data). Interleaving ensured that the test movie repetitions spanned the same period of time as the training data and therefore experienced the same range of experimental conditions (in case of neural response drifts over time). The first 4 movies shown (2 training movies and 2 repetitions of the test movie) were excluded to avoid initial transients. Test metrics are reported for the last 29 seconds of the 30 second test movie for the same reason. For further details on the experimental set-up, data preprocessing, and visual stimulation, see Heitman et al. (2016).

3.2 MODEL TRAINING
All models were implemented in Theano and trained on a combination of CPUs and GPUs (Theano Development Team, 2016). Training was performed using the Adam optimizer on the mean squared error (MSE) between predicted firing rate and true spikes (Kingma & Ba, 2014). We also experimented with optimizing a Poisson likelihood; this led to qualitatively similar results but occasionally less stable fits, so we focus on the MSE results here. All recurrent dynamics and temporal filters operated on time bins of 8.33 ms (the frame rate of the movie). Spike history terms and performance metrics were calculated for 0.833 ms bins. We used the same split of training and validation data for both experiments: 104 thirty-second movies as training data and 10 thirty-second movies as a held-out validation set.
During training, the performance on the held-out validation set is checked after every pass through the training data. After each iteration through the training data, if the model exhibits significantly better validation performance than our previous best, we reset the minimum number of iterations to be twice the current iteration number. If we make it through those iterations without another significant improvement, we stop. We train for a maximum of 150 epochs, where we define one epoch as one pass through all the training data. The model with the best validation performance is saved and used to assess test performance. All models with shared parameters were trained on a combined MSE over
all neurons and the parameters picked were those which minimized validation MSE for all neurons. For individual LNs/GLMs/RNNs, the validation MSE was minimized for each neuron separately.

3.3 RECEPTIVE FIELD CENTER ESTIMATION
In all models used in this paper, we estimate the receptive field (RF) center of each neuron in order to identify the appropriate portion of the image to use as input. We calculate a 250 ms long spike triggered average (STA) using reverse correlation of the neuron’s spikes with a white noise stimulus. We reduce the noise in this STA by using a rank 1 approximation (singular value decomposition followed by reconstruction using the primary temporal and spatial components). We then smooth each frame of the STA via convolution with a Gaussian spatial filter. The center location is defined as the pixel location that has the maximum absolute magnitude over time. The center locations were visually assessed to check accuracy of the algorithm. Rare cases where the algorithm failed to identify the correct center indicated neurons that responded to very little of the image as their receptive field was more than half-way displaced out of the image. These two neurons (two Exp 1 ON cells) were removed from further analysis. If the receptive field center is close to the edge of the image, the image patch is padded with the average training stimulus value.

3.4 PERFORMANCE EVALUATION
To quantitatively evaluate the accuracy of model spike predictions, we used the fraction of explainable variance, which has been described in previous literature (Heitman et al., 2016). Average firing rates over time are obtained after generating spikes from the model in 0.833 ms bins and smoothing with a Gaussian temporal filter (SD=10ms). The fraction of variance is computed as
F (r, rs) = 1− ∑
t(r(t)− rs(t))2∑ t(r(t)− µ)2
(1)
where r(t) is the smoothed recorded firing rate, rs(t) is the smoothed predicted firing rate, and µ is the average recorded rate. Finally, to account for the reproducibility of responses over repeated trials, we normalize by the fraction of variance captured by using the average firing rate on the odd (ro) trials of the repeated test movie to predict responses on the even (re) trials:
FV = F (r, rs)
F (re, ro) . (2)

4 MODEL ANALYSIS

4.1 NETWORK ARCHITECTURES
Individual LNs and GLMs: The linear-nonlinear model (LN) consists of a spatiotemporal filtering of the 31x31x30 movie patch (Xt, width by height by time) surrounding the estimated center of the neuron’s receptive field plus a bias term (b), followed by a sigmoid nonlinearity (f ), and Poisson spike generation to produce the responses rt. The generalized linear model (GLM), given by
rt ∼ Poiss [ f ( ~wTs (Xt ~wt) + b+ ∑ i hirt−i )] , (3)
has the same architecture with the addition of a post-spike history filter h before the nonlinearity f (Pillow et al., 2008). We used a rank 1 approximation of the full spatiotemporal filter (higher rank models did not significantly improve fits on a subset of examined neurons), resulting in a vectorized 31x31 spatial filter (~ws) and a 30 bin temporal filter (~wt) which spans 250 ms (Heitman et al., 2016). The post-spike history filter consists of a weighted sum of a basis of 20 raised cosines spanning approximately 100 ms (Pillow et al., 2008). The models with spike history were fit by initializing with the model fit without spike history. The filter either operates on the recorded spikes (training and validation) or the spikes generated by the model (testing). The nonlinearity is the logistic sigmoid: f = L/(1 + exp(−x)), which has been shown to improve fitting over an exponential nonlinearity for modeling RGC responses to natural scenes (Heitman et al., 2016).
Shared LN: In this model, the architecture is similar to the individual LNs but all cells of a given type (OFF or ON) share the same temporal and spatial filters (Figure 1A; note that the spatial filters are displaced to the RF center of each individual RGC). All other parameters are individually tuned for each observed neuron. There is an additional gain term that weights the output of the filtering individually for each observed neuron.
Two-layer RNN, 50 units: In this architecture, there are two recurrent neural network (RNN) layers between the image patch and Poisson neural unit:
~h (1) j,t = max(0, U1~sj,t + V1 ~h (1) j,t−1 + ~c) (4)
~h (2) j,t = max(0, U2 ~h (1) j,t + V2 ~h (2) j,t−1 + ~d) (5) rj,t ∼ Poiss [ f(~wTj ~h (2) j,t + bj) ] . (6)
The activity of the 50 units in the first RNN layer at time t is given by ~h(1)j,t in Eqn. 4. These units are rectified linear, and receive input from the vectorized 31x31 image patch surrounding the center of neuron j’s receptive field, ~sj,t, with weights U1, along with input from the other units in the layer with weights V1 and a bias ~c. The output of the first RNN is then fed into a second RNN with similar architecture. The firing rate for each observed neuron in the final layer is then given by Eqn. 6, and is a weighted sum of the recurrent units plus a bias bj , followed by a softplus nonlinearity f = log(1 + exp(−x)). Note that all parameters are shared across neurons except for the weights to the final layer and the final bias terms (~wj and bj).
GLM-RNN Hybrid: The GLM-RNN hybrid model consists of a spatial filter followed by a two-layer RNN. The architecture resembles that of the full two-layer RNN with 50 units, except the input to the first layer is a scalar (post multiplication with the spatial filter) at each time step instead of the full image patch; thus the RNN in this model is responsible for shaping the temporal properties of the output, but does not affect spatial processing after the first linear spatial filtering stage. All weights
are shared across neurons except for weights to the final layer (~wj) and the final bias terms (bj):
yj,t = ~w T s ~sj,t (7)
~h (1) j,t = max(0, ~u1yj,t + V1 ~h (1) j,t−1 + ~c) (8)
~h (2) j,t = max(0, U2h (1) j,t + V2 ~h (2) j,t−1 + ~d) (9) rj,t ∼ Poiss [ f(~wTj ~h (2) t + bj) ] . (10)

4.2 MODEL PERFORMANCE
RNNs of varying architectures consistently outperformed LNs and GLMs in predicting neural spiking responses to a novel natural scene movie for both OFF and ON parasol retinal ganglion cells in both experiments (Figure 2). A shared two-layer recurrent network consistently captures around 80% of the explainable variance across experiments and cell types. Other recurrent architectures (1-3 layer RNNs and a 2 layer LSTM) led to similar levels of performance (Supplementary Figure 6). The increase in performance according to the fraction of explainable variance metric was not an average effect: almost all neurons were significantly better predicted by the RNN (Figure 2B). A 2 layer RNN model with additional trained spike history filters outperformed GLMs and LNs according to a normalized log likelihood metric (Supplementary Figure 7).
Inspection of the mean predicted firing rate traces for LNs and RNNs in Figure 3 reveals that the recurrent network seems to be capturing the timing of firing more precisely. The LN often predicts a general increase in firing rate at the correct times, but the RNN captures the sudden increase in firing rate followed by decay which often occurs when the image changes. On the other hand, the LN models sometimes predict modest increases or decreases in firing rate that the recurrent nets miss.
Understanding why the recurrent models improve performance is a challenging task due to the black-box nature of deep networks. The first layer filters (U1, from image patches to recurrent units) have an interpretable structure resembling traditional receptive fields expected in the retina
(Supplementary Figure 8). However, the computations performed by the recurrent units are difficult to tease apart, because the weights are less interpretable. Thus, instead of attempting a mechanistic explanation of the internals of the RNN, we focused on what additional captured information resulted in the improved RNN performance.
One possibility is that capturing nonlinear effects in parts of the image far from the receptive field center improved predictions (McIlwain, 1964; Passaglia et al., 2009). We restricted the size of the image patch surrounding each receptive field center from 31x31 to 15x15 (Supplementary Figure 9). Shared RNNs trained on the smaller image patch size did as well, or better, than those trained on the larger patch across almost all combinations of cell type and experiment. (We see a similar small improvement when training the LN models on the small patch.) Thus we concluded that long-range nonlinear spatial interactions do not contribute to the increased performance produced by the RNNs.
We also investigated whether nonlinear spatial interactions or nonlinear temporal processing primarily contributed to better predictions. To accomplish this, we constructed a GLM-RNN hybrid, described previously, in which a single spatial filter precedes a two-layer RNN - effectively allowing only temporal nonlinearities to be captured. This model improved prediction over the LNs and GLMs but did not reach full RNN performance. The amount by which this model closed the gap differed for different experiments and cell types. We quantified this by computing the difference between multitask RNN and multitask LN performance for each neuron and the difference between multitask hybrid and multitask LN performance. We divide the latter by the former (on a cell-by-cell basis) to obtain the ratios summarized in Figure 2C. The hybrid model closed greater than half of the gap on average between multitask LN and RNN performance, indicating that the richer temporal dynamics of the RNN model account for a large part of the difference between RNN and LN performance, though spatial nonlinearities play a role too.

5 MODEST TRAINING DATA LENGTH SUFFICES FOR GOOD PERFORMANCE
Deep networks can be complex and often require large amounts of data to adequately train: convolutional neural networks used for object recognition are trained on over a million images (Krizhevsky et al., 2012). Standard neuroscience experiments yield limited data sets, so it is crucial to assess whether we have enough data to adequately fit our network architectures. We trained the RNN on varying amounts of data, and ran several different iterations of the network to explore variation over random initializations and randomly chosen training sets. These results are shown for both ON and OFF cells in Figure 4. Surprisingly small amounts of training data resulted in good predictive abilities. For larger amounts of training data, different iterations resulted in very similar mean fraction of variance values, indicating fairly robust fitting in these models. See Supplementary Figure 10 for further details.

6 BENEFITS OF MULTITASK FRAMEWORK
We investigated whether the multitask framework with shared parameters across neurons actually helps to improve predictive performance with reasonable amounts of experimental data. First, we quantified the benefits of parameter-sharing in the simple LN model. This is a highly constrained
framework: every cell has the same spatial and temporal filter. The shared LN does not improve performance for most neurons (Figure 5A).
We expected the multitask framework to be more helpful applied to the RNN model because in this case we are sharing features but not all parameters across neurons. Indeed, the multitask RNN consistently outperformed RNNs trained individually on single neurons (Figure 5B); individuallytrained RNNs also had much more variable losses than did the multitask-trained RNNs. In a realistic experimental setting with limited data, the multitask framework is a useful way to leverage all of the data collected for all neurons.

7 CONCLUSION
Using retinal neurons responding to natural scenes as an example, we showed that: using deep networks to model neural spiking responses can significantly improve prediction over current state-ofthe-art models; sharing information across neurons in a multi-task framework leads to better and more stable predictions; and these models work well even given relatively small amounts of experimental data. We believe that the multitask RNN framework presented here will enable new, richer models of complex nonlinear spiking computations in other brain areas.
While one could argue that we have merely exchanged the black box of the brain for another black box, just having a more predictive model is an important tool for research: these predictive models of the primate retina can be used in retinal prosthetics research, to probe decoding, and as a first stage of processing in the modeling of higher visual areas. Additionally, the recurrent network is more accessible and available for experimentation and quantitative analysis. For example, the trained neural network models may guide choices for more accurate simpler models by identifying key computational features that are important to include. Training smaller models on the denoised compression of spiking data (the predicted firing rate) may help them to learn features they otherwise would not (Ba & Caruana, 2014). The deep network approach allows one to determine types of information important to the neuron without having to build an exact mechanistic model of how such information is incorporated, as demonstrated by our finding that both spatial and temporal nonlinearities are not fully captured by the standard pseudo-linear models. We hope in future work to gain a more thorough and quantitative understanding of the dynamics captured by the recurrent networks and to extend this approach to higher sensory areas.

ACKNOWLEDGMENTS
Funding for this research was provided by the National Science Foundation Graduate Research Fellowship Program under grant No. DGE-114747 (NB), Grant Number No. DGE-16-44869 (EB), the National Science Foundation IGERT Training Grant No. 0801700 (NB), the National Institutes of Health Grant EY017992 (EJC), NSF CRCNS IIS-1430239 (LP, EJC) and Google Faculty Research awards (LP, EJC); in addition, this work was supported by the Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/ Interior Business Center (DoI/IBC) contract number D16PC00003 (LP). The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of IARPA, DoI/IBC, or the U.S. Government.
","Developing accurate predictive models of sensory neurons is vital to understanding sensory processing and brain computations. The current standard approach to modeling neurons is to start with simple models and to incrementally add interpretable features. An alternative approach is to start with a more complex model that captures responses accurately, and then probe the fitted model structure to understand the neural computations. Here, we show that a multitask recurrent neural network (RNN) framework provides the flexibility necessary to model complex computations of neurons that cannot be captured by previous methods. Specifically, multilayer recurrent neural networks that share features across neurons outperform generalized linear models (GLMs) in predicting the spiking responses of parasol ganglion cells in the primate retina to natural images. The networks achieve good predictive performance given surprisingly small amounts of experimental training data. Additionally, we present a novel GLM-RNN hybrid model with separate spatial and temporal processing components which provides insights into the aspects of retinal processing better captured by the recurrent neural networks.",ICLR 2017 conference submission,True,,"This paper explores the ability of nonlinear recurrent neural networks to account for neural response properties that have otherwise eluded the ability of other models.  A multilayer rnn is trained to imitate the stimulus-response mapping measured from actual retinal ganglion cells in response to a sequence of natural images.  The rnn performs significantly better, especially in accounting for transient responses, than conventional LN/GLM models.

This work is an important step in understanding the nonlinear response properties of visual neurons.  Recent results have shown that the responses of even retinal ganglion cells in response to natural movies are difficult to explain in terms of standard receptive field models.  So this presents an important challenge to the field.  If we even had *a* model that works, it would be a starting point.  So this work should be seen in that light.  The challenge now of course is to tease apart what the rnn is doing.  Perhaps it could now be pruned and simplified to see what parts are critical to performance.  It would have been nice to see such an analysis.   Nevertheless this result is a good first start and I think important for people to know about.

I am a bit confused about what is being called a ""movie.""  My understanding is that it is essentially a sequence of unrelated images shown for 1 sec. each.  But then it is stated that the ""frame rate"" is 1/8.33 ms.  I think this must refer to the refresh rate of the monitor, right?   

I would guess that the deviations from the LN model are even stronger when you show actual dynamic natural scenes - i.e., real movies.  Here I would expect the rnn to have an even more profound effect, and potentially be much more informative.

---

This work is an important step in developing the tools for understanding the nonlinear response properties of visual neurons. The methods are sound and the results are meaningful. Reviewer 3 gave a much lower score than the other two reviewers because Rev 3 does not appreciate the improvement of prediction performance as an advance in itself. For understanding of the visual algorithms in the brain, however, prediction performance is the most critical success criterion. The paper provides convincing evidence that the approach is promising and likely to facilitate further advances towards achieving this long-term goal.
 
 I am confident enough to defend acceptance of this paper for a poster.

---

We have posted a new revision of the manuscript in which we changed the subset of cells for which we show results to better align with a previous study. The results remain unchanged. One of our example cells in Figure 3 was no longer in the criteria-passing subset so we are showing responses from a different OFF example cell. Additionally, we added a supplementary figure showing comparisons using a normalized log-likelihood metric, posted a link to a video of the stimulus, added “Responses” to the title, and made minor cosmetic changes to the figures.

---

This paper explores the ability of nonlinear recurrent neural networks to account for neural response properties that have otherwise eluded the ability of other models.  A multilayer rnn is trained to imitate the stimulus-response mapping measured from actual retinal ganglion cells in response to a sequence of natural images.  The rnn performs significantly better, especially in accounting for transient responses, than conventional LN/GLM models.

This work is an important step in understanding the nonlinear response properties of visual neurons.  Recent results have shown that the responses of even retinal ganglion cells in response to natural movies are difficult to explain in terms of standard receptive field models.  So this presents an important challenge to the field.  If we even had *a* model that works, it would be a starting point.  So this work should be seen in that light.  The challenge now of course is to tease apart what the rnn is doing.  Perhaps it could now be pruned and simplified to see what parts are critical to performance.  It would have been nice to see such an analysis.   Nevertheless this result is a good first start and I think important for people to know about.

I am a bit confused about what is being called a ""movie.""  My understanding is that it is essentially a sequence of unrelated images shown for 1 sec. each.  But then it is stated that the ""frame rate"" is 1/8.33 ms.  I think this must refer to the refresh rate of the monitor, right?   

I would guess that the deviations from the LN model are even stronger when you show actual dynamic natural scenes - i.e., real movies.  Here I would expect the rnn to have an even more profound effect, and potentially be much more informative.

---

This is a clearly written paper with a nice, if straightforward, result: RNNs can be good predictive models of neuron firing rates in the retina.

On the one hand, the primary scientific contribution seems to just be to confirm that this approach works. On this particular stimulus locked task the gains from using the RNN seemed relatively modest, and it hasn't yet taught us anything new about the biology.

On the other hand, this (along with the concurrent work of McIntosh et al.) is introducing neural network modeling to a field that isn't currently using it, and where it should prove very effective.

I think it would be very interesting to see the results of applying a framework like this one with LFP and other neurons as input and on a shorter discretization time scale.

I suspect followup work building on this proof of concept will be increasingly exciting.

Minor comments:
Sec 3.2:
I didn't understand the role of the 0.833 ms bins.
Use ""epoch"" throughout, rather than alternating between ""epoch"" and ""pass through data"".

Fig. 4 would be better with the x-axis on a log scale.

---

This paper fits models to spike trains of retinal ganglion cells that are driven by natural images. I think the title should thus include the word “activity” at the end for otherwise it is actually formally incorrect.

Anyhow, this paper proposes more specifically a recurrent network for this time series prediction and compares it to what seems to be the previous approach of a generalized linear model. Overall the stated paradigm is that when one can predict the spikes well then one can look into the model and learn how nature does it. 

In general the paper sounds plausible, though I am not convinced that I learned a lot. The results in figure 2 show that the RNN model can predict the spikes a bit better. So this is nice. But now what? You have shown that a more complicated model can produce better fits to the data, though there are of course still some variations to the real data. Your initial outline was that a better predictive model helps you to better understand the neural processing in the retina. So tell us what you learned. I am not a specialist of the retina, but I know that there are several layers and recurrencies in the retina, so I am not so surprised that the new model is better than the GLM. 

It seems that more complicated recurrent models such as LSTM do not improve the performance according to a statement in the paper. However, comparisons on this level are also difficult as a more complex models needs more data. Hence, I would actually expect that more layers and even a more detailed model of the retina could eventually improve the prediction even further. 
I was also a bit puzzled that all the neurons in the network share all the same parameters (weights). While the results show that these simplified models can capture a lot of the spike train characteristics, couldn’t a model with free parameters eventually outperform this one (with correspondingly more training data)?

---

This paper explores the ability of nonlinear recurrent neural networks to account for neural response properties that have otherwise eluded the ability of other models.  A multilayer rnn is trained to imitate the stimulus-response mapping measured from actual retinal ganglion cells in response to a sequence of natural images.  The rnn performs significantly better, especially in accounting for transient responses, than conventional LN/GLM models.

This work is an important step in understanding the nonlinear response properties of visual neurons.  Recent results have shown that the responses of even retinal ganglion cells in response to natural movies are difficult to explain in terms of standard receptive field models.  So this presents an important challenge to the field.  If we even had *a* model that works, it would be a starting point.  So this work should be seen in that light.  The challenge now of course is to tease apart what the rnn is doing.  Perhaps it could now be pruned and simplified to see what parts are critical to performance.  It would have been nice to see such an analysis.   Nevertheless this result is a good first start and I think important for people to know about.

I am a bit confused about what is being called a ""movie.""  My understanding is that it is essentially a sequence of unrelated images shown for 1 sec. each.  But then it is stated that the ""frame rate"" is 1/8.33 ms.  I think this must refer to the refresh rate of the monitor, right?   

I would guess that the deviations from the LN model are even stronger when you show actual dynamic natural scenes - i.e., real movies.  Here I would expect the rnn to have an even more profound effect, and potentially be much more informative.

---

This work is an important step in developing the tools for understanding the nonlinear response properties of visual neurons. The methods are sound and the results are meaningful. Reviewer 3 gave a much lower score than the other two reviewers because Rev 3 does not appreciate the improvement of prediction performance as an advance in itself. For understanding of the visual algorithms in the brain, however, prediction performance is the most critical success criterion. The paper provides convincing evidence that the approach is promising and likely to facilitate further advances towards achieving this long-term goal.
 
 I am confident enough to defend acceptance of this paper for a poster.

---

We have posted a new revision of the manuscript in which we changed the subset of cells for which we show results to better align with a previous study. The results remain unchanged. One of our example cells in Figure 3 was no longer in the criteria-passing subset so we are showing responses from a different OFF example cell. Additionally, we added a supplementary figure showing comparisons using a normalized log-likelihood metric, posted a link to a video of the stimulus, added “Responses” to the title, and made minor cosmetic changes to the figures.

---

This paper explores the ability of nonlinear recurrent neural networks to account for neural response properties that have otherwise eluded the ability of other models.  A multilayer rnn is trained to imitate the stimulus-response mapping measured from actual retinal ganglion cells in response to a sequence of natural images.  The rnn performs significantly better, especially in accounting for transient responses, than conventional LN/GLM models.

This work is an important step in understanding the nonlinear response properties of visual neurons.  Recent results have shown that the responses of even retinal ganglion cells in response to natural movies are difficult to explain in terms of standard receptive field models.  So this presents an important challenge to the field.  If we even had *a* model that works, it would be a starting point.  So this work should be seen in that light.  The challenge now of course is to tease apart what the rnn is doing.  Perhaps it could now be pruned and simplified to see what parts are critical to performance.  It would have been nice to see such an analysis.   Nevertheless this result is a good first start and I think important for people to know about.

I am a bit confused about what is being called a ""movie.""  My understanding is that it is essentially a sequence of unrelated images shown for 1 sec. each.  But then it is stated that the ""frame rate"" is 1/8.33 ms.  I think this must refer to the refresh rate of the monitor, right?   

I would guess that the deviations from the LN model are even stronger when you show actual dynamic natural scenes - i.e., real movies.  Here I would expect the rnn to have an even more profound effect, and potentially be much more informative.

---

This is a clearly written paper with a nice, if straightforward, result: RNNs can be good predictive models of neuron firing rates in the retina.

On the one hand, the primary scientific contribution seems to just be to confirm that this approach works. On this particular stimulus locked task the gains from using the RNN seemed relatively modest, and it hasn't yet taught us anything new about the biology.

On the other hand, this (along with the concurrent work of McIntosh et al.) is introducing neural network modeling to a field that isn't currently using it, and where it should prove very effective.

I think it would be very interesting to see the results of applying a framework like this one with LFP and other neurons as input and on a shorter discretization time scale.

I suspect followup work building on this proof of concept will be increasingly exciting.

Minor comments:
Sec 3.2:
I didn't understand the role of the 0.833 ms bins.
Use ""epoch"" throughout, rather than alternating between ""epoch"" and ""pass through data"".

Fig. 4 would be better with the x-axis on a log scale.

---

This paper fits models to spike trains of retinal ganglion cells that are driven by natural images. I think the title should thus include the word “activity” at the end for otherwise it is actually formally incorrect.

Anyhow, this paper proposes more specifically a recurrent network for this time series prediction and compares it to what seems to be the previous approach of a generalized linear model. Overall the stated paradigm is that when one can predict the spikes well then one can look into the model and learn how nature does it. 

In general the paper sounds plausible, though I am not convinced that I learned a lot. The results in figure 2 show that the RNN model can predict the spikes a bit better. So this is nice. But now what? You have shown that a more complicated model can produce better fits to the data, though there are of course still some variations to the real data. Your initial outline was that a better predictive model helps you to better understand the neural processing in the retina. So tell us what you learned. I am not a specialist of the retina, but I know that there are several layers and recurrencies in the retina, so I am not so surprised that the new model is better than the GLM. 

It seems that more complicated recurrent models such as LSTM do not improve the performance according to a statement in the paper. However, comparisons on this level are also difficult as a more complex models needs more data. Hence, I would actually expect that more layers and even a more detailed model of the retina could eventually improve the prediction even further. 
I was also a bit puzzled that all the neurons in the network share all the same parameters (weights). While the results show that these simplified models can capture a lot of the spike train characteristics, couldn’t a model with free parameters eventually outperform this one (with correspondingly more training data)?",,,4.0,,4.0,6.333333333333333,5.0,,4.333333333333333,4.0,
340,"Source file: 340.pdf

ABSTRACT
We study the problem of transferring a sample in one domain to an analog sample in another domain. Given two related domains, S and T , we would like to learn a generative function G that maps an input sample from S to the domain T , such that the output of a given representation function f , which accepts inputs in either domains, would remain unchanged. Other than f , the training data is unsupervised and consist of a set of samples from each domain, without any mapping between them. The Domain Transfer Network (DTN) we present employs a compound loss function that includes a multiclass GAN loss, an f preserving component, and a regularizing component that encourages G to map samples from T to themselves. We apply our method to visual domains including digits and face images and demonstrate its ability to generate convincing novel images of previously unseen entities, while preserving their identity.

We study the problem of transferring a sample in one domain to an analog sample in another domain. Given two related domains, S and T , we would like to learn a generative function G that maps an input sample from S to the domain T , such that the output of a given representation function f , which accepts inputs in either domains, would remain unchanged. Other than f , the training data is unsupervised and consist of a set of samples from each domain, without any mapping between them. The Domain Transfer Network (DTN) we present employs a compound loss function that includes a multiclass GAN loss, an f preserving component, and a regularizing component that encourages G to map samples from T to themselves. We apply our method to visual domains including digits and face images and demonstrate its ability to generate convincing novel images of previously unseen entities, while preserving their identity.

1 INTRODUCTION
Humans excel in tasks that require making analogies between distinct domains, transferring elements from one domain to another, and using these capabilities in order to blend concepts that originated from multiple source domains. Our experience tells us that these remarkable capabilities are developed with very little, if any, supervision that is given in the form of explicit analogies.
Recent achievements replicate some of these capabilities to some degree: Generative Adversarial Networks (GANs) are able to convincingly generate novel samples that match that of a given training set; style transfer methods are able to alter the visual style of images; domain adaptation methods are able to generalize learned functions to new domains even without labeled samples in the target domain and transfer learning is now commonly used to import existing knowledge and to make learning much more efficient.
These capabilities, however, do not address the general analogy synthesis problem that we tackle in this work. Namely, given separated but otherwise unlabeled samples from domains S and T and a perceptual function f , learn a mapping G : S → T such that f(x) ∼ f(G(x)). In order to solve this problem, we make use of deep neural networks of a specific structure in which the function G is a composition of the input function f and a learned function g. A compound loss that integrates multiple terms is used. One term is a Generative Adversarial Network (GAN) term that encourages the creation of samples G(x) that are indistinguishable from the training samples of the target domain, regardless of x ∈ S or x ∈ T . The second loss term enforces that for every x in the source domain training set, ||f(x)− f(G(x))|| is small. The third loss term is a regularizer that encourages G to be the identity mapping for all x ∈ T . The type of problems we focus on in our experiments are visual, although our methods are not limited to visual or even to perceptual tasks. Typically, f would be a neural network representation that is taken as the activations of a network that was trained, e.g., by using the cross entropy loss, in order to classify or to capture identity.
As a main application challenge, we tackle the problem of emoji generation for a given facial image. Despite a growing interest in emoji and the hurdle of creating such personal emoji manually, no system has been proposed, to our knowledge, that can solve this problem. Our method is able to
produce face emoji that are visually appealing and capture much more of the facial characteristics than the emoji created by well-trained human annotators who use the conventional tools.

2 RELATED WORK
As far as we know, the domain transfer problem we formulate is novel despite being ecological (i.e., appearing naturally in the real-world), widely applicable, and related to cognitive reasoning (Fauconnier & Turner, 2003). In the discussion below, we survey recent GAN work, compare our work to the recent image synthesis work and make links to unsupervised domain adaptation.
GAN (Goodfellow et al., 2014) methods train a generator network G that synthesizes samples from a target distribution given noise vectors. G is trained jointly with a discriminator network D, which distinguishes between samples generated by G and a training set from the target distribution. The goal of G is to create samples that are classified by D as real samples.
While originally proposed for generating random samples, GANs can be used as a general tool to measure equivalence between distributions. Specifically, the optimization of D corresponds to taking the most discriminative D achievable, which in turn implies that the indistinguishability is true for every D. Formally, Ganin et al. (2016) linked the GAN loss to the H-divergence between two distributions of Ben-david et al. (2006).
The generative architecture that we employ is based on the successful architecture of Radford et al. (2015). There has recently been a growing concern about the uneven distribution of the samples generated by G – that they tend to cluster around a set of modes in the target domain (Salimans et al., 2016). In general, we do not observe such an effect in our results, due to the requirement to generate samples that satisfy specific f -constancy criteria.
A few contributions (“Conditional GANs”) have employed GANs in order to generate samples from a specific class (Mirza & Osindero, 2014), or even based on a textual description (Reed et al., 2016). When performing such conditioning, one can distinguish between samples that were correctly generated but fail to match the conditional constraint and samples that were not correctly generated. This is modeled as a ternary discriminative function D (Reed et al., 2016; Brock et al., 2016).
The recent work by Dosovitskiy & Brox (2016), has shown promising results for learning to map embeddings to their pre-images, given input-target pairs. Like us, they employ a GAN as well as additional losses in the feature- and the pixel-space. Their method is able to invert the midlevel activations of AlexNet and reconstruct the input image. In contrast, we solve the problem of unsupervised domain transfer and apply the loss terms in different domains: pixel loss in the target domain, and feature loss in the source domain.
Another class of very promising generative techniques that has recently gained traction is neural style transfer. In these methods, new images are synthesized by minimizing the content loss with respect to one input sample and the style loss with respect to one or more input samples. The content loss is typically the encoding of the image by a network training for an image categorization task, similar to our work. The style loss compares the statistics of the activations in various layers of the neural network. We do not employ style losses in our method. While initially style transfer was obtained by a slow optimization process (Gatys et al., 2016), recently, the emphasis was put on feed-forward methods (Ulyanov et al., 2016; Johnson et al., 2016).
There are many links between style transfer and our work: both are unsupervised and generate a sample under f constancy given an input sample. However, our work is much more general in its scope and does not rely on a predefined family of perceptual losses. Our method can be used in order to perform style transfer, but not the other way around. Another key difference is that the current style transfer methods are aimed at replicating the style of one or several images, while our work considers a distribution in the target space. In many applications, there is an abundance of unlabeled data in the target domain T , which can be modeled accurately in an unsupervised manner.
Given the impressive results of recent style transfer work, in particular for face images, one might get the false impression that emoji are just a different style of drawing faces. By way of analogy, this claim is similar to stating that a Siamese cat is a Labrador in a different style. Emoji differ from facial photographs in both content and style. Style transfer can create visually appealing face images; However, the properties of the target domain are compromised.
In the computer vision literature, work has been done to automatically generate sketches from images, see Kyprianidis et al. (2013) for a survey. These systems are able to emphasize image edges and facial features in a convincing way. However, unlike our method, they require matching pairs of samples, and were not shown to work across two distant domains as in our method. Due to the lack of supervised training data, we did not try to apply such methods to our problems. However, one can assume that if such methods were appropriate for emoji synthesis, automatic face emoji services would be available.
Unsupervised domain adaptation addresses the following problem: given a labeled training set in S × Y , for some target space Y , and an unlabeled set of samples from domain T , learn a function h : T → Y (Chen et al., 2012; Ganin et al., 2016). One can solve the sample transfer problem (our problem) using domain adaptation and vice versa. In both cases, the solution is indirect. In order to solve domain adaptation using domain transfer, one would learn a function from S to Y and use it as the input method of the domain transfer algorithm in order to obtain a map from S to T 1. The training samples could then be transferred to T and used to learn a classifier there.
In the other direction, given the function f , one can invert f in the domain T by generating training samples (f(x), x) for x ∈ T and learn from them a function h from f(T ) = {f(x)|x ∈ T} to T . Domain adaptation can then be used in order to map f(S) = {f(x)|x ∈ S} to T , thus achieving domain transfer. Based on the work by Zhmoginov & Sandler (2016), we expect that h, even in the target domain of emoji, will be hard to learn, making this solution hypothetical at this point.

3 A BASELINE PROBLEM FORMULATION
Given a set s of unlabeled samples in a source domain S sampled i.i.d according to some distribution DS , a set of samples in the target domain t ⊂ T sampled i.i.d from distribution DT , a function f from the domain S ∪ T , some metric d, and a weight α, we wish to learn a function G : S → T that minimizes the combined risk R = RGAN + αRCONST, which is comprised of
RGAN = max D Ex∼DS log[1−D(G(x))] + Ex∼DT log[D(x)], (1)
where D is a binary classification function from T , D(x) the probability of the class 1 it assigns for a sample x ∈ T , and
RCONST = Ex∼DS d(f(x), f(G(x))) (2)
The first term is the adversarial risk, which requires that for every discriminative function D, the samples from the target domain would be indistinguishable from the samples generated by G for samples in the source domain. An adversarial risk is not the only option. An alternative term that does not employ GANs would directly compare the distribution DT to the distribution of G(x) where x ∼ DS , e.g., by using KL-divergence. The second term is the f -constancy term, which requires that f is invariant under G. In practice, we have experimented with multiple forms of d including Mean Squared Error (MSE) and cosine distance, as well as other variants including metric learning losses (hinge) and triplet losses. The performance is mostly unchanged, and we report results using the simplest MSE solution.
Similarly to other GAN formulations, one can minimize the loss associated with the risk R over G, while maximizing it over D, where G and D are deep neural networks, and the expectations in R are replaced by summations over the corresponding training sets. However, this baseline solution, as we will show experimentally, does not produce desirable results.
1The function trained this way would be more accurate on S than on T . This asymmetry is shared with all experiments done in this work.

4 THE DOMAIN TRANSFER NETWORK
We suggest to employ a more elaborate architecture that contains two high level modifications. First, we employ f(x) as the baseline representation to the function G. Second, we consider, during training, the generated samples G(x) for x ∈ t. The first change is stated as G = g ◦ f , for some learned function g. By applying this, we focus the learning effort ofG on the aspects that are most relevant toRCONST. In addition, in most applications, f is not as accurate on T as it on S. The composed function, which is trained on samples from both S and T , adds layers on top of f , which adapt it.
The second change alters the form of LGAN, making it multiclass instead of binary. It also introduces a new term LTID that requires G to be the identity matrix on samples from T . Taken together and written in terms of training loss, we now have two losses LD and LG = LGANG + αLCONST + βLTID + γLTV, for some weights α, β, γ, where
LD = − ∑ x∈s logD1(g(f(x)))− ∑ x∈t logD2(g(f(x)))− ∑ x∈t logD3(x) (3)
LGANG = − ∑ x∈s logD3(g(f(x)))− ∑ x∈t logD3(g(f(x))) (4)
LCONST = ∑ x∈s d(f(x), f(g(f(x)))) (5)
LTID = ∑ x∈t d2(x,G(x)) (6)
and where D is a ternary classification function from the domain T to 1, 2, 3, and Di(x) is the probability it assigns to class i = 1, 2, 3 for an input sample x, and d2 is a distance function in T . During optimization, LG is minimized over g and LD is minimized over D. See Fig. 1 for an illustration of our method.
Eq. 3 and 4 make sure that the generated analogy, i.e., the output ofG, is in the target space T . Since D is ternary and can therefore confuse classes in more than one way, this role, which is captured by Eq. 1 in the baseline formulation, is split into two. However, the two equations do not enforce any similarity between the source sample x and the generated G(x). This is done by Eq. 5 and 6: Eq. 5 enforces f -constancy for x ∈ S, while Eq. 6 enforces that for samples x ∈ T , which are already in the target space, G is the identity mapping. The latter is a desirable behavior, e.g., for the cartooning task, given an input emoji, one would like it to remain constant under the mapping of G. It can also be seen as an autoencoder type of loss, applied only to samples from T . The experiments reported in Sec. 5 evaluate the contributions of LCONST and LTID and reveal that at least one of these is required, and that when employing only one loss, LCONST leads to a better performance than LTID.
The last loss, LTV is an anisotropic total variation loss (Rudin et al., 1992; Mahendran & Vedaldi, 2015), which is added in order to slightly smooth the resulting image. The loss is defined on the generated image z = [zij ] = G(x) as
LTV (z) = ∑ i,j ( (zi,j+1 − zij)2 + (zi+1,j − zij)2 )B 2 , (7)
where we employ B = 1.
In our work, MSE is used for both d and d2. We also experimented with replacing d2, which, in visual domains, compares images, with a second GAN. No noticeable improvement was observed. Throughout the experiments, the adaptive learning rate method Adam by Kingma & Ba (2016) is used as the optimization algorithm.

5 EXPERIMENTS
The Domain Transfer Network (DTN) is evaluated in two application domains: digits and face images. In the first domain, we transfer images from the Street View House Number (SVHN) dataset of Netzer et al. (2011) to the domain of the MNIST dataset by LeCun & Cortes (2010). In
the face domain, we transfer a set of random and unlabeled face images to a space of emoji images. In both cases, the source and target domains differ considerably.

5.1 DIGITS: FROM SVHN TO MNIST
For working with digits, we employ the extra training split of SVHN, which contains 531,131 images for two purposes: learning the function f and as an unsupervised training set s for the domain transfer method. The evaluation is done on the test split of SVHN, comprised of 26,032 images. The architecture of f consists of four convolutional layers with 64, 128, 256, 128 filters respectively, each followed by max pooling and ReLU non-linearity. The error on the test split is 4.95%. Even tough this accuracy is far from the best reported results, it seems to be sufficient for the purpose of domain transfer. Within the DTN, f maps a 32 × 32 RGB image to the activations of the last convolutional layer of size 128× 1× 1 (post a 4× 4 max pooling and before the ReLU). In order to apply f on MNIST images, we replicate the grayscale image three times.
The set t contains the test set of the MNIST dataset. For supporting quantitative evaluation, we have trained a classifier on the train set of the MNIST dataset, consisting of the same architecture as f . The accuracy of this classifier on the test set approaches perfect performance at 99.4% accuracy, and is, therefore, trustworthy as an evaluation metric. In comparison, the network f , achieves 76.08% accuracy on t.
Network g, inspired by Radford et al. (2015), maps SVHN-trained f ’s 128D representations to 32× 32 grayscale images. g employs four blocks of deconvolution, batch-normalization, and ReLU, with a hyperbolic tangent terminal. The architecture ofD consists of four batch-normalized convolutional layers and employs ReLU. See Radford et al. (2015) for more details on the networks architecture. In the digit experiments, the results were obtained with the tradeoff hyperparamemters α = β = 15. We did not observe a need to add a smoothness term and the weight of LTV was set to γ = 0.
Despite not being very accurate on both domains (and also considerably worse than the SVHN state of the art), we were able to achieve visually appealing domain transfer, as shown in Fig. 2(a). In order to evaluate the contribution of each of the method’s components, we have employed the MNIST network on the set of samples G(sTEST ) = {G(x)|x ∈ sTEST }, using the true SVHN labels of the test set.
We first compare to the baseline method of Sec. 3, where the generative function, which works directly with samples in S, is composed out of a few additional layers at the bottom of G. The results, shown in Tab. 1, demonstrate that DTN has a clear advantage over the baseline method. In addition, the contribution of each one of the terms in the loss function is shown in the table. The regularization term LTID seems less crucial than the constancy term. However, at least one of them is required in order to obtain good performance. The GAN constraints are also important. Finally, the inclusion of f within the generator function G has a dramatic influence on the results.
As explained in Sec. 2, domain transfer can be used in order to perform unsupervised domain adaptation. For this purposes, we transformed the set s to the MNIST domain (as above), and using the true labels of s employed a simple nearest neighbor classifier there. The choice of classifier was
to emphasize the simplicity of the approach; However, the constraints of the unsupervised domain transfer problem would be respected for any classifier trained on G(s). The results of this experiment are reported in Tab. 2, which shows a clear advantage over the state of the art method of Ganin et al. (2016). This is true both when transferring the samples of the set s and when transferring the test set of SVHN, which is much smaller and was not seen during the training of the DTN.

5.1.1 UNSEEN DIGITS
Another set of experiments was performed in order to study the ability of the domain transfer network to overcome the omission of a class of samples. This type of ablation can occur in the source or the target domain, or during the training of f and can help us understand the importance of each of these inputs. The results are shown visually in Fig. 3, and qualitatively in Tab. 3, based on the accuracy of the MNIST classifier only on the transferred samples from the test set of SVHN that belong to class ‘3’.
It is evident that not including the class in the source domain is much less detrimental than eliminating it from the target domain. This is the desirable behavior: never seeing any ‘3’-like shapes in t, the generator should not generate such samples. Results are better when not observing ‘3’ in both s, t than when not seeing it only in t since in the latter case, G learns to map source samples of ‘3’ to target images of other classes.

5.2 FACES: FROM PHOTOS TO EMOJI
For face images, we use a set s of one million random images without identity information. The set t consists of assorted facial avatars (emoji) created by an online service (bitmoji.com). The emoji images were processed by a fully automatic process that localizes, based on a set of heuristics, the center of the irides and the tip of the nose. Based on these coordinates, the emoji were centered and scaled into 152× 152 RGB images. As the function f , we employ the representation layer of the DeepFace network Taigman et al. (2014). This representation is 256-dimensional and was trained on a labeled set of four million images that does not intersect the set s. Network D takes 152 × 152 RGB images (either natural or scaled-up emoji) and consists of 6 blocks, each containing a convolution with stride 2, batch normalization, and a leaky ReLU with a parameter of 0.2. Network g maps f ’s 256D representations to 64× 64 RGB images through a network with 5 blocks, each consisting of an upscaling convolution, batch-normalization and ReLU. Adding 1 × 1 convolution to each block resulted in lower LCONST training errors, and made g 9-layers deep. We set α = 100, β = 1, γ = 0.05 as the tradeoff hyperparameters within LG via validation. As expected, higher values of α resulted in better f -constancy, however introduced artifacts such as general noise or distortions. The network was trained for 3 epochs, the point where no further reduction of validation error was observed on LCONST.
In order to upscale the 64 × 64 output to print quality, we used the method of Dong et al. (2015), which was shown to work well on art. We did not retrain this network for our application, and apply the published one to the final output of our method after its training was finished. Results without this upscale are shown, for comparison, in Appendix C.
Comparison With Human Annotators For evaluation purposes only, a team of professional annotators manually created an emoji, using a web service, for 118 random images from the CelebA dataset (Yang et al., 2015). Fig. 4 shows side by side samples of the original image, the human generated emoji and the emoji generated by the learned generator function G. As can be seen, the automatically generated emoji tend to be more informative, albeit less restrictive than the ones created manually.
In order to evaluate the identifiability of the resulting emoji, we have collected a second example for each identity in the set of 118 CelebA images and a set s′ of 100,000 random face images, which were not included in s. We then employed the VGG face CNN descriptor of Parkhi et al. (2015) in order to perform retrieval as follows. For each image x in our manually annotated set, we create a gallery s′ ∪ x′, where x′ is the other image of the person in x. We then perform retrieval using the VGG face descriptor using either the manually created emoji or G(x) as probe.
The VGG network is used in order to avoid a bias that might be caused by using f both for training the DTN and for evaluation. The results are reported in Tab. 4. As can be seen, the emoji generated by G are much more discriminative than the emoji created manually and obtain a median rank of 16 in cross-domain identification out of 105 distractors.
Multiple Images Per Person We evaluate the visual quality that is obtained per person and not just per image, by testing DTN on the Facescrub dataset (Ng & Winkler, 2014). For each person p, we considered the set of their images Xp, and selected the emoji that was most similar to their
source image: argmin x∈Xp ||f(x)− f(G(x))|| (8)
This simple heuristic seems to work well in practice; The general problem of mapping a set X ⊂ S to a single output in T is left for future work. Fig. 2(b) contains several examples from the Facescrub dataset. For the complete set of identities, see Appendix A.
Transferring both identity and expression We also experimented with multiple expressions. As it turns out the face identification network f encodes enough expression information to support a successful transfer of both identity as well as expression, see Appendix B.
Network Visualization The obtained mapping g can serve as a visualization tool for studying the properties of the face representation. This is studied in Appendix D by computing the emoji generated for the standard basis of R256. The resulting images present a large amount of variability, indicating that g does not present a significant mode effect.

5.3 STYLE TRANSFER AS A SPECIFIC DOMAIN TRANSFER TASK
Fig. 5(a-c) demonstrates that neural style transfer Gatys et al. (2016) cannot solve the photo to emoji transfer task in a convincing way. The output image is perhaps visually appealing; However, it does not belong to the space t of emoji. Our result are given in Fig. 5(d) for comparison. Note that DTN is able to fix the missing hair in the image.
Domain transfer is more general than style transfer in the sense that we can perform style transfer using a DTN. In order to show this, we have transformed, using the method of Johnson et al. (2016), the training images of CelebA based on the style of a single image (shown in Fig. 5(e)). The original photos were used as the set s, and the transformed images were used as t. Applying DTN, using face representation f , we obtained styled face images such as the one shown in the figure 5(f).

6 DISCUSSION AND LIMITATIONS
Asymmetry is central to our work. Not only does our solution handle the two domains S and T differently, the function f is unlikely to be equally effective in both domains since in most practical cases, f would be trained on samples from one domain. While an explicit domain adaptation step can be added in order to make f more effective on the second domain, we found it to be unnecessary. Adaptation of f occurs implicitly due to the application of D downstream.
Using the same function f , we can replace the roles of the two domains, S and T . For example, we can synthesize an SVHN image that resembles a given MNIST image, or synthesize a face that matches an emoji. As expected, this yields less appealing results due to the asymmetric nature of f and the lower information content in these new source domains, see Appendix E.
Domain transfer, as an unsupervised method, could prove useful across a wide variety of computational tasks. Here, we demonstrate the ability to use domain transfer in order to perform unsupervised domain adaptation. While this is currently only shown in a single experiment, the simplicity of performing domain adaptation and the fact that state of the art results were obtained effortlessly with a simple nearest neighbor classifier suggest it to be a promising direction for future research.

A FACESCRUB DATASET GENERATIONS
In Fig. 6 we show the full set of identities of the Facescrub dataset, and their corresponding generated emoji.

B TRANSFERRING NON-IDENTITY DATA
f may encode, in addition to identity, other data that is desirable to transfer. In the example of faces, this information might include expression, facial hair, glasses, pose, etc. In order to transfer such information, it is important that the set of samples in the target domain t present variability along the desirable dimensions. Otherwise, the GAN applied in the target domain (Eq. 4) would maintain these dimensions fixed. The set t employed throughout our experiments in Sec. 5.2 was constructed by sampling emoji of neutral expression. To support a smiling expression for example, we simply added to set t random smiling emoji and re-trained the DTN. The results, presented in Fig. 7, demonstrate that f contains expression information in addition to identity information, and that this information is enough in order to transfer smiling photos to smiling emoji.

C THE EFFECT OF SUPER-RESOLUTION
As mentioned in Sec. 5, in order to upscale the 64× 64 output to print quality, the method of Dong et al. (2015) is used. Fig. 8 shows the effect of applying this postprocessing step.

D THE BASIS ELEMENTS OF THE FACE REPRESENTATION
Fig. 9 depicts the face emoji generated by g for the standard basis of the face representation (Taigman et al., 2014), viewed as the vector space R256.

E DOMAIN TRANSFER IN THE REVERSE DIRECTION
For completion, we present, in Fig. 10 results obtained by performing domain transfer using DTNs in the reverse direction of the one reported in Sec. 5.
","We study the problem of transferring a sample in one domain to an analog sample in another domain. Given two related domains, S and T , we would like to learn a generative function G that maps an input sample from S to the domain T , such that the output of a given representation function f , which accepts inputs in either domains, would remain unchanged. Other than f , the training data is unsupervised and consist of a set of samples from each domain, without any mapping between them. The Domain Transfer Network (DTN) we present employs a compound loss function that includes a multiclass GAN loss, an f preserving component, and a regularizing component that encourages G to map samples from T to themselves. We apply our method to visual domains including digits and face images and demonstrate its ability to generate convincing novel images of previously unseen entities, while preserving their identity.",ICLR 2017 conference submission,True,,"Update: After reading the rebuttal comments and the revised paper, I'm leaving the rating as it was before.

This paper proposes an unsupervised algorithm for transferring samples from one domain to another (related) domain under the constraint that some predefined f returns same result for the input and the result.

Pros:
1. The paper presents an interesting idea of comparing samples from different domains using a fixed perceptual function f.

2. The proposed method produces visually appealing results on several datasets

3. The authors demonstrate how their approach can be used for domain adaptation and obtain improved results on the SVHN->MNIST task

4. The paper is well-written and easy to read

Cons:
1. The novelty of the method is relatively minor (I consider f-constancy term as the main contribution)

2. It feels like the proposed approach would break for more dissimilar domains. The method relies on a fixed f which is trained on the source domain. This f can potentially drop information important for obtaining 1) better reconstructions in the target domain  2) more tightly related x and g(f(x)). I think the authors should consider either training all the modules in the model end-to-end or incorporating target samples into the training of f.

3. A single domain adaptation experiment is definitely not enough to consider the proposed method as a universal alternative to the existing DA approaches.

I would also like to point out that using super-resolved outputs as opposed to the actual model’s outputs can produce a false impression of the visual quality of the transferred samples. I’d suggest moving original outputs from the appendix into the main part.

---

In section 5.1, you said that you employ the extra training split of SVHN for two purposes: learning the function f and as a unsupervised training set s. So can I say that, in Table 2, your results are based on SVHN 'extra' training split as s domain traning set? But it is different from other methods since SVHN dataset includes train set, test set and extra training split. As far as I know, other methods are using train set as the s domain unsupervised tranining data.

---

The authors propose a application of GANs to map images to new domains with no labels. E.g., an MNIST 3 is used to generate a SVHN 3. Ablation analysis is given to help understand the model. The results are (subjectively) impressive and the approach could be used for cross-domain transfer, an important problem. All in all, a strong paper.

---

This paper presents an unsupervised domain transfer from the image of domain S to the image of domain T. 
It was really refreshing that this conversion was possible without any mapping data. 
For example, in the paper, the model can transfer the SVHN image '3' to the MNIST image '3' without the mapping data.
The model can be roughly divided into GAN and Content Extractor (f in the paper).

1. GAN
During training, the discriminator sees the mnist image and learns to determine it as a real image. 
And with GAN loss, the generator learns to get the mnist image as output when it receives an svhn image as input to deceive the discriminator.

2. Content Extractor
If the model use only GAN loss, the content in the image may not be retained even if the domain is changed.
For example, the generator may convert the svhn image '3' to the mnist image '2' to deceive the discriminator.
In this paper, authors introduce a new function called 'f' to maintain the content.
The generator includes f and generates a fake mnist image when it receives an svhn image as input.
The original svhn image and the generated fake mnist image are put back into f.
Then additional loss function is set so that the resulting values ​​are the same.
Here, f is learning to extract content regardless of domain.


I felt very fresh in this paper so i implemented this paper myself.
Here is the code I implemented.

---

We thank the reviewers for their time and insights. All 3 reviewers seem to agree that the work is interesting, well-written and presents extensive experiments. R1 & R2 both note the f-constancy as a novelty of this work. Also, the fact that the method does not require training pairs for the two domains is noted by R3 as a major contribution, which “could be impactful in broad problem context”. R2 & R3 both agree that the output generations are visually appealing. We have no factual dispute with the reviewers and replied to each individually below.

The open review discussion has been extremely beneficial to us and the paper has been revised in order to address all actionable items raised throughout the review period. We thank the reviewers for their thoughtful and constructive reviews and all other community members who shared their comments. During the review period, the work has already been cited several times, reimplemented on github, and drawn considerable attention.

---

Following the request of the reviewing team, we have just uploaded a new version of our manuscript, which includes expression preserving experiments. 
The new experiments are in Appendix B. 

In order to provide a quick way to track the changes from the original submission, we color all modifications in red. 

Thank you for the extremely useful feedback.

---

This paper presents an unsupervised image transformation method that maps a sample from source domain to target domain. The major contribution lies in that it does not require aligned training pairs from two domains. The model is based on GANs. To make it work in the unsupervised setting, this paper decomposes the generation function into two modules: an encoder that identify a common feature space between two domains and an decoder that generates samples in the target domain. To avoid trivial solutions, this paper proposed two additional losses that penalize 1) the feature difference between a source sample and its transformed sample and 2) the pixel difference between a target sample and its re-generated sample. This paper presents extensive experiments on transferring SVHN digit images to MNIST style and transferring face images to emoji style. 
+The proposed learning method enables unsupervised domain transfer that could be impactful in broad problem contexts. 
+This paper presents careful ablation studies to analyze the effects of different components of the system, which is helpful for understanding the paper. 
+The transferred images are visually impressive and quantitative results also show the image identities are preserved across domains to some degree. 
-It will be more interesting to show results in other domains such as texts and images. 
-In addition to the face identities, it is also of great interest to analyze how well the facial attributes are preserved when mapping to target domain.

---

Update: thank you for running more experiments, and add more explanations in the manuscript. They addressed most of my concerns, so I updated the score accordingly. 


The work aims at learning a generative function G that can maps input from source domain to the target domain, such that a given representation function f remain unchanged accepting inputs from either domain. The criteria is termed f-constancy. The proposed method is evaluated on two visual domain adaptation tasks. 

The paper is relatively easy to follow, and the authors provided quite extensive experimental results on the two datasets. f-constancy is the main novelty of the work. 

It seems counter-intuitive to force the function G to be of g o f, i.e., starting from a restricted function f which might have already lost information. As in the face dataset, f is learned to optimize the performance of certain task on some external dataset. It is not clear if an input from the source or target domain can be recovered from applying G as in equation (5) and (6). Also, the f function is learned with a particular task in mind. As in the two experiments, the representation function f is learned to identify the digits in the source SVHN dataset or the identity of some face dataset.  As a result, the procedure has to be repeated if we were to perform domain adaptation for the same domains but for different tasks, such as recognizing expressions instead of identity. 

Do the authors have insight on why the baseline method proposed in equation (1) and (2) perform so poorly?

Figure 5 shows some visual comparison between style transfer and the proposed method. It is not clear though which method is better. Will it be possible to apply style transfer to generate emojis from photos and repeat the experiments shown in table 4?

---

Update: After reading the rebuttal comments and the revised paper, I'm leaving the rating as it was before.

This paper proposes an unsupervised algorithm for transferring samples from one domain to another (related) domain under the constraint that some predefined f returns same result for the input and the result.

Pros:
1. The paper presents an interesting idea of comparing samples from different domains using a fixed perceptual function f.

2. The proposed method produces visually appealing results on several datasets

3. The authors demonstrate how their approach can be used for domain adaptation and obtain improved results on the SVHN->MNIST task

4. The paper is well-written and easy to read

Cons:
1. The novelty of the method is relatively minor (I consider f-constancy term as the main contribution)

2. It feels like the proposed approach would break for more dissimilar domains. The method relies on a fixed f which is trained on the source domain. This f can potentially drop information important for obtaining 1) better reconstructions in the target domain  2) more tightly related x and g(f(x)). I think the authors should consider either training all the modules in the model end-to-end or incorporating target samples into the training of f.

3. A single domain adaptation experiment is definitely not enough to consider the proposed method as a universal alternative to the existing DA approaches.

I would also like to point out that using super-resolved outputs as opposed to the actual model’s outputs can produce a false impression of the visual quality of the transferred samples. I’d suggest moving original outputs from the appendix into the main part.

---

Following the ongoing discussion with the reviewing team as well as with other readers, we have just uploaded a new version of our manuscript, which is aimed at improving clarity. 
In order to provide a quick way to track the changes, we color all modifications in red. 
In addition, as mentioned below, we will soon share our open implementation in Torch. 
Thank you all for the extremely useful feedback.

---

Given the goal of transferring samples from source domain to target domain, I am wondering whether it is necessary to simultaneously train the model to reconstruct images in the target domain. Any ablation studies on this?

---

Hi, it is really interesting and I want to re-implement this work. But the paper doesn't provide much information about the configuration of the generator. For example, do you use a fully connected layer as the first layer? If true, what is the dimension?  How many filters of each deconvolution layer do you use ?  Thank you very much.

---

In section 5.1, you map SVHN-trained representation to 32*32 grayscale images, but the encoder f's input is three-channel, how do you solve this ? Why don't you transform RGB SVHN images into grayscale images or replicate the grayscale MNIST images three times ? I mean why don't you use the same image format for training f and the adversarial network?

---

Can the authors comment on what they are doing in terms of end-use/goal in addition to what we know from style transfer already?

---

This work is really interesting, but I find the network architecture and loss functions in this work are extremely similar to that in the work ""Generating Images with Perceptual Similarity Metrics based on Deep Networks"" by Alexey Dosoviskiy and Thomas Brox.  

The loss function in this work vs that of the latter: 
L_const is equal to L_feat;
L_tid is equall to L_img;
L_GANG is equal to L_adv.

However, I do not see the later's name in your reference paper list.  Besides, have you forgotten to print a 'log' before D_3(x) in your L_D loss  on page 4?

---

Update: After reading the rebuttal comments and the revised paper, I'm leaving the rating as it was before.

This paper proposes an unsupervised algorithm for transferring samples from one domain to another (related) domain under the constraint that some predefined f returns same result for the input and the result.

Pros:
1. The paper presents an interesting idea of comparing samples from different domains using a fixed perceptual function f.

2. The proposed method produces visually appealing results on several datasets

3. The authors demonstrate how their approach can be used for domain adaptation and obtain improved results on the SVHN->MNIST task

4. The paper is well-written and easy to read

Cons:
1. The novelty of the method is relatively minor (I consider f-constancy term as the main contribution)

2. It feels like the proposed approach would break for more dissimilar domains. The method relies on a fixed f which is trained on the source domain. This f can potentially drop information important for obtaining 1) better reconstructions in the target domain  2) more tightly related x and g(f(x)). I think the authors should consider either training all the modules in the model end-to-end or incorporating target samples into the training of f.

3. A single domain adaptation experiment is definitely not enough to consider the proposed method as a universal alternative to the existing DA approaches.

I would also like to point out that using super-resolved outputs as opposed to the actual model’s outputs can produce a false impression of the visual quality of the transferred samples. I’d suggest moving original outputs from the appendix into the main part.

---

In section 5.1, you said that you employ the extra training split of SVHN for two purposes: learning the function f and as a unsupervised training set s. So can I say that, in Table 2, your results are based on SVHN 'extra' training split as s domain traning set? But it is different from other methods since SVHN dataset includes train set, test set and extra training split. As far as I know, other methods are using train set as the s domain unsupervised tranining data.

---

The authors propose a application of GANs to map images to new domains with no labels. E.g., an MNIST 3 is used to generate a SVHN 3. Ablation analysis is given to help understand the model. The results are (subjectively) impressive and the approach could be used for cross-domain transfer, an important problem. All in all, a strong paper.

---

This paper presents an unsupervised domain transfer from the image of domain S to the image of domain T. 
It was really refreshing that this conversion was possible without any mapping data. 
For example, in the paper, the model can transfer the SVHN image '3' to the MNIST image '3' without the mapping data.
The model can be roughly divided into GAN and Content Extractor (f in the paper).

1. GAN
During training, the discriminator sees the mnist image and learns to determine it as a real image. 
And with GAN loss, the generator learns to get the mnist image as output when it receives an svhn image as input to deceive the discriminator.

2. Content Extractor
If the model use only GAN loss, the content in the image may not be retained even if the domain is changed.
For example, the generator may convert the svhn image '3' to the mnist image '2' to deceive the discriminator.
In this paper, authors introduce a new function called 'f' to maintain the content.
The generator includes f and generates a fake mnist image when it receives an svhn image as input.
The original svhn image and the generated fake mnist image are put back into f.
Then additional loss function is set so that the resulting values ​​are the same.
Here, f is learning to extract content regardless of domain.


I felt very fresh in this paper so i implemented this paper myself.
Here is the code I implemented.

---

We thank the reviewers for their time and insights. All 3 reviewers seem to agree that the work is interesting, well-written and presents extensive experiments. R1 & R2 both note the f-constancy as a novelty of this work. Also, the fact that the method does not require training pairs for the two domains is noted by R3 as a major contribution, which “could be impactful in broad problem context”. R2 & R3 both agree that the output generations are visually appealing. We have no factual dispute with the reviewers and replied to each individually below.

The open review discussion has been extremely beneficial to us and the paper has been revised in order to address all actionable items raised throughout the review period. We thank the reviewers for their thoughtful and constructive reviews and all other community members who shared their comments. During the review period, the work has already been cited several times, reimplemented on github, and drawn considerable attention.

---

Following the request of the reviewing team, we have just uploaded a new version of our manuscript, which includes expression preserving experiments. 
The new experiments are in Appendix B. 

In order to provide a quick way to track the changes from the original submission, we color all modifications in red. 

Thank you for the extremely useful feedback.

---

This paper presents an unsupervised image transformation method that maps a sample from source domain to target domain. The major contribution lies in that it does not require aligned training pairs from two domains. The model is based on GANs. To make it work in the unsupervised setting, this paper decomposes the generation function into two modules: an encoder that identify a common feature space between two domains and an decoder that generates samples in the target domain. To avoid trivial solutions, this paper proposed two additional losses that penalize 1) the feature difference between a source sample and its transformed sample and 2) the pixel difference between a target sample and its re-generated sample. This paper presents extensive experiments on transferring SVHN digit images to MNIST style and transferring face images to emoji style. 
+The proposed learning method enables unsupervised domain transfer that could be impactful in broad problem contexts. 
+This paper presents careful ablation studies to analyze the effects of different components of the system, which is helpful for understanding the paper. 
+The transferred images are visually impressive and quantitative results also show the image identities are preserved across domains to some degree. 
-It will be more interesting to show results in other domains such as texts and images. 
-In addition to the face identities, it is also of great interest to analyze how well the facial attributes are preserved when mapping to target domain.

---

Update: thank you for running more experiments, and add more explanations in the manuscript. They addressed most of my concerns, so I updated the score accordingly. 


The work aims at learning a generative function G that can maps input from source domain to the target domain, such that a given representation function f remain unchanged accepting inputs from either domain. The criteria is termed f-constancy. The proposed method is evaluated on two visual domain adaptation tasks. 

The paper is relatively easy to follow, and the authors provided quite extensive experimental results on the two datasets. f-constancy is the main novelty of the work. 

It seems counter-intuitive to force the function G to be of g o f, i.e., starting from a restricted function f which might have already lost information. As in the face dataset, f is learned to optimize the performance of certain task on some external dataset. It is not clear if an input from the source or target domain can be recovered from applying G as in equation (5) and (6). Also, the f function is learned with a particular task in mind. As in the two experiments, the representation function f is learned to identify the digits in the source SVHN dataset or the identity of some face dataset.  As a result, the procedure has to be repeated if we were to perform domain adaptation for the same domains but for different tasks, such as recognizing expressions instead of identity. 

Do the authors have insight on why the baseline method proposed in equation (1) and (2) perform so poorly?

Figure 5 shows some visual comparison between style transfer and the proposed method. It is not clear though which method is better. Will it be possible to apply style transfer to generate emojis from photos and repeat the experiments shown in table 4?

---

Update: After reading the rebuttal comments and the revised paper, I'm leaving the rating as it was before.

This paper proposes an unsupervised algorithm for transferring samples from one domain to another (related) domain under the constraint that some predefined f returns same result for the input and the result.

Pros:
1. The paper presents an interesting idea of comparing samples from different domains using a fixed perceptual function f.

2. The proposed method produces visually appealing results on several datasets

3. The authors demonstrate how their approach can be used for domain adaptation and obtain improved results on the SVHN->MNIST task

4. The paper is well-written and easy to read

Cons:
1. The novelty of the method is relatively minor (I consider f-constancy term as the main contribution)

2. It feels like the proposed approach would break for more dissimilar domains. The method relies on a fixed f which is trained on the source domain. This f can potentially drop information important for obtaining 1) better reconstructions in the target domain  2) more tightly related x and g(f(x)). I think the authors should consider either training all the modules in the model end-to-end or incorporating target samples into the training of f.

3. A single domain adaptation experiment is definitely not enough to consider the proposed method as a universal alternative to the existing DA approaches.

I would also like to point out that using super-resolved outputs as opposed to the actual model’s outputs can produce a false impression of the visual quality of the transferred samples. I’d suggest moving original outputs from the appendix into the main part.

---

Following the ongoing discussion with the reviewing team as well as with other readers, we have just uploaded a new version of our manuscript, which is aimed at improving clarity. 
In order to provide a quick way to track the changes, we color all modifications in red. 
In addition, as mentioned below, we will soon share our open implementation in Torch. 
Thank you all for the extremely useful feedback.

---

Given the goal of transferring samples from source domain to target domain, I am wondering whether it is necessary to simultaneously train the model to reconstruct images in the target domain. Any ablation studies on this?

---

Hi, it is really interesting and I want to re-implement this work. But the paper doesn't provide much information about the configuration of the generator. For example, do you use a fully connected layer as the first layer? If true, what is the dimension?  How many filters of each deconvolution layer do you use ?  Thank you very much.

---

In section 5.1, you map SVHN-trained representation to 32*32 grayscale images, but the encoder f's input is three-channel, how do you solve this ? Why don't you transform RGB SVHN images into grayscale images or replicate the grayscale MNIST images three times ? I mean why don't you use the same image format for training f and the adversarial network?

---

Can the authors comment on what they are doing in terms of end-use/goal in addition to what we know from style transfer already?

---

This work is really interesting, but I find the network architecture and loss functions in this work are extremely similar to that in the work ""Generating Images with Perceptual Similarity Metrics based on Deep Networks"" by Alexey Dosoviskiy and Thomas Brox.  

The loss function in this work vs that of the latter: 
L_const is equal to L_feat;
L_tid is equall to L_img;
L_GANG is equal to L_adv.

However, I do not see the later's name in your reference paper list.  Besides, have you forgotten to print a 'log' before D_3(x) in your L_D loss  on page 4?",,4.0,3.0,3.0,3.0,6.666666666666667,,,3.3333333333333335,5.0,3.0
350,"Authors: UNSUPERVISED LEARNING, William Lotter, Gabriel Kreiman
Source file: 350.pdf

ABSTRACT
While great strides have been made in using deep learning algorithms to solve supervised learning tasks, the problem of unsupervised learning — leveraging unlabeled examples to learn about the structure of a domain — remains a difficult unsolved challenge. Here, we explore prediction of future frames in a video sequence as an unsupervised learning rule for learning about the structure of the visual world. We describe a predictive neural network (“PredNet”) architecture that is inspired by the concept of “predictive coding” from the neuroscience literature. These networks learn to predict future frames in a video sequence, with each layer in the network making local predictions and only forwarding deviations from those predictions to subsequent network layers. We show that these networks are able to robustly learn to predict the movement of synthetic (rendered) objects, and that in doing so, the networks learn internal representations that are useful for decoding latent object parameters (e.g. pose) that support object recognition with fewer training views. We also show that these networks can scale to complex natural image streams (car-mounted camera videos), capturing key aspects of both egocentric movement and the movement of objects in the visual scene, and the representation learned in this setting is useful for estimating the steering angle. Altogether, these results suggest that prediction represents a powerful framework for unsupervised learning, allowing for implicit learning of object and scene structure.

1 INTRODUCTION
Many of the most successful current deep learning architectures for vision rely on supervised learning from large sets of labeled training images. While the performance of these networks is undoubtedly impressive, reliance on such large numbers of training examples limits the utility of deep learning in many domains where such datasets are not available. Furthermore, the need for large numbers of labeled examples stands at odds with human visual learning, where one or a few views of an object is often all that is needed to enable robust recognition of that object across a wide range of different views, lightings and contexts. The development of a representation that facilitates such abilities, especially in an unsupervised way, is a largely unsolved problem.
In addition, while computer vision models are typically trained using static images, in the real world, visual objects are rarely experienced as disjoint snapshots. Instead, the visual world is alive with movement, driven both by self-motion of the viewer and the movement of objects within the scene. Many have suggested that temporal experience with objects as they move and undergo transformations can serve as an important signal for learning about the structure of objects (Földiák, 1991; Softky, 1996; Wiskott & Sejnowski, 2002; George & Hawkins, 2005; Palm, 2012; O’Reilly et al., 2014; Agrawal et al., 2015; Goroshin et al., 2015a; Lotter et al., 2015; Mathieu et al., 2016; Srivastava et al., 2015; Wang & Gupta, 2015; Whitney et al., 2016). For instance, Wiskott and Sejnowski proposed “slow feature analysis” as a framework for exploiting temporal structure in video streams (Wiskott & Sejnowski, 2002). Their approach attempts to build feature representations that extract
Code and video examples can be found at: https://coxlab.github.io/prednet/
slowly-varying parameters, such as object identity, from parameters that produce fast changes in the image, such as movement of the object. While approaches that rely on temporal coherence have arguably not yet yielded representations as powerful as those learned by supervised methods, they nonetheless point to the potential of learning useful representations from video (Mohabi et al., 2009; Sun et al., 2014; Goroshin et al., 2015a; Maltoni & Lomonaco, 2015; Wang & Gupta, 2015).
Here, we explore another potential principle for exploiting video for unsupervised learning: prediction of future image frames (Softky, 1996; Palm, 2012; O’Reilly et al., 2014; Goroshin et al., 2015b; Srivastava et al., 2015; Mathieu et al., 2016; Patraucean et al., 2015; Finn et al., 2016; Vondrick et al., 2016). A key insight here is that in order to be able to predict how the visual world will change over time, an agent must have at least some implicit model of object structure and the possible transformations objects can undergo. To this end, we have designed a neural network architecture, which we informally call a “PredNet,” that attempts to continually predict the appearance of future video frames, using a deep, recurrent convolutional network with both bottom-up and topdown connections. Our work here builds on previous work in next-frame video prediction (Ranzato et al., 2014; Michalski et al., 2014; Srivastava et al., 2015; Mathieu et al., 2016; Lotter et al., 2015; Patraucean et al., 2015; Oh et al., 2015; Finn et al., 2016; Xue et al., 2016; Vondrick et al., 2016; Brabandere et al., 2016), but we take particular inspiration from the concept of “predictive coding” from the neuroscience literature (Rao & Ballard, 1999; Rao & Sejnowski, 2000; Lee & Mumford, 2003; Friston, 2005; Summerfield et al., 2006; Egner et al., 2010; Bastos et al., 2012; Spratling, 2012; Chalasani & Principe, 2013; Clark, 2013; O’Reilly et al., 2014; Kanai et al., 2015). Predictive coding posits that the brain is continually making predictions of incoming sensory stimuli (Rao & Ballard, 1999; Friston, 2005). Top-down (and perhaps lateral) connections convey these predictions, which are compared against actual observations to generate an error signal. The error signal is then propagated back up the hierarchy, eventually leading to an update of the predictions.
We demonstrate the effectiveness of our model for both synthetic sequences, where we have access to the underlying generative model and can investigate what the model learns, as well as natural videos. Consistent with the idea that prediction requires knowledge of object structure, we find that these networks successfully learn internal representations that are well-suited to subsequent recognition and decoding of latent object parameters (e.g. identity, view, rotation speed, etc.). We also find that our architecture can scale effectively to natural image sequences, by training using car-mounted camera videos. The network is able to successfully learn to predict both the movement of the camera and the movement of objects in the camera’s view. Again supporting the notion of prediction as an unsupervised learning rule, the model’s learned representation in this setting supports decoding of the current steering angle.

2 THE PREDNET MODEL
The PredNet architecture is diagrammed in Figure 1. The network consists of a series of repeating stacked modules that attempt to make local predictions of the input to the module, which is then subtracted from the actual input and passed along to the next layer. Briefly, each module of the network consists of four basic parts: an input convolutional layer (Al), a recurrent representation layer (Rl), a prediction layer (Âl), and an error representation (El). The representation layer, Rl, is a recurrent convolutional network that generates a prediction, Âl, of what the layer input, Al, will be on the next frame. The network takes the difference between Al and Âl and outputs an error representation, El, which is split into separate rectified positive and negative error populations. The error, El, is then passed forward through a convolutional layer to become the input to the next layer (Al+1). The recurrent prediction layerRl receives a copy of the error signalEl, along with top-down input from the representation layer of the next level of the network (Rl+1). The organization of the network is such that on the first time step of operation, the “right” side of the network (Al’s andEl’s) is equivalent to a standard deep convolutional network. Meanwhile, the “left” side of the network (the Rl’s) is equivalent to a generative deconvolutional network with local recurrence at each stage. The architecture described here is inspired by that originally proposed by (Rao & Ballard, 1999), but is formulated in a modern deep learning framework and trained end-to-end using gradient descent, with a loss function implicitly embedded in the network as the firing rates of the error neurons. Our work also shares motivation with the Deep Predictive Coding Networks of Chalasani & Principe (2013); however, their framework is based upon sparse coding and a linear dynamical system with greedy layer-wise training, whereas ours is rooted in convolutional and recurrent neural networks trained with backprop.
While the architecture is general with respect to the kinds of data it models, here we focus on image sequence (video) data. Consider a sequence of images, xt. The target for the lowest layer is set to the the actual sequence itself, i.e. At0 = xt ∀t. The targets for higher layers, Atl for l > 0, are computed by a convolution over the error units from the layer below, Etl−1, followed by rectified linear unit (ReLU) activation and max-pooling. For the representation neurons, we specifically use convolutional LSTM units (Hochreiter & Schmidhuber, 1997; Shi et al., 2015). In our setting, the Rtl hidden state is updated according to R t−1 l , E t−1 l , as well as R t l+1, which is first spatially upsampled (nearest-neighbor), due to the pooling present in the feedforward path. The predictions, Âtl are made through a convolution of the R t l stack followed by a ReLU non-linearity. For the lowest layer, Âtl is also passed through a saturating non-linearity set at the maximum pixel value: SatLU(x; pmax) := min(pmax, x). Finally, the error response, Etl , is calculated from the difference between Âtl and A t l and is split into ReLU-activated positive and negative prediction errors, which are concatenated along the feature dimension. As discussed in (Rao & Ballard, 1999), although not explicit in their model, the separate error populations are analogous to the existence of on-center, off-surround and off-center, on-surround neurons early in the visual system.
The full set of update rules are listed in Equations (1) to (4). The model is trained to minimize the weighted sum of the activity of the error units. Explicitly, the training loss is formalized in Equation 5 with weighting factors by time, λt, and layer, λl, and where nl is the number of units in the lth layer. With error units consisting of subtraction followed by ReLU activation, the loss at each layer is equivalent to an L1 error. Although not explored here, other error unit implementations, potentially even probabilistic or adversarial (Goodfellow et al., 2014), could also be used.
Atl = { xt if l = 0 MAXPOOL(RELU(CONV(Etl−1))) l > 0
(1)
Âtl = RELU(CONV(R t l)) (2)
Etl = [RELU(A t l − Âtl); RELU(Âtl −Atl)] (3) Rtl = CONVLSTM(E t−1 l , R t−1 l ,UPSAMPLE(R t l+1)) (4)
Ltrain = ∑ t λt ∑ l λl nl ∑ nl Etl (5)
Algorithm 1 Calculation of PredNet states Require: xt
1: At0 ← xt 2: E0l , R 0 l ← 0 3: for t = 1 to T do 4: for l = L to 0 do . Update Rtl states 5: if l = L then 6: RtL = CONVLSTM(E t−1 L , R t−1 L ) 7: else 8: Rtl = CONVLSTM(E t−1 l , R t−1 l ,UPSAMPLE(R t l+1))
9: for l = 0 to L do . Update Âtl , Atl , Etl states 10: if l = 0 then 11: Ât0 = SATLU(RELU(CONV(R t 0))) 12: else 13: Âtl = RELU(CONV(R t l )) 14: Etl = [RELU(A t l − Âtl); RELU(Âtl −Alt)] 15: if l < L then 16: Atl+1 = MAXPOOL(CONV(E l t))
The order in which each unit in the model is updated must also be specified, and our implementation is described in Algorithm 1. Updating of states occurs through two passes: a top-down pass where the Rtl states are computed, and then a forward pass to calculate the predictions, errors, and higher level targets. A last detail of note is that Rl and El are initialized to zero, which, due to the convolutional nature of the network, means that the initial prediction is spatially uniform.

3 EXPERIMENTS

3.1 RENDERED IMAGE SEQUENCES
To gain an understanding of the representations learned in the proposed framework, we first trained PredNet models using synthetic images, for which we have access to the underlying generative stimulus model and all latent parameters. We created sequences of rendered faces rotating with two degrees of freedom, along the “pan” (out-of-plane) and “roll” (in-plane) axes. The faces start at a random orientation and rotate at a random constant velocity for a total of 10 frames. A different face was sampled for each sequence. The images were processed to be grayscale, with values normalized between 0 and 1, and 64x64 pixels in size. We used 16K sequences for training and 800 for both validation and testing.
Predictions generated by a PredNet model are shown in Figure 2. The model is able to accumulate information over time to make accurate predictions of future frames. Since the representation neurons are initialized to zero, the prediction at the first time step is uniform. On the second time step, with no motion information yet, the prediction is a blurry reconstruction of the first time step. After further iterations, the model adapts to the underlying dynamics to generate predictions that closely match the incoming frame.
For choosing the hyperparameters of the model, we performed a random search and chose the model that had the lowest L1 error in frame prediction averaged over time steps 2-10 on a validation set. Given this selection criteria, the best performing models tended to have a loss solely concentrated at the lowest layer (i.e. λ0 = 1, λl>0 = 0), which is the case for the model shown. Using an equal loss at each layer considerably degraded predictions, but enforcing a moderate loss on upper layers that was one magnitude smaller than the lowest layer (i.e. λ0 = 1, λl>0 = 0.1) led to only slightly worse predictions, as illustrated in Figure 9 in the Appendix. In all cases, the time loss weight, λt, was set to zero for the first time step and then one for all time steps after. As for the remaining hyperparameters, the model shown has 5 layers with 3x3 filter sizes for all convolutions, max-pooling of stride 2, and number of channels per layer, for bothAl andRl units, of (1, 32, 64, 128, 256). Model weights were optimized using the Adam algorithm (Kingma & Ba, 2014).
Actual
Predicted
Actual
Predicted
Actual
Predicted
Quantitative evaluation of generative models is a difficult, unsolved problem (Theis et al., 2016), but here we report prediction error in terms of meansquared error (MSE) and the Structural Similarity Index Measure (SSIM) (Wang et al., 2004). SSIM is designed to be more correlated with perceptual judgments, and ranges from−1 and 1, with a larger score indicating greater similarity. We compare the PredNet to the trivial solution of copying the last
frame, as well as a control model that shares the overall architecture and training scheme of the PredNet, but that sends forward the layer-wise activations (Al) rather than the errors (El). This model thus takes the form of a more traditional encoder-decoder pair, with a CNN encoder that has lateral skip connections to a convolutional LSTM decoder. The performance of all models on the rotating faces dataset is summarized in Table 1, where the scores were calculated as an average over all predictions after the first frame. We report results for the PredNet model trained with loss only on the lowest layer, denoted as PredNet L0, as well as the model trained with an 0.1 weight on upper layers, denoted as PredNet Lall. Both PredNet models outperformed the baselines on both measures, with the L0 model slightly outperforming Lall, as expected for evaluating the pixel-level predictions.
Synthetic sequences were chosen as the initial training set in order to better understand what is learned in different layers of the model, specifically with respect to the underlying generative model (Kulkarni et al., 2015). The rotating faces were generated using the FaceGen software package (Singular Inversions, Inc.), which internally generates 3D face meshes by a principal component analysis in “face space”, derived from a corpus of 3D face scans. Thus, the latent parameters of the image sequences used here consist of the initial pan and roll angles, the pan and roll velocities, and the principal component (PC) values, which control the “identity” of the face. To understand the information contained in the trained models, we decoded the latent parameters from the representation neurons (Rl) in different layers, using a ridge regression. The Rl states were taken at the earliest possible informative time steps, which, in the our notation, are the second and third steps, respectively, for the static and dynamic parameters. The regression was trained using 4K sequences with 500 for validation and 1K for testing. For a baseline comparison of the information implicitly embedded in the network architecture, we compare to the decoding accuracies of an untrained network with random initial weights. Note that in this randomly initialized case, we still expect above-chance decoding performance, given past theoretical and empirical work with random networks (Pinto et al., 2009; Jarrett et al., 2009; Saxe et al., 2010).
Latent variable decoding accuracies of the pan and roll velocities, pan initial angle, and first PC are shown in the left panel of Figure 3. There are several interesting patterns. First, the trained models learn a representation that generally permits a better linear decoding of the underlying latent factors than the randomly initialized model, with the most striking difference in terms of the the pan rotation speed (αpan). Second, the most notable difference between the Lall and L0 versions occurs with the first principle component, where the model trained with loss on all layers has a higher decoding accuracy than the model trained with loss only on the lowest layer.
The latent variable decoding analysis suggests that the model learns a representation that may generalize well to other tasks for which it was not explicitly trained. To investigate this further, we assessed the models in a classification task from single, static images. We created a dataset of 25 previously unseen FaceGen faces at 7 pan angles, equally spaced between [−π2 , π 2 ], and 8 roll angles, equally spaced between [0, 2π). There were therefore 7 · 8 = 56 orientations per identity, which were tested in a cross-validated fashion. A linear SVM to decode face identity was fit on a model’s representation of a random subset of orientations and then tested on the remaining angles. For each size of the SVM training set, ranging from 1-40 orientations per face, 50 different random splits were generated, with results averaged over the splits.
For the static face classification task, we compare the PredNets to a standard autoencoder and a variant of the Ladder Network (Valpola, 2015; Rasmus et al., 2015). Both models were constructed to have the same number of layers and channel sizes as the PredNets, as well as a similar alternating convolution/max-pooling, then upsampling/convolution scheme. As both networks are autoencoders, they were trained with a reconstruction loss, with a dataset consisting of all of the individual frames from the sequences used to train the PredNets. For the Ladder Network, which is a denoising autoencoder with lateral skip connections, one must also choose a noise parameter, as well as the relative weights of each layer in the total cost. We tested noise levels ranging from 0 to 0.5 in increments of 0.1, with loss weights either evenly distributed across layers, solely concentrated at the pixel layer, or 1 at the bottom layer and 0.1 at upper layers (analogous to the PredNet Lall model). Shown is the model that performed best for classification, which consisted of 0.4 noise and only pixel weighting. Lastly, as in our architecture, the Ladder Network has lateral and top-down streams that are combined by a combinator function. Inspired by (Pezeshki et al., 2015), where a learnable MLP improved results, and to be consistent in comparing to the PredNet, we used a purely convolutional combinator. Given the distributed representation in both networks, we decoded from a concatenation of the feature representations at all layers, except the pixel layer. For the PredNets, the representation units were used and features were extracted after processing one input frame.
Face classification accuracies using the representations learned by the L0 and Lall PredNets, a standard autoencoder, and a Ladder Network variant are shown in the right panel of Figure 3. Both PredNets compare favorably to the other models at all sizes of the training set, suggesting they learn a representation that is relatively tolerant to object transformations. Similar to the decoding accuracy of the first principle component, the PredNet Lall model actually outperformed the L0 variant. Altogether, these results suggest that predictive training with the PredNet can be a viable alternative to other models trained with a more traditional reconstructive or denoising loss, and that the relative layer loss weightings (λl’s) may be important for the particular task at hand.

3.2 NATURAL IMAGE SEQUENCES
We next sought to test the PredNet architecture on complex, real-world sequences. As a testbed, we chose car-mounted camera videos, since these videos span across a wide range of settings and are characterized by rich temporal dynamics, including both self-motion of the vehicle and the motion of other objects in the scene (Agrawal et al., 2015). Models were trained using the raw videos from the KITTI dataset (Geiger et al., 2013), which were captured by a roof-mounted camera on a car driving around an urban environment in Germany. Sequences of 10 frames were sampled from the “City”, “Residential”, and “Road” categories, with 57 recording sessions used for training and 4 used for validation. Frames were center-cropped and downsampled to 128x160 pixels. In total, the training set consisted of roughly 41K frames.
A random hyperparameter search, with model selection based on the validation set, resulted in a 4 layer model with 3x3 convolutions and layer channel sizes of (3, 48, 96, 192). Models were again trained with Adam (Kingma & Ba, 2014) using a loss either solely computed on the lowest layer (L0) or with a weight of 1 on the lowest layer and 0.1 on the upper layers (Lall). Adam parameters were initially set to their default values (α = 0.001, β1 = 0.9, β2 = 0.999) with the learning rate, α, decreasing by a factor of 10 halfway through training. To assess that the network had indeed learned a robust representation, we tested on the CalTech Pedestrian dataset (Dollár et al., 2009), which consists of videos from a dashboard-mounted camera on a vehicle driving around Los Angeles. Testing sequences were made to match the frame rate of the KITTI dataset and again cropped to 128x160 pixels. Quantitative evaluation was performed on the entire CalTech test partition, split into sequences of 10 frames.
Sample PredNet predictions (for the L0 model) on the CalTech Pedestrian dataset are shown in Figure 4, and example videos can be found at https://coxlab.github.io/prednet/. The model is able to make fairly accurate predictions in a wide range of scenarios. In the top sequence of Fig. 4, a car is passing in the opposite direction, and the model, while not perfect, is able to predict its trajectory, as well as fill in the ground it leaves behind. Similarly in Sequence 3, the model is able to predict the motion of a vehicle completing a left turn. Sequences 2 and 5 illustrate that the PredNet can judge its own movement, as it predicts the appearance of shadows and a stationary vehicle as they approach. The model makes reasonable predictions even in difficult scenarios, such as when the camera-mounted vehicle is turning. In Sequence 4, the model predicts the position of a tree, as the vehicle turns onto a road. The turning sequences also further illustrate the model’s ability to “fill-in”, as it is able to extrapolate sky and tree textures as unseen regions come into view. As an additional control, we show a sequence at the bottom of Fig. 4, where the input has been temporally scrambled. In this case, the model generates blurry frames, which mostly just resemble the previous frame. Finally, although the PredNet shown here was trained to predict one frame ahead, it is also possible to predict multiple frames into the future, by feeding back predictions as the inputs and recursively iterating. We explore this in Appendix 5.3.
Quantitatively, the PredNet models again outperformed the CNN-LSTM EncoderDecoder. To ensure that the difference in performance was not simply because of the choice of hyperparameters, we trained models with four other sets of hyperparameters, which were sampled from the initial random search over the number of layers, fil-
ter sizes, and number of filters per layer. For each of the four additional sets, the PredNet L0 had the best performance, with an average error reduction of 14.7% and 14.9% for MSE and SSIM,
respectively, compared to the CNN-LSTM Encoder-Decoder. More details, as well as a thorough investigation of systematically simplified models on the continuum between the PredNet and the CNN-LSTM Encoder-Decoder can be found in Appendix 5.1. Briefly, the elementwise subtraction operation in the PredNet seems to be beneficial, and the nonlinearity of positive/negative splitting also adds modest improvements. Finally, while these experiments measure the benefits of each component of our model, we also directly compare against recent work in a similar car-cam setting, by reporting results on a 64x64 pixel, grayscale car-cam dataset released by Brabandere et al. (2016). Our PredNet model outperforms the model by Brabandere et al. (2016) by 29%. Details can be found in Appendix 5.2. Also in Appendix 5.2, we present results for the Human3.6M (Ionescu et al., 2014) dataset, as reported by Finn et al. (2016). Without re-optimizing hyperparameters, our
model underperforms the concurrently developed DNA model by Finn et al. (2016), but outperforms the model by Mathieu et al. (2016).
To test the implicit encoding of latent parameters in the car-cam setting, we used the internal representation in the PredNet to estimate the car’s steering angle (Bojarski et al., 2016; Biasini et al., 2016). We used a dataset released by Comma.ai (Biasini et al., 2016) consisting of 11 videos totaling about 7 hours of mostly highway driving. We first trained networks for next-frame prediction and then fit a linear fully-connected layer on the learned representation to estimate the steering angle, using a MSE loss. We again concatenate the Rl representation at all layers, but first spatially average pool lower layers to match the spatial size of the upper layer, in order to reduce dimensionality. Steering angle estimation results, using the representation on the 10th time step, are shown in Figure 5. Given just 1K labeled training examples, a simple linear readout on the PredNet L0 representation explains 74% of the variance in the steering angle and outperforms the CNN-LSTM Enc.-Dec. by 35%. With 25K labeled training examples, the PredNet L0 has a MSE (in degrees2) of 2.14. As a point of reference, a CNN model designed to predict the steering angle (Biasini et al., 2016), albeit from a single frame instead of multiple frames, achieve a MSE of ~4 when trained end-to-end using 396K labeled training examples. Details of this analysis can be found in Appendix 8. Interestingly, in this task, the PredNet Lall model actually underperformed the L0 model and slightly underperformed the CNN-LSTM Enc.-Dec, again suggesting that the λl parameter can affect the representation learned, and different values may be preferable in different end tasks. Nonetheless, the readout from the Lall model still explained a substantial proportion of the steering angle variance and strongly outperformed the random initial weights. Overall, this analysis again demonstrates that a representation learned through prediction, and particularly with the PredNet model with appropriate hyperparameters, can contain useful information about underlying latent parameters.

4 DISCUSSION
Above, we have demonstrated a predictive coding inspired architecture that is able to predict future frames in both synthetic and natural image sequences. Importantly, we have shown that learning to predict how an object or scene will move in a future frame confers advantages in decoding latent parameters (such as viewing angle) that give rise to an object’s appearance, and can improve recognition performance. More generally, we argue that prediction can serve as a powerful unsupervised learning signal, since accurately predicting future frames requires at least an implicit model of the objects that make up the scene and how they are allowed to move. Developing a deeper understanding of the nature of the representations learned by the networks, and extending the architecture, by, for instance, allowing sampling, are important future directions.

ACKNOWLEDGMENTS
We would like to thank Rasmus Berg Palm for fruitful discussions and early brainstorming. We would also like to thank the developers of Keras (Chollet, 2016). This work was supported by IARPA (contract D16PC00002), the National Science Foundation (NSF IIS 1409097), and the Center for Brains, Minds and Machines (CBMM, NSF STC award CCF-1231216).

5 APPENDIX

5.1 ADDITIONAL CONTROL MODELS
Table 3 contains results for additional variations of the PredNet and CNN-LSTM Encoder-Decoder evaluated on the CalTech Pedestrian Dataset after being trained on KITTI. We evaluate the models in terms of pixel prediction, thus using the PredNet model trained with loss only on the lowest layer (PredNet L0) as the base model. In addition to mean-squared error (MSE) and the Structural Similarity Index Measure (SSIM), we include calculations of the Peak Signal-To-Noise Ratio (PSNR). For each model, we evaluate it with the original set of hyperparameters (controlling the number of layers, filter sizes, and number of filters per layer), as well as with the four additional sets of hyperparameters that were randomly sampled from the initial random search (see main text for more details). Below is an explanation of the additional control models:
MSE (x 10−3) PSNR SSIM PredNet 3.13 (3.33) 25.8 (25.5) 0.884 (0.878) PredNet (no El split) 3.20 (3.37) 25.6 (25.4) 0.883 (0.878) CNN-LSTM Enc.-Dec. 3.67 (3.91) 25.0 (24.6) 0.865 (0.856) CNN-LSTM Enc.-Dec. (2x Al filts) 3.82 (3.97) 24.8 (24.6) 0.857 (0.853) CNN-LSTM Enc.-Dec. (except pass E0) 3.41 (3.61) 25.4 (25.1) 0.873 (0.866) CNN-LSTM Enc.-Dec. (+/- split) 3.71 (3.84) 24.9 (24.7) 0.861 (0.857) Copy Last Frame 7.95 20.0 0.762
Equalizing the number of filters in the CNN-LSTM Encoder-Decoder (2x Al filts) cannot account for its performance difference with the PredNet, and actually leads to overfitting and a decrease in performance. Passing the error at the lowest layer (E0) in the CNN-LSTM Enc.-Dec. improves performance, but still does not match the PredNet, where errors are passed at all layers. Finally, splitting the activationsAl into positive and negative populations in the CNN-LSTM Enc.-Dec. does not help, but the PredNet with linear error activation (“no El split”) performs slightly worse than the original split version. Together, these results suggest that the PredNet’s error passing operation can lead to improvements in next-frame prediction performance.

5.2 COMPARING AGAINST OTHER MODELS
While our main comparison in the text was a control model that isolates the effects of the more unique components in the PredNet, here we directly compare against other published models. We report results on a 64x64 pixel, grayscale car-cam dataset and the Human3.6M dataset (Ionescu et al., 2014) to compare against the two concurrently developed models by Brabandere et al. (2016)
and Finn et al. (2016), respectively. For both comparisons, we use a model with the same hyperparameters (# of layers, # of filters, etc.) of the PredNet L0 model trained on KITTI, but train from scratch on the new datasets. The only modification we make is to train using an L2 loss instead of the effective L1 loss, since both models train with an L2 loss and report results using L2-based metrics (MSE for Brabandere et al. (2016) and PSNR for Finn et al. (2016)). That is, we keep the original PredNet model intact but directly optimize using MSE between actual and predicted frames. We measure next-frame prediction performance after inputting 3 frames and 10 frames, respectively, for the 64x64 car-cam and Human3.6M datasets, to be consistent with the published works. We also include the results using a feedforward multi-scale network, similar to the model of Mathieu et al. (2016), on Human3.6M, as reported by Finn et al. (2016).
On a dataset similar to KITTI, our model outperforms the model proposed by Brabandere et al. (2016). On Human3.6M, our model outperforms a model similar to (Mathieu et al., 2016), but underperforms Finn et al. (2016), although we note we did not perform any hyperparameter optimization.

5.3 MULTIPLE TIME STEP PREDICTION
While the models presented here were originally trained to predict one frame ahead, they can be made to predict multiple frames by treating predictions as actual input and recursively iterating. Examples of this process are shown in Figure 6 for the PredNet L0 model. Although the next frame predictions are reasonably accurate, the model naturally breaks down when extrapolating further into the future. This is not surprising since the predictions will unavoidably have different statistics than the natural images for which the model was trained to handle (Bengio et al., 2015). If we additionally train the model to process its own predictions, the model is better able to extrapolate. The third row for every sequence shows the output of the original PredNet fine-tuned for extrapolation. Starting from the trained weights, the model was trained with a loss over 15 time steps, where the actual frame was inputted for the first 10 and then the model’s predictions were used as input to the network for the last 5. For the first 10 time steps, the training loss was calculated on the El activations as usual, and for the last 5, it was calculated directly as the mean absolute error with respect to the ground truth frames. Despite eventual blurriness (which might be expected to some extent due to uncertainty), the fine-tuned model captures some key structure in its extrapolations after the tenth time step. For instance, in the first sequence, the model estimates the general shape of an upcoming shadow, despite minimal information in the last seen frame. In the second sequence, the model is able to extrapolate the motion of a car moving to the right. The reader is again encouraged to visit https://coxlab.github.io/prednet/ to view the predictions in video form. Quantitatively, the MSE of the model’s predictions stay well below the trivial solution of copying the last seen frame, as illustrated in Fig 7. The MSE increases fairly linearly from time steps 2-10, even though the model was only trained for up to t+ 5 prediction.

5.4 ADDITIONAL STEERING ANGLE ANALYSIS
In Figure 8, we show the steering angle estimation accuracy on the Comma.ai (Biasini et al., 2016) dataset using the representation learned by the PredNet L0 model, as a function of the number of frames inputted into the model. The PredNet’s representation at all layers was concatenated (after spatially pooling lower layers to a common spatial resolution) and a fully-connected readout was fit using MSE. For each level of the number of training examples, we average over 10 cross-validation splits. To serve as points of reference, we include results for two static models. The first model is an autoencoder trained on single frame reconstruction with appropriately matching hyperparameters. A fully-connected layer was fit on the autoencoder’s representation to estimate the steering angle in the same fashion as the PredNet. The second model is the default model in the posted Comma.ai code (Biasini et al., 2016), which is a five layer CNN. This model is trained end-to-end to estimate
the steering angle given the current frame as input, with a MSE loss. In addition to 25K examples, we trained a version using all of the frames in the Comma dataset (~396K). For all models, the final weights were chosen at the minimum validation error during training. Given the relatively small number of videos in the dataset compared to the average duration of each video, we used 5% of each video for validation and testing, chosen as a random continuous chunk, and discarded the 10 frames before and after the chosen segments from the training set.
As illustrated in Figure 8, the PredNet’s performance gets better over time, as one might expect, as the model is able to accumulate more information. Interestingly, it performs reasonably well after just one time step, in a regime that is orthogonal to the training procedure of the PredNet where there are no dynamics. Altogether, these results again point to the usefulness of the model in learning underlying latent parameters.
5.5 PREDNET Lall NEXT-FRAME PREDICTIONS
Figures 9 and 10 compare next-frame predictions by the PredNet Lall model, trained with a prediction loss on all layers (λ0 = 1, λl>0 = 0.1), and the PredNet L0 model, trained with a loss only on the lowest layer. At first glance, the difference in predictions seem fairly minor, and indeed, in terms of MSE, the Lall model only underperformed the L0 version by 3% and 6%, respectively, for the rotating faces and CalTech Pedestrian datasets. Upon careful inspection, however, it is apparent that the Lall predictions lack some of the finer details of the L0 predictions and are more blurry in regions of high variance. For instance, with the rotating faces, the facial features are less defined and with CalTech, details of approaching shadows and cars are less precise.
Actual
PredNet 𝐿0
PredNet 𝐿𝑎𝑙𝑙
Error 𝐿𝑎𝑙𝑙 - 𝐿0
Actual
PredNet 𝐿0
PredNet 𝐿𝑎𝑙𝑙
Error 𝐿𝑎𝑙𝑙 - 𝐿0
Actual
PredNet 𝐿0
PredNet 𝐿𝑎𝑙𝑙
Error 𝐿𝑎𝑙𝑙 - 𝐿0
Actual
PredNet 𝐿0
PredNet 𝐿𝑎𝑙𝑙
Error 𝐿𝑎𝑙𝑙 - 𝐿0
","While great strides have been made in using deep learning algorithms to solve supervised learning tasks, the problem of unsupervised learning — leveraging unlabeled examples to learn about the structure of a domain — remains a difficult unsolved challenge. Here, we explore prediction of future frames in a video sequence as an unsupervised learning rule for learning about the structure of the visual world. We describe a predictive neural network (“PredNet”) architecture that is inspired by the concept of “predictive coding” from the neuroscience literature. These networks learn to predict future frames in a video sequence, with each layer in the network making local predictions and only forwarding deviations from those predictions to subsequent network layers. We show that these networks are able to robustly learn to predict the movement of synthetic (rendered) objects, and that in doing so, the networks learn internal representations that are useful for decoding latent object parameters (e.g. pose) that support object recognition with fewer training views. We also show that these networks can scale to complex natural image streams (car-mounted camera videos), capturing key aspects of both egocentric movement and the movement of objects in the visual scene, and the representation learned in this setting is useful for estimating the steering angle. Altogether, these results suggest that prediction represents a powerful framework for unsupervised learning, allowing for implicit learning of object and scene structure.",ICLR 2017 conference submission,True,,"Paper Summary
This paper proposes an unsupervised learning model in which the network
predicts what its state would look like at the next time step (at input layer
and potentially other layers).  When these states are observed, an error signal
is computed by comparing the predictions and the observations. This error
signal is fed back into the model. The authors show that this model is able to
make good predictions on a toy dataset of rotating 3D faces as well as on
natural videos. They also show that these features help perform supervised
tasks.

Strengths
- The model is an interesting embodiment of the idea of predictive coding
  implemented using a end-to-end backpropable recurrent neural network architecture.
- The idea of feeding forward an error signal is perhaps not used as widely as it could
  be, and this work shows a compelling example of using it. 
- Strong empirical results and relevant comparisons show that the model works well.
- The authors present a detailed ablative analysis of the proposed model.

Weaknesses
- The model (esp. in Fig 1) is presented as a generalized predictive model
  where next step predictions are made at each layer. However, as discovered by
running the experiments, only the predictions at the input layer are the ones
that actually matter and the optimal choice seems to be to turn off the error
signal from the higher layers. While the authors intend to address this in future
work, I think this point merits some more discussion in the current work, given
the way this model is presented.
- The network currently lacks stochasticity and does not model the future as a
  multimodal distribution (However, this is mentioned as potential future work).

Quality
The experiments are well-designed and a detailed analysis is provided
in the appendix.

Clarity
The paper is well-written and easy to follow.

Originality
Some deep models have previously been proposed that use predictive coding.
However, the proposed model is most probably novel in the way it feds back the
error signal and implements the entire model as a single differentiable
network.

Significance
This paper will be of wide interest to the growing set of researchers working
in unsupervised learning of time series. This helps draw attention to
predictive coding as an important learning paradigm.

Overall
Good paper with detailed and well-designed experiments. The idea of feeding
forward the error signal is not being used as much as it could be in our
community. This work helps to draw the community's attention to this idea.

---

This paper proposes an interesting architecture for predicting future frames of videos using end-to-end trained deep predictive coding.
  The architecture is well presented and the paper is clearly written. The experiments are extensive and convincing, include ablation analyses, and show that this architecture performs well compared to other current methods.
 Overall, this is an interesting, solid contribution.

---

An interesting architecture that accumulates and continuously corrects mistakes as you see more and more of a video sequence.

Clarity: The video you generated seems very helpful towards understanding the information flow in your network, it would be nice to link to it from the paper.

 ""Our model with hyperparameters optimized for KITTI underperforms the model of Finn et al. (2016), but outperforms the previous state-of-the-art model by Mathieu et al. (2016).""

 It is not clear how different are the train and test sequences at the moment, since standard benchmarks do not really exist for video prediction and each author picks his/her favorite. Underperforming Finn et al 206 at the H3.6m Walking videos is a bit disappointing.

---

Learning about the physical structure and semantics of the world from video (without supervision) is a very hot area in computer vision and machine learning.
In this paper, the authors investigate how the prediction of future image frames (inherently unsupervised) can help to deduce object/s structure and it's properties (in this case single object pose, category, and steering angle, (after a supervised linear readout step))

I enjoyed reading this paper, it is clear, interesting and proposes an original network architecture (PredNet) for video frame prediction that has produced promising results on both synthetic and natural images.
Moreover, the extensive experimental evaluation and analysis the authors provide puts it on solid ground to which others can compare.

The weaknesses:
- the link to predictive coding should be better explained in the paper if it is to be used as a motivation for the prednet model.
- any idea that the proposed method is learning an implicit `model' of the `objects' that make up the `scene' is vague and far fetched, but it sounds great.

Minor comment:
Next to the number of labeled training examples (Fig.5), it would be interesting to see how much unsupervised training data was used to train your representations.

---

Paper Summary
This paper proposes an unsupervised learning model in which the network
predicts what its state would look like at the next time step (at input layer
and potentially other layers).  When these states are observed, an error signal
is computed by comparing the predictions and the observations. This error
signal is fed back into the model. The authors show that this model is able to
make good predictions on a toy dataset of rotating 3D faces as well as on
natural videos. They also show that these features help perform supervised
tasks.

Strengths
- The model is an interesting embodiment of the idea of predictive coding
  implemented using a end-to-end backpropable recurrent neural network architecture.
- The idea of feeding forward an error signal is perhaps not used as widely as it could
  be, and this work shows a compelling example of using it. 
- Strong empirical results and relevant comparisons show that the model works well.
- The authors present a detailed ablative analysis of the proposed model.

Weaknesses
- The model (esp. in Fig 1) is presented as a generalized predictive model
  where next step predictions are made at each layer. However, as discovered by
running the experiments, only the predictions at the input layer are the ones
that actually matter and the optimal choice seems to be to turn off the error
signal from the higher layers. While the authors intend to address this in future
work, I think this point merits some more discussion in the current work, given
the way this model is presented.
- The network currently lacks stochasticity and does not model the future as a
  multimodal distribution (However, this is mentioned as potential future work).

Quality
The experiments are well-designed and a detailed analysis is provided
in the appendix.

Clarity
The paper is well-written and easy to follow.

Originality
Some deep models have previously been proposed that use predictive coding.
However, the proposed model is most probably novel in the way it feds back the
error signal and implements the entire model as a single differentiable
network.

Significance
This paper will be of wide interest to the growing set of researchers working
in unsupervised learning of time series. This helps draw attention to
predictive coding as an important learning paradigm.

Overall
Good paper with detailed and well-designed experiments. The idea of feeding
forward the error signal is not being used as much as it could be in our
community. This work helps to draw the community's attention to this idea.

---

In response to the helpful comments and questions, we have made several changes to the manuscript:

1.  In our original manuscript, we primarily compared the PredNet to a CNN-LSTM Encoder-Decoder, which we chose because it serves as a tight control for the more novel elements of our architecture. However, we agree that it is useful to compare against other published architectures.  One reason that this isn’t a trivial task is because a standard benchmark for next frame prediction arguably has yet to be established.  Another issue is that published models are often optimized for performance on particular datasets, so evaluating competing models on KITTI/CalTech isn’t necessarily fair to those models.  Searching the very recent literature, we found that the most relevant comparison to make is probably against the DFN model by Brabandere et al. (2016), which was recently presented at NIPS and was developed concurrently with our work.  One of their experiments was on a 64x64 pixel, grayscale car-cam dataset.  Training our KITTI model on this dataset, we outperform their results by 29%.  To compare against another concurrently developed model, also published at NIPS 2016, we have additionally evaluated on the Human3.6M dataset (Ionescu et al., 2014).  Our model with hyperparameters optimized for KITTI underperforms the model of Finn et al. (2016), but outperforms the previous state-of-the-art model by Mathieu et al. (2016).  We have added all of these comparisons to the appendix.

2.  To make the main text more clear and concise, and to properly explain all of the necessary details, we have moved portions of the steering angle analysis to the appendix.  Our main point has been to demonstrate that our model learns a representation of important underlying factors, using other models as points of reference, so we have emphasized this.

At the reviewer’s suggestion, we have added a video clip to help illustrate the flow of information in the network:

---

What's the difference between this work and ""Deep Predictive Coding Networks"" (ref [4]).
Please explain it clearly in the text.

Why not comparing the performance of prednet with the previous work of authors(ref[25])?

The improvement in MSE of prednet over previous frame prediction is 0.005. How significant is this?
Maybe you could provide images showing the difference between images in t+1 and t, and between t+1 and your prediction.
This could reveal in which regions prednet does better than previous frame prediction.

---

Paper Summary
This paper proposes an unsupervised learning model in which the network
predicts what its state would look like at the next time step (at input layer
and potentially other layers).  When these states are observed, an error signal
is computed by comparing the predictions and the observations. This error
signal is fed back into the model. The authors show that this model is able to
make good predictions on a toy dataset of rotating 3D faces as well as on
natural videos. They also show that these features help perform supervised
tasks.

Strengths
- The model is an interesting embodiment of the idea of predictive coding
  implemented using a end-to-end backpropable recurrent neural network architecture.
- The idea of feeding forward an error signal is perhaps not used as widely as it could
  be, and this work shows a compelling example of using it. 
- Strong empirical results and relevant comparisons show that the model works well.
- The authors present a detailed ablative analysis of the proposed model.

Weaknesses
- The model (esp. in Fig 1) is presented as a generalized predictive model
  where next step predictions are made at each layer. However, as discovered by
running the experiments, only the predictions at the input layer are the ones
that actually matter and the optimal choice seems to be to turn off the error
signal from the higher layers. While the authors intend to address this in future
work, I think this point merits some more discussion in the current work, given
the way this model is presented.
- The network currently lacks stochasticity and does not model the future as a
  multimodal distribution (However, this is mentioned as potential future work).

Quality
The experiments are well-designed and a detailed analysis is provided
in the appendix.

Clarity
The paper is well-written and easy to follow.

Originality
Some deep models have previously been proposed that use predictive coding.
However, the proposed model is most probably novel in the way it feds back the
error signal and implements the entire model as a single differentiable
network.

Significance
This paper will be of wide interest to the growing set of researchers working
in unsupervised learning of time series. This helps draw attention to
predictive coding as an important learning paradigm.

Overall
Good paper with detailed and well-designed experiments. The idea of feeding
forward the error signal is not being used as much as it could be in our
community. This work helps to draw the community's attention to this idea.

---

This paper proposes an interesting architecture for predicting future frames of videos using end-to-end trained deep predictive coding.
  The architecture is well presented and the paper is clearly written. The experiments are extensive and convincing, include ablation analyses, and show that this architecture performs well compared to other current methods.
 Overall, this is an interesting, solid contribution.

---

An interesting architecture that accumulates and continuously corrects mistakes as you see more and more of a video sequence.

Clarity: The video you generated seems very helpful towards understanding the information flow in your network, it would be nice to link to it from the paper.

 ""Our model with hyperparameters optimized for KITTI underperforms the model of Finn et al. (2016), but outperforms the previous state-of-the-art model by Mathieu et al. (2016).""

 It is not clear how different are the train and test sequences at the moment, since standard benchmarks do not really exist for video prediction and each author picks his/her favorite. Underperforming Finn et al 206 at the H3.6m Walking videos is a bit disappointing.

---

Learning about the physical structure and semantics of the world from video (without supervision) is a very hot area in computer vision and machine learning.
In this paper, the authors investigate how the prediction of future image frames (inherently unsupervised) can help to deduce object/s structure and it's properties (in this case single object pose, category, and steering angle, (after a supervised linear readout step))

I enjoyed reading this paper, it is clear, interesting and proposes an original network architecture (PredNet) for video frame prediction that has produced promising results on both synthetic and natural images.
Moreover, the extensive experimental evaluation and analysis the authors provide puts it on solid ground to which others can compare.

The weaknesses:
- the link to predictive coding should be better explained in the paper if it is to be used as a motivation for the prednet model.
- any idea that the proposed method is learning an implicit `model' of the `objects' that make up the `scene' is vague and far fetched, but it sounds great.

Minor comment:
Next to the number of labeled training examples (Fig.5), it would be interesting to see how much unsupervised training data was used to train your representations.

---

Paper Summary
This paper proposes an unsupervised learning model in which the network
predicts what its state would look like at the next time step (at input layer
and potentially other layers).  When these states are observed, an error signal
is computed by comparing the predictions and the observations. This error
signal is fed back into the model. The authors show that this model is able to
make good predictions on a toy dataset of rotating 3D faces as well as on
natural videos. They also show that these features help perform supervised
tasks.

Strengths
- The model is an interesting embodiment of the idea of predictive coding
  implemented using a end-to-end backpropable recurrent neural network architecture.
- The idea of feeding forward an error signal is perhaps not used as widely as it could
  be, and this work shows a compelling example of using it. 
- Strong empirical results and relevant comparisons show that the model works well.
- The authors present a detailed ablative analysis of the proposed model.

Weaknesses
- The model (esp. in Fig 1) is presented as a generalized predictive model
  where next step predictions are made at each layer. However, as discovered by
running the experiments, only the predictions at the input layer are the ones
that actually matter and the optimal choice seems to be to turn off the error
signal from the higher layers. While the authors intend to address this in future
work, I think this point merits some more discussion in the current work, given
the way this model is presented.
- The network currently lacks stochasticity and does not model the future as a
  multimodal distribution (However, this is mentioned as potential future work).

Quality
The experiments are well-designed and a detailed analysis is provided
in the appendix.

Clarity
The paper is well-written and easy to follow.

Originality
Some deep models have previously been proposed that use predictive coding.
However, the proposed model is most probably novel in the way it feds back the
error signal and implements the entire model as a single differentiable
network.

Significance
This paper will be of wide interest to the growing set of researchers working
in unsupervised learning of time series. This helps draw attention to
predictive coding as an important learning paradigm.

Overall
Good paper with detailed and well-designed experiments. The idea of feeding
forward the error signal is not being used as much as it could be in our
community. This work helps to draw the community's attention to this idea.

---

In response to the helpful comments and questions, we have made several changes to the manuscript:

1.  In our original manuscript, we primarily compared the PredNet to a CNN-LSTM Encoder-Decoder, which we chose because it serves as a tight control for the more novel elements of our architecture. However, we agree that it is useful to compare against other published architectures.  One reason that this isn’t a trivial task is because a standard benchmark for next frame prediction arguably has yet to be established.  Another issue is that published models are often optimized for performance on particular datasets, so evaluating competing models on KITTI/CalTech isn’t necessarily fair to those models.  Searching the very recent literature, we found that the most relevant comparison to make is probably against the DFN model by Brabandere et al. (2016), which was recently presented at NIPS and was developed concurrently with our work.  One of their experiments was on a 64x64 pixel, grayscale car-cam dataset.  Training our KITTI model on this dataset, we outperform their results by 29%.  To compare against another concurrently developed model, also published at NIPS 2016, we have additionally evaluated on the Human3.6M dataset (Ionescu et al., 2014).  Our model with hyperparameters optimized for KITTI underperforms the model of Finn et al. (2016), but outperforms the previous state-of-the-art model by Mathieu et al. (2016).  We have added all of these comparisons to the appendix.

2.  To make the main text more clear and concise, and to properly explain all of the necessary details, we have moved portions of the steering angle analysis to the appendix.  Our main point has been to demonstrate that our model learns a representation of important underlying factors, using other models as points of reference, so we have emphasized this.

At the reviewer’s suggestion, we have added a video clip to help illustrate the flow of information in the network:

---

What's the difference between this work and ""Deep Predictive Coding Networks"" (ref [4]).
Please explain it clearly in the text.

Why not comparing the performance of prednet with the previous work of authors(ref[25])?

The improvement in MSE of prednet over previous frame prediction is 0.005. How significant is this?
Maybe you could provide images showing the difference between images in t+1 and t, and between t+1 and your prediction.
This could reveal in which regions prednet does better than previous frame prediction.",,5.0,,3.5,4.0,7.333333333333333,,,4.0,,
355,"Authors: Yuxin Wu, Yuandong Tian
Source file: 355.pdf

ABSTRACT
In this paper, we propose a new framework for training vision-based agent for First-Person Shooter (FPS) Game, in particular Doom. Our framework combines the state-of-the-art reinforcement learning approach (Asynchronous Advantage Actor-Critic (A3C) model [Mnih et al. (2016)]) with curriculum learning. Our model is simple in design and only uses game states from the AI side, rather than using opponents’ information [Lample & Chaplot (2016)]. On a known map, our agent won 10 out of the 11 attended games and the champion of Track1 in ViZDoom AI Competition 2016 by a large margin, 35% higher score than the second place.

1 INTRODUCTION
Deep Reinforcement Learning has achieved super-human performance in fully observable environments, e.g., in Atari Games [Mnih et al. (2015)] and Computer Go [Silver et al. (2016)]. Recently, Asynchronous Advantage Actor-Critic (A3C) [Mnih et al. (2016)] model shows good performance for 3D environment exploration, e.g. labyrinth exploration. However, in general, to train an agent in a partially observable 3D environment from raw frames remains an open challenge. Direct application of A3C to competitive 3D scenarios, e.g. 3D games, is nontrivial, partly due to sparse and long-term rewards in such scenarios.
Doom is a 1993 First-Person Shooter (FPS) game in which a player fights against other computercontrolled agents or human players in an adversarial 3D environment. Previous works on FPS AI [van Waveren (2001)] focused on using hand-tuned state machines and privileged information, e.g., the geometry of the map, the precise location of all players, to design playable agents. Although state-machine is conceptually simple and computationally efficient, it does not operate like human players, who only rely on visual (and possibly audio) inputs. Also, many complicated situations require manually-designed rules which could be time-consuming to tune.
In this paper, we train an AI agent in Doom with a framework that based on A3C with convolutional neural networks (CNN). This model uses only the recent 4 frames and game variables from the AI side, to predict the next action of the agent and the value of the current situation. We follow the curriculum learning paradigm [Bengio et al. (2009); Jiang et al. (2015)]: start from simple tasks and then gradually try harder ones. The difficulty of the task is controlled by a variety of parameters in Doom environment, including different types of maps, strength of the opponents and the design of the reward function. We also develop adaptive curriculum training that samples from a varying distribution of tasks to train the model, which is more stable and achieves higher score than A3C with the same number of epoch. As a result, our trained agent, named F1, won the champion in Track 1 of ViZDoom Competition 1 by a large margin.
There are many contemporary efforts on training a Doom AI based on the VizDoom platform [Kempka et al. (2016)] since its release. Arnold [Lample & Chaplot (2016)] also uses game frames and trains an action network using Deep Recurrent Q-learning [Hausknecht & Stone (2015)], and a navigation network with DQN [Mnih et al. (2015)]. However, there are several important differences. To predict the next action, they use a hybrid architecture (CNN+LSTM) that involves more complicated training procedure. Second, in addition to game frames, they require internal
1http://vizdoom.cs.put.edu.pl/competition-cig-2016/results
game status about the opponents as extra supervision during training, e.g., whether enemy is present in the current frame. IntelAct [Dosovitskiy & Koltun (2017)] models the Doom AI bot training in a supervised manner by predicting the future values of game variables (e.g., health, amount of ammo, etc) and acting accordingly. In comparison, we use curriculum learning with asynchronized actorcritic models and use stacked frames (4 most recent frames) and resized frames to mimic short-term memory and attention. Our approach requires no opponent’s information, and is thus suitable as a general framework to train agents for close-source games.
In VizDoom AI Competition 2016 at IEEE Computational Intelligence And Games (CIG) Conference2, our AI won the champion of Track1 (limited deathmatch with known map), and IntelAct won the champion of Track2 (full deathmatch with unknown maps). Neither of the two teams attends the other track. Arnold won the second places of both tracks and CLYDE [Ratcliffe et al. (2017)] won the third place of Track1.

2 THE ACTOR-CRITIC MODEL
The goal of Reinforcement Learning (RL) is to train an agent so that its behavior maximizes/minimizes expected future rewards/penalties it receives from a given environment [Sutton & Barto (1998)]. Two functions play important roles: a value function V (s) that gives the expected reward of the current state s, and a policy function π(a|s) that gives a probability distribution on the candidate actions a for the current state s. Getting the groundtruth value of either function would largely solve RL: the agent just follows π(a|s) to act, or jumps in the best state provided by V (s) when the number of candidate next states is finite and practically enumerable. However, neither is trivial.
Actor-critic models [Barto et al. (1983); Sutton (1984); Konda & Tsitsiklis (1999); Grondman et al. (2012)] aim to jointly estimate V (s) and π(a|s): from the current state st, the agent explores the environment by iteratively sampling the policy function π(at|st;wπ) and receives positive/negative reward, until the terminal state or a maximum number of iterations are reached. The exploration gives a trajectory {(st, at, rt), (st+1, at+1, rt+1), · · · }, from which the policy function and value function are updated. Specifically, to update the value function, we use the expected reward Rt along the trajectory as the ground truth; to update the policy function, we encourage actions that lead to high rewards, and penalize actions that lead to low rewards. To determine whether an action leads to high- or low-rewarding state, a reference point, called baseline [Williams (1992)], is usually needed. Using zero baseline might increase the estimation variance. [Peters & Schaal (2008)] gives a way to estimate the best baseline (a weighted sum of cumulative rewards) that minimizes the variance of the gradient estimation, in the scenario of episodic REINFORCE [Williams (1992)].
In actor-critic frameworks, we pick the baseline as the expected cumulative reward V (s) of the current state, which couples the two functions V (s) and π(a|s) together in the training, as shown in Fig. 1. Here the two functions reinforce each other: a correct π(a|s) gives high-rewarding trajectories which update V (s) towards the right direction; a correct V (s) picks out the correct actions for π(a|s) to reinforce. This mutual reinforcement behavior makes actor-critic model converge faster, but is also prone to converge to bad local minima, in particular for on-policy models that follow the very recent policy to sample trajectory during training. If the experience received by the agent in consecutive batches is highly correlated and biased towards a particular subset of the environment, then both π(a|s) and V (s) will be updated towards a biased direction and the agent may never see
2http://vizdoom.cs.put.edu.pl/competition-cig-2016
the whole picture. To reduce the correlation of game experience, Asynchronous Advantage ActorCritic Model [Mnih et al. (2016)] runs independent multiple threads of the game environment in parallel. These game instances are likely uncorrelated, therefore their experience in combination would be less biased.
For on-policy models, the same mutual reinforcement behavior will also lead to highly-peaked π(a|s) towards a few actions (or a few fixed action sequences), since it is always easy for both actor and critic to over-optimize on a small portion of the environment, and end up “living in their own realities”. To reduce the problem, [Mnih et al. (2016)] added an entropy term to the loss to encourage diversity, which we find to be critical. The final gradient update rules are listed as follows:
wπ ← wπ + α(Rt − V (st))∇wπ log π(at|st) + β∇wπH(π(·|st)) (1) wV ← wV − α∇wV (Rt − V (st)) 2 (2)
where Rt = ∑ T t′=t γt ′ −trt′ is the expected discounted reward at time t and α, β are the learning rate. In this work, we use Huber loss instead of the L2 loss in Eqn. 2.
Architecture. While [Mnih et al. (2016)] keeps a separate model for each asynchronous agent and perform model synchronization once in a while, we use an alternative approach called BatchA3C, in which all agents act on the same model and send batches to the main process for gradient descent optimization. The agents’ models are updated after each gradient update. Note that the contemporary work GA3C [Babaeizadeh et al. (2017)] also proposes a similar architecture. In their architecture, there is a prediction queue that collects agents’ experience and sends them to multiple predictors, and a training queue that collects experience to feed the optimization.

3 DOOM AS A REINFORCEMENT LEARNING PLATFORM
In Doom, the player controls the agent to fight against enemies in a 3D environment (e.g., in a maze). The agent can only see the environment from his viewpoint and thus receives partial information upon which it makes decisions. On modern computers, the original Doom runs in thousands of frames per second, making it suitable as a platform for training AI agent. ViZDoom [Kempka et al. (2016)] is an open-source platform that offers programming interface to communicate with Doom engine, ZDoom3. From the interface, users can obtain current frames of the game, and control the agent’s action. ViZDoom offers much flexibility, including:
Rich Scenarios. Many customized scenarios are made due to the popularity of the game, offering a variety of environments to train from. A scenario consists of many components, including 2D maps for the environment, scripts to control characters and events. Open-source tools, such as
3https://zdoom.org/
SLADE4, are also widely available to build new scenarios. We built our customized map (Fig. 2(b)) for training.
Game variables. In addition to image frames, ViZDoom environment also offers many games variables revealing the internal state of the game. This includes HEALTH, AMMO ? (agent’s health and ammunition), FRAG COUNT (current score) and so on. ViZDoom also offers USER? variables that are computed on the fly via scenario scripts. These USER? variables can provide more information of the agent, e.g., their spatial locations. Enemy information could also be obtained by modifying ViZDoom [Lample & Chaplot (2016)]. Such information is used to construct a reward function, or as a direct supervision to accelerate training [Lample & Chaplot (2016)].
Built-in bots. Built-in bots can be inserted in the battle. They are state machines with privileged information over the map and the player, which results in apparently decent intelligence with minimal computational cost. By competing against built-in bots, the agent learns to improve.
Evaluation Criterion. In FPS games, to evaluate their strength, multiple AIs are placed to a scenario for a deathmatch, in which every AI plays for itself against the remaining AIs. Frags per episode, the number of kills minus the number of suicides for the agent in one round of game, is often used as a metric. An AI is stronger if its frags is ranked higher against others. In this work, we use an episode of 2-minute game time (4200 frames in total) for all our evaluations unless noted otherwise.

4 METHOD

4.1 NETWORK ARCHITECTURE
We use convolutional neural networks to extract features from the game frames and then combine its output representation with game variables. Fig. 3 shows the network architecture and Tbl. 1 gives the parameters. It takes the frames as the input (i.e., the state s) and outputs two branches, one that outputs the value function V (s) by regression, while the other outputs the policy function π(s|a) by a regular softmax. The parameters of the two functions are shared before the branch.
For input, we use the most recent 4 frames plus the center part of them, scaled to the same size (120 × 120). Therefore, these centered “attention frames” have higher resolution than regular game frames, and greatly increase the aiming accuracy. The policy network will give 6 actions, namely MOVE FORWARD, MOVE LEFT, MOVE RIGHT, TURN LEFT, TURN RIGHT, and ATTACK. We found other on-off actions (e.g., MOVE BACKWARD) offered by ViZDoom less important. After feature extraction by convolutional network, game variables are incorporated. This includes the agent’s Health (0-100) and Ammo (how many bullets left). They are related to AI itself and thus legal in the game environment for training, testing and ViZDoom AI competition.

4.2 TRAINING PIPELINE
Our training procedure is implemented with TensorFlow [Abadi et al. (2016)] and tensorpack5. We open 255 processes, each running one Doom instance, and sending experience (st, at, rt) to the
4http://slade.mancubus.net/ 5https://github.com/ppwwyyxx/tensorpack
main process which runs the training procedure. The main process collects frames from different game instances to create batches, and optimizes on these batches asynchronously on one or more GPUs using Eqn. 1 and Eqn. 2. The frames from different processes running independent game instances, are likely to be uncorrelated, which stabilizes the training. This procedure is slightly different from the original A3C, where each game instance collects their own experience and updates the parameters asynchronously.
Despite the use of entropy term, we still find that π(·|s) is highly peaked. Therefore, during trajectory exploration, we encourage exploration by the following changes: a) multiply the policy output of the network by an exploration factor (0.2) before softmax b) uniformly randomize the action for 10% random frames.
As mentioned in [Kempka et al. (2016)], care should be taken for frame skips. Small frame skip introduces strong correlation in the training set, while big frame skip reduces effective training samples. We set frame skip to be 3. We choose 640x480 as the input frame resolution and do not use high aspect ratio resolution [Lample & Chaplot (2016)] to increase the field of view.
We use Adam [Kingma & Ba (2014)] with ǫ = 10−3 for training. Batch size is 128, discount factor γ = 0.99, learning rate α = 10−4 and the policy learning rate β = 0.08α. The model is trained from scratch. The training procedure runs on Intel Xeon CPU E5-2680v2 at 2. 80GHz, and 2 TitanX GPUs. It takes several days to obtain a decent result. Our final model, namely the F1 bot, is trained for around 3 million mini-batches on multiple different scenarios.

4.3 CURRICULUM LEARNING
When the environment only gives very sparse rewards, or adversarial, A3C takes a long time to converge to a satisfying solution. A direct training with A3C on the map CIGTrack1 with 8 builtin bots does not yield sensible performance. To address this, we use curriculum learning [Bengio et al. (2009)] that trains an agent with a sequence of progressively more difficult environments. By varying parameters in Doom (Sec. 3), we could control its difficulty level.
Reward Shaping. Reward shaping has been shown to be an effective technique to apply reinforcement learning in a complicated environment with delayed reward [Ng et al. (1999); Devlin et al. (2011)]. In our case, besides the basic reward for kills (+1) and death (-1), intermediate rewards are used as shown in Tbl. 2. We penalize agent with a living state, encouraging it to explore and encounter more enemies. health loss and ammo loss place linear reward for a decrement of health and ammunition. ammo pickup and health pickup place reward for picking up these two items. In addition, there is extra reward for picking up ammunition when in need (e.g. almost out of ammo). dist penalty and dist reward push the agent away from the previous locations, encouraging it to explore. The penalty is applied every action, when the displacement of the bot relative to the last state is less than a threshold dist penalty thres. And dist reward is applied for every unit displacement the agent makes. Similar to [Lample & Chaplot (2016)], the displacement information is computed from the ground truth location variables provided by Doom engine, and will not be used in the competition. However, unlike [Lample & Chaplot (2016)] that uses enemy-in-sight signal for training, locations can be extracted directly from USER? variables, or can easily be computed roughly with action history.
Curriculum Design. We train the bot on FlatMap that contains a simple square with a few pillars (Fig. 2(a)) with several curricula (Tbl. 3), and then proceed to CIGTrack1. For each map, we design curricula by varying the strength of built-in bots, i.e., their moving speed, initial health and initial weapon. Our agent always uses RocketLauncher as its only weapon. Training on FlatMap leads to a capable initial model which is quickly adapted to more complicated maps. As shown in Tbl. 2, for CIGTrack1 we increase dist penalty thres to keep the agent moving, and increase num bots so that the agent encounters more enemies per episode.
Adaptive Curriculum. In addition to staged curriculum learning, we also design adaptive curriculum learning by assigning a probability distribution on different levels for each thread that runs a Doom instance. The probability distribution shifts towards more difficult curriculum when the agent performs well on the current distribution, and shifts towards easier level otherwise. We consider the agent to perform well if its frag count is greater than 10 points.

4.4 POST-TRAINING RULES
For a better performance in the competition, we also put several rules to process the action given by the trained policy network, called post-training (PT) rules. There are two sets of buttons in ViZDoom: on-off buttons and delta buttons. While on-off button maps to the binary states of a keystroke (e.g., pressing the up arrow key will move the agent forward), delta buttons mimic the mouse behavior and could act faster in certain situations. Therefore, we setup rules that detect the intention of the agent and accelerate with delta button. For example, when the agent turns by invoking TURN LEFT repeatedly, we convert its action to TURN LEFT RIGHT DELTA for acceleration. Besides, the trained model might get stuck in rare situations, e.g., keep moving forward but blocked by an explosive bucket. We also designed rules to detect and fix them.

5 EXPERIMENT
In this section, we show the training procedure (Sec. 5.1), evaluate our AIs with ablation analysis (Sec. 5.2) and ViZDoom AI Competition (Sec. 5.3). We mainly compare among three AIs: (1) F1Pre, the bot trained with FlatMap only, (2) F1Plain, the bot trained on both FlatMap and CIGTrack1, but without post-training rules, and (3) the final F1 bot that attends competition.
5.1 CURRICULUM LEARNING ON FLATMAP
Fig. 4 shows that the curriculum learning increases the performance of the agents over all levels. When an agent becomes stronger in the higher level of class, it is also stronger in the lower level of class without overfitting. Fig. 5 shows comparison between adaptive curriculum learning with pure A3C. We can see that pure A3C can learn on FlatMap but is slower. Moreover, in CIGTrack1, a direct application of A3C does not yield sensible performance.

5.2 ABLATION ANALYSIS
Visualization. Fig. 6 shows the visualization of the first convolutional layer of the trained AI agent. We could see that the convolutional kernels of the current frame is less noisy than the kernels of previous frames. This means that the agent makes the most use of the current frames.
Effect of History Frames. Interestingly, while the agent focuses on the current frame, it also uses motion information. For this, we use (1) 4 duplicated current frames (2) 4 recent frames in reverse order, as the input. This gives 8.50 and 2.39 mean frags, compared to 10.34 in the normal case, showing that the agent heavily uses the motion information for better decision. In particular, the bot is totally confused with the reversed motion feature. Detailed results are shown in Tbl. 5.
Post-training Rules. Tbl. 5 shows that the post-training rules improve the performance. As a future work, an end-to-end training involving delta buttons could make the bot better.
Internal Tournament. We also evaluate our AIs with internal tournaments (Tbl. 4). All our bots beat the performance of built-in bots by a large margin, even though they use privileged information. F1Pre, trained with only FlatMap, shows decent performance, but is not as good as the models trained with both FlatMap and CIGTrack1. The final bot F1 performs the best.
Behaviors. Visually, the three bots behave differently. F1Pre is a bit overtrained in FlatMap and does not move too often, but when it sees enemies, even faraway, it will start to shoot. Occasionally it will move to the corner and pick medkits. In CIGTrack1, F1Pre stays in one place and ambushes opponents who pass by. On the other hand, F1Plain and F1 always move forwards and turn at the corner. As expected, F1 moves and turns faster.
Tactics All bots develop interesting local tactics when exchanging fire with enemy: they slide around when shooting the enemy. This is quite effective for dodging others’ attack. Also when they shoot the enemy, they usually take advantage of the splashing effect of rocket to cause additional damage for enemy, e.g., shooting the wall when the enemy is moving. They do not pick ammunition too often, even if they can no longer shoot. However, such disadvantage is mitigated by the nature of deathmatch: when a player dies, it will respawn with ammunition. We also check states with highest/lowest estimated future value V (s) over a 10-episode evaluation of F1 bot, from which we can speculate its tactics. The highest value is V = 0.97 when the agent fired, and about to hit the enemy. One low value is V = −0.44, ammo = 0, when the agent encountered an enemy at the corner but is out of ammunition. Both cases are reasonable.

5.3 COMPETITION
We attended the ViZDoom AI Competition hosted by IEEE CIG. There are 2 tracks in the competition. Track 1 (Limited Deathmatch) uses a known map and fixed weapons, while Track 2 (Full Deathmatch) uses 3 unknown maps and a variety of weapons. Each bot fights against all others for 12 rounds of 10 minutes each. Due to server capacity, each bot skips one match in the first 9 rounds. All bots are supposed to run in real-time (>35fps) on a GTX960 GPU.
Our F1 bot won 10 out of 11 attended games and won the champion for Track 1 by a large margin. We have achieved 559 frags, 35.4% higher than 413 frags achieved by Arnold [Lample & Chaplot (2016)], that uses extra game state for model training. On the other hand, IntelAct [Dosovitskiy & Koltun (2017)] won Track 2. The full videos for the two tracks have been released67, as well as an additional game between Human and AIs8. Our bot behaves reasonable and very human-like in Track 1. In the match between Human and AIs, our bot was even ahead of the human player for a short period (6:30 to 7:00).

6 CONCLUSION
Teaching agents to act properly in complicated and adversarial 3D environment is a very challenging task. In this paper, we propose a new framework to train a strong AI agent in a First-Person Shooter (FPS) game, Doom, using a combination of state-of-the-art Deep Reinforcement Learning and Curriculum Training. Via playing against built-in bots in a progressive manner, our bot wins the champion of Track1 (known map) in ViZDoom AI Competition. Furthermore, it learns to use motion features and build its own tactics during the game, which is never taught explicitly.
Currently, our bot is still an reactive agent that only remembers the last 4 frames to act. Ideally, a bot should be able to build a map from an unknown environment and localize itself, is able to have a global plan to act, and visualize its reasoning process. We leave them to future works.
","In this paper, we propose a new framework for training vision-based agent for First-Person Shooter (FPS) Game, in particular Doom. Our framework combines the state-of-the-art reinforcement learning approach (Asynchronous Advantage Actor-Critic (A3C) model [Mnih et al. (2016)]) with curriculum learning. Our model is simple in design and only uses game states from the AI side, rather than using opponents’ information [Lample & Chaplot (2016)]. On a known map, our agent won 10 out of the 11 attended games and the champion of Track1 in ViZDoom AI Competition 2016 by a large margin, 35% higher score than the second place.",ICLR 2017 conference submission,True,,"This paper provides a number of performance enhancements inspired by domain knowledge. Taken together, these produce a compelling system that has shown itself to be the best-in-class as per the related competition.
 Experts agree that the authors do a good job at justifying the majority of the design decisions.
 
 pros:
 - insights into the SOTA Doom player
 
 cons:
 - lack of pure technical novelty: the various elements have existed previously
 
 This paper comes down to a matter of taste in terms of appreciation of SOTA systems or technical novelty.
 With the code being released, I believe that this work will have impact as a benchmark, and as a guidebook
 as to how features can be combined for SOTA performance on FPS-style scenarios.

---

We thank the reviewers for their insightful comments!

All reviewers agree that this paper makes a solid contribution with good experimental results. It is not uncommon to see application-oriented papers using a combination of multiple techniques to achieve strong performance. This category covers many seminar works, e.g., deep reinforcement learning for Atari games (applying deep models to traditional Q-learning), or even AlphaGo (supervised learning, policy gradient, value function, Monte-Carlo Tree Search, self-play). It may be a bit shortsighted to judge such strong performing papers with a single criterion.

Confusion about the domain:
Reviewer3 mentions that the paper ""basically applies A3C to 3D spatial navigation tasks."", which is not true. In the deathmatch game of Doom, multiple players explore the maze and fight against each other to get a higher score, which is defined as #kills - #suicide. In this task, part of the goal is to learn anti-enemy tactics (e.g., dodging the rocket shot from the enemy, e.g., video:

---

The paper describes approaches taken to train learning agents for the 3D game Doom. The authors propose a number of performance enhancements (curriculum learning, attention (zoomed-in centered) frames, reward shaping, game variables, post-training rules) inspired by domain knowledge.

The enhancements together lead to a clear win as demonstrated by the competition results. From Fig 4, the curriculum learning clearly helps with learning over increasingly difficult settings. A nice result is that there is no overfitting to the harder classes once they have learned (probably because the curriculum is health and speed). The authors conclude from Fig 5 that the adaptive curriculum is better and more stable that pure A3C; however, this is a bit of a stretch given that graph. They go on to say that Pure A3C doesn't learn at all in the harder map but then show no result/graph to back this claim. Tbl 5 shows a clear benefit of the post-training rules.

If the goal is to solve problems like these (3D shooters), then this paper makes a significant contribution in that it shows which techniques are practical for solving the problem and ultimately improving performance in these kinds of tasks. Still, I am just not excited about this paper, mainly because it relies so heavily of many sources of domain knowledge, it is quite far from the pure reinforcement learning problem. The results are relatively unsurprising. Maybe they are novel for this problem, though.

I'm not sure we can realistically draw any conclusions about Figure 6 in the paper's current form. I recommend the authors increase the resolution or run some actual metrics to determine the fuzziness/clarity of each row/image: something more concrete than an arrow of already low-resolution images.

--- Added after rebuttal:

I still do not see any high-res images for Figure 6 or any link to them, but I trust that the authors will add them if accepted.

---

This paper basically applies A3C to 3D spatial navigation tasks. 

- This is not the first time A3C has been applied to 3D navigation. In fact the original paper reported these experiments. Although the experimental results are great, I am not sure if this paper has any additional insights to warrant itself as a conference paper. It might make more sense as a workshop paper

-  Are the graphs in Fig 5 constructed using a single hyper-parameter sweep? I think the authors should report results with many random initializations to make the comparisons more robust

- Overall the two main ideas in this paper -- A3C and curriculums -- are not really novel but the authors do make use of them in a real system.

---

This is a solid paper that applies A3C to Doom, enhancing it with a collection of tricks so as to win one of the VizDoom competitions. I think it is fair to expect the competition aspect to overshadow the more scientific approach of justifying every design decision in isolation, but in fact the authors do a decent job at the latter.

Two of my concerns have remained unanswered (see AnonReviewer2, below). 

In addition, the citation list is rather thin, for example reward shaping has a rich literature, as do incrementally more difficult task setups, dating back at least to Mark Ring’s work in the 1990s. There has also been a lot of complementary work on other FPS games. I’m not asking that the authors do any direct comparisons, but to give the reader a sense of context in which to place this.

---

This paper provides a number of performance enhancements inspired by domain knowledge. Taken together, these produce a compelling system that has shown itself to be the best-in-class as per the related competition.
 Experts agree that the authors do a good job at justifying the majority of the design decisions.
 
 pros:
 - insights into the SOTA Doom player
 
 cons:
 - lack of pure technical novelty: the various elements have existed previously
 
 This paper comes down to a matter of taste in terms of appreciation of SOTA systems or technical novelty.
 With the code being released, I believe that this work will have impact as a benchmark, and as a guidebook
 as to how features can be combined for SOTA performance on FPS-style scenarios.

---

We thank the reviewers for their insightful comments!

All reviewers agree that this paper makes a solid contribution with good experimental results. It is not uncommon to see application-oriented papers using a combination of multiple techniques to achieve strong performance. This category covers many seminar works, e.g., deep reinforcement learning for Atari games (applying deep models to traditional Q-learning), or even AlphaGo (supervised learning, policy gradient, value function, Monte-Carlo Tree Search, self-play). It may be a bit shortsighted to judge such strong performing papers with a single criterion.

Confusion about the domain:
Reviewer3 mentions that the paper ""basically applies A3C to 3D spatial navigation tasks."", which is not true. In the deathmatch game of Doom, multiple players explore the maze and fight against each other to get a higher score, which is defined as #kills - #suicide. In this task, part of the goal is to learn anti-enemy tactics (e.g., dodging the rocket shot from the enemy, e.g., video:

---

The paper describes approaches taken to train learning agents for the 3D game Doom. The authors propose a number of performance enhancements (curriculum learning, attention (zoomed-in centered) frames, reward shaping, game variables, post-training rules) inspired by domain knowledge.

The enhancements together lead to a clear win as demonstrated by the competition results. From Fig 4, the curriculum learning clearly helps with learning over increasingly difficult settings. A nice result is that there is no overfitting to the harder classes once they have learned (probably because the curriculum is health and speed). The authors conclude from Fig 5 that the adaptive curriculum is better and more stable that pure A3C; however, this is a bit of a stretch given that graph. They go on to say that Pure A3C doesn't learn at all in the harder map but then show no result/graph to back this claim. Tbl 5 shows a clear benefit of the post-training rules.

If the goal is to solve problems like these (3D shooters), then this paper makes a significant contribution in that it shows which techniques are practical for solving the problem and ultimately improving performance in these kinds of tasks. Still, I am just not excited about this paper, mainly because it relies so heavily of many sources of domain knowledge, it is quite far from the pure reinforcement learning problem. The results are relatively unsurprising. Maybe they are novel for this problem, though.

I'm not sure we can realistically draw any conclusions about Figure 6 in the paper's current form. I recommend the authors increase the resolution or run some actual metrics to determine the fuzziness/clarity of each row/image: something more concrete than an arrow of already low-resolution images.

--- Added after rebuttal:

I still do not see any high-res images for Figure 6 or any link to them, but I trust that the authors will add them if accepted.

---

This paper basically applies A3C to 3D spatial navigation tasks. 

- This is not the first time A3C has been applied to 3D navigation. In fact the original paper reported these experiments. Although the experimental results are great, I am not sure if this paper has any additional insights to warrant itself as a conference paper. It might make more sense as a workshop paper

-  Are the graphs in Fig 5 constructed using a single hyper-parameter sweep? I think the authors should report results with many random initializations to make the comparisons more robust

- Overall the two main ideas in this paper -- A3C and curriculums -- are not really novel but the authors do make use of them in a real system.

---

This is a solid paper that applies A3C to Doom, enhancing it with a collection of tricks so as to win one of the VizDoom competitions. I think it is fair to expect the competition aspect to overshadow the more scientific approach of justifying every design decision in isolation, but in fact the authors do a decent job at the latter.

Two of my concerns have remained unanswered (see AnonReviewer2, below). 

In addition, the citation list is rather thin, for example reward shaping has a rich literature, as do incrementally more difficult task setups, dating back at least to Mark Ring’s work in the 1990s. There has also been a lot of complementary work on other FPS games. I’m not asking that the authors do any direct comparisons, but to give the reader a sense of context in which to place this.",,,,,4.0,5.666666666666667,,,4.333333333333333,3.0,
356,"NEURO-SYMBOLIC PROGRAM SYNTHESIS
Authors: Emilio Parisotto, Abdel-rahman Mohamed, Rishabh Singh, Lihong Li, Dengyong Zhou, Pushmeet Kohli
Source file: 356.pdf

ABSTRACT
Recent years have seen the proposal of a number of neural architectures for the problem of Program Induction. Given a set of input-output examples, these architectures are able to learn mappings that generalize to new test inputs. While achieving impressive results, these approaches have a number of important limitations: (a) they are computationally expensive and hard to train, (b) a model has to be trained for each task (program) separately, and (c) it is hard to interpret or verify the correctness of the learnt mapping (as it is defined by a neural network). In this paper, we propose a novel technique, Neuro-Symbolic Program Synthesis, to overcome the above-mentioned problems. Once trained, our approach can automatically construct computer programs in a domain-specific language that are consistent with a set of input-output examples provided at test time. Our method is based on two novel neural modules. The first module, called the cross correlation I/O network, given a set of input-output examples, produces a continuous representation of the set of I/O examples. The second module, the RecursiveReverse-Recursive Neural Network (R3NN), given the continuous representation of the examples, synthesizes a program by incrementally expanding partial programs. We demonstrate the effectiveness of our approach by applying it to the rich and complex domain of regular expression based string transformations. Experiments show that the R3NN model is not only able to construct programs from new input-output examples, but it is also able to construct new programs for tasks that it had never observed before during training.

1 INTRODUCTION
The act of programming, i.e., developing a procedure to accomplish a task, is a remarkable demonstration of the reasoning abilities of the human mind. Expectedly, Program Induction is considered as one of the fundamental problems in Machine Learning and Artificial Intelligence. Recent progress on deep learning has led to the proposal of a number of promising neural architectures for this problem. Many of these models are inspired from computation modules (CPU, RAM, GPU) (Graves et al., 2014; Kurach et al., 2015; Reed & de Freitas, 2015; Neelakantan et al., 2015) or common data structures used in many algorithms (stack) (Joulin & Mikolov, 2015). A common thread in this line of work is to specify the atomic operations of the network in some differentiable form, allowing efficient end-to-end training of a neural controller, or to use reinforcement learning to make hard choices about which operation to perform. While these results are impressive, these approaches have a number of important limitations: (a) they are computationally expensive and hard to train, (b) a model has to be trained for each task (program) separately, and (c) it is hard to interpret or verify the correctness of the learnt mapping (as it is defined by a neural network). While some recently proposed methods (Kurach et al., 2015; Gaunt et al., 2016; Riedel et al., 2016; Bunel et al., 2016) do learn interpretable programs, they still need to learn a separate neural network model for each individual task.
Motivated by the need for model interpretability and scalability to multiple tasks, we address the problem of Program Synthesis. Program Synthesis, the problem of automatically constructing programs that are consistent with a given specification, has long been a subject of research in Computer Science (Biermann, 1978; Summers, 1977). This interest has been reinvigorated in recent years on
the back of the development of methods for learning programs in various domains, ranging from low-level bit manipulation code (Solar-Lezama et al., 2005) to data structure manipulations (Singh & Solar-Lezama, 2011) and regular expression based string transformations (Gulwani, 2011).
Most of the recently proposed methods for program synthesis operate by searching the space of programs in a Domain-Specific Language (DSL) instead of arbitrary Turing-complete languages. This hypothesis space of possible programs is huge (potentially infinite) and searching over it is a challenging problem. Several search techniques including enumerative (Udupa et al., 2013), stochastic (Schkufza et al., 2013), constraint-based (Solar-Lezama, 2008), and version-space algebra based algorithms (Gulwani et al., 2012) have been developed to search over the space of programs in the DSL, which support different kinds of specifications (examples, partial programs, natural language etc.) and domains. These techniques not only require significant engineering and research effort to develop carefully-designed heuristics for efficient search, but also have limited applicability and can only synthesize programs of limited sizes and types.
In this paper, we present a novel technique called Neuro-Symbolic Program Synthesis (NSPS) that learns to generate a program incrementally without the need for an explicit search. Once trained, NSPS can automatically construct computer programs that are consistent with any set of input-output examples provided at test time. Our method is based on two novel module neural architectures. The first module, called the cross correlation I/O network, produces a continuous representation of any given set of input-output examples. The second module, the Recursive-Reverse-Recursive Neural Network (R3NN), given the continuous representation of the input-output examples, synthesizes a program by incrementally expanding partial programs. R3NN employs a tree-based neural architecture that sequentially constructs a parse tree by selecting which non-terminal symbol to expand using rules from a context-free grammar (i.e., the DSL).
We demonstrate the efficacy of our method by applying it to the rich and complex domain of regularexpression-based syntactic string transformations, using a DSL based on the one used by FlashFill (Gulwani, 2011; Gulwani et al., 2012), a Programming-By-Example (PBE) system in Microsoft Excel 2013. Given a few input-output examples of strings, the task is to synthesize a program built on regular expressions to perform the desired string transformation. An example task that can be expressed in this DSL is shown in Figure 1, which also shows the DSL.
Our evaluation shows that NSPS is not only able to construct programs for known tasks from new input-output examples, but it is also able to construct completely new programs that it had not observed during training. Specifically, the proposed system is able to synthesize string transformation programs for 63% of tasks that it had not observed at training time, and for 94% of tasks when 100 program samples are taken from the model. Moreover, our system is able to learn 38% of 238 real-world FlashFill benchmarks.
To summarize, the key contributions of our work are:
• A novel Neuro-Symbolic program synthesis technique to encode neural search over the space of programs defined using a Domain-Specific Language (DSL).
• The R3NN model that encodes and expands partial programs in the DSL, where each node has a global representation of the program tree.
• A novel cross-correlation based neural architecture for learning continuous representation of sets of input-output examples.
• Evaluation of the NSPS approach on the complex domain of regular expression based string transformations.

2 PROBLEM DEFINITION
In this section, we formally define the DSL-based program synthesis problem that we consider in this paper. Given a DSL L, we want to automatically construct a synthesis algorithm A such that given a set of input-output example, {(i1, o1), · · · , (in, on)}, A returns a program P ∈ L that conforms to the input-output examples, i.e.,
∀j : 1 ≤ j ≤ n P (ij) = oj . (1)
The syntax and semantics of the DSL for string transformations is shown in Figure 1(b) and Figure 8 respectively. The DSL corresponds to a large subset of FlashFill DSL (except conditionals), and allows for a richer class of substring operations than FlashFill. A DSL program takes as input a string v and returns an output string o. The top-level string expression e is a concatenation of a finite list of substring expressions f1, · · · , fn. A substring expression f can either be a constant string s or a substring expression, which is defined using two position logics pl (left) and pr (right). A position logic corresponds to a symbolic expression that evaluates to an index in the string. A position logic p can either be a constant position k or a token match expression (r, k,Dir), which denotes the Start or End of the kth match of token r in input string v. A regex token can either be a constant string s or one of 8 regular expression tokens: p (ProperCase), C (CAPS), l (lowercase), d (Digits), α (Alphabets), αn (Alphanumeric), ∧ (StartOfString), and $ (EndOfString). The semantics of the DSL programs is described in the appendix.
A DSL program for the name transformation task shown in Figure 1(a) that is consistent with the examples is: Concat(f1,ConstStr(“, ”), f2,ConstStr(“.”)), where f1 ≡ SubStr(v, (“ ”,−1,End),ConstPos(−1)) and f2 ≡ SubStr(v,ConstPos(0),ConstPos(1)). The program concatenates the following 4 strings: i) substring between the end of last whitespace and end of string, ii) constant string “, ”, iii) first character of input string, and iv) constant string “.”.

3 OVERVIEW OF OUR APPROACH
We now present an overview of our approach. Given a DSL L, we learn a generative model of programs in the DSL L that is conditioned on input-output examples to efficiently search for consistent programs. The workflow of our system is shown in Figure 2, which is trained end-to-end using a large training set of programs in the DSL together with their corresponding input-output examples. To generate a large training set, we uniformly sample programs from the DSL and then use a rule-based strategy to compute well-formed input strings. Given a program P (sampled from the DSL), the rule-based strategy generates input strings for the program P ensuring that the preconditions of P are met (i.e. P doesn’t throw an exception on the input strings). It collects the pre-conditions of all Substring expressions present in the sampled program P and then generates inputs conforming to them. For example, let’s assume the sampled program is SubStr(v,(CAPS, 2, Start), (“ ”, 3, Start)), which extracts the substring between the start of 2nd capital letter and start of 3rd whitespace. The rule-based strategy would ensure that all the generated input strings consist of at least 2 capital letters and 3 whitespaces in addition to other randomly generated characters. The corresponding output strings are obtained by running the programs on the input strings.
A DSL can be considered as a context-free grammar with a start symbol S and a set of non-terminals with corresponding expansion rules. The (partial) grammar derivations or trees correspond to (partial) programs. A naı̈ve way to perform a search over the programs in a DSL is to start from the start symbol S and then randomly choose non-terminals to expand with randomly chosen expansion rules until reaching a derivation with only terminals. We, instead, learn a generative model over partial derivations in the DSL that assigns probabilities to different non-terminals in a partial derivation and corresponding expansions to guide the search for complete derivations.
Our generative model uses a Recursive-Reverse-Recursive Neural Network (R3NN) to encode partial trees (derivations) in L, where each node in the partial tree encodes global information about every other node in the tree. The model assigns a vector representation for every symbol and every expansion rule in the grammar. Given a partial tree, the model first assigns a vector representation to each leaf node, and then performs a recursive pass going up in the tree to assign a global tree representation to the root. It then performs a reverse-recursive pass starting from the root to assign a global tree representation to each node in the tree.
The generative process is conditioned on a set of input-output examples to learn a program that is consistent with this set of examples. We experiment with multiple input-output encoders including an LSTM encoder that concatenates the hidden vectors of two deep bidirectional LSTM networks for input and output strings in the examples, and a Cross Correlation encoder that computes the cross correlation between the LSTM tensor representations of input and output strings in the examples. This vector is then used as an additional input in the R3NN model to condition the generative model.

4 TREE-STRUCTURED GENERATION MODEL
We define a program t-steps into construction as a partial program tree (PPT) (see Figure 3 for a visual depiction). A PPT has two types of nodes: leaf (symbol) nodes and inner non-leaf (rule) nodes. A leaf node represents a symbol, whether non-terminal or terminal. An inner non-leaf node represents a particular production rule of the DSL, where the number of children of the non-leaf node is equivalent to the arity of the RHS of the rule it represents. A PPT is called a program tree (PT) whenever all the leaves of the tree are terminal symbols. Such a tree represents a completed program under the DSL and can be executed. We define an expansion as the valid application of a specific production rule (e → e op2 e) to a specific non-terminal leaf node within a PPT (leaf with symbol e). We refer to the specific production rule that an expansion is derived from as the expansion type. It can be seen that if there exist two leaf nodes (l1 and l2) with the same symbol then for every expansion specific to l1 there exists an expansion specific to l2 with the same type.

4.1 RECURSIVE-REVERSE-RECURSIVE NEURAL NETWORK
In order to define a generation model over PPTs, we need an efficient way of assigning probabilities to every valid expansion in the current PPT. A valid expansion has two components: first the production rule used, and second the position of the expanded leaf node relative to every other node in the tree. To account for the first component, a separate distributed representation for each production rule is maintained. The second component is handled using an architecture where the forward propagation resembles belief propagation on trees, allowing a notion of global tree state at every node within the tree. A given expansion probability is then calculated as being proportional to the inner product between the production rule representation and the global-tree representation of the leaf-level non-terminal node. We now describe the design of this architecture in more detail.
The R3NN has the following parameters for the grammar described by a DSL (see Figure 3):
1. For every symbol s ∈ S, an M−dimensional representation φ(s) ∈ RM . 2. For every production rule r ∈ R, an M−dimensional representation ω(r) ∈ RM .
3. For every production rule r ∈ R, a deep neural network fr which takes as input a vector x ∈ RQ·M , with Q being the number of symbols on the RHS of the production rule r, and outputs a vector y ∈ RM . Therefore, the production-rule network fr takes as input a concatenation of the distributed representations of each of its RHS symbols and produces a distributed representation for the LHS symbol.
4. For every production rule r ∈ R, an additional deep neural network gr which takes as input a vector x′ ∈ RM and outputs a vector y′ ∈ RQ·M . We can think of gr as a reverse production-rule network that takes as input a vector representation of the LHS and produces a concatenation of the distributed representations of each of the rule’s RHS symbols.
Let E be the set of all valid expansions in a PPT T , let L be the current leaf nodes of T and N be the current non-leaf (rule) nodes of T . Let S(l) be the symbol of leaf l ∈ L and R(n) represent the production rule of non-leaf node n ∈ N .

4.1.1 GLOBAL TREE INFORMATION AT THE LEAVES
To compute the probability distribution over the set E, the R3NN first computes a distributed representation for each leaf node that contains global tree information. To accomplish this, for every leaf node l ∈ L in the tree we retrieve its distributed representation φ(S(l)) . We now do a standard recursive bottom-to-top, RHS→LHS pass on the network, by going up the tree and applying fR(n) for every non-leaf node n ∈ N on its RHS node representations (see Figure 3(a)). These networks fR(n) produce a node representation which is input into the parent’s rule network and so on until we reach the root node.
Once at the root node, we effectively have a fixed-dimensionality global tree representation φ(root) for the start symbol. The problem is that this representation has lost any notion of tree position. To solve this problem R3NN now does what is effectively a reverse-recursive pass which starts at the root node with φ(root) as input and moves towards the leaf nodes (see Figure 3(b)).
More concretely, we start with the root node representation φ(root) and use that as input into the rule network gR(root) where R(root) is the production rule that is applied to the start symbol in T . This produces a representation φ′(c) for each RHS node c of R(root). If c is a non-leaf node, we iteratively apply this procedure to c, i.e., process φ′(c) using gR(c) to get representations φ′(cc) for every RHS node cc of R(c), etc. If c is a leaf node, we now have a leaf representation φ′(c) which has an information path to φ(root) and thus to every other leaf node in the tree. Once the reverse-recursive process is complete, we now have a distributed representation φ′(l) for every leaf node l which contains global tree information. While φ(l1) and φ(l2) could be equal for leaf nodes which have the same symbol type, φ′(l1) and φ′(l2) will not be equal even if they have the same symbol type because they are at different positions in the tree.

4.1.2 EXPANSION PROBABILITIES
Given the global leaf representations φ′(l), we can now straightforwardly acquire scores for each e ∈ E. For expansion e, let e.r be the expansion type (production rule r ∈ R that e applies) and let e.l be the leaf node l that e.r is applied to. ze = φ′(e.l) · ω(e.r) The score of an expansion is calculated using ze = φ′(e.l) · ω(e.r). The probability of expansion e is simply the exponentiated normalized sum over all scores: π(e) = e
ze∑ e′∈E e z e′ .
An additional improvement that was found to help was to add a bidirectional LSTM (BLSTM) to process the global leaf representations right before calculating the scores. To do this, we first order the global leaf representations sequentially from left-most leaf node to right-mode leaf node. We then treat each leaf node as a time step for a BLSTM to process. This provides a sort of skip connection between leaf nodes, which potentially reduces the path length that information needs to travel between leaf nodes in the tree. The BLSTM hidden states are then used in the score calculation rather than the leaves themselves.
The R3NN can be seen as an extension and combination of several previous tree-based models, which were mainly developed in the context of natural language processing (Le & Zuidema, 2014; Paulus et al., 2014; Irsoy & Cardie, 2013).

5 CONDITIONING WITH INPUT/OUTPUT EXAMPLES
Now that we have defined a generation process over tree-structured programs, we need a way of conditioning this generation process on a set of input/output examples. The set of input/output examples provide a nearly complete specification for the desired output program, and so a good encoding of the examples is crucial to the success of our program generator. For the most part, this example encoding needs to be domain-specific, since different DSLs have different inputs (some may operate over integers, some over strings, etc.). Therefore, in our case, we use an encoding adapted to the input-output strings that our DSL operates over. We also investigate different ways of conditioning program search on the learnt example input-output encodings.

5.1 ENCODING INPUT/OUTPUT EXAMPLES
There are two types of information that string manipulation programs need to extract from inputoutput examples: 1) constant strings, such as “@domain.com” or “.”, which appear in all output examples; 2) substring indices in input where the index might be further defined by a regular expression. These indices determine which parts of the input are also present in the output. To simplify the DSL, we assume that there is a fixed finite universe of possible constant strings that could appear in programs. Therefore we focus on extracting the second type of information, the substring indices.
In earlier hand-engineered systems such as FlashFill, this information was extracted from the inputoutput strings by running the Longest Common Substring algorithm, a dynamic programming algorithm that efficiently finds matching substrings in string pairs. To extract substrings, FlashFill runs LCS on every input-output string pair in the I/O set to get a set of substring candidates. It then takes the entire set of substring candidates and simply tries every possible regex and constant index that can be used at substring boundaries, exhaustively searching for the one which is the most “general”, where generality is specified by hand-engineered heuristics.
In contrast to these previous methods, instead of hand-designing a complicated algorithm to extract regex-based substrings, we develop neural network based architectures that are capable of learning to extract and produce continuous representations of the likely regular expressions given I/O examples.

5.1.1 BASELINE LSTM ENCODER
Our first I/O encoding network involves running two separate deep bidirectional LSTM networks for processing the input and the output string in each example pair. For each pair, it then concatenates the topmost hidden representation at every time step to produce a 4HT -dimensional feature vector per I/O pair, where T is the maximum string length for any input or output string, and H is the topmost LSTM hidden dimension.
We then concatenate the encoding vectors across all I/O pairs to get a vector representation of the entire I/O set. This encoding is conceptually straightforward and has very little prior knowledge about what operations are being performed over the strings, i.e., substring, constant, etc., which might make it difficult to discover substring indices, especially the ones based on regular expressions.

5.1.2 CROSS CORRELATION ENCODER
To help the model discover input substrings that are copied to the output, we designed an novel I/O example encoder to compute the cross correlation between each input and output example representation. We used the two output tensors of the LSTM encoder (discussed above) as inputs to this encoder. For each example pair, we first slide the output feature block over the input feature block and compute the dot product between the respective position representation. Then, we sum over all overlapping time steps. Features of all pairs are then concatenated to form a 2∗ (T −1)-dimensional vector encoding for all example pairs. There are 2 ∗ (T − 1) possible alignments in total between input and output feature blocks. An illustration of the cross-correlation encoder is shown in Figure 9. We also designed the following variants of this encoder.
Diffused Cross Correlation Encoder: This encoder is identical to the Cross Correlation encoder except that instead of summing over overlapping time steps after the element-wise dot product, we simply concatenate the vectors corresponding to all time steps, resulting in a final representation that contains 2 ∗ (T − 1) ∗ T features for each example pair. LSTM-Sum Cross Correlation Encoder: In this variant of the Cross Correlation encoder, instead of doing an element-wise dot product, we run a bidirectional LSTM over the concatenated feature blocks of each alignment. We represent each alignment by the LSTM hidden representation of the final time step leading to a total of 2 ∗H ∗ 2 ∗ (T − 1) features for each example pair. Augmented Diffused Cross Correlation Encoder: For this encoder, the output of each character position of the Diffused Cross Correlation encoder is combined with the character embedding at this position, then a basic LSTM encoder is run over the combined features to extract a 4∗H-dimensional vector for both the input and the output streams. The LSTM encoder output is then concatenated with the output of the Diffused Cross Correlation encoder forming a (4∗H+T ∗(T−1))-dimensional feature vector for each example pair.

5.2 CONDITIONING PROGRAM SEARCH ON EXAMPLE ENCODINGS
Once the I/O example encodings have been computed, we can use them to perform conditional generation of the program tree using the R3NN model. There are a number of ways in which the PPT generation model can be conditioned using the I/O example encodings depending on where the I/O example information is inserted in the R3NN model. We investigated three locations to inject example encodings:
1) Pre-conditioning: where example encodings are concatenated to the encoding of each tree leaf, and then passed to a conditioning network before the bottom-up recursive pass over the program tree. The conditioning network can be either a multi-layer feedforward network, or a bidirectional LSTM network running over tree leaves. Running an LSTM over tree leaves allows the model to learn more about the relative position of each leaf node in the tree.
2) Post-conditioning: After the reverse-recursive pass, example encodings are concatenated to the updated representation of each tree leaf and then fed to a conditioning network before computing the expansion scores.
3) Root-conditioning: After the recursive pass over the tree, the root encoding is concatenated to the example encodings and passed to a conditioning network. The updated root representation is then used to drive the reverse-recursive pass.
Empirically, pre-conditioning worked better than either root- or post- conditioning. In addition, conditioning at all 3 places simultaneously did not cause a significant improvement over just pre-conditioning. Therefore, for the experimental section, we report models which only use preconditioning.

6 EXPERIMENTS
In order to evaluate and compare variants of the previously described models, we generate a dataset randomly from the DSL. To do so, we first enumerate all possible programs under the DSL up to a specific number of instructions, which are then partitioned into training, validation and test sets. In order to have a tractable number of programs, we limited the maximum number of instructions for programs to be 13. Length 13 programs are important for this specific DSL because all larger programs can be written as compositions of sub-programs of length at most 13. The semantics of length 13 programs therefore constitute the “atoms” of this particular DSL.
In testing our model, there are two different categories of generalization. The first is input/output generalization, where we are given a new set of input/output examples as well as a program with a specific tree that we have seen during training. This represents the model’s capacity to be applied on new data. The second category is program generalization, where we are given both a previously unseen program tree in addition to unseen input/output examples. Therefore the model needs to have a sufficient enough understanding of the semantics of the DSL that it can construct novel combinations of operations. For all reported results, training sets correspond to the first type of generalization since we have seen the program tree but not the input/output pairs. Test sets represent the second type of generalization, as they are trees which have not been seen before on input/output pairs that have also not been seen before.
In this section, we compare several different variants of our model. We first evaluate the effect of each of the previously described input/output encoders. We then evaluate the R3NN model against a simple recurrent model called io2seq, which is basically an LSTM that takes as input the input/output conditioning vector and outputs a sequence of DSL symbols that represents a linearized program tree. Finally, we report the results of the best model on the length 13 training and testing sets, as well as on a set of 238 benchmark functions.

6.1 SETUP AND HYPERPARAMETERS SETTINGS
For training the R3NN, two hyperparameters that were crucial for stabilizing training were the use of hyperbolic tangent activation functions in both R3NN (other activations such as ReLU more consistently diverged during our initial experiments) and cross-correlation I/O encoders and the use of minibatches of length 8. Additionally, for all results, the program tree generation is conditioned on a set of 10 input/output string pairs. We used ADAM (Kingma & Ba, 2014) to optimize the networks with a learning rate of 0.001. Network weights used the default torch initializations.
Due to the difficulty of batching tree-based neural networks since each sample in a batch has a potentially different tree structure, we needed to do batching sequentially. Therefore for each minibatch of size N , we accumulated the gradients for each sample. After all N sample gradients were accumulated, we updated the parameters and reset the accumulated gradients. Due to this sequential processing, in order to train models in a reasonable time, we limited our batch sizes to between 8-12. Despite the computational inefficiency, batching was critical to successfully train an R3NN, as online learning often caused the network to diverge.
For each latent function and set of input/output examples that we test on, we report whether we had a success after sampling 100 functions from the model and testing all 100 to see if one of these functions is equivalent to the latent function. Here we consider two functions to be equivalent with respect to a specific input/output example set if the functions output the same strings when run on the inputs. Under this definition, two functions can have a different set of operations but still be equivalent with respect to a specific input-output set.
We restricted the maximum size of training programs to be 13 because of two computational considerations. As described earlier, one difficulty was in batching tree-based neural networks of different structure and the computational cost of batching increases with the increase in size of the program trees. The second issue is that valid I/O strings for programs often grow with the program length, in the sense that for programs of length 40 a minimal valid I/O string will typically be much longer than a minimal valid I/O string for length 20 programs. For example, for a program such as (Concat (ConstStr “longstring”) (Concat (ConstStr “longstring”) (Concat (ConstStr “longstring”) ...))), the valid output string would be “longstringlongstringlongstring...” which could be many
hundreds of characters long. Because of limited GPU memory, the I/O encoder models can quickly run out of memory.

6.2 EXAMPLE ENCODING
In this section, we evaluate the effect of several different input/output example encoders. To control for the effect of the tree model, all results here used an R3NN with fixed hyperparameters to generate the program tree. Table 1 shows the performance of several of these input/output example encoders. We can see that the summed cross-correlation encoder did not perform well, which can be due to the fact that the sum destroys positional information that might be useful for determining specific substring indices. The LSTM-sum and the augmented diffused cross-correlation models did the best. Surprisingly, the LSTM encoder was capable of finding nearly 88% of all programs without having any prior knowledge explicitly built into the architecture. We use 100 samples for evaluating the Train and Test sets. The training performance is sometimes slightly lower because there are close to 5 million training programs but we only look at less than 2 million of these programs during training. We sample a subset of only 1000 training programs from the 5 million program set to report the training results in the tables. The test sets also consist of 1000 programs.

6.3 IO2SEQ
In this section, we motivate the use of the R3NN by testing whether a simpler model can also be used to generate programs. The io2seq model is an LSTM whose initial hidden and cell states are a function of the input/output encoding vector. The io2seq model then generates a linearized tree of a program symbol-by-symbol. An example of what a linearized program tree looks like is (S(e(f (ConstStr“@”)ConstStr)f )e)S , which represents the program tree that returns the constant string “@”. Predicting a linearized tree using an LSTM was also done in the context of parsing (Vinyals et al., 2015). For the io2seq model, we used the LSTM-sum cross-correlation I/O conditioning model.
The results in Table 2 show that the performance of the io2seq model at 100 samples per latent test function is far worse than the R3NN, at around 42% versus 91%, respectively. The reasons for that could be that the io2seq model needs to perform far more decisions than the R3NN, since the io2seq model has to predict the parentheses symbols that determine at which level of the tree a particular symbol is at. For example, the io2seq model requires on the order of 100 decisions for length 13 programs, while the R3NN requires no more than 13.

6.4 EFFECT OF SAMPLING MULTIPLE PROGRAMS
For the best R3NN model that we trained, we also evaluated the effect that a different number of samples per latent function had on performance. The results are shown in Table 3. The increase of the model’s performance as the sample size increases hints that the model has a notion of what type of program satisfies a given I/O pair, but it might not be that certain about the details such as which regex to use, etc. By 300 samples, the model is nearing perfect accuracy on the test sets.

6.5 EFFECT OF NUMBER OF INPUT-OUTPUT EXAMPLES
We evaluate the effect of varying the number of input-output examples used to train the Input-output encoders. The 1-best accuracy for train and test data for models trained for 74 epochs is shown in Figure 4. As expected, the accuracy increases with increase in number of input-output examples, since more examples add more information to the encoder and constrain the space of consistent programs in the DSL.

6.6 FLASHFILL BENCHMARKS
We also evaluate our learnt models on 238 real-world FlashFill benchmarks obtained from the Microsoft Excel team and online help-forums. These benchmarks involve string manipulation tasks described using input-output examples. We evaluate two models – one with a cross correlation encoder trained on 5 input-output examples and another trained on 10 input-output examples. Both the models were trained on randomly sampled programs from the DSL upto size 13 with randomly generated input-output examples.
The distribution of the size of smallest DSL programs needed to solve the benchmark tasks is shown in Figure 5(a), which varies from 4 to 63. The figure also shows the number of benchmarks for which our model was able to learn the program using 5 input-output examples using samples of top-2000 learnt programs. In total, the model is able to learn programs for 91 tasks (38.2%). Since the model was trained for programs upto size 13, it is not surprising that it is not able to solve tasks that need larger program size. There are 110 FlashFill benchmarks that require programs upto size 13, out of which the model is able to solve 82.7% of them.
The effect of sampling multiple learnt programs instead of only top program is shown in Figure 5(b). With only 10 samples, the model can already learn about 13% of the benchmarks. We observe a steady increase in performance upto about 2000 samples, after which we do not observe any significant improvement. Since there are more than 2 million programs in the DSL of length 11 itself, the enumerative techniques with uniform search do not scale well (Alur et al., 2015).
We also evaluate a model that is learnt with 10 input-output examples per benchmark. This model can only learn programs for about 29% of the FlashFill benchmarks. Since the FlashFill benchmarks contained only 5 input-output examples for each task, to run the model that took 10 examples as input, we duplicated the I/O examples. Our models are trained on the synthetic training dataset
that is generated uniformly from the DSL. Because of the discrepancy between the training data distribution (uniform) and auxiliary task data distribution, the model with 10 input/output examples might not perform the best on the FlashFill benchmark distribution, even though it performs better on the synthetic data distribution (on which it is trained) as shown in Figure 4.
Our model is able to solve majority of FlashFill benchmarks that require learning programs with upto 3 Concat operations. We now describe a few of these benchmarks, also shown in Figure 6. An Excel user wanted to clean a set of medical billing records by adding a missing “]” to medical codes as shown in Figure 6(a). Our system learns the following program given these 5 input-output examples: Concat(SubStr(v,ConstPos(0),(d,-1,End)), ConstStr(“]”)). The program concatenates the substring between the start of the input string and the position of the last digit regular expression with the constant string “]”. Another task that required user to transform some numbers into a hex format is shown in Figure 6(b). Our system learns the following program: Concat(ConstStr(“0x”),SubStr(v,ConstPos(0),ConstPos(2))). For some benchmarks with long input strings, it is still able to learn regular expressions to extract the desired substring, e.g. it learns a program to extract “NancyF” from the string “123456789,freehafer ,drew ,nancy,19700101,11/1/2007,NancyF@north.com,1230102,123 1st Avenue,Seattle,wa,09999”.
Our system is currently not able to learn programs for benchmarks that require 4 or more Concat operations. Two such benchmarks are shown in Figure 7. The task of combining names in Figure 7(a) requires 6 Concat arguments, whereas the phone number transformation task in Figure 7(b) requires 5 Concat arguments. This is mainly because of the scalability issues in training with programs of larger size. There are also a few interesting benchmarks where the R3NN models gets very close to learning the desired program. For example, for the task “Bill Gates” → “Mr. Bill Gates”, it learns a program that generates “Mr.Bill Gates” (without the whitespace), and for the task “617-444-5454” → “(617) 444-5454”, it learns a program that generates the string “(617 444-5454”.

7 RELATED WORK
We have seen a renewed interest in recent years in the area of Program Induction and Synthesis.
In the machine learning community, a number of promising neural architectures have been proposed to perform program induction. These methods have employed architectures inspired from computation modules (Turing Machines, RAM) (Graves et al., 2014; Kurach et al., 2015; Reed & de Freitas, 2015; Neelakantan et al., 2015) or common data structures such as stacks used in many algorithms (Joulin & Mikolov, 2015). These approaches represent the atomic operations of the network in a differentiable form, which allows for efficient end-to-end training of a neural controller. However, unlike our approach that learns comprehensible complete programs, many of these approaches learn only the program behavior (i.e., they produce desired outputs on new input data). Some recently proposed methods (Kurach et al., 2015; Gaunt et al., 2016; Riedel et al., 2016; Bunel et al., 2016) do learn interpretable programs but these techniques require learning a separate neural network model for each individual task, which is undesirable in many synthesis settings where we would like to learn programs in real-time for a large number of tasks. Liang et al. (2010) restrict the problem space with a probabilistic context-free grammar and introduce a new representation of programs based on combinatory logic, which allows for sharing sub-programs across multiple tasks. They then take a hierarchical Bayesian approach to learn frequently occurring substructures of programs. Our approach, instead, uses neural architectures to condition the search space of programs, and does not require additional step of representing program space using combinatory logic for allowing sharing.
The DSL-based program synthesis approach has also seen a renewed interest recently (Alur et al., 2015). It has been used for many applications including synthesizing low-level bitvector implementations (Solar-Lezama et al., 2005), Excel macros for data manipulation (Gulwani, 2011; Gulwani et al., 2012), superoptimization by finding smaller equivalent loop bodies (Schkufza et al., 2013), protocol synthesis from scenarios (Udupa et al., 2013), synthesis of loop-free programs (Gulwani et al., 2011), and automated feedback generation for programming assignments (Singh et al., 2013). The synthesis techniques proposed in the literature generally employ various search techniques including enumeration with pruning, symbolic constraint solving, and stochastic search, while supporting different forms of specifications including input-output examples, partial programs, program invariants, and reference implementation.
In this paper, we consider input-output example based specification over the hypothesis space defined by a DSL of string transformations, similar to that of FlashFill (without conditionals) (Gulwani, 2011). The key difference between our approach over previous techniques is that our system is trained completely in an end-to-end fashion, while previous techniques require significant manual effort to design heuristics for efficient search. There is some work on guiding the program search using learnt clues that suggest likely DSL expansions, but the clues are learnt over hand-coded textual features of examples (Menon et al., 2013). Moreover, their DSL consists of composition of about 100 high-level text transformation functions such as count and dedup, whereas our DSL consists of tree structured programs over richer regular expression based substring constructs.
There is also a recent line of work on learning probabilistic models of code from a large number of code repositories (big code) (Raychev et al., 2015; Bielik et al., 2016; Hindle et al., 2016), which are then used for applications such as auto-completion of partial programs, inference of variable and method names, program repair, etc. These language models typically capture only the syntactic
properties of code, unlike our approach that also tries to capture the semantics to learn the desired program. The work by Maddison & Tarlow (2014) addresses the problem of learning structured generative models of source code but both their model and application domain are different from ours. Piech et al. (2015) use an NPM-RNN model to embed program ASTs, where a subtree of the AST rooted at a node n is represented by a matrix obtained by combining representations of the children of node n and the embedding matrix of the node n itself (which corresponds to its functional behavior). The forward pass in our R3NN architecture from leaf nodes to the root node is, at a high-level, similar, but we use a distributed representation for each grammar symbol that leads to a different root representation. Moreover, R3NN also performs a reverse-recursive pass to ensure all nodes in the tree encode global information about other nodes in the tree. Finally, the R3NN network is then used to incrementally build a tree to synthesize a program.
The R3NN model employed in our work is related to several tree and graph structured neural networks present in the NLP literature (Le & Zuidema, 2014; Paulus et al., 2014; Irsoy & Cardie, 2013). The Inside-Outside Recursive Neural Network (Le & Zuidema, 2014) in particular is most similar to the R3NN, where they generate a parse tree incrementally by using global leaf-level representations to determine which expansions in the parse tree to take next.

8 CONCLUSION
We have proposed a novel technique called Neuro-Symbolic Program Synthesis that is able to construct a program incrementally based on given input-output examples. To do so, a new neural architecture called Recursive-Reverse-Recursive Neural Network is used to encode and expand a partial program tree into a full program tree. Its effectiveness at example-based program synthesis is demonstrated, even when the program has not been seen during training.
These promising results open up a number of interesting directions for future research. For example, we took a supervised-learning approach here, assuming availability of target programs during training. In some scenarios, we may only have access to an oracle that returns the desired output given an input. In this case, reinforcement learning is a promising framework for program synthesis.

A DOMAIN-SPECIFIC LANGUAGE FOR STRING TRANSFORMATIONS
The semantics of the DSL programs is shown in Figure 8. The semantics of a Concat expression is to concatenate the results of recursively evaluating the constituent substring expressions fi. The semantics of ConstStr(s) is to simply return the constant string s. The semantics of a substring expression is to first evaluate the two position logics pl and pr to p1 and p2 respectively, and then return the substring corresponding to v[p1..p2]. We denote s[i..j] to denote the substring of string s starting at index i (inclusive) and ending at index j (exclusive), and len(s) denotes its length. The semantics of ConstPos(k) expression is to return k if k > 0 or return len + k (if k < 0). The semantics of position logic (r, k, Start) is to return the Start of kth match of r in v from the beginning (if k > 0) or from the end (if k < 0).
","Recent years have seen the proposal of a number of neural architectures for the problem of Program Induction. Given a set of input-output examples, these architectures are able to learn mappings that generalize to new test inputs. While achieving impressive results, these approaches have a number of important limitations: (a) they are computationally expensive and hard to train, (b) a model has to be trained for each task (program) separately, and (c) it is hard to interpret or verify the correctness of the learnt mapping (as it is defined by a neural network). In this paper, we propose a novel technique, Neuro-Symbolic Program Synthesis, to overcome the above-mentioned problems. Once trained, our approach can automatically construct computer programs in a domain-specific language that are consistent with a set of input-output examples provided at test time. Our method is based on two novel neural modules. The first module, called the cross correlation I/O network, given a set of input-output examples, produces a continuous representation of the set of I/O examples. The second module, the RecursiveReverse-Recursive Neural Network (R3NN), given the continuous representation of the examples, synthesizes a program by incrementally expanding partial programs. We demonstrate the effectiveness of our approach by applying it to the rich and complex domain of regular expression based string transformations. Experiments show that the R3NN model is not only able to construct programs from new input-output examples, but it is also able to construct new programs for tasks that it had never observed before during training.",ICLR 2017 conference submission,True,,"This paper proposes a model that is able to infer a program from input/output example pairs, focusing on a restricted domain-specific language that captures a fairly wide variety of string transformations, similar to that used by Flash Fill in Excel.  The approach is to model successive “extensions” of a program tree conditioned on some embedding of the input/output pairs.  Extension probabilities are computed as a function of leaf and production rule embeddings — one of the main contributions is the so-called “Recursive-Reverse-Recursive Neural Net” which computes a globally aware embedding of a leaf by doing something that looks like belief propagation on a tree (but training this operation in an end-to-end differentiable way).

There are many strong points about this paper.  In contrast with some of the related work in the deep learning community, I can imagine this being used in an actual application in the near future.  The R3NN idea is a good one and the authors motivate it quite well.  Moreover, the authors have explored many variants of this model to understand what works well and what does not.  Finally, the exposition is clear (even if it is a long paper), which made this paper a pleasure to read.  Some weaknesses of this paper: the results are still not super accurate, perhaps because the model has only been trained on small programs but is being asked to infer programs that should be much longer.  And it’s unclear why the authors did not simply train on longer programs…  It also seems that the number of I/O pairs is fixed?  So if I had more I/O pairs, the model might not be able to use those additional pairs (and based on the experiments, more pairs can hurt…).  Overall however, I would certainly like to see this paper accepted at ICLR.

Other miscellaneous comments:
* Too many e’s in the expansion probability expression — might be better just to write “Softmax”.
* There is a comment about adding a bidirectional LSTM to process the global leaf representations before calculating scores, but no details are given on how this is done (as far as I can see).
* The authors claim that using hyperbolic tangent activation functions is important — I’d be interested in some more discussion on this and why something like ReLU would not be good.
* It’s unclear to me how batching was done in this setting since each program has a different tree topology.  More discussion on this would be appreciated.  Related to this, it would be good to add details on optimization algorithm (SGD?  Adagrad?  Adam?), learning rate schedules and how weights were initialized.  At the moment, the results are not particularly reproducible.
* In Figure 6 (unsolved benchmarks), it would be great to add the program sizes for these harder examples (i.e., did the approach fail because these benchmarks require long programs?  Or was it some other reason?)
* There is a missing related work by Piech et al (Learning Program Embeddings…) where the authors trained a recursive neural network (that matched abstract syntax trees for programs submitted to an online course) to predict program output (but did not synthesize programs).

---

There is a bit of a spread in the reviewer scores and unfortunately it wasn't possible to entice the reviewers to participate in a discussion. The area chair therefore discounts the late review of reviewer3, who seems to have had a misunderstanding that was successfully rebutted by the authors. The other reviewers are supportive of the paper.

---

Dear reviewers, do you have any reactions after the authors responded to your reviews?

---

The paper presents a method to synthesize string manipulation programs based on a set of input output pairs. The paper focuses on a restricted class of programs based on a simple context free grammar sufficient to solve string manipulation tasks from the FlashFill benchmark. A probabilistic generative model called Recursive-Reverse-Recursive Neural Network (R3NN) is presented that assigns a probability to each program's parse tree after a bottom-up and a top-down pass. Results are presented on a synthetic dataset and a Microsoft Excel benchmark called FlashFill.

The problem of program synthesis is important with a lot of recent interest from the deep learning community. The approach taken in the paper based on parse trees and recursive neural networks seems interesting and promising. However, the model seems too complicated and unclear at several places (details below). On the negative side, the experiments are particularly weak, and the paper does not seem ready for publication based on its experimental results. I was positive about the paper until I realized that the method obtains an accuracy of 38% on FlashFill benchmark when presented with only 5 input-output examples but the performance degrades to 29% when 10 input-output examples are used. This was surprising to the authors too, and they came up with some hypothesis to explain this phenomenon. To me, this is a big problem indicating either a bug in the code or a severe shortcoming of the model. Any model useful for program synthesis needs to be applicable to many input-output examples because most complicated programs require many examples to disambiguate the details of the program.

Given the shortcoming of the experiments, I am not convinced that the paper is ready for publication. Thus, I recommend weak reject. I encourage the authors to address the comments below and resubmit as the general idea seems promising.

More comments:

I am unclear about the model at several places:
- How is the probability distribution normalized? Given the nature of bottom-up top-down evaluation of the potentials, should one enumerate over different completions of a program and the compare their exponentiated potentials? If so, does this restrict the applicability of the model to long programs as the enumeration of the completions gets prohibitively slow?
- What if you only use 1 input-output pair for each program instead of 5? Do the results get better?
- Section 5.1.2 is not clear to me. Can you elaborate by potentially including some examples? Does your input-output representation pre-supposes a fixed number of input-output examples across tasks (e.g. 5 or 10 for all of the tasks)?

Regarding the experiments,
- Could you present some baseline results on FlashFill benchmark based on previous work?
- Is your method only applicable to short programs? (based on the choice of 13 for the number of instructions)
- Does a program considered correct when it is identical to a test program, or is it considered correct when it succeeds on a set of held-out input-output pairs?
- When using 100 or more program samples, do you report the accuracy of the best program out of 100 (i.e. recall) or do you first filter the programs based on training input-output pairs and then evaluate a program that is selected?

Your paper is well beyond the recommended limit of 8 pages. please consider making it shorter.

---

This paper sets out to tackle the program synthesis problem: given a set of input/output pairs discover the program that generated them. The authors propose a bipartite model, with one component that is a generative model of tree-structured programs and the other component an input/output pair encoder for conditioning. They consider applying many variants of this basic model to a FlashFill DSL. The experiments explore a practical dataset and achieve fine numbers. The range of models considered, carefulness of the exposition, and basic experimental setup make this a valuable paper for an important area of research. I have a few questions, which I think would strengthen the paper, but think it's worth accepting as is.

Questions/Comments:

- The dataset is a good choice, because it is simple and easy to understand. What is the effect of the ""rule based strategy"" for computing well formed input strings?

- Clarify what ""backtracking search"" is? I assume it is the same as trying to generate the latent function? 

- In general describing the accuracy as you increase the sample size could be summarize simply by reporting the log-probability of the latent function. Perhaps it's worth reporting that? Not sure if I missed something.

---

This paper proposes a model that is able to infer a program from input/output example pairs, focusing on a restricted domain-specific language that captures a fairly wide variety of string transformations, similar to that used by Flash Fill in Excel.  The approach is to model successive “extensions” of a program tree conditioned on some embedding of the input/output pairs.  Extension probabilities are computed as a function of leaf and production rule embeddings — one of the main contributions is the so-called “Recursive-Reverse-Recursive Neural Net” which computes a globally aware embedding of a leaf by doing something that looks like belief propagation on a tree (but training this operation in an end-to-end differentiable way).

There are many strong points about this paper.  In contrast with some of the related work in the deep learning community, I can imagine this being used in an actual application in the near future.  The R3NN idea is a good one and the authors motivate it quite well.  Moreover, the authors have explored many variants of this model to understand what works well and what does not.  Finally, the exposition is clear (even if it is a long paper), which made this paper a pleasure to read.  Some weaknesses of this paper: the results are still not super accurate, perhaps because the model has only been trained on small programs but is being asked to infer programs that should be much longer.  And it’s unclear why the authors did not simply train on longer programs…  It also seems that the number of I/O pairs is fixed?  So if I had more I/O pairs, the model might not be able to use those additional pairs (and based on the experiments, more pairs can hurt…).  Overall however, I would certainly like to see this paper accepted at ICLR.

Other miscellaneous comments:
* Too many e’s in the expansion probability expression — might be better just to write “Softmax”.
* There is a comment about adding a bidirectional LSTM to process the global leaf representations before calculating scores, but no details are given on how this is done (as far as I can see).
* The authors claim that using hyperbolic tangent activation functions is important — I’d be interested in some more discussion on this and why something like ReLU would not be good.
* It’s unclear to me how batching was done in this setting since each program has a different tree topology.  More discussion on this would be appreciated.  Related to this, it would be good to add details on optimization algorithm (SGD?  Adagrad?  Adam?), learning rate schedules and how weights were initialized.  At the moment, the results are not particularly reproducible.
* In Figure 6 (unsolved benchmarks), it would be great to add the program sizes for these harder examples (i.e., did the approach fail because these benchmarks require long programs?  Or was it some other reason?)
* There is a missing related work by Piech et al (Learning Program Embeddings…) where the authors trained a recursive neural network (that matched abstract syntax trees for programs submitted to an online course) to predict program output (but did not synthesize programs).

---

Few remarks:

The paper explains what the tree is and how examples are encoded, but its missing important explanation on:

- how the I/O examples are encoded in the tree during training exactly.
- it is not well explained how the prediction works during testing when the examples are given.

It looks like the DSL is restricted to know all constant strings that will be used, which seems difficult in realistic scenarios.

---

This paper proposes a model that is able to infer a program from input/output example pairs, focusing on a restricted domain-specific language that captures a fairly wide variety of string transformations, similar to that used by Flash Fill in Excel.  The approach is to model successive “extensions” of a program tree conditioned on some embedding of the input/output pairs.  Extension probabilities are computed as a function of leaf and production rule embeddings — one of the main contributions is the so-called “Recursive-Reverse-Recursive Neural Net” which computes a globally aware embedding of a leaf by doing something that looks like belief propagation on a tree (but training this operation in an end-to-end differentiable way).

There are many strong points about this paper.  In contrast with some of the related work in the deep learning community, I can imagine this being used in an actual application in the near future.  The R3NN idea is a good one and the authors motivate it quite well.  Moreover, the authors have explored many variants of this model to understand what works well and what does not.  Finally, the exposition is clear (even if it is a long paper), which made this paper a pleasure to read.  Some weaknesses of this paper: the results are still not super accurate, perhaps because the model has only been trained on small programs but is being asked to infer programs that should be much longer.  And it’s unclear why the authors did not simply train on longer programs…  It also seems that the number of I/O pairs is fixed?  So if I had more I/O pairs, the model might not be able to use those additional pairs (and based on the experiments, more pairs can hurt…).  Overall however, I would certainly like to see this paper accepted at ICLR.

Other miscellaneous comments:
* Too many e’s in the expansion probability expression — might be better just to write “Softmax”.
* There is a comment about adding a bidirectional LSTM to process the global leaf representations before calculating scores, but no details are given on how this is done (as far as I can see).
* The authors claim that using hyperbolic tangent activation functions is important — I’d be interested in some more discussion on this and why something like ReLU would not be good.
* It’s unclear to me how batching was done in this setting since each program has a different tree topology.  More discussion on this would be appreciated.  Related to this, it would be good to add details on optimization algorithm (SGD?  Adagrad?  Adam?), learning rate schedules and how weights were initialized.  At the moment, the results are not particularly reproducible.
* In Figure 6 (unsolved benchmarks), it would be great to add the program sizes for these harder examples (i.e., did the approach fail because these benchmarks require long programs?  Or was it some other reason?)
* There is a missing related work by Piech et al (Learning Program Embeddings…) where the authors trained a recursive neural network (that matched abstract syntax trees for programs submitted to an online course) to predict program output (but did not synthesize programs).

---

There is a bit of a spread in the reviewer scores and unfortunately it wasn't possible to entice the reviewers to participate in a discussion. The area chair therefore discounts the late review of reviewer3, who seems to have had a misunderstanding that was successfully rebutted by the authors. The other reviewers are supportive of the paper.

---

Dear reviewers, do you have any reactions after the authors responded to your reviews?

---

The paper presents a method to synthesize string manipulation programs based on a set of input output pairs. The paper focuses on a restricted class of programs based on a simple context free grammar sufficient to solve string manipulation tasks from the FlashFill benchmark. A probabilistic generative model called Recursive-Reverse-Recursive Neural Network (R3NN) is presented that assigns a probability to each program's parse tree after a bottom-up and a top-down pass. Results are presented on a synthetic dataset and a Microsoft Excel benchmark called FlashFill.

The problem of program synthesis is important with a lot of recent interest from the deep learning community. The approach taken in the paper based on parse trees and recursive neural networks seems interesting and promising. However, the model seems too complicated and unclear at several places (details below). On the negative side, the experiments are particularly weak, and the paper does not seem ready for publication based on its experimental results. I was positive about the paper until I realized that the method obtains an accuracy of 38% on FlashFill benchmark when presented with only 5 input-output examples but the performance degrades to 29% when 10 input-output examples are used. This was surprising to the authors too, and they came up with some hypothesis to explain this phenomenon. To me, this is a big problem indicating either a bug in the code or a severe shortcoming of the model. Any model useful for program synthesis needs to be applicable to many input-output examples because most complicated programs require many examples to disambiguate the details of the program.

Given the shortcoming of the experiments, I am not convinced that the paper is ready for publication. Thus, I recommend weak reject. I encourage the authors to address the comments below and resubmit as the general idea seems promising.

More comments:

I am unclear about the model at several places:
- How is the probability distribution normalized? Given the nature of bottom-up top-down evaluation of the potentials, should one enumerate over different completions of a program and the compare their exponentiated potentials? If so, does this restrict the applicability of the model to long programs as the enumeration of the completions gets prohibitively slow?
- What if you only use 1 input-output pair for each program instead of 5? Do the results get better?
- Section 5.1.2 is not clear to me. Can you elaborate by potentially including some examples? Does your input-output representation pre-supposes a fixed number of input-output examples across tasks (e.g. 5 or 10 for all of the tasks)?

Regarding the experiments,
- Could you present some baseline results on FlashFill benchmark based on previous work?
- Is your method only applicable to short programs? (based on the choice of 13 for the number of instructions)
- Does a program considered correct when it is identical to a test program, or is it considered correct when it succeeds on a set of held-out input-output pairs?
- When using 100 or more program samples, do you report the accuracy of the best program out of 100 (i.e. recall) or do you first filter the programs based on training input-output pairs and then evaluate a program that is selected?

Your paper is well beyond the recommended limit of 8 pages. please consider making it shorter.

---

This paper sets out to tackle the program synthesis problem: given a set of input/output pairs discover the program that generated them. The authors propose a bipartite model, with one component that is a generative model of tree-structured programs and the other component an input/output pair encoder for conditioning. They consider applying many variants of this basic model to a FlashFill DSL. The experiments explore a practical dataset and achieve fine numbers. The range of models considered, carefulness of the exposition, and basic experimental setup make this a valuable paper for an important area of research. I have a few questions, which I think would strengthen the paper, but think it's worth accepting as is.

Questions/Comments:

- The dataset is a good choice, because it is simple and easy to understand. What is the effect of the ""rule based strategy"" for computing well formed input strings?

- Clarify what ""backtracking search"" is? I assume it is the same as trying to generate the latent function? 

- In general describing the accuracy as you increase the sample size could be summarize simply by reporting the log-probability of the latent function. Perhaps it's worth reporting that? Not sure if I missed something.

---

This paper proposes a model that is able to infer a program from input/output example pairs, focusing on a restricted domain-specific language that captures a fairly wide variety of string transformations, similar to that used by Flash Fill in Excel.  The approach is to model successive “extensions” of a program tree conditioned on some embedding of the input/output pairs.  Extension probabilities are computed as a function of leaf and production rule embeddings — one of the main contributions is the so-called “Recursive-Reverse-Recursive Neural Net” which computes a globally aware embedding of a leaf by doing something that looks like belief propagation on a tree (but training this operation in an end-to-end differentiable way).

There are many strong points about this paper.  In contrast with some of the related work in the deep learning community, I can imagine this being used in an actual application in the near future.  The R3NN idea is a good one and the authors motivate it quite well.  Moreover, the authors have explored many variants of this model to understand what works well and what does not.  Finally, the exposition is clear (even if it is a long paper), which made this paper a pleasure to read.  Some weaknesses of this paper: the results are still not super accurate, perhaps because the model has only been trained on small programs but is being asked to infer programs that should be much longer.  And it’s unclear why the authors did not simply train on longer programs…  It also seems that the number of I/O pairs is fixed?  So if I had more I/O pairs, the model might not be able to use those additional pairs (and based on the experiments, more pairs can hurt…).  Overall however, I would certainly like to see this paper accepted at ICLR.

Other miscellaneous comments:
* Too many e’s in the expansion probability expression — might be better just to write “Softmax”.
* There is a comment about adding a bidirectional LSTM to process the global leaf representations before calculating scores, but no details are given on how this is done (as far as I can see).
* The authors claim that using hyperbolic tangent activation functions is important — I’d be interested in some more discussion on this and why something like ReLU would not be good.
* It’s unclear to me how batching was done in this setting since each program has a different tree topology.  More discussion on this would be appreciated.  Related to this, it would be good to add details on optimization algorithm (SGD?  Adagrad?  Adam?), learning rate schedules and how weights were initialized.  At the moment, the results are not particularly reproducible.
* In Figure 6 (unsolved benchmarks), it would be great to add the program sizes for these harder examples (i.e., did the approach fail because these benchmarks require long programs?  Or was it some other reason?)
* There is a missing related work by Piech et al (Learning Program Embeddings…) where the authors trained a recursive neural network (that matched abstract syntax trees for programs submitted to an online course) to predict program output (but did not synthesize programs).

---

Few remarks:

The paper explains what the tree is and how examples are encoded, but its missing important explanation on:

- how the I/O examples are encoded in the tree during training exactly.
- it is not well explained how the prediction works during testing when the examples are given.

It looks like the DSL is restricted to know all constant strings that will be used, which seems difficult in realistic scenarios.",1.0,4.0,2.6666666666666665,2.0,3.3333333333333335,6.666666666666667,1.5,,3.6666666666666665,,1.5
366,"AUTOENCODING VARIATIONAL INFERENCE FOR TOPIC MODELS
Authors: Akash Srivastava, Charles Sutton
Source file: 366.pdf

ABSTRACT
Topic models are one of the most popular methods for learning representations of text, but a major challenge is that any change to the topic model requires mathematically deriving a new inference algorithm. A promising approach to address this problem is autoencoding variational Bayes (AEVB), but it has proven difficult to apply to topic models in practice. We present what is to our knowledge the first effective AEVB based inference method for latent Dirichlet allocation (LDA), which we call Autoencoded Variational Inference For Topic Model (AVITM). This model tackles the problems caused for AEVB by the Dirichlet prior and by component collapsing. We find that AVITM matches traditional methods in accuracy with much better inference time. Indeed, because of the inference network, we find that it is unnecessary to pay the computational cost of running variational optimization on test data. Because AVITM is black box, it is readily applied to new topic models. As a dramatic illustration of this, we present a new topic model called ProdLDA, that replaces the mixture model in LDA with a product of experts. By changing only one line of code from LDA, we find that ProdLDA yields much more interpretable topics, even if LDA is trained via collapsed Gibbs sampling.

1 INTRODUCTION
Topic models (Blei, 2012) are among the most widely used models for learning unsupervised representations of text, with hundreds of different model variants in the literature, and have have found applications ranging from the exploration of the scientific literature (Blei & Lafferty, 2007) to computer vision (Fei-Fei & Perona, 2005), bioinformatics (Rogers et al., 2005), and archaeology (Mimno, 2009). A major challenge in applying topic models and developing new models is the computational cost of computing the posterior distribution. Therefore a large body of work has considered approximate inference methods, the most popular methods being variational methods, especially mean field methods, and Markov chain Monte Carlo, particularly methods based on collapsed Gibbs sampling.
Both mean-field and collapsed Gibbs have the drawback that applying them to new topic models, even if there is only a small change to the modeling assumptions, requires re-deriving the inference methods, which can be mathematically arduous and time consuming, and limits the ability of practitioners to freely explore the space of different modeling assumptions. This has motivated the development of black-box inference methods (Ranganath et al., 2014; Mnih & Gregor, 2014; Kucukelbir et al., 2016; Kingma & Welling, 2014) which require only very limited and easy to compute information from the model, and hence can be applied automatically to new models given a simple declarative specification of the generative process.
Autoencoding variational Bayes (AEVB) (Kingma & Welling, 2014; Rezende et al., 2014) is a particularly natural choice for topic models, because it trains an inference network (Dayan et al., 1995), a neural network that directly maps a document to an approximate posterior distribution,
∗Additional affiliation: Alan Turing Institute, British Library, 96 Euston Road, London NW1 2DB
without the need to run further variational updates. This is intuitively appealing because in topic models, we expect the mapping from documents to posterior distributions to be well behaved, that is, that a small change in the document will produce only a small change in topics. This is exactly the type of mapping that a universal function approximator like a neural network should be good at representing. Essentially, the inference network learns to mimic the effect of probabilistic inference, so that on test data, we can enjoy the benefits of probabilistic modeling without paying a further cost for inference.
However, despite some notable successes for latent Gaussian models, black box inference methods are significantly more challenging to apply to topic models. For example, in initial experiments, we tried to apply ADVI (Kucukelbir et al., 2016), a recent black-box variational method, but it was difficult to obtain any meaningful topics. Two main challenges are: first, the Dirichlet prior is not a location scale family, which hinders reparameterisation, and second, the well known problem of component collapsing (Dinh & Dumoulin, 2016), in which the inference network becomes stuck in a bad local optimum in which all topics are identical.
In this paper, we present what is, to our knowledge, the first effective AEVB inference method for topic models, which we call Autoencoded Variational Inference for Topic Models or AVITM1. On several data sets, we find that AVITM yields topics of equivalent quality to standard mean-field inference, with a large decrease in training time. We also find that the inference network learns to mimic the process of approximate inference highly accurately, so that it is not necessary to run variational optimization at all on test data.
But perhaps more important is that AVITM is a black-box method that is easy to apply to new models. To illustrate this, we present a new topic model, called ProdLDA, in which the distribution over individual words is a product of experts rather than the mixture model used in LDA. We find that ProdLDA consistently produces better topics than standard LDA, whether measured by automatically determined topic coherence or qualitative examination. Furthermore, because we perform probabilistic inference using a neural network, we can fit a topic model on roughly a one million documents in under 80 minutes on a single GPU, and because we are using a black box inference method, implementing ProdLDA requires a change of only one line of code from our implementation of standard LDA.
To summarize, the main advantages of our methods are:
1. Topic coherence: ProdLDA returns consistently better topics than LDA, even when LDA is trained using Gibbs sampling.
2. Computational efficiency: Training AVITM is fast and efficient like standard mean-field. On new data, AVITM is much faster than standard mean field, because it requires only one forward pass through a neural network.
3. Black box: AVITM does not require rigorous mathematical derivations to handle changes in the model, and can be easily applied to a wide range of topic models.
Overall, our results suggest that AVITM is ready to take its place alongside mean field and collapsed Gibbs as one of the workhorse inference methods for topic models.

2 BACKGROUND
To fix notation, we begin by describing topic modelling and AVITM.

2.1 LATENT DIRICHLET ALLOCATION
We describe the most popular topic model, latent Dirichlet allocation (LDA). In LDA, each document of the collection is represented as a mixture of topics, where each topic βk is a probability distribution over the vocabulary. We also use β to denote the matrix β = (β1 . . . βK). The generative process is then as described in Algorithm 1. Under this generative model, the marginal likelihood of
1Code available at https://github.com/akashgit/autoencoding_vi_for_topic_models
for each document w do Draw topic distribution θ ∼ Dirichlet(α); for each word at position n do
Sample topic zn ∼ Multinomial(1, θ); Sample word wn ∼ Multinomial(1, βzn);
end end
Algorithm 1: LDA as a generative model.
a document w is
(1)p(w|α, β) = ∫ θ ( N∏ n=1 k∑ zn=1 p(wn|zn, β)p(zn|θ) ) p(θ|α)dθ.
Posterior inference over the hidden variables θ and z is intractable due to the coupling between the θ and β under the multinomial assumption (Dickey, 1983).

2.2 MEAN FIELD AND AEVB
A popular approximation for efficient inference in topic models is mean field variational inference, which breaks the coupling between θ and z by introducing free variational parameters γ over θ and φ over z and dropping the edges between them. This results in an approximate variational posterior q(θ, z|γ, φ) = qγ(θ) ∏ n qφ(zn), which is optimized to best approximate the true posterior p(θ, z|w, α, β). The optimization problem is to minimize
(2)L(γ, φ | α, β) = DKL [q(θ, z|γ, φ)||p(θ, z|w, α, β)]− log p(w|α, β).
In fact the above equation is a lower bound to the marginal log likelihood, sometimes called an evidence lower bound (ELBO), a fact which can be easily verified by multiplying and dividing (1) by the variational posterior and then applying Jensen’s inequality on its logarithm. Note that the mean field method optimizes over an independent set of variational parameters for each document. To emphasize this, we will refer to this standard method by the non-standard name of Decoupled Mean-Field Variational Inference (DMFVI).
For LDA, this optimization has closed form coordinate descent equations due to the conjugacy between the Dirichlet and multinomial distributions. Although this is a computationally convenient aspect of DMFVI, it also limits its flexibility. Applying DMFVI to new models relies on the practitioner’s ability to derive the closed form updates, which can be impractical and sometimes impossible.
AEVB (Kingma & Welling, 2014; Rezende et al., 2014) is one of several recent methods that aims at “black box” inference methods to sidestep this issue. First, rewrite the ELBO as
(3)L(γ, φ | α, β) = −DKL [q(θ, z|γ, φ)||p(θ, z|α)] + Eq(θ,z|γ,φ)[log p(w|z, θ, α, β)]
This form is intuitive. The first term attempts to match the variational posterior over latent variables to the prior on the latent variables, while the second term ensures that the variational posterior favors values of the latent variables that are good at explaining the data. By analogy to autoencoders, this second term is referred to as a reconstruction term.
What makes this method “Autoencoding,” and in fact the main difference from DMFVI, is the parameterization of the variational distribution. In AEVB, the variational parameters are computed by using a neural network called an inference network that takes the observed data as input. For example, if the model prior p(θ) were Gaussian, we might define the inference network as a feedforward neural network (µ(w),v(w)) = f(w, γ), where µ(w) and v(w) are both vectors of length k, and γ are the network’s parameters. Then we might choose a Gaussian variational distribution qγ(θ) = N(θ;µ(w), diag(v(w))), where diag(· · ·) produces a diagonal matrix from a column vector. The variational parameters γ can then be chosen by optimizing the ELBO (3). Note that we have
now, unlike DMFVI, coupled the variational parameters for different documents because they are all computed from the same neural network. To compute the expectations with respect to q in (3), Kingma & Welling (2014); Rezende et al. (2014) use a Monte Carlo estimator which they call the “reparameterization trick” (RT; appears also in Williams (1992)). In the RT, we define a variate U with a simple distribution that is independent of all variational parameters, like a uniform or standard normal, and a reparameterization function F such that F (U, γ) has distribution qγ . This is always possible, as we could choose F to be the inverse cumulative distribution function of qγ , although we will additionally want F to be easy to compute and differentiable. If we can determine a suitable F , then we can approximate (3) by taking Monte Carlo samples of U , and optimize γ using stochastic gradient descent.

3 AUTOENCODING VARIATIONAL BAYES IN LATENT DIRICHLET ALLOCATION
Although simple conceptually, applying AEVB to topic models raises several practical challenges. The first is the need to determine a reparameterization function for q(θ) and q(zn) to use the RT. The zn are easily dealt with, but θ is more difficult; if we choose q(θ) to be Dirichlet, it is difficult to apply the RT, whereas if we choose q to be Gaussian or logistic normal, then the KL divergence in (3) becomes more problematic. The second issue is the well known problem of component collapsing (Dinh & Dumoulin, 2016), which a type of bad local optimum that is particularly endemic to AEVB and similar methods. We describe our solutions to each of those problems in the next few subsections.
3.1 COLLAPSING z’S
Dealing with discrete variables like z using reparameterization can be problematic, but fortunately in LDA the variable z can be conveniently summed out. By collapsing z we are left with having to sample from θ only, reducing (1) to
(4)p(w|α, β) = ∫ θ ( N∏ n=1 p(wn|β, θ) ) p(θ|α)dθ.
where the distribution of wn|β, θ is Multinomial(1, βθ), recalling that β denotes the matrix of all topic-word probability vectors.

3.2 WORKING WITH DIRICHLET BELIEFS: LAPLACE APPROXIMATION
LDA gets its name from the Dirichlet prior on the topic proportions θ, and the choice of Dirichlet prior is important to obtaining interpretable topics (Wallach et al., 2009). But it is difficult to handle the Dirichlet within AEVB because it is difficult to develop an effective reparameterization function for the RT. Fortunately, a RT does exist for the Gaussian distribution and has been shown to perform quite well in the context of variational autoencoder (VAE) (Kingma & Welling, 2014).
We resolve this issue by constructing a Laplace approximation to the Dirichlet prior. Following MacKay (1998), we do so in the softmax basis instead of the simplex. There are two benefits of this choice. First, Dirichlet distributions are unimodal in the softmax basis with their modes coinciding with the means of the transformed densities. Second, the softmax basis also allows for carrying out unconstrained optimization of the cost function without the simplex constraints. The Dirichlet probability density function in this basis over the softmax variable h is given by
(5)P (θ(h)|α) = Γ( ∑ k αk)∏
k Γ(αk) ∏ k θαkk g(1 Th).
Here θ = σ(h), where σ(.) represents the softmax function. Recall that the Jacobian of σ is proportional to ∏ k θk and g(·) is an arbitrary density that ensures integrability by constraining the redundant degree of freedom. We use the Laplace approximation of Hennig et al. (2012), which
has the property that the covariance matrix becomes diagonal for large k (number of topics). This approximation to the Dirichlet prior p(θ|α) is results in the distribution over the softmax variables h as a multivariate normal with mean µ1 and covariance matrix Σ1 where
µ1k = logαk − 1
K ∑ i logαi
Σ1kk = 1
αk
( 1− 2
K
) + 1
K2 ∑ i 1 αk . (6)
Finally, we approximate p(θ|α) in the simplex basis with p̂(θ|µ1,Σ1) = LN (θ|µ1,Σ1) where LN is a logistic normal distribution with parameters µ1,Σ1. Although we approximate the Dirichlet prior in LDA with a logistic normal, this is not the same idea as a correlated topic model (Blei & Lafferty, 2006), because we use a diagonal covariance matrix. Rather, it is an approximation to standard LDA.

3.3 VARIATIONAL OBJECTIVE
Now we can write the modified variational objective function. We use a logistic normal variational distribution over θ with diagonal covariance. More precisely, we define two inference networks as feed forward neural networks fµ and fΣ with parameters δ; the output of each network is a vector in RK . Then for a document w, we define q(θ) to be logistic normal with mean µ0 = fµ(w, δ) and diagonal covariance Σ0 = diag(fΣ(w, δ)), where diag converts a column vector to a diagonal matrix. Note that we can generate samples from q(θ) by sampling ∼ N (0, I) and computing θ = σ(µ0 + Σ 1/2 0 ).
We can now write the ELBO as
L(Θ) = D∑ d=1
[ − ( 1
2
{ tr(Σ−11 Σ0) + (µ1 −µ0)TΣ −1 1 (µ1 −µ0)−K + log
|Σ1| |Σ0|
}) (7)
+E ∼N (0,I) [ w>d log ( σ(β)σ(µ0 + Σ 1/2 0 ) )]] ,
where Θ represents the set of all the model and variational parameters and w1 . . .wD are the documents in the corpus. The first line in this equation arises from the KL divergence between the two logistic normal distributions q and p̂, while the second line is the reconstruction error.
In order to impose the simplex constraint on the β matrix during the optimization, we apply the softmax transformation. That is, each topic βk ∈ RV is unconstrained, and the notation σ(β) means to apply the softmax function separately to each column of the matrix β. Note that the mixture of multinomials for each word wn can then be written as p(wn|β, θ) = [σ(β)θ]wn , which explains the dot product in (7). To optimize (7), we use stochastic gradient descent using Monte Carlo samples from , following the Law of the Unconscious Statistician.

3.4 TRAINING AND PRACTICAL CONSIDERATIONS: DEALING WITH COMPONENT COLLAPSING
AEVB is prone to component collapsing (Dinh & Dumoulin, 2016), which is a particular type of local optimum very close to the prior belief, early on in the training. As the latent dimensionality of the model is increased, the KL regularization in the variational objective dominates, so that the outgoing decoder weights collapse for the components of the latent variable that reach close to the prior and do not show any posterior divergence. In our case, the collapsing specifically occurs because of the inclusion of the softmax transformation to produce θ. The result is that the k inferred topics are identical as shown in table 7.
We were able to resolve this issue by tweaking the optimization. Specifically, we train the network with the ADAM optimizer (Kingma & Ba, 2015) using high moment weight (β1) and learning rate (η). Through training at higher rates, early peaks in the functional space can be easily avoided. The
problem is that momentum based training coupled with higher learning rate causes the optimizer to diverge. While explicit gradient clipping helps to a certain extent, we found that batch normalization (Ioffe & Szegedy, 2015) does even better by smoothing out the functional space and hence curbing sudden divergence.
Finally, we also found an increase in performance with dropout units when applied to θ to force the network to use more of its capacity.
While more prominent in the AEVB framework, the collapsing can also occurs in DMFVI if the learning offset (referred to as the τ parameter (Hofmann, 1999)) is not set properly. Interestingly, a similar learning offset or annealing based approach can also be used to down-weight the KL term in early iterations of the training to avoid local optima.

4 PRODLDA: LATENT DIRICHLET ALLOCATION WITH PRODUCTS OF EXPERTS
In LDA, the distribution p(w|θ, β) is a mixture of multinomials. A problem with this assumption is that it can never make any predictions that are sharper than the components that are being mixed (Hinton & Salakhutdinov, 2009). This can result in some topics appearing that are poor quality and do not correspond well with human judgment. One way to resolve this issue is to replace this word-level mixture with a weighted product of experts which by definition is capable of making sharper predictions than any of the constituent experts (Hinton, 2002). In this section we present a novel topic model PRODLDA that replaces the mixture assumption at the word-level in LDA with a weighted product of experts, resulting in a drastic improvement in topic coherence. This is a good illustration of the benefits of a black box inference method, like AVITM, to allow exploration of new models.

4.1 MODEL
The PRODLDA model can be simply described as latent Dirichlet allocation where the word-level mixture over topics is carried out in natural parameter space, i.e. the topic matrix is not constrained to exist in a multinomial simplex prior to mixing. In other words, the only changes from LDA are that β is unnormalized, and that the conditional distribution of wn is defined as wn|β, θ ∼ Multinomial(1, σ(βθ)).
The connection to a product of experts is straightforward, as for the multinomial, a mixture of natural parameters corresponds to a weighted geometric average of the mean parameters. That is, consider two N dimensional multinomials parametrized by mean vectors p and q. Define the corresponding natural parameters as p = σ(r) and q = σ(s), and let δ ∈ [0, 1]. It is then easy to show that
P ( x|δr + (1− δ)s ) ∝ N∏ i=1 σ(δri + (1− δ)si)xi ∝ N∏ i=1 [rδi · s (1−δ) i ] xi .
So the PRODLDA model can be simply described as a product of experts, that is, p(wn|θ, β) ∝∏ k p(wn|zn = k, β)θk . PRODLDA is an instance of the exponential-family PCA (Collins et al., 2001) class, and relates to the exponential-family harmoniums (Welling et al., 2004) but with nonGaussian priors.

5 RELATED WORK
For an overview of topic modeling, see Blei (2012). There are several examples of topic models based on neural networks and neural variational inference (Hinton & Salakhutdinov, 2009; Larochelle & Lauly, 2012; Mnih & Gregor, 2014; Miao et al., 2016) but we are unaware of methods that apply AEVB generically to a topic model specified by an analyst, or even of a successful application of AEVB to the most widely used topic model, latent Dirichlet allocation.
Recently, Miao et al. (2016) introduced a closely related model called the Neural Variational Document Model (NVDM). This method uses a latent Gaussian distribution over topics, like probabilistic latent semantic indexing, and averages over topic-word distributions in the logit space. However,
they do not use either of the two key aspects of our work: explicitly approximating the Dirichlet prior using a Gaussian, or high-momentum training. In the experiments we show that these aspects lead to much improved training and much better topics.

6 EXPERIMENTS AND RESULTS
Qualitative evaluation of topic models is a challenging task and consequently a large body of work has developed automatic evaluation metrics that attempt to match human judgment of topic quality. Traditionally, perplexity has been used to measure the goodness-of-fit of the model but it has been repeatedly shown that perplexity is not a good metric for qualitative evaluation of topics (Newman et al., 2010). Several new metrics of topic coherence evaluation have thus been proposed; see Lau et al. (2014) for a comparative review. Lau et al. (2014) showed that among all the competing metrics, normalized pointwise mutual information (NPMI) between all the pairs of words in a set of topics matches human judgment most closely, so we adopt it in this work. We also report perplexity, primarily as a way of evaluating the capability of different optimizers. Following standard practice (Blei et al., 2003), for variational methods we use the ELBO to calculate perplexity. For AEVB methods, we calculate the ELBO using the same Monte Carlo approximation as for training.
We run experiments on both the 20 Newsgroups (11,000 training instances with 2000 word vocabulary) and RCV1 Volume 2 ( 800K training instances with 10000 word vocabulary) datasets. Our preprocessing involves tokenization, removal of some non UTF-8 characters for 20 Newsgroups and English stop word removal. We first compare our AVITM inference method with the standard online mean-field variational inference (Hoffman et al., 2010) and collapsed Gibbs sampling (Griffiths & Steyvers, 2004) on the LDA model. We use standard implementations of both methods, scikit-learn for DMFVI and mallet (McCallum, 2002) for collapsed Gibbs. Then we compare two autoencoding inference methods on three different topic models: standard LDA, PRODLDA using our inference method and the Neural Variational Document Model (NVDM) (Miao et al., 2016), using the inference described in the paper.2
Tables 1 and 2 show the average topic coherence values for all the models for two different settings of k, the number of topics. Comparing the different inference methods for LDA, we find that, consistent with previous work, collapsed Gibbs sampling yields better topics than mean-field methods. Among the variational methods, we find that VAE-LDA model (AVITM) 3 yields similar topic coherence and perplexity to the standard DMFVI (although in some cases, VAE-LDA yields significantly better topics). However, AVITM is significantly faster to train than DMFVI. It takes 46 seconds on 20 Newsgroup compared to 18 minutes for DMFVI. Whereas for a million document corpus of RCV1 it only under 1.5 hours while scikit-learn’s implementation of DMFVI failed to return any results even after running for 24 hours.4
Comparing the new topic models than LDA, it is clear that PRODLDA finds significantly better topics than LDA, even when trained by collapsed Gibbs sampling. To verify this qualitatively, we display examples of topics from all the models in Table 6. The topics from ProdLDA appear visually more coherent than NVDM or LDA. Unfortunately, NVDM does not perform comparatively to LDA
2We have used both https://github.com/carpedm20/variational-text-tensorflow and the NVDM author’s (Miao et al., 2016) implementation.
3We recently found that ’whitening’ the topic matrix significantly improves the topic coherence for VAELDA. Manuscript in preparation.
4Therefore, we were not able to report topic coherence for DMFVI on RCV1
for any value of k. To avoid any training dissimilarities we train all the competing models until we reach the perplexities that were reported in previous work. These are reported in Table 35.
A major benefit of AVITM inference is that it does not require running variational optimization, which can be costly, for new data. Rather, the inference network can be used to obtain topic proportions for new documents for new data points without running any optimization. We evaluate whether this approximation is accurate, i.e. whether the neural network effectively learns to mimic probabilistic inference. We verify this by training the model on the training set, then on the test set, holding the topics (β matrix) fixed, and comparing the test perplexity if we obtain topic proportions by running the inference neural network directly, or by the standard method of variational optimization of the inference network on the test set. As shown in Table 4, the perplexity remains practically un-changed. The computational benefits of this are remarkable. On both the datasets, computing perplexity using the neural network takes well under a minute, while running the standard variational approximation takes ∼ 3 minutes even on the smaller 20 Newsgroups data. Finally, we investigate the reasons behind the improved topic coherence in PRODLDA. First, Table 5 explores the effects of each of our two main ideas separately. In this table, “Dirichlet” means that the prior is the Laplace approximation to Dirichlet(α = 0.02), while “Gaussian” indicates that we use a standard Gaussian as prior. ‘High Learning Rate” training means we use β1 > 0.8 and 0.1 > η > 0.0016 with batch normalization, whereas “Low Learning Rate” means β1 > 0.8 and 0.0009 > η > 0.00009 without batch normalization. (For both parameters, the precise value was chosen by Bayesian optimization. We found that these values in the ”with BN” cases were close to the default settings in the Adam optimizer.) We find that the high topic coherence that we achieve in this work is only possible if we use both tricks together. In fact the high learning rates with momentum is required to avoid local minima in the beginning of the training and batch-normalization is required to be able to train the network at these values without diverging. If trained at a lower momentum value or at a lower learning rate PRODLDA shows component collapsing. Interestingly, if we choose a Gaussian prior, rather than the logistic normal approximation used in ProdLDA or NVLDA, the model is easier to train even with low learning rate without any momentum or batch normalization.
The main advantage of AVITM topic models as opposed to NVDM is that the Laplace approximation allows us to match a specific Dirichlet prior of interest. As pointed out by Wallach et al. (2009), the choice of Dirichlet hyperparameter is important to the topic quality of LDA. Following this reasoning, we hypothesize that AVITM topics are higher quality than those of NVDM because they are much more focused, i.e., apply to a more specific subset of documents of interest. We provide support for this hypothesis in Figure 1, by evaluating the sparsity of the posterior proportions over topics, that is, how many of the model’s topics are typically used to explain each document. In order to estimate the sparsity in topic proportions, we project samples from the Gaussian latent spaces of PRODLDA and NVDM in the simplex and average them across documents. We compare the topic
5We note that much recent work follows Hinton & Salakhutdinov (2009) in reporting perplexity for the LDA Gibbs sampler on only a small subset of the test data. Our results are different because we use the entire test dataset.
6β1 is the weight on the average of the gradients from the previous time step and η refers to the learning rate.
sparsity for the standard Gaussian prior used by NVDM to the Laplace approximation of Dirichlet priors with different hyperparameters. Clearly the Laplace approximation to the Dirichlet prior significantly promotes sparsity, providing support for our hypothesis that preserving the Dirichlet prior explains the the increased topic coherence in our method.
The inference network architecture can be found in figure 2 in the appendix.

7 DISCUSSION AND FUTURE WORK
We present what is to our knowledge the first effective AEVB inference algorithm for latent Dirichlet allocation. Although this combination may seem simple in principle, in practice this method is difficult to train because of the Dirichlet prior and because of the component collapsing problem. By addressing both of these problems, we presented a black-box inference method for topic models with the notable advantage that the neural network allows computing topic proportions for new documents without the need to run any variational optimization. As an illustration of the advantages of

1. write article get thanks like anyone please know look one
black box inference techniques, we presented a new topic model, ProdLDA, which achieves significantly better topics than LDA, while requiring a change of only one line of code from AVITM for LDA. Our results suggest that AVITM inference is ready to take its place alongside mean field and collapsed Gibbs as one of the workhorse inference methods for topic models. Future work could include extending our inference methods to handle dynamic and correlated topic models.

ACKNOWLEDGMENTS
We thank Andriy Mnih, Chris Dyer, Chris Russell, David Blei, Hannah Wallach, Max Welling, Mirella Lapata and Yishu Miao for helpful comments, discussions and feedback.

A NETWORK ARCHITECTURE
","Topic models are one of the most popular methods for learning representations of text, but a major challenge is that any change to the topic model requires mathematically deriving a new inference algorithm. A promising approach to address this problem is autoencoding variational Bayes (AEVB), but it has proven difficult to apply to topic models in practice. We present what is to our knowledge the first effective AEVB based inference method for latent Dirichlet allocation (LDA), which we call Autoencoded Variational Inference For Topic Model (AVITM). This model tackles the problems caused for AEVB by the Dirichlet prior and by component collapsing. We find that AVITM matches traditional methods in accuracy with much better inference time. Indeed, because of the inference network, we find that it is unnecessary to pay the computational cost of running variational optimization on test data. Because AVITM is black box, it is readily applied to new topic models. As a dramatic illustration of this, we present a new topic model called ProdLDA, that replaces the mixture model in LDA with a product of experts. By changing only one line of code from LDA, we find that ProdLDA yields much more interpretable topics, even if LDA is trained via collapsed Gibbs sampling.",ICLR 2017 conference submission,True,,"This is an interesting paper on a VAE framework for topic models. The main idea is to train a recognition model for the inference phase which, because of so called “amortized inference” can be much faster than normal inference where inference must be run iteratively for every document. Some comments:
Eqn 5: I find the notation p(theta(h)|alpha) awkward. Why not P(h|alpha) ?
The generative model seems agnostic to document length, meaning that the latent variables only generate probabilities over word space. However, the recognition model is happy to radically change the probabilities q(z|x) if the document length changes because the input to q changes. This seems undesirable. Maybe they should normalize the input to the recognition network?
The ProdLDA model might well be equivalent to exponential family PCA or some variant thereof:

---

The reviewers agree that the approach is interesting and the paper presents useful findings. They also raise enough questions and suggestions for improvements that I believe the paper will be much stronger after further revision, though these seem straightforward to address.

---

The authors propose NVI for LDA variants. The authors compare NVI-LDA to standard inference schemes such as CGS and online SVI. The authors also evaluate NVI on a different model ProdLDA (not sure this model has been proposed before in the topic modeling literature though?)

In general, I like the direction of this paper and NVI looks promising for LDA. The experimental results however confound model vs inference which makes it hard to understand the significance of the results. Furthermore, the authors don't discuss hyper-parameter selection which is known to significantly impact performance of topic models. This makes it hard to understand when the proposed method can be expected to work. 

Can you maybe generate synthetic datasets with different Dirichlet distributions and assess when the proposed method recovers the true parameters?

Figure 1: Is this prior or posterior? The text talks about sparsity whereas the y-axis reads ""log p(topic proportions)"" which is a bit confusing.

Section 3.2: it is not clear what you mean by unimodal in softmax basis. Consider a Dirichlet on K-dimensional simplex with concentration parameter alpha/K where alpha<1 makes it multimodal. Isn't the softmax basis still multimodal?

None of the numbers include error bars. Are the results statistically significant?


Minor comments:

Last term in equation (3) is not ""error""; reconstruction accuracy or negative reconstruction error perhaps?

The idea of using an inference network is much older, cf. Helmholtz machine.

---

This paper proposes the use of neural variational inference method for topic models. The paper shows a nice trick to approximate Dirichlet prior using softmax basis with a Gaussian and then the model is trained to maximize the variational lower bound. Also, the authors study a better way to alleviate the component collapsing issue, which has been problematic for continuous latent variables that follow Gaussian distribution. The results look promising and the experimental protocol sounds fine.

Minor comments:
Please add citation to [1] or [2] for neural variational inference, and [2] for VAE. 
A typo in “This approximation to the Dirichlet prior p(θ|α) is results in the distribution”, it should be “This approximation to the Dirichlet prior p(θ|α) results in the distribution”

In table 2, it is written that DMFVI was trained more than 24hrs but failed to deliver any result, but why not wait until the end and report the numbers?

In table 3, why are the perplexities of LDA-Collapsed Gibbs and NVDM are lower while the proposed models (ProdLDA) generates more coherent topics? What is your intuition on this?

How does the training speed (until the convergence) differs by using different learning-rate and momentum scheduling approaches shown as in figure 1?

It may be also interesting to add some more analysis on the latent variables z (component collapsing and etc., although your results indirectly show that the learning-rate and momentum scheduling trick removes this issue).

Overall, the paper clearly proposes its main idea, explain why it is good to use NVI, and its experimental results support the original claim. It explains well what are the challenges and demonstrate their solutions. 

[1] Minh et al., Neural Variational Inference and Learning in Belief Networks, ICML’14
[2] Rezende et al., Stochastic Backpropagation and Approximate Inference in Deep Generative Models, ICML’14

---

This is an interesting paper on a VAE framework for topic models. The main idea is to train a recognition model for the inference phase which, because of so called “amortized inference” can be much faster than normal inference where inference must be run iteratively for every document. Some comments:
Eqn 5: I find the notation p(theta(h)|alpha) awkward. Why not P(h|alpha) ?
The generative model seems agnostic to document length, meaning that the latent variables only generate probabilities over word space. However, the recognition model is happy to radically change the probabilities q(z|x) if the document length changes because the input to q changes. This seems undesirable. Maybe they should normalize the input to the recognition network?
The ProdLDA model might well be equivalent to exponential family PCA or some variant thereof:

---

The comparison to NVDM looks unfair since the user introduces a couples tricks (Dirichlet prior, batch normalisation, high momentum training, etc.) which NVDM doesn't use. A more convincing experimental design is to explore the effect of each trick separately in neural variational inference.

---

The perplexity you're reporting for the 20 Newsgroups dataset using LDA Collapsed Gibbs is better than for any other method I've seen.  Would you mind sharing the parameters you used and/or the preprocessed dataset?

---

This is an interesting paper on a VAE framework for topic models. The main idea is to train a recognition model for the inference phase which, because of so called “amortized inference” can be much faster than normal inference where inference must be run iteratively for every document. Some comments:
Eqn 5: I find the notation p(theta(h)|alpha) awkward. Why not P(h|alpha) ?
The generative model seems agnostic to document length, meaning that the latent variables only generate probabilities over word space. However, the recognition model is happy to radically change the probabilities q(z|x) if the document length changes because the input to q changes. This seems undesirable. Maybe they should normalize the input to the recognition network?
The ProdLDA model might well be equivalent to exponential family PCA or some variant thereof:

---

The reviewers agree that the approach is interesting and the paper presents useful findings. They also raise enough questions and suggestions for improvements that I believe the paper will be much stronger after further revision, though these seem straightforward to address.

---

The authors propose NVI for LDA variants. The authors compare NVI-LDA to standard inference schemes such as CGS and online SVI. The authors also evaluate NVI on a different model ProdLDA (not sure this model has been proposed before in the topic modeling literature though?)

In general, I like the direction of this paper and NVI looks promising for LDA. The experimental results however confound model vs inference which makes it hard to understand the significance of the results. Furthermore, the authors don't discuss hyper-parameter selection which is known to significantly impact performance of topic models. This makes it hard to understand when the proposed method can be expected to work. 

Can you maybe generate synthetic datasets with different Dirichlet distributions and assess when the proposed method recovers the true parameters?

Figure 1: Is this prior or posterior? The text talks about sparsity whereas the y-axis reads ""log p(topic proportions)"" which is a bit confusing.

Section 3.2: it is not clear what you mean by unimodal in softmax basis. Consider a Dirichlet on K-dimensional simplex with concentration parameter alpha/K where alpha<1 makes it multimodal. Isn't the softmax basis still multimodal?

None of the numbers include error bars. Are the results statistically significant?


Minor comments:

Last term in equation (3) is not ""error""; reconstruction accuracy or negative reconstruction error perhaps?

The idea of using an inference network is much older, cf. Helmholtz machine.

---

This paper proposes the use of neural variational inference method for topic models. The paper shows a nice trick to approximate Dirichlet prior using softmax basis with a Gaussian and then the model is trained to maximize the variational lower bound. Also, the authors study a better way to alleviate the component collapsing issue, which has been problematic for continuous latent variables that follow Gaussian distribution. The results look promising and the experimental protocol sounds fine.

Minor comments:
Please add citation to [1] or [2] for neural variational inference, and [2] for VAE. 
A typo in “This approximation to the Dirichlet prior p(θ|α) is results in the distribution”, it should be “This approximation to the Dirichlet prior p(θ|α) results in the distribution”

In table 2, it is written that DMFVI was trained more than 24hrs but failed to deliver any result, but why not wait until the end and report the numbers?

In table 3, why are the perplexities of LDA-Collapsed Gibbs and NVDM are lower while the proposed models (ProdLDA) generates more coherent topics? What is your intuition on this?

How does the training speed (until the convergence) differs by using different learning-rate and momentum scheduling approaches shown as in figure 1?

It may be also interesting to add some more analysis on the latent variables z (component collapsing and etc., although your results indirectly show that the learning-rate and momentum scheduling trick removes this issue).

Overall, the paper clearly proposes its main idea, explain why it is good to use NVI, and its experimental results support the original claim. It explains well what are the challenges and demonstrate their solutions. 

[1] Minh et al., Neural Variational Inference and Learning in Belief Networks, ICML’14
[2] Rezende et al., Stochastic Backpropagation and Approximate Inference in Deep Generative Models, ICML’14

---

This is an interesting paper on a VAE framework for topic models. The main idea is to train a recognition model for the inference phase which, because of so called “amortized inference” can be much faster than normal inference where inference must be run iteratively for every document. Some comments:
Eqn 5: I find the notation p(theta(h)|alpha) awkward. Why not P(h|alpha) ?
The generative model seems agnostic to document length, meaning that the latent variables only generate probabilities over word space. However, the recognition model is happy to radically change the probabilities q(z|x) if the document length changes because the input to q changes. This seems undesirable. Maybe they should normalize the input to the recognition network?
The ProdLDA model might well be equivalent to exponential family PCA or some variant thereof:

---

The comparison to NVDM looks unfair since the user introduces a couples tricks (Dirichlet prior, batch normalisation, high momentum training, etc.) which NVDM doesn't use. A more convincing experimental design is to explore the effect of each trick separately in neural variational inference.

---

The perplexity you're reporting for the 20 Newsgroups dataset using LDA Collapsed Gibbs is better than for any other method I've seen.  Would you mind sharing the parameters you used and/or the preprocessed dataset?",,,3.0,3.0,4.0,6.0,,,4.0,2.3333333333333335,2.0
375,"Authors: Sanjeev Arora, Yingyu Liang, Tengyu Ma
Source file: 375.pdf

ABSTRACT
The success of neural network methods for computing word embeddings has motivated methods for generating semantic embeddings of longer pieces of text, such as sentences and paragraphs. Surprisingly, Wieting et al (ICLR’16) showed that such complicated methods are outperformed, especially in out-of-domain (transfer learning) settings, by simpler methods involving mild retraining of word embeddings and basic linear regression. The method of Wieting et al. requires retraining with a substantial labeled dataset such as Paraphrase Database (Ganitkevitch et al., 2013). The current paper goes further, showing that the following completely unsupervised sentence embedding is a formidable baseline: Use word embeddings computed using one of the popular methods on unlabeled corpus like Wikipedia, represent the sentence by a weighted average of the word vectors, and then modify them a bit using PCA/SVD. This weighting improves performance by about 10% to 30% in textual similarity tasks, and beats sophisticated supervised methods including RNN’s and LSTM’s. It even improves Wieting et al.’s embeddings. This simple method should be used as the baseline to beat in future, especially when labeled training data is scarce or nonexistent. The paper also gives a theoretical explanation of the success of the above unsupervised method using a latent variable generative model for sentences, which is a simple extension of the model in Arora et al. (TACL’16) with new “smoothing” terms that allow for words occurring out of context, as well as high probabilities for words like and, not in all contexts.

1 INTRODUCTION
Word embeddings computed using diverse methods are basic building blocks for Natural Language Processing (NLP) and Information Retrieval (IR). They capture the similarities between words (e.g., (Bengio et al., 2003; Collobert & Weston, 2008; Mikolov et al., 2013a; Pennington et al., 2014)). Recent work has tried to compute embeddings that capture the semantics of word sequences (phrases, sentences, and paragraphs), with methods ranging from simple additional composition of the word vectors to sophisticated architectures such as convolutional neural networks and recurrent neural networks (e.g., (Iyyer et al., 2015; Le & Mikolov, 2014; Kiros et al., 2015; Socher et al., 2011; Blunsom et al., 2014; Tai et al., 2015; Wang et al., 2016)). Recently, (Wieting et al., 2016) learned general-purpose, paraphrastic sentence embeddings by starting with standard word embeddings and modifying them based on supervision from the Paraphrase pairs dataset (PPDB), and constructing sentence embeddings by training a simple word averaging model. This simple method leads to better performance on textual similarity tasks than a wide variety of methods and serves as a good initialization for textual classification tasks. However, supervision from the paraphrase dataset seems crucial, since they report that simple average of the initial word embeddings does not work very well.
Here we give a new sentence embedding method that is embarrassingly simple: just compute the weighted average of the word vectors in the sentence and then remove the projections of the average vectors on their first principal component (“common component removal”). Here the weight of a word w is a/(a+ p(w)) with a being a parameter and p(w) the (estimated) word frequency; we call
this smooth inverse frequency (SIF). This method achieves significantly better performance than the unweighted average on a variety of textual similarity tasks, and on most of these tasks even beats some sophisticated supervised methods tested in (Wieting et al., 2016), including some RNN and LSTM models. The method is well-suited for domain adaptation settings, i.e., word vectors trained on various kinds of corpora are used for computing the sentence embeddings in different testbeds. It is also fairly robust to the weighting scheme: using the word frequencies estimated from different corpora does not harm the performance; a wide range of the parameters a can achieve close-to-best results, and an even wider range can achieve significant improvement over unweighted average.
Of course, this SIF reweighting is highly reminiscent of TF-IDF reweighting from information retrieval (Sparck Jones, 1972; Robertson, 2004) if one treats a “sentence” as a “document” and make the reasonable assumption that the sentence doesn’t typically contain repeated words. Such reweightings (or related ideas like removing frequent words from the vocabulary) are a good rule of thumb but has not had theoretical justification in a word embedding setting.
The current paper provides a theoretical justification for the reweighting using a generative model for sentences, which is a simple modification for the Random Walk on Discourses model for generating text in (Arora et al., 2016). In that paper, it was noted that the model theoretically implies a sentence embedding, namely, simple average of embeddings of all the words in it.
We modify this theoretical model, motivated by the empirical observation that most word embedding methods, since they seek to capture word cooccurence probabilities using vector inner product, end up giving large vectors to frequent words, as well as giving unnecessarily large inner products to word pairs, simply to fit the empirical observation that words sometimes occur out of context in documents. These anomalies cause the average of word vectors to have huge components along semantically meaningless directions. Our modification to the generative model of (Arora et al., 2016) allows “smoothing” terms, and then a max likelihood calculation leads to our SIF reweighting.
Interestingly, this theoretically derived SIF does better (by a few percent points) than traditional TFIDF in our setting. The method also improves the sentence embeddings of Wieting et al., as seen in Table 1. Finally, we discovered that —contrary to widespread belief—Word2Vec(CBOW) also does not use simple average of word vectors in the model, as misleadingly suggested by the usual expression Pr[w|w1, w2, . . . , w5] ∝ exp(vw · ( 15 ∑ i vwi)). A dig into the implementation shows it implicitly uses a weighted average of word vectors —again, different from TF-IDF— and this weighting turns out to be quite similar in effect to ours. (See Section 3.1.)

2 RELATED WORK
Word embeddings. Word embedding methods represent words as continuous vectors in a low dimensional space which capture lexical and semantic properties of words. They can be obtained from the internal representations from neural network models of text (Bengio et al., 2003; Collobert & Weston, 2008; Mikolov et al., 2013a) or by low rank approximation of co-occurrence statistics (Deerwester et al., 1990; Pennington et al., 2014). The two approaches are known to be closely related (Levy & Goldberg, 2014; Hashimoto et al., 2016; Arora et al., 2016).
Our work is most directly related work to (Arora et al., 2016), which proposed a random walk model for generating words in the documents. Our sentence vector can be seen as approximate inference of the latent variables in their generative model.
Phrase/Sentence/Paragraph embeddings. Previous works have computed phrase or sentence embeddings by composing word embeddings using operations on vectors and matrices e.g., (Mitchell & Lapata, 2008; 2010; Blacoe & Lapata, 2012). They found that coordinate-wise multiplication of the vectors performed very well among the binary operations studied. Unweighted averaging is also found to do well in representing short phrases (Mikolov et al., 2013a). Another approach is recursive neural networks (RNNs) defined on the parse tree, trained with supervision (Socher et al., 2011) or without (Socher et al., 2014). Simple RNNs can be viewed as a special case where the parse tree is replaced by a simple linear chain. For example, the skip-gram model (Mikolov et al., 2013b) is extended to incorporate a latent vector for the sequence, or to treat the sequences rather than the word as basic units. In (Le & Mikolov, 2014) each paragraph was assumed to have a latent paragraph vector, which influences the distribution of the words in the paragraph. Skip-thought
of (Kiros et al., 2015) tries to reconstruct the surrounding sentences from surrounded one and treats the hidden parameters as their vector representations. RNNs using long short-term memory (LSTM) capture long-distance dependency and have also been used for modeling sentences (Tai et al., 2015). Other neural network structures include convolution neural networks, such as (Blunsom et al., 2014) that uses a dynamic pooling to handle input sentences of varying length and do well in sentiment prediction and classification tasks.
The directed inspiration for our work is (Wieting et al., 2016) which learned paraphrastic sentence embeddings by using simple word averaging and also updating standard word embeddings based on supervision from paraphrase pairs; the supervision being used for both initialization and training.

3 A SIMPLE METHOD FOR SENTENCE EMBEDDING
We briefly recall the latent variable generative model for text in (Arora et al., 2016). The model treats corpus generation as a dynamic process, where the t-th word is produced at step t. The process is driven by the random walk of a discourse vector ct ∈ <d. Each word w in the vocabulary has a vector in <d as well; these are latent variables of the model. The discourse vector represents “what is being talked about.” The inner product between the discourse vector ct and the (time-invariant) word vector vw for word w captures the correlations between the discourse and the word. The probability of observing a word w at time t is given by a log-linear word production model from Mnih and Hinton:
Pr[w emitted at time t | ct] ∝ exp (〈ct, vw〉) . (1)
The discourse vector ct does a slow random walk (meaning that ct+1 is obtained from ct by adding a small random displacement vector), so that nearby words are generated under similar discourses. It was shown in (Arora et al., 2016) that under some reasonable assumptions this model generates behavior –in terms of word-word cooccurence probabilities—that fits empirical works like word2vec and Glove. The random walk model can be relaxed to allow occasional big jumps in ct, since a simple calculation shows that they have negligible effect on cooccurrence probabilities of words. The word vectors computed using this model are reported to be similar to those from Glove and word2vec(CBOW).
Our improved Random Walk model. Clearly, it is tempting to define the sentence embedding as follows: given a sentence s, do a MAP estimate of the discourse vectors that govern this sentence. We note that we assume the discourse vector ct doesn’t change much while the words in the sentence were emitted, and thus we can replace for simplicity all the ct’s in the sentence s by a single discourse vector cs. In the paper (Arora et al., 2016), it was shown that the MAP estimate of cs is —up to multiplication by scalar—the average of the embeddings of the words in the sentence.
In this paper, towards more realistic modeling, we change the model (1) as follows. This model has two types of “smoothing term”, which are meant to account for the fact that some words occur out of context, and that some frequent words (presumably “the”, “and ” etc.) appear often regardless of the discourse. We first introduce an additive term αp(w) in the log-linear model, where p(w) is the unigram probability (in the entire corpus) of word and α is a scalar. This allows words to occur even if their vectors have very low inner products with cs. Secondly, we introduce a common discourse vector c0 ∈ <d which serves as a correction term for the most frequent discourse that is often related to syntax. (Other possible correction is left to future work.) It boosts the co-occurrence probability of words that have a high component along c0.
Concretely, given the discourse vector cs, the probability of a word w is emitted in the sentence s is modeled by,
Pr[w emitted in sentence s | cs] = αp(w) + (1− α) exp (〈c̃s, vw〉)
Zc̃s , (2)
where c̃s = βc0 + (1− β)cs, c0 ⊥ cs where α and β are scalar hyperparameters, and Zc̃s = ∑ w∈V exp (〈c̃s, vw〉) is the normalizing constant (the partition function). We see that the model allows a word w unrelated to the discourse cs to be omitted for two reasons: a) by chance from the term αp(w); b) if w is correlated with the common discourse vector c0.
Algorithm 1 Sentence Embedding Input: Word embeddings {vw : w ∈ V}, a set of sentences S, parameter a and estimated probabil-
ities {p(w) : w ∈ V} of the words. Output: Sentence embeddings {vs : s ∈ S}
1: for all sentence s in S do 2: vs ← 1|s| ∑ w∈s a a+p(w)vw 3: end for 4: Compute the first principal component u of {vs : s ∈ S} 5: for all sentence s in S do 6: vs ← vs − uu>vs 7: end for
Computing the sentence embedding. The word embeddings yielded by our model are actually the same 1 The sentence embedding will be defined as the max likelihood estimate for the vector cs that generated it. ( In this case MLE is the same as MAP since the prior is uniform.) We borrow the key modeling assumption of (Arora et al., 2016), namely that the word vw’s are roughly uniformly dispersed, which implies that the partition function Zc is roughly the same in all directions. So assume that Zc̃s is roughly the same, say Z for all c̃s. By the model (2) the likelihood for the sentence is
p[s | cs] = ∏ w∈s p(w | cs) = ∏ w∈s [ αp(w) + (1− α)exp (〈vw, c̃s〉) Z ] .
Let
fw(c̃s) = log
[ αp(w) + (1− α)exp (〈vw, c̃s〉)
Z ] denote the log likelihood of sentence s. Then, by simple calculus we have,
∇fw(c̃s) = 1 αp(w) + (1− α) exp (〈vw, c̃s〉) /Z 1− α Z exp (〈vw, c̃s〉) vw.
Then by Taylor expansion, we have,
fw(c̃s) ≈ fw(0) +∇fw(0)>c̃s
= constant + (1− α)/(αZ)
p(w) + (1− α)/(αZ) 〈vw, c̃s〉 .
Therefore, the maximum likelihood estimator for c̃s on the unit sphere (ignoring normalization) is approximately,2
arg max ∑ w∈s fw(c̃s) ∝ ∑ w∈s
a
p(w) + a vw, where a = 1− α αZ . (3)
That is, the MLE is approximately a weighted average of the vectors of the words in the sentence. Note that for more frequent words w, the weight a/(p(w) + a) is smaller, so this naturally leads to a down weighting of the frequent words.
To estimate cs, we estimate the direction c0 by computing the first principal component of c̃s’s for a set of sentences. In other words, the final sentence embedding is obtained by subtracting the projection of c̃s’s to their first principal component. This is summarized in Algorithm 1.

3.1 CONNECTION TO SUBSAMPLING PROBABILITIES IN WORD2VEC
Word2vec (Mikolov et al., 2013b) uses a sub-sampling technique which downsamples word w with probability proportional to 1/ √ p(w) where p(w) is the marginal probability of the word w. This
1We empirically discovered the significant common component c0 in word vectors built by existing methods, which inspired us to propose our theoretical model of this paper.
2Note that maxc:‖c‖=1 C + 〈c, g〉 = g/‖g‖ for any constant C.
heuristic not only speeds up the training but also learns more regular word representations. Here we explain that this corresponds to an implicit reweighting of the word vectors in the model and therefore the statistical benefit should be of no surprise.
Recall the vanilla CBOW model of word2vec:
Pr[wt | wt−1, . . . , wt−5] ∝ exp (〈v̄t, vw〉) , where v̄t = 1
5 5∑ i=1 vwt−i . (4)
It can be shown that the loss (MLE) for the single word vector vw (from this occurrence) can be abstractly written in the form,
g(vw) = γ(〈v̄t, vw〉) + negative sampling terms ,
where γ(x) = log(1/(1 + e−x)) is the logistic function. Therefore, the gradient of g(vw) is
∇g(vw) = γ′(〈v̄t, vw〉)v̄t = α(vwt−5 + vwt−4 + vwt−3 + vwt−2 + vwt−1) , (5)
where α is a scalar. That is, without the sub-sampling trick, the update direction is the average of the word vectors in the window.
The sub-sampling trick in (Mikolov et al., 2013b) randomly selects the summands in equation (5) to “estimate” the gradient. Specifically, the sampled update direction is
∇̃g(vw) = α(J5vwt−5 + J4vwt−4 + J3vwt−3 + J2vwt−2 + J1vwt−1) (6)
where Jk’s are Bernoulli random variables with Pr [Jk = 1] = q(wt−k) , min { 1, √ 10−5
p(wt−k)
} .
However, we note that ∇̃g(vw) is (very) biased estimator! We have that the expectation of ∇̃g(vw) is a weighted sum of the word vectors,
E [ ∇̃g(vw) ] = α(q(wt−5)vwt−5 + q(wt−4)vwt−4 + q(wt−3)vwt−3 + q(wt−2)vwt−2 + q(wt−1)vwt−1) .
In fact, the expectation E[∇̃g(vw)] corresponds to the gradient of a modified word2vec model with the average v̄t (in equation (4)) being replaced by the weighted average ∑5 k=1 q(wt−k)vwt−k . Such a weighted model can also share the same form of what we derive from our random walk model as in equation (3). Moreover, the weighting q(wi) closely tracks our weighting scheme a/(a + p(w)) when using parameter a = 10−4; see Figure 1 for an illustration. Therefore, the expected gradient here is approximately the estimated discourse vector in our model! Thus, word2vec with sub-sampling gradient heuristic corresponds to a stochastic gradient update method for using our weighting scheme.

4 EXPERIMENTS

4.1 TEXTUAL SIMILARITY TASKS
Datasets. We test our methods on the 22 textual similarity datasets including all the datasets from SemEval semantic textual similarity (STS) tasks (2012-2015) (Agirre et al., 2012; 2013; 2014; Agirrea et al., 2015), and the SemEval 2015 Twitter task (Xu et al., 2015) and the SemEval 2014 Semantic Relatedness task (Marelli et al., 2014). The objective of these tasks is to predict the similarity between two given sentences. The evaluation criterion is the Pearson’s coefficient between the predicted scores and the ground-truth scores.
Experimental settings. We will compare our method with the following:
1. Unsupervised: ST, avg-GloVe, tfidf-GloVe. ST denotes the skip-thought vectors (Kiros et al., 2015), avg-GloVe denotes the unweighted average of the GloVe vectors (Pennington et al., 2014),3 and tfidf-GloVe denotes the weighted average of GloVe vectors using TF-IDF weights.
2. Semi-supervised: avg-PSL. This method uses the unweighted average of the PARAGRAMSL999 (PSL) word vectors from (Wieting et al., 2015). The word vectors are trained using labeled data, but the sentence embedding are computed by unweighted average without training.
3. Supervised: PP, PP-proj., DAN, RNN, iRNN, LSTM (o.g.), LSTM (no). All these methods are initialized with PSL word vectors and then trained on the PPDB dataset. PP and PPproj. are proposed in (Wieting et al., 2016). The first is an average of the word vectors, and the second additionally adds a linear projection. The word vectors are updated during the training. DAN denotes the deep averaging network of (Iyyer et al., 2015). RNN denotes the classical recurrent neural network, and iRNN denotes a variant with the activation being the identity, and the weight matrices initialized to identity. The LSTM is the version from (Gers et al., 2002), either with output gates (denoted as LSTM(o.g.)) or without (denoted as LSTM (no)).
Our method can be applied to any types of word embeddings. So we denote the sentence embeddings obtained by applying our method to word embeddings method “XXX” as “XXX+WR”.4 To get a completely unsupervised method, we apply it to the GloVe vectors, denoted as GloVe+WR. The weighting parameter a is fixed to 10−3, and the word frequencies p(w) are estimated from the
3We used the vectors that are publicly available at http://nlp.stanford.edu/projects/glove/. They are 300- dimensional vectors that were trained on the 840 billion token Common Crawl corpus.
4“W” stands for the smooth inverse frequency weighting scheme, and “R” stands for removing the common components.
commoncrawl dataset.5 This is denoted by GloVe+WR in Table 1. We also apply our method on the PSL vectors, denoted as PSL+WR, which is a semi-supervised method.
Results. The results are reported in Table 1. Each year there are 4 to 6 STS tasks. For clarity, we only report the average result for the STS tasks each year; the detailed results are in the appendix.
The unsupervised method GloVe+WR improves upon avg-GloVe significantly by 10% to 30%, and beats the baselines by large margins. It achieves better performance than LSTM and RNN and is comparable to DAN, even though the later three use supervision. This demonstrates the power of this simple method: it can be even stronger than highly-tuned supervisedly trained sophisticated models. Using TF-IDF weighting scheme also improves over the unweighted average, but not as much as our method.
The semi-supervised method PSL+WR achieves the best results for four out of the six tasks and is comparable to the best in the rest of two tasks. Overall, it outperforms the avg-PSL baseline and all the supervised models initialized with the same PSL vectors. This demonstrates the advantage of our method over the training for those models.
We also note that the top singular vectors c0 of the datasets seem to roughly correspond to the syntactic information or common words. For example, closest words (by cosine similarity) to c0 in the SICK dataset are “just”, “when”, “even”, “one”, “up”, “little”, “way”, “there”, “while”, and “but.”
Finally, in the appendix, we showed that our two ideas all contribute to the improvement: for GloVe vectors, using smooth inverse frequency weighting alone improves over unweighted average by about 5%, using common component removal alone improves by 10%, and using both improves by 13%.

4.1.1 EFFECT OF WEIGHTING PARAMETER ON PERFORMANCE
We study the sensitivity of our method to the weighting parameter a, the method for computing word vectors, and the estimated word probabilities p(w). First, we test the performance of three
5It is possible to tune the parameter a to get better results. The effect of a and the corpus for estimating word frequencies are studied in Section 4.1.1.
types of word vectors (PSL, GloVe, and SN) on the STS 2012 tasks. SN vectors are trained on the enwiki dataset (Wikimedia, 2012) using the method in (Arora et al., 2016), while PSL and GloVe vectors are those used in Table 1. We enumerate a ∈ {10−i, 3 × 10−i : 1 ≤ i ≤ 5} and use the p(w) estimated on the enwiki dataset. Figure 2a shows that for all three kinds of word vectors, a wide range of a leads to significantly improved performance over the unweighted average. Best performance occurs from a = 10−3 to a = 10−4.
Next, we fix a = 10−3 and use four very different datasets to estimate p(w): enwiki (wikipedia, 3 billion tokens), poliblogs (Yano et al., 2009) (political blogs, 5 million), commoncrawl (Buck et al., 2014) (Internet crawl, 800 billion), text8 (Mahoney, 2008) (wiki subset, 1 million). Figure 2b shows performance is almost the same for all four settings.
The fact that our method can be applied on different types of word vectors trained on different corpora also suggests it should be useful across different domains. This is especially important for unsupervised methods, since the unlabeled data available may be collected in a different domain from the target application.

4.2 SUPERVISED TASKS
The sentence embeddings obtained by our method can be used as features for downstream supervised tasks. We consider three tasks: the SICK similarity task, the SICK entailment task, and the Stanford Sentiment Treebank (SST) binary classification task (Socher et al., 2013). To highlight the representation power of the sentence embeddings learned unsupervisedly, we fix the embeddings and only learn the classifier. Setup of supervised tasks mostly follow (Wieting et al., 2016) to allow fair comparison, i.e., the classifier a linear projection followed by the classifier in (Kiros et al., 2015). The linear projection maps the sentence embeddings into 2400 dimension (the same as the skip-thought vectors), and is learned during the training. We compare our method to PP, DAN, RNN, and LSTM, which are the methods used in Section 4.1. We also compare to the skip-thought vectors (with improved training in (Lei Ba et al., 2016)).
Results. Our method gets better or comparable performance compared to the competitors. It gets the best results for two of the tasks. This demonstrates the power of our simple method. We emphasize that our embeddings are unsupervisedly learned, while DAN, RNN, LSTM are trained with supervision. Furthermore, skip-thought vectors are much higher dimensional than ours (though projected into higher dimension, the original 300 dimensional embeddings contain all the information).
The advantage is not as significant as in the textual similarity tasks. This is possibly because similarity tasks rely directly upon cosine similarity, which favors our method’s approach of removing the common components (which can be viewed as a form of denoising), while in supervised tasks, with the cost of some label information, the classifier can pick out the useful components and ignore the common ones.
Finally, we speculate that our method doesn’t outperform RNN’s and LSTM’s for sentiment tasks because (a) the word vectors —and more generally the distributional hypothesis of meaning —has known limitations for capturing sentiment due to the “antonym problem”, (b) also in our weighted average scheme, words like “not” that may be important for sentiment analysis are downweighted a lot. To address (a), there is existing work on learning better word embeddings for sentiment analysis (e.g., (Maas et al., 2011)). To address (b), it is possible to design weighting scheme (or learn weights) for this specific task.

4.3 THE EFFECT OF THE ORDER OF WORDS IN SENTENCES
A interesting feature of our method is that it ignores the word order. This is in contrast to that RNN’s and LSTM’s can potentially take advantage of the word order. The fact that our method achieves better or comparable performance on these benchmarks raise the following question: is word order not important in these benchmarks? We conducted an experiment suggesting that word order does play some role.
We trained and tested RNN/LSTM on the supervised tasks where the words in each sentence are randomly shuffled, and the results are reported in Table 3.6 It can be observed that the performance drops noticeably. Thus our method —which ignores word order—must be much better at exploiting the semantics than RNN’s and LSTM’s. An interesting future direction is to explore if some ensemble idea can combine the advantages of both approaches.

5 CONCLUSIONS
This work provided a simple approach to sentence embedding, based on the discourse vectors in the random walk model for generating text (Arora et al., 2016). It is simple and unsupervised, but achieves significantly better performance than baselines on various textual similarity tasks, and can even beat sophisticated supervised methods such as some RNN and LSTM models. The sentence embeddings obtained can be used as features in downstream supervised tasks, which also leads to better or comparable results compared to the sophisticated methods.

6 ACKNOWLEDGEMENTS
We thank the reviewers for insightful comments. We also thank the authors of (Wieting et al., 2016; Bowman et al., 2015) for sharing their code or the preprocessed datasets.
This work was supported in part by NSF grants CCF-1527371, DMS-1317308, Simons Investigator Award, Simons Collaboration Grant, and ONRN00014- 16-1-2329. Tengyu Ma was supported in addition by Simons Award in Theoretical Computer Science and IBM PhD Fellowship.

A DETAILS OF EXPERIMENTAL SETTING
A.1 UNSUPERVISED TASK: TEXTUAL SIMILARITY
The competitors. We give a brief overview of the competitors. RNN is the classical recurrent neural network:
ht = f(WxW xt w +Whht−1 + b)
where f is the activation, Wx,Wh and b are parameters, and xt is the t-th token in the sentence. The sentence embedding of RNN is just the hidden vector of the last token. iRNN is a special RNN with the activation being the identity, the weight matrices initialized to identity, and b initialized to zero. LSTM (Hochreiter & Schmidhuber, 1997) is a recurrent neural network architecture designed to capture long-distance dependencies. Here, the version from (Gers et al., 2002) is used.
The supervised methods are initialized with PARAGRAM-SL999 (PSL) vectors, and trained using the approach of (Wieting et al., 2016) on the XL section of the PPDB data (Pavlick et al., 2015) which contains about 3 million unique phrase pairs. After training, the final models can be used to generate sentence embeddings on the test data. For hyperparameter tuning they used 100k examples sampled from PPDB XXL and trained for 5 epochs. Then after finding the hyperparameters that maximize Spearmans coefficients on the Pavlick et al. PPDB task, they are trained on the entire XL section of PPDB for 10 epochs. See (Wieting et al., 2016) and related papers for more details about these methods.
The tfidf-GloVe method is a weighted average of the GloVe vectors, where the weights are defined by the TF-IDF scheme. More precisely, the embedding of a sentence s is
vs = 1 |s| ∑ w∈s IDFwvw (7)
where IDFw is the inverse document frequency of w, and |s| denotes the number of words in the sentence. Here, the TF part of the TF-IDF scheme is taken into account by the sum over w ∈ s. Furthermore, when computing IDFw, each sentence is viewed as a “document”:
IDFw := log 1 +N
1 +Nw
where N is the total number of sentences and Nw is the number of sentences containing w, and 1 is added to avoid division by 0. In the experiments, we use all the textual similarity datasets to compute IDFw.
Detailed experimental results. In the main body we present the average results for STS tasks by year. Each year there are actually 4 to 6 STS tasks, as shown in Table 4. Note that tasks with the same name in different years are actually different tasks. Here we provide the results for each tasks in Table 5. PSL+WR achieves the best results on 12 out of 22 tasks, PP and PP-proj. achieves on 3, tfidf-GloVe achieves on 2, and DAN, iRNN, and GloVe+WR achieves on 1. In general, our method improves the performance significantly compared to the unweighted average, though on rare cases such as MSRpar it can decrease the performance.
Effects of smooth inverse frequency and common component removal. There are two key ideas in our methods: smooth inverse frequency weighting (W) and common component removal (R). It is instructive to see their effects separately. Let GloVe+W denote the embeddings using only smooth inverse frequency, and GloVe+R denote that using only common component removal. Similarly define PSL+W and PSL+R. The results for these methods are shown in Table 6. When using GloVe vectors, W alone improves the performance of the unweighted average baseline by about 5%, R alone improves by 10%, W and R together improves by 13%. When using PSL vectors, W gets 10%, R gets 10%, W and R together gets 13%. In summary, both techniques are important for obtaining significant advantage over the unweighted average.
A.2 SUPERVISED TASKS
Setup of supervised tasks mostly follow (Wieting et al., 2016) to allow fair comparison: the sentence embeddings are fixed and fed into some classifier which are trained. For the SICK similarity task, given a pair of sentences with embeddings vL and vR, first do a linear projection:
hL = WpvL, hR = WpvR
where Wp is of size 300 × dp and is learned during training. dp = 2400 matches the dimension of the skip-thought vectors. Then hL and hR are used in the objective function from (Tai et al., 2015). Almost the same approach is used for the entailment task.For the sentiment task, the classifier has a fully-connected layer with a sigmoid activation followed by a softmax layer.
Recall that our method has two steps: smooth inverse frequency weighting and common component removal. For the experiments here, we do not perform the common component removal, since it can be absorbed into the projection step. For the weighted average, the hyperparameter a is enumerated in {10−i, 3 × 10−i : 2 ≤ i ≤ 3}. The other hyperparameters are enumerated as in (Wieting et al., 2016), and the same validation approach is used to select the final values.
A.3 ADDITIONAL SUPERVISED TASKS
Here we report the experimental results on two more datasets, comparing to known results on them.
SNLI. The first experiment is for the 3-class classification task on the SNLI dataset (Bowman et al., 2015). To compare to the results in (Bowman et al., 2015), we used their experimental setup. In particular, we applied our method to 300 dimensional GloVe vectors and used an additional tanh neural network layer to map these 300d embeddings into 300 dimensional space, then used the code provided by the authors of (Bowman et al., 2015), trained the classifier on our 100 dimensional sentence embedding for 120 passes over the data, using their default hyperparameters. The results are shown in Table 7. Our method indeed gets slightly better performance.
Our test accuracy is worse than those using more sophisticated models (e.g., using attention mechanism), which are typically 83% - 88%; see the website of the SNLI project7 for a summary. An interesting direction is to study whether our idea can be combined with these sophisticated models to get improved performance.
IMDB. The second experiment is the sentiment analysis task on the IMDB dataset, studied in (Wang & Manning, 2012). Since the intended application is semi-supervised or transfer learning, we also compared performance with fewer labeled examples.
7http://nlp.stanford.edu/projects/snli/
Our method gets worse performance on the full dataset, but its decrease in performance is better with less labeled examples, showing the benefit of using word embeddings. Note that our sentence embeddings are unsupervised, while that in the NB-SVM method takes advantage of the labels. Another comment is that sentiment analysis appears to be the best case for Bag-Of-Word methods, whereas it may be the worst case for word embedding methods (See Table 2) due to the well-known antonymy problem —distributional hypothesis fails for distinguishing “good” from “bad.”
","The success of neural network methods for computing word embeddings has motivated methods for generating semantic embeddings of longer pieces of text, such as sentences and paragraphs. Surprisingly, Wieting et al (ICLR’16) showed that such complicated methods are outperformed, especially in out-of-domain (transfer learning) settings, by simpler methods involving mild retraining of word embeddings and basic linear regression. The method of Wieting et al. requires retraining with a substantial labeled dataset such as Paraphrase Database (Ganitkevitch et al., 2013). The current paper goes further, showing that the following completely unsupervised sentence embedding is a formidable baseline: Use word embeddings computed using one of the popular methods on unlabeled corpus like Wikipedia, represent the sentence by a weighted average of the word vectors, and then modify them a bit using PCA/SVD. This weighting improves performance by about 10% to 30% in textual similarity tasks, and beats sophisticated supervised methods including RNN’s and LSTM’s. It even improves Wieting et al.’s embeddings. This simple method should be used as the baseline to beat in future, especially when labeled training data is scarce or nonexistent. The paper also gives a theoretical explanation of the success of the above unsupervised method using a latent variable generative model for sentences, which is a simple extension of the model in Arora et al. (TACL’16) with new “smoothing” terms that allow for words occurring out of context, as well as high probabilities for words like and, not in all contexts.",ICLR 2017 conference submission,True,,"This paper presents a new theoretically-principled method of representing sentences as vectors. The experiments show that vectors produced by this method perform well on similarity and entailment benchmarks, surpassing some RNN-based methods too.

Overall, this is an interesting empirical result, especially since the model is not order-sensitive (as far as I can tell). I would like to see some more discussion on why such a simple model does better than LSTMs at capturing similarity and entailment. Could this be an artifact of these benchmarks?

---

A new method for sentence embedding that is simple and performs well. Important contribution that will attract attention and help move the field forward.

---

This is a good paper with an interesting probabilistic motivation for weighted bag of words models.
The (hopefully soon) added comparison to Wang and Manning will make it stronger. 
Though it is sad that for sufficiently large datasets, NB-SVM still works better.

In the second to last paragraph of the introduction you describe a problem of large cooccurrence counts which was already fixed by the Glove embeddings with their weighting function f.

Minor comments:

""The capturing the similarities"" -- typo in line 2 of intro.
""Recently, (Wieting et al.,2016) learned"" -- use citet instead of parenthesized citation

---

This paper proposes a simple way to reweight the word embedding in the simple composition function for sentence representation. This paper also shows the connection between this new weighting scheme and some previous work.

Here are some comments on technical details:

- The word ""discourse"" is confusing. I am not sure whether the words ""discourse"" in ""discourse vector c_s"" and the one in ""most frequent discourse"" have the same meaning.
- Is there any justification about $c_0$ related to syntac?
- Not sure what thie line means: ""In fact the new model was discovered by our detecting the common component c0 in existing embeddings."" in section ""Computing the sentence embedding""
- Is there any explanation about the results on sentiment in Table 2?

---

This paper presents a new theoretically-principled method of representing sentences as vectors. The experiments show that vectors produced by this method perform well on similarity and entailment benchmarks, surpassing some RNN-based methods too.

Overall, this is an interesting empirical result, especially since the model is not order-sensitive (as far as I can tell). I would like to see some more discussion on why such a simple model does better than LSTMs at capturing similarity and entailment. Could this be an artifact of these benchmarks?

---

We (J. Mu and P. Viswanath) thoroughly enjoyed the authors' previous work on linear algebraic structure of word senses (cf.

---

Dear Authors,

Please resubmit your paper in the ICLR 2017 format with the correct font for your submission to be considered. Thank you!

---

This paper presents a new theoretically-principled method of representing sentences as vectors. The experiments show that vectors produced by this method perform well on similarity and entailment benchmarks, surpassing some RNN-based methods too.

Overall, this is an interesting empirical result, especially since the model is not order-sensitive (as far as I can tell). I would like to see some more discussion on why such a simple model does better than LSTMs at capturing similarity and entailment. Could this be an artifact of these benchmarks?

---

A new method for sentence embedding that is simple and performs well. Important contribution that will attract attention and help move the field forward.

---

This is a good paper with an interesting probabilistic motivation for weighted bag of words models.
The (hopefully soon) added comparison to Wang and Manning will make it stronger. 
Though it is sad that for sufficiently large datasets, NB-SVM still works better.

In the second to last paragraph of the introduction you describe a problem of large cooccurrence counts which was already fixed by the Glove embeddings with their weighting function f.

Minor comments:

""The capturing the similarities"" -- typo in line 2 of intro.
""Recently, (Wieting et al.,2016) learned"" -- use citet instead of parenthesized citation

---

This paper proposes a simple way to reweight the word embedding in the simple composition function for sentence representation. This paper also shows the connection between this new weighting scheme and some previous work.

Here are some comments on technical details:

- The word ""discourse"" is confusing. I am not sure whether the words ""discourse"" in ""discourse vector c_s"" and the one in ""most frequent discourse"" have the same meaning.
- Is there any justification about $c_0$ related to syntac?
- Not sure what thie line means: ""In fact the new model was discovered by our detecting the common component c0 in existing embeddings."" in section ""Computing the sentence embedding""
- Is there any explanation about the results on sentiment in Table 2?

---

This paper presents a new theoretically-principled method of representing sentences as vectors. The experiments show that vectors produced by this method perform well on similarity and entailment benchmarks, surpassing some RNN-based methods too.

Overall, this is an interesting empirical result, especially since the model is not order-sensitive (as far as I can tell). I would like to see some more discussion on why such a simple model does better than LSTMs at capturing similarity and entailment. Could this be an artifact of these benchmarks?

---

We (J. Mu and P. Viswanath) thoroughly enjoyed the authors' previous work on linear algebraic structure of word senses (cf.

---

Dear Authors,

Please resubmit your paper in the ICLR 2017 format with the correct font for your submission to be considered. Thank you!",,4.5,,,4.0,7.333333333333333,3.0,,3.6666666666666665,3.5,4.0
377,"DEEP REINFORCEMENT LEARNING
Authors: Misha Denil, Pulkit Agrawal, Tejas D Kulkarni, Tom Erez, Peter Battaglia, Nando de Freitas
Source file: 377.pdf

ABSTRACT
When encountering novel objects, humans are able to infer a wide range of physical properties such as mass, friction and deformability by interacting with them in a goal driven way. This process of active interaction is in the same spirit as a scientist performing experiments to discover hidden facts. Recent advances in artificial intelligence have yielded machines that can achieve superhuman performance in Go, Atari, natural language processing, and complex control problems; however, it is not clear that these systems can rival the scientific intuition of even a young child. In this work we introduce a basic set of tasks that require agents to estimate properties such as mass and cohesion of objects in an interactive simulated environment where they can manipulate the objects and observe the consequences. We found that deep reinforcement learning methods can learn to perform the experiments necessary to discover such hidden properties. By systematically manipulating the problem difficulty and the cost incurred by the agent for performing experiments, we found that agents learn different strategies that balance the cost of gathering information against the cost of making mistakes in different situations. We also compare our learned experimentation policies to randomized baselines and show that the learned policies lead to better predictions.

1 INTRODUCTION
Our work is inspired by empirical findings and theories in psychology indicating that infant learning and thinking is similar to that of adult scientists (Gopnik, 2012). One important view in developmental science is that babies are endowed with a small number of separable systems of core knowledge for reasoning about objects, actions, number, space, and possibly social interactions (Spelke & Kinzler, 2007). The object core system covering aspects such as cohesion, continuity, and contact, enables babies and other animals to solve object related tasks such as reasoning about oclusion and predicting how objects behave.
Core knowledge research has motivated the development of methods that endow agents with physics priors and perception modules so as to infer intrinsic physical properties rapidly from data (Battaglia et al., 2013; Wu et al., 2015; 2016; Stewart & Ermon, 2016). For instance, using physics engines and mental simulation, it becomes possible to infer quantities such as mass from visual input (Hamrick et al., 2016; Wu et al., 2015).
In early stages of life, infants spend a lot of time interacting with objects in a seemingly random manner (Smith & Gasser, 2005). They interact with objects in multiple ways, including throwing, pushing, pulling, breaking, and biting. It is quite possible that this process of actively engaging with objects and watching the consequences of their actions helps infants understand different physical properties of the object which cannot be observed directly using their sensory systems. It seems infants run a series of “physical” experiments to enhance their knowledge about the world (Gopnik, 2012). The act of performing an experiment is useful both for quickly adapting an agent’s policy to a new environment and for understanding object properties in a holistic manner. Despite impressive advances in artificial intelligence that have led to superhuman performance in Go, Atari and natural language processing, it is still unclear if these systems behind these advances can rival the scientific intuition of even a small child.
While we draw inspiration from child development, it must be emphasized that our purpose is not to provide an account of learning and thinking in humans, but rather to explore how similar types of understanding might be learned by artificial agents in a grounded way. To this end we show that we can build agents that can learn to experiment so as to learn representations that are informative about physical properties of objects, using deep reinforcement learning. The act of conducting an experiment involves the agent having a belief about the world, which it then updates by observing the consequences of actions it performs.
We investigate the ability of agents to learn to perform experiments to infer object properties through two environments—Which is Heavier and Towers. In the Which is Heavier environment, the agent is able to apply forces to blocks and it must infer which of the blocks is the heaviest. In the Towers environment the agent’s task is to infer how many rigid bodies a tower is composed of by knocking it down. Unlike Wu et al. (2015), we assume that the agent has no prior knowledge about physical properties of objects, or the laws of physics, and hence must interact with the objects in order to learn to answer questions about these properties.
Our results indicate that in the case Which is Heavier environment our agents learn experimentation strategies that are similar to those we would expect from an algorithm designed with knowledge of the underlying structure of the environment. In the Towers environment we show that our agents learn a closed loop policy that can adapt to a varying time scale. In both environments we show that when using the learned interaction policies agents are more accurate and often take less time to produce correct answers than when following randomized interaction policies.

2 WHAT IS THIS PAPER ABOUT?
This is an unusual paper in that it does not present a new model or propose a new algorithm. There is a reinforcement learning task at the core of each of our experiments, but the algorithm and models we use to solve it are not new, and many other existing approaches should be expected to perform equally well if they were to be substituted in the same setting.
This paper is a step towards agents that understand objects and intuitive reasoning in physical worlds. Our best AI agents currently fail on simple control tasks and simple games, such as Montezuma’s Revenge, because when they look at a screen that has a ladder, a key and a skull they don’t immediately know that keys open doors, that skulls are probably hazardous and best avoided, that ladders allow us to defy gravity, etc. The understanding of physics, relations and objects enables children to solve seemingly simple problems that our best existing AI agents do not come close to begin to solve.
Endowing our agents with knowledge of objects would help enormously with planning, reasoning and exploration, and yet, doing so is far from trivial. What is an object? It turns out this question does not have a straightforward answer, and this paper is based around the idea that staring at a thing is not enough to understand what it is.
Children understand their world by engaging with it. Poking something to find that it is soft, tasting it to discover it is delicious, or hitting it to see if it falls down. Much of the knowledge people have of the world is the result of interaction. Vision or open loop perception alone is not enough.
This paper introduces tasks where we can evaluate the ability of agents to learn about these “hidden” properties of objects. This requires environments where the tasks depend on these properties (otherwise the agents have no incentive to learn about them) and also that we have a way to probe for this understanding in agents that complete the tasks.
Previous approaches to this problem have relied on either explicit knowledge of the underlying structure of the environment (e.g. hard-wired physical laws) or on exploiting correlations between material appearance and physical properties (see Section 7 for much more detail). One of the contributions of this paper is to show that our agents can still learn about properties of objects, even when the connection between material appearance and physical properties is broken. This setting allows us to show that our agents are not merely learning that blocks are heavy; they are learning how to check if blocks are heavy.
None of the previous approaches give a complete account of how agents could come to understand the physical properties of the world around them. Specifying a model manually is difficult to scale,
generalize and to ground in perception. Making predictions from only visual properties will fail to distinguish between objects that look similar, and it will certainly be unable to distinguish between a sack full of rocks and a sack full of tennis balls.

3 ANSWERING QUESTIONS THROUGH INTERACTION
We pose the problem of experimentation as that of answering questions about non-visual properties of objects present in the environment. We design environments that ask questions about these properties by providing rewards when the agent is able to infer them correctly, and we train agents to answer these questions using reinforcement learning.
We design environments that follow a three phase structure:
Interaction Initially there is an exploration phase, where the agent is free to interact with the environment and gather information.
Labeling The interaction phase ends when the agent produces a labeling action through which it communicates its answer to the implicit question posed by the environment.
Reward When the agent produces as labeling action, the environment responds with a reward, positive for a correct answer and negative for incorrect, and the episode terminates. The episode terminates automatically with a negative reward if the agent does not produce a labeling action before a maximum time limit is reached.
Crucially, the transition between interaction and labeling does not happen at a fixed time, but is initiated by the agent. This is achieved by providing the agent with the ability to produce either an interaction action or a labeling action at every time step. This allows the agent to decide when enough information has been gathered, and forces it to balance the trade-off between answering now given its current knowledge, or delaying its answer to gather more information.
The optimal trade-off between information gathering and risk of answering incorrectly depends on two factors. The first factor is the difficulty of the question and the second is the cost of information. The difficulty is environment specific and is addressed later when we describe the environments. The cost of information can be generically controlled by varying the discount factor during learning. A small discount factor places less emphasis on future rewards and encourages the agent to answer as quickly as possible. On the other hand, a large discount factor encourages the agent to spend more time gathering information in order to increase the likelihood of choosing the correct answer.
Our use of “questions” and “answers” differs from how these terms are used elsewhere in the literature. Sutton et al. (2011) talk about a value function as a question, and the agent provides an answer in the form of an approximation of the value. The answer incorporates the agent’s knowledge, and the match between the actual value and the agent’s approximation grounds what it means for this knowledge to be accurate.
In our usage the environment (or episode) itself is the question, and answers come in the form of labeling actions. In each episode there is a correct answer whose semantics is grounded in the sign of the reward function, and the accuracy of an agents knowledge is assessed by the frequency with which it is able to choose the correct answer.
Using reward (rather than value) to ground our semantics means that we have a straightforward way to ask questions that do not depend on the agent’s behavior. For example, we can easily ask the question “Which block is heaviest?” without making the question contingent on a particular information acquisition strategy.

4 AGENT ARCHITECTURE AND TRAINING
We use the same basic agent architecture and training procedure for all of our experiments, making only minimal modifications in order to adapt the agents to different observation spaces and actuators. For all experiments we train recurrent agents using an LSTM with 100 hidden units. When working from features we feed the observations into the LSTM directly. When training from pixels we first scale the observations to 84x84 pixels and feed them through a three convolution layers, each
followed by a ReLU non-linearity. The three layers have 32, 64, 64 square filters with sizes 8, 4, 3, which are applied at strides of 4, 2, 1 respectively. We train the agents using Asynchronous Advantage Actor Critic (Mnih et al., 2016), but ensure that the unroll length is always greater than the timeout length so the agent network is unrolled over the entirety of each episode.

5 WHICH IS HEAVIER
The Which is Heavier environment is designed to ask a question about the relative masses of different objects in a scene. We assign masses to objects in a way that is uncorrelated with their appearance in order to ensure that the task is not solvable without interaction.

5.1 ENVIRONMENT
The environment is diagrammed in the left panel of Figure 1. It consists of four blocks, which are constrained to only move vertically. The blocks are always the same size, but vary in mass between episodes. The agent’s strength (i.e. magnitude of force it can apply) remains constant between episodes.
The question to answer in this environment is which of the four blocks is the heaviest. Since the mass of each block is randomly assigned in each episode, the agent must poke the blocks and observe how they respond in order to make this determination. Assigning masses randomly ensures it is not possible to solve this task from vision (or features) alone, since the appearance and identity of each block imparts no information about its mass in the current episode. The only way to obtain information about the masses of the blocks is to interact with them and watch how they respond.
The Which is Heavier environment is designed to encode a latent bandit problem through a “physical” lens. Each block corresponds to an arm of the bandit, and the reward obtained by pulling each arm is proportional to the mass of the block. Identifying the heaviest block can then be seen as a best arm identification problem (Audibert & Bubeck, 2010). Best arm identification is a well studied problem in experimental design, and understanding of how an optimal solution to the latent bandit should behave is used to guide our analysis of the agents we train on this task.
It is important to emphasize that we cannot simply apply standard bandit algorithms here, because we impose a much higher level of prior ignorance on our algorithms than that setting allows. Bandit algorithms assume that rewards are observed directly, whereas our agents observe mass through its role in dynamics (and in the case of learning from pixels, through the lens of vision as well). To maintain a bandit setting one could imagine parameterizing this transformation from reward to observation, and perhaps even learning the mapping as well; however, doing so requires explicitly acknowledging the mapping in the design of the learning algorithm, which we avoid doing. Moreover, acknowledging this mapping in any way requires the a-priori recognition of the existence of the latent bandit structure. From the perspective of our learning algorithm the mere existence of such a structure also lies beyond the veil of ignorance.
Controlling the distribution of masses allows us to control the difficulty of this task. In particular, by controlling the size of the mass gap between the two heaviest blocks we can make the task more
or less difficult. We generate masses in the range [0, 1] and scale them to an appropriate range for the agent’s strength.
We use the following scheme for controlling the difficulty of the Which is Heavier environment. First we select one of the blocks uniformly at random to be the “heavy” block and designate the remaining three as “light” blocks. We sample the mass of the heavy block from Beta(β, 1) and the mass of the light blocks from Beta(1, β). The single parameter β effectively controls the distribution of mass gaps (and thus controls the difficulty), with large values of β leading to easier problems. Figure 1 shows the distribution of mass gaps for three values of β that we use in our experiments.
We distinguish between problem level and instance level difficulty for this domain. Instance level difficulty refers to the size of the mass gap in a single episode. If the mass gap is small it is harder to determine which block is heaviest, and we say that one episode is more difficult than another by comparing their mass gaps. Problem level difficulty refers to the shape of the generating distribution of mass gaps (e.g. as shown in the right panel of Figure 1). A distribution that puts more mass on configurations that have a small mass gap will tend to generate more episodes that are difficult at the instance level, and we say that one distribution is more difficult than another if it is more likely to generate instances with small mass gaps. We control the problem level difficulty through β, but we incorporate both problem and instance level difficulty in our analysis.
We set the episode length limit to 100 steps in this environment, which is sufficient time to be much longer than a typical episode by a successfully trained agent.

5.2 ACTUATORS
The obvious choice for actuation in physical domains is some kind of arm or hand based manipulator. However, controlling an arm or hand is quite challenging on its own, requiring a fair amount of dexterity on the part of the agent. The manipulation problem, while very interesting in its own right, is orthogonal to our goals in this work. Therefore we avoid the problem of learning dexterous manipulation by providing the agent with a much simpler form of actuation.
We call the actuation strategy for this environment direct actuation, which allows the agent to affect forces on the different blocks directly. At every time step the agent can output one out of eight possible actions. The first four actions result in an application of a vertical force of fixed magnitude to center of mass of each of the four blocks respectively. The remaining actions are labeling actions and correspond to agent’s selection of which is the heaviest block.

5.3 EXPERIMENTS
Our first experiment is a sanity check to show that we can train agents successfully on the Which is Heavier environment using both features and pixels. This experiment is designed simply to show that our task is solvable, and to illustrate that by changing the problem difficulty we can make the task very hard.
We present two additional experiments showing how varying difficulty leads to differentiated behavior both at the problem level and at the instance level. In both cases knowledge of the latent bandit problem allows us to make predictions about how an experimenting agent should behave, and our experiments are designed to show that qualitatively correct behavior is obtained by our agents in spite of their a-priori ignorance of the underlying bandit problem.
We show that as we increase the problem difficulty the learned policies transition from guessing immediately when a heavy block is found to strongly preferring to poke all blocks before making a decision. This corresponds to the observation that if it is unlikely for more than one arm to give high reward then any high reward arm is likely to be best.
We also observe that our agents can adapt their behavior to the difficulty of individual problem instances. We show that a single agent will tend to spend longer gathering information when the particular problem instance is more difficult. This corresponds to the observation that when the two best arms have similar reward then more information is required to accurately distinguish them.
Finally, we conduct an experiment comparing our learned information gathering policies to a randomized baseline method. This experiment shows that agents more reliably produce the correct label by following their learned interaction policies than by observing the environment being driven by random actions.
Success in learning For this experiment we trained several agents at three different difficulties corresponding to β ∈ {3, 5, 10}. For each problem difficulty we trained agents on both feature observations, which includes the z coordinate of each of the four blocks; and also using raw pixels, providing 84 × 84 pixel RGB rendering of the scene to the agent. Representative learning curves for each condition are shown in Figure 2. The curves are smoothed over time and show a running estimate of the probability of success, rather than showing the reward directly.
The agents do not reach perfect performance on this task, with more difficult problems plateauing at progressively lower performance. This can be explained by looking at the distributions of instance level difficulties generated by different settings of β, which is shown in the right panel of Figure 1. For higher difficulties (lower values of β) there is a substantial probability of generating problem instances where the mass gap is near 0, which makes distinguishing between the two heaviest blocks very difficult.
Population strategy differentiation For this experiment we trained agents at three different difficulties corresponding to β ∈ {3, 5, 10} all using a discount factor of γ = 0.95 which corresponds a relatively high cost of gathering information. We trained three agents for each difficulty and show results aggregated across the different replicas.
After training, each agent was run for 10,000 steps under the same conditions they were exposed to during training. We record the number and length of episodes executed during the testing period as well as the outcome of each episode. Episodes are terminated by timeout after 100 steps, but the vast majority of episodes are terminated in < 30 steps by the agent producing a label. Since episodes vary in length not all agents complete the same number of episodes during testing.
The left plot in Figure 3 shows histograms of the episode lengths broken down by task difficulty. The dashed vertical line indicates an episode length of four interaction steps, which is the minimum number of actions required for the agents to interact with every block. At a task difficulty of β = 10 the agents appear to learn simply to search for a single heavy block (which can be found with an average of two interactions). However, at a task difficulty of β = 3 we see a strong bias away from terminating the episode before taking at least four exploratory actions.
Individual strategy differentiation For this experiment we trained agents using the same three task difficulties as in the previous experiment, but with an increased discount factor of γ = 0.99.
This decreases the cost of exploration and encourages the agents to gather more information before producing a label, leading to longer episodes.
After training, each agent was run for 100,000 steps under the same conditions they were exposed to during training. We record the length of each episode, as well as the mass gap between the two heaviest blocks in each episode. In the same way that we use the distribution of mass gaps as a measure of task difficulty, we can use the mass gap in a single episode as a measure of the difficulty of that specific problem instance. We again exclude from analysis the very small proportion of episodes that terminate by timeout.
The right plots in Figure 3 show the relationship between the mass gap and episode length across the testing runs of two different agents. From these plots we can see how a single agent has learned to adapt its behavior based on the difficulty of a single problem instance. Although the variance is high, there is a clear correlation between the mass gap and the length of the episodes. This behavior reflects what we would expect from a solution to the latent bandit problem; more information is required to identify the best arm when the second best arm is nearly as good.
Randomized interaction For this experiment we trained several agents using both feature and pixel observations at the same three task difficulties with a discount of γ = 0.95. In total we trained six sets of agents for this experiment.
After training, each agent was run for 10,000 steps under the same conditions used during training. We record the outcome of each episode, as well as the number of steps taken by each agent before it chooses a label. For each agent we repeat the experiment using both the agent’s learned interaction policy as well as a randomized interaction policy.
The randomized interaction policy is obtained as follows: At each step the agent chooses a candidate action using its learned policy. If the candidate action is a labeling action then it is passed to the environment unchanged (and the episode terminates). However, if the candidate action is an interaction action then we replace the agent action with a new interaction action chosen uniformly at random from the available action set. When following the randomized interaction policy the agent has no control over the information gathering process, but still controls when each episode ends, and what label is chosen.
Figure 4 compares the learned interaction policies to the randomized interaction baselines. The results show that the effect on episode length is small, with no consistent bias towards longer or shorter episodes across difficulties and observation types. However, the learned interaction policies produce more accurate labels across all permutations.

6 TOWERS
The Towers environment is designed to ask agents to count the number of cohesive rigid bodies in a scene. The environment is designed so that in its initial configuration it is not possible to determine the number of rigid bodies from vision or features alone.

6.1 ENVIRONMENT
The environment is diagrammed in the left panel of Figure 5. It consists of a tower of five blocks which can move freely in three dimensions. The initial block tower is always in the same configuration but in each episode we bolt together different subsets of the blocks to form larger rigid bodies as shown in the figure.
The question to answer in this environment is how many rigid bodies are formed from the primitive blocks. Since which blocks are bound together is randomly assigned in each episode, and binding forces are invisible, the agent must poke the tower and observe how it falls down in order to determine how many rigid bodies it is composed of. We parameterize the environment in such a way that the distribution over the number of separate blocks in the tower is uniform. This ensures that there is no single action strategy that achieves high reward.

6.2 ACTUATORS
In the Towers environment, we used two actuators: direct actuation, which is similar to the Which is Heavier environment; and the fist actuator, described below. In case of the direct actuation, the agent can output one out of 25 actions. At every time step, the agent can apply a force of fixed magnitude in either of +x, -x, +y or -y direction to one out of the five blocks. If two blocks are glued together, both blocks move under the effect of force. We use towers of five blocks, which results in 20 different possible actions. The remaining actions are labeling actions that are used by the agent to indicate the number of distinct blocks in the tower.
The fist is a large spherical object that the agent can actuate by setting velocities in a 2D horizontal plane. Unlike direct actuation, the agent cannot apply any direct forces to the objects that constitute the tower, but only manipulate them by pushing or hitting them with the fist. At every time step agent can output one of nine actions. The first four actions corresponds to setting the velocity of the fist to a constant amount in (+x, -x, +y, -y) directions respectively. The remaining actions are labeling actions, that are used by the agent to indicate the number of distinct blocks in the tower.
In order to investigate if the agent learns a strategy of stopping after a fixed number of time steps or whether it integrates sensory information in a non-trivial manner we used a notion of “control time step”. The idea of control time step is similar to that of action repeats and if the physics simulation time step is 0.025s and control time step is 0.1s, it means that the same action is repeated 4 times. For the direct actuators we use an episode timeout of 26 steps and for both actuator types.

6.3 EXPERIMENTS
Our first experiment is again intended to show that we can train agents in this environment. We show simply that the task is solvable by our agents using both types of actuation.
The second experiment shows that the agents learn to wait for an observation where they can identify the number of rigid bodies before producing an answer. This is designed to show that the agents find a closed loop strategy for counting the number of rigid bodies. An alternative hypothesis would be that agents learn to wait for (approximately) the same number of steps each time and then take their best guess.
Our third experiment compares the learned policy to a randomized interaction policy and shows that agents are able to determine the correct number of blocks in the tower more quickly and more reliably when using their learned policy to gather information.
Success in learning For this experiment we trained several agents on the Towers environment using different pairings of actuators and perception. The features observations include the 3d position of each primitive block, and when training using raw pixels we provide an 84× 84 pixel RGB rendering of the scene as the agent observation. Figure 6 shows learning curves for each combination of actuator and observation type.
In all cases we obtain agents that solve the task nearly perfectly, although when training from pixels we find that the range of hyperparameters which train successfully is narrower than when training from features. Interestingly, the fist actuators lead to the fastest learning, in spite of the fact that the agent must manipulate the blocks indirectly through the fist. One possible explanation is that the fist can affect multiple blocks in one action step, whereas in the direct actuation only one block can be affected per time step.
Waiting for information For this experiment we trained an agent with pixel observations and the fist actuator on the towers task with an control time step of 0.1 seconds and examine its behavior at test time with a smaller delay between actions. Reducing the control time step means that from the agent perspective time has been slowed down. Moving the fist a fixed amount of distance takes longer, as does waiting for the block tower to collapse once it has been hit.
After training the agent was run for 10000 steps for a range of different control time steps. We record the outcome of each episode, as well as the number of steps taken by the agent before it chooses a label. None of the test episodes terminate by timeout, so we include all of them in the analysis.
The plot in Figure 5 shows the probability of answering correctly, as well as the median length of each episode measured in seconds. In terms of absolute performance we see a small drop compared to the training setting, where the agent is essentially perfect, but the agent performance remains good even for substantially smaller control timesteps than were used during training.
We also observe that the episodes with different time steps take approximate the same amount of real time across the majority of the tested range. This corresponds to a large change in episode length as measured by number of agent actions, since with an control time step of 0.01 the agent must execute 10x as many actions to cover the same amount of real time as compared to the control time step used during training. From this we can infer that the agent has learned to wait for an informative observation before producing a label, as opposed to a simpler degenerate strategy of waiting a fixed amount of steps before answering.
Randomized interaction For this experiment we trained several agents for each combination of actuator and observation type, and examine their behavior when observing an environment driven by a random interaction policy. The randomized interaction policy is identical to the randomized baseline used in the Which is Heavier environment.
After training, each agent was run for 10,000 steps. We record the outcome of each episode, as well as the number of steps taken by the agent before it chooses a label. For each agent we repeat the experiment using both the agent’s learned interaction policy as well as the randomized interaction policy.
Figure 7 compares the learned interaction policies to the randomized interaction baselines. The results show that the agents tend to produce labels more quickly when following their learned interaction policies, and also that the labels they produce in this way are much more accurate.

7 RELATED WORK
Deep learning techniques in conjunction with vast labeled datasets have yielded powerful models for image classification (Krizhevsky et al., 2012; He et al., 2016) and speech recognition (Hinton et al., 2012). In recent years, as we have approached human level performance on these tasks, there has been a strong interest in the computer vision field in moving beyond semantic classification, to tasks that require a deeper and more nuanced understanding of the world.
Inspired by developmental studies (Smith & Gasser, 2005), some recent works have focused on learning representations by predicting physical embodiment quantities such as ego-motion (Agrawal et al., 2015; Jayaraman & Grauman, 2015), instead of symbolic labels. Extending the realm of things-to-be-predicted to include quantities beyond class labels, such as viewer centric parameters (Doersch et al., 2015) or the poses of humans within a scene (Delaitre et al., 2012; Fouhey et al., 2014), has been shown to improve the quality of feature learning and scene understanding. Researchers have looked at cross modal learning, for example synthesizing sounds from visual images (Owens et al., 2015), using summary statistics of audio to learn features for object recognition (Owens et al., 2016) or image colorization (Zhang et al., 2016).
Inverting the prediction tower, another line of work has focused on learning about the visual world by synthesizing, rather than analyzing, images. Major cornerstones of recent work in this area include the Variational Autoencoders of Kingma & Welling (2014), the Generative Adversarial Networks
of (Goodfellow et al., 2014), and more recently autoregressive models have been very successful (van den Oord et al., 2016).
Building on models of single image synthesis there have been many works on predicting the evolution of video frames over time (Ranzato et al., 2014; Srivastava et al., 2015; van den Oord et al., 2016). Xue et al. (2016) have approached this problem by designing a variational autoencoder architecture that uses the latent stochastic units of the VAE to make choices about the direction of motion of objects, and generates future frames conditioned on these choices.
A different form of uncertainty in video prediction can arise from the effect of actions taken by an agent. In environments with deterministic dynamics (where the possibility of “known unknowns” can, in principle, be eliminated), very accurate action-conditional predictions of future frames can be made (Oh et al., 2015). Introducing actions into the prediction process amounts to learning a latent forward dynamics model, which can be exploited to plan actions to achieve novel goals (Watter et al., 2015; Assael et al., 2015; Fragkiadaki et al., 2016). In these works, frame synthesis plays the role of a regularizer, preventing collapse of the feature space where the dynamics model lives.
Agrawal et al. (2016) break the dependency between frame synthesis and dynamics learning by replacing frame synthesis with an inverse dynamics model. The forward model plays the same role as in the earlier works, but here feature space collapse is prevented by ensuring that the model can decode actions from pairs of time-adjacent images. Several works, including Agrawal et al. (2016) and Assael et al. (2015) mentioned above but also Pinto et al. (2016); Pinto & Gupta (2016); Levine et al. (2016), have gone further in coupling feature learning and dynamics. The learned dynamics models can be used for control not only after learning but also during the learning process in order to collect data in a more targeted way, which has been shown to improve the speed and quality of learning in robot manipulation tasks.
A key challenge of learning from dynamics is collecting the appropriate data. An ingenious solution to this is to import real world data into a physics engine and simulate the application of forces in order to generate ground truth data. This is the approach taken by Mottaghi et al. (2016), who generate an “interactable” data set of scenes, which they use to generate a static data set of image and force pairs, along with the ground truth trajectory of a target object in response to the application of the indicated force.
When the purpose is learning an intuitive understanding of dynamics it is possible to do interesting work with entirely synthetic data (Fragkiadaki et al., 2016; Lerer et al., 2016). Lerer et al. (2016) show that convolutional networks can learn to make judgments about the stability of synthetic block towers based on a single image of the tower. They also show that their model trained on synthetic data is able to generalize to make accurate judgments about photographs of similar block towers built in the real world.
Making intuitive judgments about block towers has been extensively studied in the psychophysics literature. There is substantial evidence connecting the behavior of human judgments to inference over an explicit latent physics model (Hegarty, 2004; Hamrick et al., 2011; Battaglia et al., 2013). Humans can infer mass by watching movies of complex rigid body dynamics (Hamrick et al., 2016).
A major component of the above line of work is analysis by synthesis, in which understanding of a physical process is obtained by learning to invert it. Observations are assumed to be generated from an explicitly parameterized generative model of the true physical process, and provide constraints to an inference process run over the parameters of this model. The analysis by synthesis approach has been extremely influential due to its power to explain human judgments and generalization patterns in a variety of situations (Lake et al., 2015).
Galileo (Wu et al., 2015) is a particularly relevant instance of tying together analysis by synthesis and deep learning for understanding dynamics. This system first infers the physical parameters (mass and friction coefficient) of a variety of blocks by watching videos of them sliding down slopes and colliding with other blocks. This stage of the system uses an off-the-shelf object tracker to ground inference over the parameters of a physical simulator, and the inference is achieved by matching simulated and observed block trajectories. The inferred physical parameters are used to train a deep network to predict the physical parameters from the initial frame of video. At test time the system is evaluated by using the deep network to infer physical parameters of new blocks, which can be fed into the physics engine and used to answer questions about behaviors not observed at training time.
Physics 101 (Wu et al., 2016) is an extension of Galileo that more fully embraces deep learning. Instead of using a first pass of analysis by synthesis to infer physical parameters based on observations, a deep network is trained to regress the output of an object tracker directly, and the relevant physical laws are encoded directly into the architecture of the model. The authors show that they can use latent intrinsic physical properties inferred in this way to make novel predictions. The approach of encoding physical models as architecture constraints has also been proposed by Stewart & Ermon (2016).
Many of the works discussed thus far, including Galileo and Physics 101, are restricted to passive sensing. Pinto et al. (2016); Pinto & Gupta (2016); Agrawal et al. (2016); Levine et al. (2016) are exceptions to this because they learn their models using a sequential greedy data collection bootstrapping strategy. Active sensing, it appears, is an important aspect of visual object learning in toddlers as argued by Bambach et al. (2016), providing motivation for the approach presented here.
In computer vision, it is well known that recognition performance can be improved by moving so as to acquire new views of an object or scene. Jayaraman & Grauman (2016), for example, apply deep reinforcement learning to construct an agent that chooses how to acquire new views of an object so as to classify it into a semantic category, and their related work section surveys many other efforts in active vision.
While Jayaraman & Grauman (2016) and others share deep reinforcement learning and active sensing in common with our work, their goal is to learn a policy that can be applied to images to make decisions based on vision. In contrast, the goal in this paper is to study how agents learn to experiment continually so as to learn representations to answer questions about intrinsic properties of objects. In particular, our focus is on tasks that can only be solved by interaction and not by vision alone.

8 CONCLUSION AND FUTURE DIRECTIONS
Despite recent advances in artificial intelligence, machines still lack a common sense understanding of our physical world. There has been impressive progress in recognizing objects, segmenting object boundaries and even describing visual scenes with natural language. However, these tasks are not enough for machines to infer physical properties of objects such as mass, friction or deformability.
We introduce a deep reinforcement learning agent that actively interacts with physical objects to infer their hidden properties. Our approach is inspired by findings from the developmental psychology literature indicating that infants spend a lot of their early time experimenting with objects through random exploration (Smith & Gasser, 2005; Gopnik, 2012; Spelke & Kinzler, 2007). By letting our agents conduct physical experiments in an interactive simulated environment, they learn to manip-
ulate objects and observe the consequences to infer hidden object properties. We demonstrate the efficacy of our approach on two important physical understanding tasks—inferring mass and counting the number of objects under strong visual ambiguities. Our empirical findings suggest that our agents learn different strategies for these tasks that balance the cost of gathering information against the cost of making mistakes in different situations.
Scientists and children are able not only to probe the environment to discover things about it, but they can also leverage their findings to answer new questions. In this paper we have shown that agents can be trained to gather knowledge to answer questions about hidden properties, but we have not addressed the larger issue of theory building, or transfer of this information. Given agents that can make judgments about mass and numerosity, how can they be enticed to leverage this knowledge to solve new tasks?
Another important aspect of understanding through interaction is that that the shape of the interactions influences behavior. We touched on this in the Towers environment where we looked at two different actuation styles, but there is much more to be done here. Thinking along these lines leads naturally to exploring tool use. We showed that agents can make judgments about object mass by hitting them, but could we train an agent to make similar judgments using a scale?
Finally, we have made no attempt in this work to optimize data efficiency, but learning physical properties from fewer samples is an important direction to pursue.

ACKNOWLEDGMENTS
We would like to thank Matt Hoffman for several enlightening discussions about bandits. We would also like to thank the ICLR reviewers, whose helpful feedback allowed us to greatly improve the paper.
","When encountering novel objects, humans are able to infer a wide range of physical properties such as mass, friction and deformability by interacting with them in a goal driven way. This process of active interaction is in the same spirit as a scientist performing experiments to discover hidden facts. Recent advances in artificial intelligence have yielded machines that can achieve superhuman performance in Go, Atari, natural language processing, and complex control problems; however, it is not clear that these systems can rival the scientific intuition of even a young child. In this work we introduce a basic set of tasks that require agents to estimate properties such as mass and cohesion of objects in an interactive simulated environment where they can manipulate the objects and observe the consequences. We found that deep reinforcement learning methods can learn to perform the experiments necessary to discover such hidden properties. By systematically manipulating the problem difficulty and the cost incurred by the agent for performing experiments, we found that agents learn different strategies that balance the cost of gathering information against the cost of making mistakes in different situations. We also compare our learned experimentation policies to randomized baselines and show that the learned policies lead to better predictions.",ICLR 2017 conference submission,True,,"The following statement best summarizes the contribution: ""This paper shows that model free RL methods can learn how to gather information about physical properties of objects, even when this information is not available to a passive observer, and use this information to make decisions."" So this is not a paper about new theory or algorithms, but rather about solving the problem of acquiring knowledge about the physics of the world around us, which is important for many problems and helps explain human performance in many tasks. There are still some concerns about the depth-of-analysis of the paper, but on balance, it is seen as an unconventional but interesting paper. As per AnonReviewer6, the final version could still aim to better address ""What should other researchers focus on if they are trying to build agents that can understand physics intuitively (building off this work)?""
 -- Area chair

---

The machine learning field has profited from hundreds of yearly publications that propose a new algorithm, which outperforms baseline algorithms and has a theoretical proof for some idealized form of the algorithm. This paper, however, is different. It does not propose a new algorithm. It incidentally uses an RL algorithm, but many other RL algorithms could have been used just as well. This paper is not about new models or algorithms.

So what is this paper about? It is an initial research step toward understanding objects and intuitive reasoning in physical worlds. Why is this important? Our best AI agents currently fail on simple control tasks and simple games, such as Montezuma’s Revenge, because when they look at a screen that has a ladder, a key and a skull they don’t immediately know that keys open doors, that skulls are probably hazardous and best avoided, that ladders allow us to defy gravity, etc. The understanding of physics, relations and objects enables children to solve seemingly simple problems that our best existing AI agents do not come close to begin to solve.   

Endowing our agents with knowledge of objects would help enormously with planning, reasoning and exploration. Yet, this is far from trivial. First, what is an object? It turns out this is not an easy question to answer and has baffled psychologists and philosophers. To simplify matters, in this paper we restrict our attention to rigid bodies. Staring at a thing in one’s hands is not enough to understand what it is. Vision or open loop perception in general is not enough. Trying to pull the thing apart to see whether it is a single entity might help. (Children do indeed love to tear things like paper apart to understand them.) Touching the thing to see what happens also helps --- perhaps it lights up and starts beeping. Further interaction might reveal that it enables us to remotely talk to someone else, to tweet, etc. Much of the knowledge gained is the result of interaction, that is perception by action. 

This paper is also about designing tasks to understand how agents acquire intuitive reasoning and experimentation strategies in physical worlds. An abundant body of evidence in psychology --- see eg the works of Gerd Gigerenzer

---

We would like to highlight to all reviewers that the experiments have been updated substantially since the previous version of the paper.  In particular, we now include the missing results for Fist Pixels as requested by Reviewer 3, as well as additional experiments comparing our models to random baseline policies in both environments.

In the process of making these updates we have have made several minor changes to the environments themselves. These changes have made the tower environment harder to solve, but do not alter any conclusions drawn from the experiments.

---

This paper addresses the question of how to utilize physical interactions to answer questions about physical outcomes. This question falls into a popular stream in ML community -- understanding physics. The paper moved a step further and worked on experimental setups where there is no prior about the physical properties/rules and it uses a deep reinforcement learning (DRL) technique to address the problem. My overall opinion about this paper is: an interesting attempt and idea, yet without a clear contribution.

The experimental setups are quite interesting. The goal is to figure out which blocks are heavier or which blocks are glued together -- only by pushing and pulling objects around without any prior. The paper also shows reasonable performances on each task with detailed scenarios.

While these experiments and results are interesting, the contribution is unclear. My main question is: does this result bring us any new insight? While the scenarios are interesting and focused on physical experiments, this is not any more different (potentially easier) than learning from playing games (e.g. Atari). In other words, are the tasks really different from other typical popular DRL tasks? To this end, I would have been more excited if authors showed some more new insights or experiments on learned representations and etc. Currently, the paper only discusses the factual outcome. For example, it describes the experimental setup and how much performances an agent could achieve. The authors could probably dissect the learned representations further, or discuss how the experimental results are linked to the human behavior or physical properties/laws.

I am very in-between for my overall rating. I think the paper could have a deeper analysis. I however recommend the acceptance because of its merit of the idea.



The followings are some detailed questions (not directly impacting my overall rating):
(1) Page 2 ""we assume that the agent has no prior knowledge about the physical properties of objects, or the laws of physics, and hence must interact with the objects in order to learn to answer questions about these properties."": why does one ""must"" interact with objects in order to learn about the properties? Can't we also learn through observation?

(2) Figure 1right is missing a Y-axis label.

(3) Page 3: A relating to bandit is interesting, but the formal approach is all based on DRL.

(4) Page 5 ""which makes distinguishing between the two heaviest blocks very difficult"": I am a bit confused why having a small mass gap makes the task harder (unless it's really close to 0). Shouldn't a machine be possible to distinguish even a pixel difference of speed? If not, isn't this just because of the network architecture?

(5) Page 5 ""Since the agents exhibit similar performance using pixels and features we conduct the remaining experiments in this section using feature observations, since these agents are substantially faster to train."": How about at least showing a correlation of performances at the instance level (rather than average performances)? Even so, I think this is a bit of big conclusion.

(6) Throughout the papers, I felt that many conclusions (e.g. difficulty and etc) are based on a particularly chosen training distribution. For example, how does an agent really know when the instance is any more difficult? Doesn't this really depend on the empirically learned distribution of training samples (i.e. P(m_3 | m_1, m_2), where m_i indicates masses of object 1, 2, and 3)? In other words, does what's hard/easy matter much unless this is more thoroughly tested over various types of distributions?

(7) Any baseline approach?

---

This paper investigates the question of gathering information (answering question)
through direct interaction with the environment. In that sense, it is closely
related to ""active learning"" in supervised learning, or to the fundamental
problem of exploration-exploitation in RL. The authors consider a specific 
instance of this problem in a physics domain and learn
information-seeking policies using recent deep RL methods.

The paper is mostly empirical and explores the effect of changing the
cost of information (via the discount factor) on the structure of the learned
policies. It also shows that general-purpose deep policy gradient methods are
sufficient powerful to learn such tasks. The proposed environment is, to my knowledge,
novel as well the task formulation in section 2. (And it would be very valuable to the
the community if the environment would be open-sourced)

The expression ""latent structure/dynamics"" is used throughout the text and the connection
with bandits is mentioned in section 4. It therefore seems that authors aspire
for more generality with their approach but the paper doesn't quite fully ground
the proposed approach formally in any existing framework nor does it provide a
new one completely.

For example: how does your approach formalize the concept of ""questions"" and ""answers"" ?
What makes a question ""difficult"" ? How do you quantify ""difficulty"" ?
How do you define the ""cost of information""? What are its units (bits, scalar reward), its semantics ?
Do you you have an MDP or a POMDP ? What kind of MDP do you consider ?
How do you define your discounted MDP ? What is the state and action spaces ?
Some important problem structure under the ""interaction/labeling/reward""
paragraph of section 2 would be worth expressing directly in your definition
of the MDP: labeling actions can only occur during the ""labeling phase"" and that the transition
and reward functions have a specific structure (positive/negative, lead to absorbing state).
The notion of ""phase"" could perhaps be implemented by considering an augmented state space : $\tilde s = (s, phase)$

---

This paper purports to investigate the ability of RL agents to perform ‘physics experiments’ in an environment, to infer physical properties about the objects in that environment. The problem is very well motivated; indeed, inferring the physical properties of objects is a crucial skill for intelligent agents, and there has been relatively little work in this direction, particularly in deep RL. The paper is also well-written.

As there are no architectural or theoretical contributions of the paper (and none are claimed), the main novelty comes in the task application – using a recurrent A3C model for two tasks that simulate an agent interacting with an environment to infer physical properties of objects. More specifically, two tasks are considered – moving blocks to determine their mass, and poking towers such that they fall to determine the number of rigid bodies they are composed of. These of course represent a very limited cross-section of the prerequisite abilities for an agent to understand physics. This in itself is not a bad thing, but since there is no comparison of different (simpler) RL agents on the tasks, it is difficult to determine if the tasks selected are challenging. As mentioned in the pre-review question, the ‘Which is Heavier’ task seems quite easy due to the actuator set-up, and the fact that the model simply must learn to take the difference between successive block positions (which are directly encoded as features in most experiments).  Thus, it is not particularly surprising that the RL agent can solve the proposed tasks. 

The main claim beyond solving two proposed tasks related to physics simulation is that “the agents learn different strategies for these tasks that balance the cost of gathering information against the cost of making mistakes”. The ‘cost of gathering information’ is implemented by multiplying the reward with a value of gamma < 1. This is somewhat interesting behaviour, but is hardly surprising given the problem setup.

One item the authors highlight is that their approach of learning about physical object properties through interaction is different from many previous approaches, which use visual cues. However, the authors also note that this in itself is not novel, and has been explored in other work (e.g. Agrawal et al. (2016)). I think it’s crucial for the authors to discuss these approaches in more detail (potentially along with removing some other, less relevant information from the related work section), and specifically highlight why the proposed tasks in this paper are interesting compared to, for example, learning to move objects towards certain end positions by poking them.

To discern the level of contribution of the paper, one must ask the following questions: 

1)	how much do these two tasks contribute (above previous work) to the goal of having agents learn the properties of objects by interaction; and
2)	how much do the results of the RL agent on these tasks contribute to our understanding of agents that interact with their environment to learn physical properties of objects? 

It is difficult to know exactly, but due to the concerns outlined above, I am not convinced that the answers to (1) or (2) are “to a significant extent”. In particular, for (1), since the proposed agent is able to essentially solve both tasks, it is not clear that the tasks can be used to benchmark more advanced agents (e.g. it can’t be used as a set of bAbI-like tasks). 

Another possible concern, as pointed out by Reviewer 3, is that the description of the model is extremely concise. It would be nice to have, for example, a diagram illustrating the inputs and outputs to the model at each time step, to ease replication.

Overall, it is important to make progress towards agents that can learn to discover physical properties of their environment, and the paper contributes in this direction. However, the technical contributions of this paper are rather limited – thus, it is not clear to what extent the paper pushes forward research in this direction beyond previous work that is mentioned. It would be nice, for example, to have some discussion about the future of agents that learn physics from interaction (speculation on more difficult versions of the tasks in this paper), and how the proposed approach fits into that picture.  

---------------
EDIT: score updated, see comments below

---

This paper presents interesting experimental findings that state-of-the-art deep reinforcement learning methods enable agent learning of latent (physical) properties in its environment. The paper formulates the problem of an agent labeling environmental properties after interacting with the environment based on its actions, and applies the deep reinforcement learning model to evaluate whether such learning is possible. The approach jointly learns the convolutional layers for pixel-based perception and its later layers for learning actions based on reinforcement signals.

We have a mixed opinion about this paper. The paper is written clearly and presents interesting experimental findings. It introduces and formulates a problem potentially important for many robotics applications. Simultaneously, the paper suffers from lacking algorithmic contributions and missing (some of) crucial experiments to confirm its true benefits.

Pros:

+ This paper introduces a new problem of learning latent properties in the agent's environment.

+ The paper presents a framework to appropriately combine existing tools to address the formulated problem.

+ The paper tries reinforcement learning with image inputs and fist-like actuator actions. This will lead to its direct application to robots.

Cons:

- Lacking algorithmic contribution: this paper applies existing tools/methods to solve the problem rather than developing something new or extending them. The approach essentially is training LSTMs with convolutional layers using the previous Asynchronous Advantage Actor Critic.

- In the Towers experiment, the results of probably the most important setting, ""Fist Pixels"", are missing. This setting receiving pixel inputs and using the Fist actuator in a continuous space is the setting closest to real-world robots, and thus is very important to confirm whether the proposed approach will be directly applicable to real-world robots. However, Figure 5 is missing the results with this setting. Is there any reason behind this?

- The paper lacks its comparison to any baseline methods. Without explicit baselines, it is difficult to see what the agent is really learning and what aspect of the proposed approach is benefitting the task. For instance, in the Towers task, how would an agent randomly pushing/hitting the tower (using 'Fist') a number of times and then passively observing its consequence to produce a label perform compared to this approach? That is, how would an approach with a fixed action policy (but with everything else) perform compared to the full deep reinforcement learning version?

---

The following statement best summarizes the contribution: ""This paper shows that model free RL methods can learn how to gather information about physical properties of objects, even when this information is not available to a passive observer, and use this information to make decisions."" So this is not a paper about new theory or algorithms, but rather about solving the problem of acquiring knowledge about the physics of the world around us, which is important for many problems and helps explain human performance in many tasks. There are still some concerns about the depth-of-analysis of the paper, but on balance, it is seen as an unconventional but interesting paper. As per AnonReviewer6, the final version could still aim to better address ""What should other researchers focus on if they are trying to build agents that can understand physics intuitively (building off this work)?""
 -- Area chair

---

The machine learning field has profited from hundreds of yearly publications that propose a new algorithm, which outperforms baseline algorithms and has a theoretical proof for some idealized form of the algorithm. This paper, however, is different. It does not propose a new algorithm. It incidentally uses an RL algorithm, but many other RL algorithms could have been used just as well. This paper is not about new models or algorithms.

So what is this paper about? It is an initial research step toward understanding objects and intuitive reasoning in physical worlds. Why is this important? Our best AI agents currently fail on simple control tasks and simple games, such as Montezuma’s Revenge, because when they look at a screen that has a ladder, a key and a skull they don’t immediately know that keys open doors, that skulls are probably hazardous and best avoided, that ladders allow us to defy gravity, etc. The understanding of physics, relations and objects enables children to solve seemingly simple problems that our best existing AI agents do not come close to begin to solve.   

Endowing our agents with knowledge of objects would help enormously with planning, reasoning and exploration. Yet, this is far from trivial. First, what is an object? It turns out this is not an easy question to answer and has baffled psychologists and philosophers. To simplify matters, in this paper we restrict our attention to rigid bodies. Staring at a thing in one’s hands is not enough to understand what it is. Vision or open loop perception in general is not enough. Trying to pull the thing apart to see whether it is a single entity might help. (Children do indeed love to tear things like paper apart to understand them.) Touching the thing to see what happens also helps --- perhaps it lights up and starts beeping. Further interaction might reveal that it enables us to remotely talk to someone else, to tweet, etc. Much of the knowledge gained is the result of interaction, that is perception by action. 

This paper is also about designing tasks to understand how agents acquire intuitive reasoning and experimentation strategies in physical worlds. An abundant body of evidence in psychology --- see eg the works of Gerd Gigerenzer

---

We would like to highlight to all reviewers that the experiments have been updated substantially since the previous version of the paper.  In particular, we now include the missing results for Fist Pixels as requested by Reviewer 3, as well as additional experiments comparing our models to random baseline policies in both environments.

In the process of making these updates we have have made several minor changes to the environments themselves. These changes have made the tower environment harder to solve, but do not alter any conclusions drawn from the experiments.

---

This paper addresses the question of how to utilize physical interactions to answer questions about physical outcomes. This question falls into a popular stream in ML community -- understanding physics. The paper moved a step further and worked on experimental setups where there is no prior about the physical properties/rules and it uses a deep reinforcement learning (DRL) technique to address the problem. My overall opinion about this paper is: an interesting attempt and idea, yet without a clear contribution.

The experimental setups are quite interesting. The goal is to figure out which blocks are heavier or which blocks are glued together -- only by pushing and pulling objects around without any prior. The paper also shows reasonable performances on each task with detailed scenarios.

While these experiments and results are interesting, the contribution is unclear. My main question is: does this result bring us any new insight? While the scenarios are interesting and focused on physical experiments, this is not any more different (potentially easier) than learning from playing games (e.g. Atari). In other words, are the tasks really different from other typical popular DRL tasks? To this end, I would have been more excited if authors showed some more new insights or experiments on learned representations and etc. Currently, the paper only discusses the factual outcome. For example, it describes the experimental setup and how much performances an agent could achieve. The authors could probably dissect the learned representations further, or discuss how the experimental results are linked to the human behavior or physical properties/laws.

I am very in-between for my overall rating. I think the paper could have a deeper analysis. I however recommend the acceptance because of its merit of the idea.



The followings are some detailed questions (not directly impacting my overall rating):
(1) Page 2 ""we assume that the agent has no prior knowledge about the physical properties of objects, or the laws of physics, and hence must interact with the objects in order to learn to answer questions about these properties."": why does one ""must"" interact with objects in order to learn about the properties? Can't we also learn through observation?

(2) Figure 1right is missing a Y-axis label.

(3) Page 3: A relating to bandit is interesting, but the formal approach is all based on DRL.

(4) Page 5 ""which makes distinguishing between the two heaviest blocks very difficult"": I am a bit confused why having a small mass gap makes the task harder (unless it's really close to 0). Shouldn't a machine be possible to distinguish even a pixel difference of speed? If not, isn't this just because of the network architecture?

(5) Page 5 ""Since the agents exhibit similar performance using pixels and features we conduct the remaining experiments in this section using feature observations, since these agents are substantially faster to train."": How about at least showing a correlation of performances at the instance level (rather than average performances)? Even so, I think this is a bit of big conclusion.

(6) Throughout the papers, I felt that many conclusions (e.g. difficulty and etc) are based on a particularly chosen training distribution. For example, how does an agent really know when the instance is any more difficult? Doesn't this really depend on the empirically learned distribution of training samples (i.e. P(m_3 | m_1, m_2), where m_i indicates masses of object 1, 2, and 3)? In other words, does what's hard/easy matter much unless this is more thoroughly tested over various types of distributions?

(7) Any baseline approach?

---

This paper investigates the question of gathering information (answering question)
through direct interaction with the environment. In that sense, it is closely
related to ""active learning"" in supervised learning, or to the fundamental
problem of exploration-exploitation in RL. The authors consider a specific 
instance of this problem in a physics domain and learn
information-seeking policies using recent deep RL methods.

The paper is mostly empirical and explores the effect of changing the
cost of information (via the discount factor) on the structure of the learned
policies. It also shows that general-purpose deep policy gradient methods are
sufficient powerful to learn such tasks. The proposed environment is, to my knowledge,
novel as well the task formulation in section 2. (And it would be very valuable to the
the community if the environment would be open-sourced)

The expression ""latent structure/dynamics"" is used throughout the text and the connection
with bandits is mentioned in section 4. It therefore seems that authors aspire
for more generality with their approach but the paper doesn't quite fully ground
the proposed approach formally in any existing framework nor does it provide a
new one completely.

For example: how does your approach formalize the concept of ""questions"" and ""answers"" ?
What makes a question ""difficult"" ? How do you quantify ""difficulty"" ?
How do you define the ""cost of information""? What are its units (bits, scalar reward), its semantics ?
Do you you have an MDP or a POMDP ? What kind of MDP do you consider ?
How do you define your discounted MDP ? What is the state and action spaces ?
Some important problem structure under the ""interaction/labeling/reward""
paragraph of section 2 would be worth expressing directly in your definition
of the MDP: labeling actions can only occur during the ""labeling phase"" and that the transition
and reward functions have a specific structure (positive/negative, lead to absorbing state).
The notion of ""phase"" could perhaps be implemented by considering an augmented state space : $\tilde s = (s, phase)$

---

This paper purports to investigate the ability of RL agents to perform ‘physics experiments’ in an environment, to infer physical properties about the objects in that environment. The problem is very well motivated; indeed, inferring the physical properties of objects is a crucial skill for intelligent agents, and there has been relatively little work in this direction, particularly in deep RL. The paper is also well-written.

As there are no architectural or theoretical contributions of the paper (and none are claimed), the main novelty comes in the task application – using a recurrent A3C model for two tasks that simulate an agent interacting with an environment to infer physical properties of objects. More specifically, two tasks are considered – moving blocks to determine their mass, and poking towers such that they fall to determine the number of rigid bodies they are composed of. These of course represent a very limited cross-section of the prerequisite abilities for an agent to understand physics. This in itself is not a bad thing, but since there is no comparison of different (simpler) RL agents on the tasks, it is difficult to determine if the tasks selected are challenging. As mentioned in the pre-review question, the ‘Which is Heavier’ task seems quite easy due to the actuator set-up, and the fact that the model simply must learn to take the difference between successive block positions (which are directly encoded as features in most experiments).  Thus, it is not particularly surprising that the RL agent can solve the proposed tasks. 

The main claim beyond solving two proposed tasks related to physics simulation is that “the agents learn different strategies for these tasks that balance the cost of gathering information against the cost of making mistakes”. The ‘cost of gathering information’ is implemented by multiplying the reward with a value of gamma < 1. This is somewhat interesting behaviour, but is hardly surprising given the problem setup.

One item the authors highlight is that their approach of learning about physical object properties through interaction is different from many previous approaches, which use visual cues. However, the authors also note that this in itself is not novel, and has been explored in other work (e.g. Agrawal et al. (2016)). I think it’s crucial for the authors to discuss these approaches in more detail (potentially along with removing some other, less relevant information from the related work section), and specifically highlight why the proposed tasks in this paper are interesting compared to, for example, learning to move objects towards certain end positions by poking them.

To discern the level of contribution of the paper, one must ask the following questions: 

1)	how much do these two tasks contribute (above previous work) to the goal of having agents learn the properties of objects by interaction; and
2)	how much do the results of the RL agent on these tasks contribute to our understanding of agents that interact with their environment to learn physical properties of objects? 

It is difficult to know exactly, but due to the concerns outlined above, I am not convinced that the answers to (1) or (2) are “to a significant extent”. In particular, for (1), since the proposed agent is able to essentially solve both tasks, it is not clear that the tasks can be used to benchmark more advanced agents (e.g. it can’t be used as a set of bAbI-like tasks). 

Another possible concern, as pointed out by Reviewer 3, is that the description of the model is extremely concise. It would be nice to have, for example, a diagram illustrating the inputs and outputs to the model at each time step, to ease replication.

Overall, it is important to make progress towards agents that can learn to discover physical properties of their environment, and the paper contributes in this direction. However, the technical contributions of this paper are rather limited – thus, it is not clear to what extent the paper pushes forward research in this direction beyond previous work that is mentioned. It would be nice, for example, to have some discussion about the future of agents that learn physics from interaction (speculation on more difficult versions of the tasks in this paper), and how the proposed approach fits into that picture.  

---------------
EDIT: score updated, see comments below

---

This paper presents interesting experimental findings that state-of-the-art deep reinforcement learning methods enable agent learning of latent (physical) properties in its environment. The paper formulates the problem of an agent labeling environmental properties after interacting with the environment based on its actions, and applies the deep reinforcement learning model to evaluate whether such learning is possible. The approach jointly learns the convolutional layers for pixel-based perception and its later layers for learning actions based on reinforcement signals.

We have a mixed opinion about this paper. The paper is written clearly and presents interesting experimental findings. It introduces and formulates a problem potentially important for many robotics applications. Simultaneously, the paper suffers from lacking algorithmic contributions and missing (some of) crucial experiments to confirm its true benefits.

Pros:

+ This paper introduces a new problem of learning latent properties in the agent's environment.

+ The paper presents a framework to appropriately combine existing tools to address the formulated problem.

+ The paper tries reinforcement learning with image inputs and fist-like actuator actions. This will lead to its direct application to robots.

Cons:

- Lacking algorithmic contribution: this paper applies existing tools/methods to solve the problem rather than developing something new or extending them. The approach essentially is training LSTMs with convolutional layers using the previous Asynchronous Advantage Actor Critic.

- In the Towers experiment, the results of probably the most important setting, ""Fist Pixels"", are missing. This setting receiving pixel inputs and using the Fist actuator in a continuous space is the setting closest to real-world robots, and thus is very important to confirm whether the proposed approach will be directly applicable to real-world robots. However, Figure 5 is missing the results with this setting. Is there any reason behind this?

- The paper lacks its comparison to any baseline methods. Without explicit baselines, it is difficult to see what the agent is really learning and what aspect of the proposed approach is benefitting the task. For instance, in the Towers task, how would an agent randomly pushing/hitting the tower (using 'Fist') a number of times and then passively observing its consequence to produce a label perform compared to this approach? That is, how would an approach with a fixed action policy (but with everything else) perform compared to the full deep reinforcement learning version?",,,4.0,,,6.75,,,3.25,,
378,"EXPLORING UNDER-APPRECIATED REWARDS
Authors: Ofir Nachum, Mohammad Norouzi, Dale Schuurmans
Source file: 378.pdf

ABSTRACT
This paper presents a novel form of policy gradient for model-free reinforcement learning (RL) with improved exploration properties. Current policy-based methods use entropy regularization to encourage undirected exploration of the reward landscape, which is ineffective in high dimensional spaces with sparse rewards. We propose a more directed exploration strategy that promotes exploration of under-appreciated reward regions. An action sequence is considered under-appreciated if its log-probability under the current policy under-estimates its resulting reward. The proposed exploration strategy is easy to implement, requiring small modifications to the REINFORCE algorithm. We evaluate the approach on a set of algorithmic tasks that have long challenged RL methods. Our approach reduces hyper-parameter sensitivity and demonstrates significant improvements over baseline methods. The proposed algorithm successfully solves a benchmark multi-digit addition task and generalizes to long sequences, which, to our knowledge, is the first time that a pure RL method has solved addition using only reward feedback.

1 INTRODUCTION
Humans can reason about symbolic objects and solve algorithmic problems. After learning to count and then manipulate numbers via simple arithmetic, people eventually learn to invent new algorithms and even reason about their correctness and efficiency. The ability to invent new algorithms is fundamental to artificial intelligence (AI). Although symbolic reasoning has a long history in AI (Russell et al., 2003), only recently have statistical machine learning and neural network approaches begun to make headway in automated algorithm discovery (Reed & de Freitas, 2016; Kaiser & Sutskever, 2016; Neelakantan et al., 2016), which would constitute an important milestone on the path to AI. Nevertheless, most of the recent successes depend on the use of strong supervision to learn a mapping from a set of training inputs to outputs by maximizing a conditional log-likelihood, very much like neural machine translation systems (Sutskever et al., 2014; Bahdanau et al., 2015). Such a dependence on strong supervision is a significant limitation that does not match the ability of people to invent new algorithmic procedures based solely on trial and error.
By contrast, reinforcement learning (RL) methods (Sutton & Barto, 1998) hold the promise of searching over discrete objects such as symbolic representations of algorithms by considering much weaker feedback in the form of a simple verifier that tests the correctness of a program execution on a given problem instance. Despite the recent excitement around the use of RL to tackle Atari games (Mnih et al., 2015) and Go (Silver et al., 2016), standard RL methods are not yet able to consistently and reliably solve algorithmic tasks in all but the simplest cases (Zaremba & Sutskever, 2014). A key property of algorithmic problems that makes them challenging for RL is reward sparsity, i.e., a policy usually has to get a long action sequence exactly right to obtain a non-zero reward.
∗Work done as a member of the Google Brain Residency program (g.co/brainresidency) †Also at the Department of Computing Science, University of Alberta, daes@ualberta.ca
We believe one of the key factors limiting the effectiveness of current RL methods in a sparse reward setting is the use of undirected exploration strategies (Thrun, 1992), such as -greedy and entropy regularization (Williams & Peng, 1991). For long action sequences with delayed sparse reward, it is hopeless to explore the space uniformly and blindly. Instead, we propose a formulation to encourage exploration of action sequences that are under-appreciated by the current policy. Our formulation considers an action sequence to be under-appreciated if the model’s log-probability assigned to an action sequence under-estimates the resulting reward from the action sequence. Exploring underappreciated states and actions encourages the policy to have a better calibration between its logprobabilities and observed reward values, even for action sequences with negligible rewards. This effectively increases exploration around neglected action sequences.
We term our proposed technique under-appreciated reward exploration (UREX). We show that the objective given by UREX is a combination of a mode seeking objective (standard REINFORCE) and a mean seeking term, which provides a well motivated trade-off between exploitation and exploration. To empirically evaluate our method, we take a set of algorithmic tasks such as sequence reversal, multi-digit addition, and binary search. We choose to focus on these tasks because, although simple, they present a difficult sparse reward setting which has limited the success of standard RL approaches. The experiments demonstrate that UREX significantly outperforms baseline RL methods, such as entropy regularized REINFORCE and one-step Q-learning, especially on the more difficult tasks, such as multi-digit addition. Moreover, UREX is shown to be more robust to changes of hyper-parameters, which makes hyper-parameter tuning less tedious in practice. In addition to introducing a new variant of policy gradient with improved performance, our paper is the first to demonstrate strong results for an RL method on algorithmic tasks. To our knowledge, the addition task has not been solved by any model-free reinforcement learning approach. We observe that some of the policies learned by UREX can successfully generalize to long sequences; e.g., in 2 out of 5 random restarts, the policy learned by UREX for the addition task correctly generalizes to addition of numbers with 2000 digits with no mistakes, even though training sequences are at most 33 digits long.

2 NEURAL NETWORKS FOR LEARNING ALGORITHMS
Although research on using neural networks to learn algorithms has witnessed a surge of recent interest, the problem of program induction from examples has a long history in many fields, including program induction, inductive logic programming (Lavrac & Dzeroski, 1994), relational learning (Kemp et al., 2007) and regular language learning (Angulin, 1987). Rather than presenting a comprehensive survey of program induction here, we focus on neural network approaches to algorithmic tasks and highlight the relative simplicity of our neural network architecture.
Most successful applications of neural networks to algorithmic tasks rely on strong supervision, where the inputs and target outputs are completely known a priori. Given a dataset of examples, one learns the network parameters by maximizing the conditional likelihood of the outputs via backpropagation (e.g., Reed & de Freitas (2016); Kaiser & Sutskever (2016); Vinyals et al. (2015)). However, target outputs may not be available for novel tasks, for which no prior algorithm is known to be available. A more desirable approach to inducing algorithms, followed in this paper, advocates using self-driven learning strategies that only receive reinforcement based on the outputs produced. Hence, just by having access to a verifier for an algorithmic problem, one can aim to learn an algorithm. For example, if one does not know how to sort an array, but can check the extent to which an array is sorted, then one can provide the reward signal necessary for learning sorting algorithms.
We formulate learning algorithms as an RL problem and make use of model-free policy gradient methods to optimize a set parameters associated with the algorithm. In this setting, the goal is to learn a policy πθ that given an observed state st at step t, estimates a distribution over the next action at, denoted πθ(at | st). Actions represent the commands within the algorithm and states represent the joint state of the algorithm and the environment. Previous work in this area has focused on augmenting a neural network with additional structure and increased capabilities (Zaremba & Sutskever, 2015; Graves et al., 2016). In contrast, we utilize a simple architecture based on a standard recurrent neural network (RNN) with LSTM cells (Hochreiter & Schmidhuber, 1997) as depicted in Figure 1. At each episode, the environment is initialized with a latent state h, unknown to the agent, which determines s1 and the subsequent state transition and reward functions. Once the agent observes s1
as the input to the RNN, the network outputs a distribution πθ(a1 | s1), from which an action a1 is sampled. This action is applied to the environment, and the agent receives a new state observation s2. The state s2 and the previous action a1 are then fed into the RNN and the process repeats until the end of the episode. Upon termination, a reward signal is received.

3 LEARNING A POLICY BY MAXIMIZING EXPECTED REWARD
We start by discussing the most common form of policy gradient, REINFORCE (Williams, 1992), and its entropy regularized variant (Williams & Peng, 1991). REINFORCE has been applied to model-free policy-based learning with neural networks and algorithmic domains (Zaremba & Sutskever, 2015; Graves et al., 2016).
The goal is to learn a policy πθ that, given an observed state st at step t, estimates a distribution over the next action at, denoted πθ(at |st). The environment is initialized with a latent vector, h, which determines the initial observed state s1 = g(h), and the transition function st+1 = f(st,at | h). Note that the use of nondeterministic transitions f as in Markov decision processes (MDP) may be recovered by assuming that h includes the random seed for the any nondeterministic functions. Given a latent state h, and s1:T ≡ (s1, . . . , sT ), the model probability of an action sequence a1:T ≡ (a1, . . . ,aT ) is expressed as,
πθ(a1:T | h) = T∏ t=1 πθ(at | st) , where s1 = g(h), st+1 = f(st,at | h) for 1 ≤ t < T .
The environment provides a reward at the end of the episode, denoted r(a1:T | h). For ease of readability we drop the subscript from a1:T and simply write πθ(a | h) and r(a | h). The objective used to optimize the policy parameters, θ, consists of maximizing expected reward under actions drawn from the policy, plus an optional maximum entropy regularizer. Given a distribution over initial latent environment states p(h), we express the regularized expected reward as,
ORL(θ; τ) = Eh∼p(h) {∑
a∈A πθ(a | h)
[ r(a | h)− τ log πθ(a | h) ]} . (1)
When πθ is a non-linear function defined by a neural network, finding the global optimum of θ is challenging, and one often resorts to gradient-based methods to find a local optimum of ORL(θ; τ). Given that ddθπθ(a) = πθ(a) d dθ log πθ(a) for any a such that πθ(a) > 0, one can verify that,
d
dθ ORL(θ; τ | h) = ∑ a∈A πθ(a | h) d dθ log πθ(a | h) [ r(a | h)− τ log πθ(a | h)− τ ] . (2)
Because the space of possible actionsA is large, enumerating over all of the actions to compute this gradient is infeasible. Williams (1992) proposed to compute the stochastic gradient of the expected
reward by using Monte Carlo samples. Using Monte Carlo samples, one first drawsN i.i.d. samples from the latent environment states {h(n)}Nn=1, and then draws K i.i.d. samples {a(k)}Kk=1 from πθ(a | h(n)) to approximate the gradient of (1) by using (2) as,
d
dθ ORL(θ; τ) ≈
1
NK N∑ n=1 K∑ k=1 d dθ log πθ(a (k) | h(n)) [ r(a(k) | h(n))− τ log πθ(a(k) | h(n))− τ ] .
(3) This reparametrization of the gradients is the key to the REINFORCE algorithm. To reduce the variance of (3), one uses rewards r̂ that are shifted by some offset values,
r̂ (a(k) | h) = r(a(k) | h)− b(h) , (4) where b is known as a baseline or sometimes called a critic. Note that subtracting any offset from the rewards in (1) simply results in shifting the objective ORL by a constant. Unfortunately, directly maximizing expected reward (i.e., when τ = 0) is prone to getting trapped in a local optimum. To combat this tendency, Williams & Peng (1991) augmented the expected reward objective by including a maximum entropy regularizer (τ > 0) to promote greater exploration. We will refer to this variant of REINFORCE as MENT (maximum entropy exploration).

4 UNDER-APPRECIATED REWARD EXPLORATION (UREX)
To explain our novel form of policy gradient, we first note that the optimal policy π∗τ , which globally maximizes ORL(θ; τ | h) in (1) for any τ > 0, can be expressed as,
π∗τ (a | h) = 1
Z(h) exp {1 τ r(a | h) } , (5)
where Z(h) is a normalization constant making π∗τ a distribution over the space of action sequences A. One can verify this by first acknowledging that,
ORL(θ; τ | h) = −τ DKL (πθ(· | h) ‖ π∗τ (· | h)) . (6) Since DKL (p ‖ q) is non-negative and zero iff p = q, then π∗τ defined in (5) maximizes ORL. That said, given a particular form of πθ, finding θ that exactly characterizes π∗τ may not be feasible.
The KL divergence DKL (πθ ‖ π∗τ ) is known to be mode seeking (Murphy, 2012, Section 21.2.2) even with entropy regularization (τ > 0). Learning a policy by optimizing this direction of the KL is prone to falling into a local optimum resulting in a sub-optimal policy that omits some of the modes of π∗τ . Although entropy regularization helps mitigate the issues as confirmed in our experiments, it is not an effective exploration strategy as it is undirected and requires a small regularization coefficient τ to avoid too much random exploration. Instead, we propose a directed exploration strategy that improves the mean seeking behavior of policy gradient in a principled way.
We start by considering the alternate mean seeking direction of the KL divergence, DKL (π∗τ ‖ πθ). Norouzi et al. (2016) considered this direction of the KL to directly learn a policy by optimizing
ORAML(θ; τ) = Eh∼p(h) { τ ∑ a∈A π∗τ (a | h) log πθ(a | h) } , (7)
for structured prediction. This objective has the same optimal solution π∗τ as ORL since, ORAML(θ; τ | h) = −τ DKL (π∗τ (· | h) ‖ πθ(· | h)) + const . (8)
Norouzi et al. (2016) argue that in some structured prediction problems when one can draw samples from π∗τ , optimizing (7) is more effective than (1), since no sampling from a non-stationary policy πθ is required. If πθ is a log-linear model of a set of features,ORAML is convex in θ whereasORL is not, even in the log-linear case. Unfortunately, in scenarios that the reward landscape is unknown or computing the normalization constant Z(h) is intractable, sampling from π∗τ is not straightforward.
In RL problems, the reward landscape is completely unknown, hence sampling from π∗τ is intractable. This paper proposes to approximate the expectation with respect to π∗τ by using selfnormalized importance sampling (Owen, 2013), where the proposal distribution is πθ and the reference distribution is π∗τ . For importance sampling, one draws K i.i.d. samples {a(k)}Kk=1 from
πθ(a | h) and computes a set of normalized importance weights to approximate ORAML(θ; τ | h) as,
ORAML(θ; τ | h) ≈ τ K∑ k=1 wτ (a (k) | h)∑K m=1 wτ (a (m) | h) log πθ(a (k) | h) , (9)
where wτ (a(k) | h) ∝ π∗τ/πθ denotes an importance weight defined by,
wτ (a (k) | h) = exp {1 τ r(a(k) | h)− log πθ(a(k) | h) } . (10)
One can view these importance weights as evaluating the discrepancy between scaled rewards r/τ and the policy’s log-probabilities log πθ. Among the K samples, a sample that is least appreciated by the model, i.e., has the largest r/τ − log πθ, receives the largest positive feedback in (9). In practice, we have found that just using the importance sampling RAML objective in (9) does not always yield promising solutions. Particularly, at the beginning of training, when πθ is still far away from π∗τ , the variance of importance weights is too large, and the self-normalized importance sampling procedure results in poor approximations. To stabilize early phases of training and ensure that the model distribution πθ achieves large expected reward scores, we combine the expected reward and RAML objectives to benefit from the best of their mode and mean seeking behaviors. Accordingly, we propose the following objective that we call under-appreciated reward exploration (UREX),
OUREX(θ; τ) = Eh∼p(h) {∑
a∈A
[ πθ(a | h) r(a | h) + τ π∗τ (a | h) log πθ(a | h) ]} , (11)
which is the sum of the expected reward and RAML objectives. In our preliminary experiments, we considered a composite objective of ORL + ORAML, but we found that removing the entropy term is beneficial. Hence, theOUREX objective does not include entropy regularization. Accordingly, the optimum policy for OUREX is no longer π∗τ , as it was for ORL and ORAML. Appendix A derives the optimal policy for OUREX as a function of the optimal policy for ORL. We find that the optimal policy of UREX is more sharply concentrated on the high reward regions of the action space, which may be an advantage for UREX, but we leave more analysis of this behavior to future work.
To compute the gradient ofOUREX(θ; τ), we use the self-normalized importance sampling estimate outlined in (9). We assume that the importance weights are constant and contribute no gradient to d dθOUREX(θ; τ). To approximate the gradient, one draws N i.i.d. samples from the latent environment states {h(n)}Nn=1, and then draws K i.i.d. samples {a(k)}Kk=1 from πθ(a |h (n)) to obtain
d
dθ OUREX(θ; τ) ≈
1
N N∑ n=1 K∑ k=1 d dθ log πθ(a (k) |h(n)) [ 1 K r̂ (a(k) | h(n))+τ wτ (a (k) |h(n))∑K m=1wτ (a (m) |h(n)) ] .
(12) As with REINFORCE, the rewards are shifted by an offset b(h). In this gradient, the model logprobability of a sample action sequence a(k) is reinforced if the corresponding reward is large, or the corresponding importance weights are large, meaning that the action sequence is under-appreciated. The normalized importance weights are computed using a softmax operator softmax(r/τ − log πθ).

5 RELATED WORK
Before presenting the experimental results, we briefly review some pieces of previous work that closely relate to the UREX approach.
Reward-Weighted Regression. Both RAML and UREX objectives bear some similarity to a method in continuous control known as Reward-Weighted Regression (RWR) (Peters & Schaal, 2007; Wierstra et al., 2008). Using our notation, the RWR objective is expressed as,
ORWR(θ; τ | h) = log ∑ a∈A π∗τ (a | h)πθ(a | h) (13)
≥ ∑ a∈A q(a | h) log π ∗ τ (a | h)πθ(a | h) q(a | h) . (14)
To optimize ORWR, Peters & Schaal (2007) propose a technique inspired by the EM algorithm to maximize a variational lower bound in (14) based on a variational distribution q(a | h). The RWR objective can be interpreted as a log of the correlation between π∗τ and πθ. By contrast, the RAML and UREX objectives are both based on a KL divergence between π∗τ and πθ.
To optimize the RWR objective, one formulates the gradient as, d
dθ ORWR(θ; τ | h) = ∑ a∈A π∗τ (a | h)πθ(a | h) C d dθ log πθ(a | h), (15)
where C denotes the normalization factor, i.e., C = ∑
a∈A π ∗ τ (a | h)πθ(a | h). The expectation
with respect to π∗τ (a | h)πθ(a | h)/C on the RHS can be approximated by self-normalized importance sampling,1 where the proposal distribution is πθ. Accordingly, one draws K Monte Carlo samples {a(k)}Kk=1 i.i.d. from πθ(a |h) and formulates the gradient as,
d
dθ ORWR(θ; τ | h) ≈
1
K K∑ k=1 u(a(k) | h)∑K m=1 u(a (m) | h) d dθ log πθ(a (k) | h), (16)
where u(a(k) | h) = exp{ 1τ r(a (k) | h)}. There is some similarity between (16) and (9) in that they both use self-normalized importance sampling, but note the critical difference that (16) and (9) estimate the gradients of two different objectives, and hence the importance weights in (16) do not correct for the sampling distribution πθ(a |h) as opposed to (9). Beyond important technical differences, the optimal policy of ORWR is a one hot distribution with all probability mass concentrated on an action sequence with maximal reward, whereas the optimal policies for RAML and UREX are everywhere nonzero, with the probability of different action sequences being assigned proportionally to their exponentiated reward (with UREX introducing an additional re-scaling; see Appendix A). Further, the notion of under-appreciated reward exploration evident in OUREX, which is key to UREX’s performance, is missing in the RWR formulation. Exploration. The RL literature contains many different attempts at incorporating exploration that may be compared with our method. The most common exploration strategy considered in valuebased RL is -greedy Q-learning, where at each step the agent either takes the best action according to its current value approximation or with probability takes an action sampled uniformly at random. Like entropy regularization, such an approach applies undirected exploration, but it has achieved recent success in game playing environments (Mnih et al., 2013; Van Hasselt et al., 2016; Mnih et al., 2016).
Prominent approaches to improving exploration beyond -greedy in value-based or model-based RL have focused on reducing uncertainty by prioritizing exploration toward states and actions where the agent knows the least. This basic intuition underlies work on counter and recency methods (Thrun, 1992), exploration methods based on uncertainty estimates of values (Kaelbling, 1993; Tokic, 2010), methods that prioritize learning environment dynamics (Kearns & Singh, 2002; Stadie et al., 2015), and methods that provide an intrinsic motivation or curiosity bonus for exploring unknown states (Schmidhuber, 2006; Bellemare et al., 2016).
In contrast to value-based methods, exploration for policy-based RL methods is often a by-product of the optimization algorithm itself. Since algorithms like REINFORCE and Thompson sampling choose actions according to a stochastic policy, sub-optimal actions are chosen with some non-zero probability. The Q-learning algorithm may also be modified to sample an action from the softmax of the Q values rather than the argmax (Sutton & Barto, 1998).
Asynchronous training has also been reported to have an exploration effect on both value- and policy-based methods. Mnih et al. (2016) report that asynchronous training can stabilize training by reducing the bias experienced by a single trainer. By using multiple separate trainers, an agent is less likely to become trapped at a policy found to be locally optimal only due to local conditions. In the same spirit, Osband et al. (2016) use multiple Q value approximators and sample only one to act for each episode as a way to implicitly incorporate exploration.
By relating the concepts of value and policy in RL, the exploration strategy we propose tries to bridge the discrepancy between the two. In particular, UREX can be viewed as a hybrid combination of value-based and policy-based exploration strategies that attempts to capture the benefits of each.
1Bornschein & Bengio (2014) apply the same trick to optimize the log-likelihood of latent variable models.
Per-step Reward. Finally, while we restrict ourselves to episodic settings where a reward is associated with an entire episode of states and actions, much work has been done to take advantage of environments that provide per-step rewards. These include policy-based methods such as actor-critic (Mnih et al., 2016; Schulman et al., 2016) and value-based approaches based on Qlearning (Van Hasselt et al., 2016; Schaul et al., 2016). Some of these value-based methods have proposed a softening of Q-values which can be interpreted as adding a form of maximum-entropy regularizer (Asadi & Littman, 2016; Azar et al., 2012; Fox et al., 2016; Ziebart, 2010). The episodic total-reward setting that we consider is naturally harder since the credit assignment to individual actions within an episode is unclear.

6 SIX ALGORITHMIC TASKS
We assess the effectiveness of the proposed approach on five algorithmic tasks from the OpenAI Gym (Brockman et al., 2016), as well as a new binary search problem. Each task is summarized below, with further details available on the Gym website2 or in the corresponding open-source code.3 In each case, the environment has a hidden tape and a hidden sequence. The agent observes the sequence via a pointer to a single character, which can be moved by a set of pointer control actions. Thus an action at is represented as a tuple (m,w, o) where m denotes how to move, w is a boolean denoting whether to write, and o is the output symbol to write.
1. Copy: The agent should emit a copy of the sequence. The pointer actions are move left and right. 2. DuplicatedInput: In the hidden tape, each character is repeated twice. The agent must dedupli-
cate the sequence and emit every other character. The pointer actions are move left and right.
3. RepeatCopy: The agent should emit the hidden sequence once, then emit the sequence in the reverse order, then emit the original sequence again. The pointer actions are move left and right.
4. Reverse: The agent should emit the hidden sequence in the reverse order. As before, the pointer actions are move left and right.
5. ReversedAddition: The hidden tape is a 2×n grid of digits representing two numbers in base 3 in little-endian order. The agent must emit the sum of the two numbers, in little-endian order. The allowed pointer actions are move left, right, up, or down.
The OpenAI Gym provides an additional harder task called ReversedAddition3, which involves adding three numbers. We omit this task, since none of the methods make much progress on it.
For these tasks, the input sequences encountered during training range from a length of 2 to 33 characters. A reward of 1 is given for each correct emission. On an incorrect emission, a small penalty of−0.5 is incurred and the episode is terminated. The agent is also terminated and penalized with a reward of −1 if the episode exceeds a certain number of steps. For the experiments using UREX and MENT, we associate an episodic sequence of actions with the total reward, defined as the sum of the per-step rewards. The experiments using Q-learning, on the other hand, used the per-step rewards. Each of the Gym tasks has a success threshold, which determines the required average reward over 100 episodes for the agent to be considered successful.
We also conduct experiments on an additional algorithmic task described below: 6. BinarySearch: Given an integer n, the environment has a hidden array of n distinct numbers
stored in ascending order. The environment also has a query number x unknown to the agent that is contained somewhere in the array. The goal of the agent is to find the query number in the array in a small number of actions. The environment has three integer registers initialized at (n, 0, 0). At each step, the agent can interact with the environment via the four following actions: • INC(i): increment the value of the register i for i ∈ {1, 2, 3}. • DIV(i): divide the value of the register i by 2 for i ∈ {1, 2, 3}. • AVG(i): replace the value of the register i with the average of the two other registers. • CMP(i): compare the value of the register i with x and receive a signal indicating which
value is greater. The agent succeeds when it calls CMP on an array cell holding the value x.
2gym.openai.com 3github.com/openai/gym
We set the maximum number of steps to 2n+1 to allow the agent to perform a full linear search. A policy performing full linear search achieves an average reward of 5, because x is chosen uniformly at random from the elements of the array. A policy employing binary search can find the number x in at most 2 log2 n + 1 steps. If n is selected uniformly at random from the range 32 ≤ n ≤ 512, binary search yields an optimal average reward above 9.55. We set the success threshold for this task to an average reward of 9.

7 EXPERIMENTS
We compare our policy gradient method using under-appreciated reward exploration (UREX) against two main RL baselines: (1) REINFORCE with entropy regularization termed MENT (Williams & Peng, 1991), where the value of τ determines the degree of regularization. When τ = 0, standard REINFORCE is obtained. (2) one-step double Q-learning based on bootstrapping one step future rewards.

7.1 ROBUSTNESS TO HYPER-PARAMETERS
Hyper-parameter tuning is often tedious for RL algorithms. We found that the proposed UREX method significantly improves robustness to changes in hyper-parameters when compared to MENT. For our experiments, we perform a careful grid search over a set of hyper-parameters for both MENT and UREX. For any hyper-parameter setting, we run the MENT and UREX methods 5 times with different random restarts. We explore the following main hyper-parameters:
• The learning rate denoted η chosen from a set of 3 possible values η ∈ {0.1, 0.01, 0.001}. • The maximum L2 norm of the gradients, beyond which the gradients are clipped. This parame-
ter, denoted c, matters for training RNNs. The value of c is selected from c ∈ {1, 10, 40, 100}. • The temperature parameter τ that controls the degree of exploration for both MENT and UREX.
For MENT, we use τ ∈ {0, 0.005, 0.01, 0.1}. For UREX, we only consider τ = 0.1, which consistently performs well across the tasks.
In all of the experiments, both MENT and UREX are treated exactly the same. In fact, the change of implementation is just a few lines of code. Given a value of τ , for each task, we run 60 training jobs comprising 3 learning rates, 4 clipping values, and 5 random restarts. We run each algorithm for a maximum number of steps determined based on the difficulty of the task. The training jobs for Copy, DuplicatedInput, RepeatCopy, Reverse, ReversedAddition, and BinarySearch are run for 2K, 500, 50K, 5K, 50K, and 2K stochastic gradient steps, respectively. We find that running a trainer job longer does not result in a better performance. Our policy network comprises a single LSTM layer with 128 nodes. We use the Adam optimizer (Kingma & Ba, 2015) for the experiments.
Table 1 shows the percentage of 60 trials on different hyper-parameters (η, c) and random restarts which successfully solve each of the algorithmic tasks. It is clear that UREX is more robust than
MENT to changes in hyper-parameters, even though we only report the results of UREX for a single temperature. See Appendix B for more detailed tables on hyper-parameter robustness.

7.2 RESULTS
Table 2 presents the number of successful attempts (out of 5 random restarts) and the expected reward values (averaged over 5 trials) for each RL algorithm given the best hyper-parameters. Onestep Q-learning results are also included in the table. We also present the training curves for MENT and UREX in Figure 2. It is clear that UREX outperforms the baselines on these tasks. On the more difficult tasks, such as Reverse and ReverseAddition, UREX is able to consistently find an appropriate algorithm, but MENT and Q-learning fall behind. Importantly, for the BinarySearch task, which exhibits many local maxima and necessitates smart exploration, UREX is the only method that can solve it consistently. The Q-learning baseline solves some of the simple tasks, but it makes little headway on the harder tasks. We believe that entropy regularization for policy gradient and - greedy for Q-learning are relatively weak exploration strategies in long episodic tasks with delayed rewards. On such tasks, one random exploratory step in the wrong direction can take the agent off the optimal policy, hampering its ability to learn. In contrast, UREX provides a form of adaptive and smart exploration. In fact, we observe that the variance of the importance weights decreases as the agent approaches the optimal policy, effectively reducing exploration when it is no longer necessary; see Appendix E.

7.3 GENERALIZATION TO LONGER SEQUENCES
To confirm whether our method is able to find the correct algorithm for multi-digit addition, we investigate its generalization to longer input sequences than provided during training. We evaluate the trained models on inputs up to a length of 2000 digits, even though training sequences were at most 33 characters. For each length, we test the model on 100 randomly generated inputs, stopping when the accuracy falls below 100%. Out of the 60 models trained on addition with UREX, we find that 5 models generalize to numbers up to 2000 digits without any observed mistakes. On the best UREX hyper-parameters, 2 out of the 5 random restarts are able to generalize successfully. For more detailed results on the generalization performance on 3 different tasks including Copy,
DuplicatedInput, and ReversedAddition, see Appendix C. During these evaluations, we take the action with largest probability from πθ(a | h) at each time step rather than sampling randomly. We also looked into the generalization of the models trained on the BinarySearch task. We found that none of the agents perform proper binary search. Rather, those that solved the task perform a hybrid of binary and linear search: first actions follow a binary search pattern, but then the agent switches to a linear search procedure once it narrows down the search space; see Appendix D for some execution traces for BinarySearch and ReversedAddition. Thus, on longer input sequences, the agent’s running time complexity approaches linear rather than logarithmic. We hope that future work will make more progress on this task. This task is especially interesting because the reward signal should incorporate both correctness and efficiency of the algorithm.

7.4 IMPLEMENTATION DETAILS
In all of the experiments, we make use of curriculum learning. The environment begins by only providing small inputs and moves on to longer sequences once the agent achieves close to maximal reward over a number of steps. For policy gradient methods including MENT and UREX, we only provide the agent with a reward at the end of the episode, and there is no notion of intermediate reward. For the value-based baseline, we implement one-step Q-learning as described in Mnih et al. (2016)-Alg. 1, employing double Q-learning with -greedy exploration. We use the same RNN in our policy-based approaches to estimate the Q values. A grid search over exploration rate, exploration rate decay, learning rate, and sync frequency (between online and target network) is conducted to find the best hyper-parameters. Unlike our other methods, the Q-learning baseline uses intermediate rewards, as given by the OpenAI Gym on a per-step basis. Hence, the Q-learning baseline has a slight advantage over the policy gradient methods.
In all of the tasks except Copy, our stochastic optimizer uses mini-batches comprising 400 policy samples from the model. These 400 samples correspond to 40 different random sequences drawn from the environment, and 10 random policy trajectories per sequence. In other words, we set K = 10 and N = 40 as defined in (3) and (12). For MENT, we use the 10 samples to subtract the mean of the coefficient of ddθ log πθ(a | h) which includes the contribution of the reward and entropy regularization. For UREX, we use the 10 trajectories to subtract the mean reward and normalize the importance sampling weights. We do not subtract the mean of the normalized importance weights. For the Copy task, we use mini-batches with 200 samples using K = 10 and N = 20. Experiments are conducted using Tensorflow (Abadi et al., 2016).

8 CONCLUSION
We present a variant of policy gradient, called UREX, which promotes the exploration of action sequences that yield rewards larger than what the model expects. This exploration strategy is the result of importance sampling from the optimal policy. Our experimental results demonstrate that UREX significantly outperforms other value and policy based methods, while being more robust
to changes of hyper-parameters. By using UREX, we can solve algorithmic tasks like multi-digit addition from only episodic reward, which other methods cannot reliably solve even given the best hyper-parameters. We introduce a new algorithmic task based on binary search to advocate more research in this area, especially when the computational complexity of the solution is also of interest. Solving these tasks is not only important for developing more human-like intelligence in learning algorithms, but also important for generic reinforcement learning, where smart and efficient exploration is the key to successful methods.

9 ACKNOWLEDGMENT
We thank Sergey Levine, Irwan Bello, Corey Lynch, George Tucker, Kelvin Xu, Volodymyr Mnih, and the Google Brain team for insightful comments and discussions.

A OPTIMAL POLICY FOR THE UREX OBJECTIVE
To derive the form of the optimal policy for the UREX objective (11), note that for each h one would like to maximize ∑
a∈A
[ πθ(a) r(a) + τ π ∗ τ (a) log πθ(a) ] , (17)
subject to the constraint ∑
a∈A πθ(a) = 1. To enforce the constraint, we introduce a Lagrange multiplier α and aim to maximize∑
a∈A
[ πθ(a) r(a) + τ π ∗ τ (a) log πθ(a)− απθ(a) ] + α . (18)
Since the gradient of the Lagrangian (18) with respect to θ is given by∑ a∈A dπθ(a) dθ [ r(a) + τ π∗τ (a) πθ(a) − α ] , (19)
the optimal choice for πθ is achieved by setting
πθ(a) = τ π∗τ (a)
α− r(a) for all a ∈ A , (20)
forcing the gradient to be zero. The Lagrange multiplier α can then be chosen so that ∑
a∈A πθ(a) = 1 while also satisfying α > maxa∈A r(a); see e.g. (Golub, 1987).

B ROBUSTNESS TO HYPER-PARAMETERS
Tables 3–8 provide more details on different cells of Table 1. Each table presents the results of MENT using the best temperature τ vs. UREX with τ = 0.1 on a variety of learning rates and clipping values. Each cell is the number of trials out of 5 random restarts that succeed at solving the task using a specific η and c.

C GENERALIZATION TO LONGER SEQUENCES
Table 9 provides a more detailed look into the generalization performance of the trained models on Copy, DuplicatedInput, and ReversedAddition. The tables show how the number of models which can solve the task correctly drops off as the length of the input increases.

D EXAMPLE EXECUTION TRACES
We provide the traces of two trained agents on the ReversedAddition task (Figure 3) and the BinarySearch task (Table 10).
E VARIANCE OF IMPORTANCE WEIGHTS

F A SIMPLE BANDIT TASK
","This paper presents a novel form of policy gradient for model-free reinforcement learning (RL) with improved exploration properties. Current policy-based methods use entropy regularization to encourage undirected exploration of the reward landscape, which is ineffective in high dimensional spaces with sparse rewards. We propose a more directed exploration strategy that promotes exploration of under-appreciated reward regions. An action sequence is considered under-appreciated if its log-probability under the current policy under-estimates its resulting reward. The proposed exploration strategy is easy to implement, requiring small modifications to the REINFORCE algorithm. We evaluate the approach on a set of algorithmic tasks that have long challenged RL methods. Our approach reduces hyper-parameter sensitivity and demonstrates significant improvements over baseline methods. The proposed algorithm successfully solves a benchmark multi-digit addition task and generalizes to long sequences, which, to our knowledge, is the first time that a pure RL method has solved addition using only reward feedback.",ICLR 2017 conference submission,True,,"It has recently come to my attention that the objective proposed by the authors in this paper has in fact already been studied in the literature under the name 'reward-weighted regression', from e.g. ICML 2007 [1]. This has spawned several other works using the same objective (e.g. [2]). One can examine for instance the objective proposed in Section 3.4 from [2], from ICANN 2008.

While this paper has already been accepted to ICLR, it would be beneficial for the authors to at least cite these works (and other related works) so that readers are aware of the previous origins of this idea.

[1]

---

This paper proposes a nice algorithm for improved exploration in policy search RL settings. The method essentially optimizes a weighted combination of expected reward (essentially the REINFORCE objective without an entropy regularizer) plus a term from the reward augmented maximum likelihood objective (from a recent NIPS paper), and show that the resulting update can be made with a fairly small modification to the REINFORCE algorithm. The authors show improved performance on several sequential ""program-like"" domains like copying a string, adding, etc.
 
 I'm recommending this paper for acceptance, as I think the contribution here is a good one, and the basic approach very nicely offers a better exploration policy than the typical Boltzmann policy, using a fairly trivial modification to REINFORCE. But after re-reading I'm less enthusiastic, simply because the delta over previous work (namely the RAML paper) doesn't seem incredibly substantial. None of the domains in the experiments seem substantially challenges, and the fact that it can improve over REINFORCE isn't necessarily amazing.
 
 Pros:
 + Well-motivated (and simple) modification to REINFORCE to get better exploration
 + Demonstrably better performance with seemingly less hyperparameter tunies
 
 Cons:
 - Delta over RAML work isn't that clear, essentially is just a weighted combination between REINFORCE and RAML (though in RL context)
 - Experiments are good, but not outstanding relative to simple baselines

---

We thank all the reviewers for their thoughtful comments.  We have made several updates to the paper in response:

-- We have made adjustments to the introduction to make it clear that the motivation of the paper is twofold: (1) to introduce an RL algorithm that improves the performance and the exploration behavior of REINFORCE (2) to improve the behavior of RL methods on algorithmic tasks.

-- In Section 6, we have made it clear that we only use total-episode reward for UREX and MENT.

-- For more results, we have revised the paper to include an appendix with a simple bandit-like task with a large action space.

---

This paper proposes a novel exploration strategy that promotes exploration of under-appreciated reward regions. Proposed importance sampling based approach is a simple modification to REINFORCE and experiments in several algorithmic toy tasks show that the proposed model is performing better than REINFORCE and Q-learning.

This paper shows promising results in automated algorithm discovery using reinforcement learning. However it is not very clear what is the main motivation of the paper. Is the main motivation better exploration for policy gradient methods? If so, authors should have benchmarked their algorithm with standard reinforcement learning tasks. While there is a huge body of literature on improving REINFORCE, authors have considered a simple version of REINFORCE on a non-standard task and say that UREX is better. If the main motivation is improving the performance in algorithm learning tasks, then the baselines are still weak. Authors should make it clear which is the main motivation.

Also the action space is too small. In the beginning authors raise the concern that entropy regularization might not scale to larger action spaces. So a comparison of MENT and UREX in a large action space problem would give more insights on whether UREX is not affected by large action space.

--------------------------
After rebuttal:
I missed the action sequences argument when I pointed about small action space issue.

For question regarding weak baseline, there are several tricks used in the literature to tackle high-variance issue for REINFORCE. For example, see Mnih & Gregor, 2014.

I have increased my rating from 6 to 7. I still encourage the authors to improve their baseline.

---

overview:
This work proposes to link trajectory log-probabilities and rewards by defining under-appreciated rewards. This suggests that there is a linear relationship between trajectory rewards and their log-probability which can be exploited by measuring the resulting mismatch.
That is, when an action sequence under-appreciates its reward, its log-probability is increased.
This method is a simple modification to the well-known REINFORCE method, requiring only one extra hyperparameter \tau, and intuitively provides us with a better exploration mechanism than \epsilon-greedy or random exploration.
The method is tested on algorithmic environments, and compared to entropy-regularized REINFORCE and double Q-learning, and performs equally or better than those two baselines, especially in more complex environments.


remarks:
- the focus in the introduction on algorithmic tasks may be a double-edged sword. It is an interesting domain to test your hypothesis and benchmark your method. At the same time, it distracts the reader from the (IMO) generality of the proposed method.
- in the introduction you say the reward is sparse, in section 6, on tasks 1-5, you then say there is a reward at each correct emission, i.e. each time step. This is only 'corrected' to end-of-episode-reward in section 7.4, after having discussed results. I'd move or mention this in section 6.
- approach seems quite sensible to tau being in the same range as logpi(a|h), but you only try tau=0.1 for UREX. I'm not sure I understand nor agree with this experimentation choice.
- an alternative to grid search is random search (Bergstra&Bengio, 2012). It may illustrate better hyperparameter robustness, and allow you to explore more in the same number of experiment.

opinion:
- An interesting approach to policy-gradient, to be sure. It tackles the very important question of ""how should agents explore?""
- I'm ambivalent to claiming that an algorithm is robust to hyperparmeters simply because it performs better on the selected hyperparameter range. All you really show is that it performs well some amount of time when the hyperparams lay in that range. Couldn't it be that MENT needs different hyperparameters? (Just being devil's advocate here)
- I see why matching 1/tau with logpi is the obvious choice, but it implies a very strong prior: that the reward (to a factor of 1/tau) lies in the same space as the log policy. One point of failure I see (but correct me if I'm wrong) is that as the length of the trajectory grows the reward is expected to grow linearly, so short ways to get some reward will be less explored than long ways of getting the same reward, creating an imbalance unless the reward is shaped such that shorter trajectories get more reward (which is only the case in task 6).
- It might have been good to also compare with methods explicitly trying to explore better with value-functions (e.g. prioritized experience replay, Schaul et al 2015)
- At the risk of repeating myself, tau plays a major role in this method, but there is little analysis on its effect on experiments.


The methodology and reasoning is clearly explained and I think this paper communicates its message very well.
That message is novel, albeit a minor modification to a well-known algorithm, it is well motivated and, I think, a welcome addition to literature concerning exploration in RL.
The experiments are chosen accordingly, and results seem to reflect the hypothesis of the authors.

I realize the tyranny of extensive experimentation and the scarcity of time, but I do think that this paper would benefit from more (or cleverer) experimentation, as well as demonstrating more explicitly the impact of the method on exploration.
Reading this paper convinced me that measuring mismatch between a trajectory's observed reward and its probability given the current policy is a clever (and well motivated) thing to do. Yet, I think that the paper could have a more convincing empirical argument, even if it is for toy tasks.

---

The paper proposes a new algorithm based on REINFORCE which aims at exploring under-appreciate action sequences. The idea is to compare the probability of a sequence of actions under the current policy with the estimated reward. Actions where the current policy under-estimate the reward will provide a higher feedback, thus encouraging exploration of particular sequences of actions. The UREX model is tested on 6 algortihmic RL problems and show interesting properties in comparison to the standard regularized REINFORCE (MENT) model and to Q-Learning.

The model is interesting, well defined and well explained. As far as I know, the UREX model is an original model which will certainly be useful for the RL community. The only drawback of the paper is to restrict the evaluation of this algortihm to algorithmic problems that are specific while it would be easy to test the proposed model onto other standard RL problems. This would clearly help to make the article stronger and I greatly encourage the authors to add some other tasks in their paper.

---

Hi,

I feel the weight ""w_\tau(a^k|h)"" proposed in Equation (10) is quite crucial to model the *under appreciation*. Equation (10) is linked to the optimal policy distribution proposed by Equation (5). The weight should be proportional  to π*/π_\theta, and π* is proposed in Equation (5) based on the rewards given optimal policy. But apparently, in Equation (10), the reward term ""r"" used is from the underlying policy, which is not optimal. Otherwise, it's not quite possible to compute that for gradient update. 

Hope you could clarify on this. Thank you!

---

It has recently come to my attention that the objective proposed by the authors in this paper has in fact already been studied in the literature under the name 'reward-weighted regression', from e.g. ICML 2007 [1]. This has spawned several other works using the same objective (e.g. [2]). One can examine for instance the objective proposed in Section 3.4 from [2], from ICANN 2008.

While this paper has already been accepted to ICLR, it would be beneficial for the authors to at least cite these works (and other related works) so that readers are aware of the previous origins of this idea.

[1]

---

This paper proposes a nice algorithm for improved exploration in policy search RL settings. The method essentially optimizes a weighted combination of expected reward (essentially the REINFORCE objective without an entropy regularizer) plus a term from the reward augmented maximum likelihood objective (from a recent NIPS paper), and show that the resulting update can be made with a fairly small modification to the REINFORCE algorithm. The authors show improved performance on several sequential ""program-like"" domains like copying a string, adding, etc.
 
 I'm recommending this paper for acceptance, as I think the contribution here is a good one, and the basic approach very nicely offers a better exploration policy than the typical Boltzmann policy, using a fairly trivial modification to REINFORCE. But after re-reading I'm less enthusiastic, simply because the delta over previous work (namely the RAML paper) doesn't seem incredibly substantial. None of the domains in the experiments seem substantially challenges, and the fact that it can improve over REINFORCE isn't necessarily amazing.
 
 Pros:
 + Well-motivated (and simple) modification to REINFORCE to get better exploration
 + Demonstrably better performance with seemingly less hyperparameter tunies
 
 Cons:
 - Delta over RAML work isn't that clear, essentially is just a weighted combination between REINFORCE and RAML (though in RL context)
 - Experiments are good, but not outstanding relative to simple baselines

---

We thank all the reviewers for their thoughtful comments.  We have made several updates to the paper in response:

-- We have made adjustments to the introduction to make it clear that the motivation of the paper is twofold: (1) to introduce an RL algorithm that improves the performance and the exploration behavior of REINFORCE (2) to improve the behavior of RL methods on algorithmic tasks.

-- In Section 6, we have made it clear that we only use total-episode reward for UREX and MENT.

-- For more results, we have revised the paper to include an appendix with a simple bandit-like task with a large action space.

---

This paper proposes a novel exploration strategy that promotes exploration of under-appreciated reward regions. Proposed importance sampling based approach is a simple modification to REINFORCE and experiments in several algorithmic toy tasks show that the proposed model is performing better than REINFORCE and Q-learning.

This paper shows promising results in automated algorithm discovery using reinforcement learning. However it is not very clear what is the main motivation of the paper. Is the main motivation better exploration for policy gradient methods? If so, authors should have benchmarked their algorithm with standard reinforcement learning tasks. While there is a huge body of literature on improving REINFORCE, authors have considered a simple version of REINFORCE on a non-standard task and say that UREX is better. If the main motivation is improving the performance in algorithm learning tasks, then the baselines are still weak. Authors should make it clear which is the main motivation.

Also the action space is too small. In the beginning authors raise the concern that entropy regularization might not scale to larger action spaces. So a comparison of MENT and UREX in a large action space problem would give more insights on whether UREX is not affected by large action space.

--------------------------
After rebuttal:
I missed the action sequences argument when I pointed about small action space issue.

For question regarding weak baseline, there are several tricks used in the literature to tackle high-variance issue for REINFORCE. For example, see Mnih & Gregor, 2014.

I have increased my rating from 6 to 7. I still encourage the authors to improve their baseline.

---

overview:
This work proposes to link trajectory log-probabilities and rewards by defining under-appreciated rewards. This suggests that there is a linear relationship between trajectory rewards and their log-probability which can be exploited by measuring the resulting mismatch.
That is, when an action sequence under-appreciates its reward, its log-probability is increased.
This method is a simple modification to the well-known REINFORCE method, requiring only one extra hyperparameter \tau, and intuitively provides us with a better exploration mechanism than \epsilon-greedy or random exploration.
The method is tested on algorithmic environments, and compared to entropy-regularized REINFORCE and double Q-learning, and performs equally or better than those two baselines, especially in more complex environments.


remarks:
- the focus in the introduction on algorithmic tasks may be a double-edged sword. It is an interesting domain to test your hypothesis and benchmark your method. At the same time, it distracts the reader from the (IMO) generality of the proposed method.
- in the introduction you say the reward is sparse, in section 6, on tasks 1-5, you then say there is a reward at each correct emission, i.e. each time step. This is only 'corrected' to end-of-episode-reward in section 7.4, after having discussed results. I'd move or mention this in section 6.
- approach seems quite sensible to tau being in the same range as logpi(a|h), but you only try tau=0.1 for UREX. I'm not sure I understand nor agree with this experimentation choice.
- an alternative to grid search is random search (Bergstra&Bengio, 2012). It may illustrate better hyperparameter robustness, and allow you to explore more in the same number of experiment.

opinion:
- An interesting approach to policy-gradient, to be sure. It tackles the very important question of ""how should agents explore?""
- I'm ambivalent to claiming that an algorithm is robust to hyperparmeters simply because it performs better on the selected hyperparameter range. All you really show is that it performs well some amount of time when the hyperparams lay in that range. Couldn't it be that MENT needs different hyperparameters? (Just being devil's advocate here)
- I see why matching 1/tau with logpi is the obvious choice, but it implies a very strong prior: that the reward (to a factor of 1/tau) lies in the same space as the log policy. One point of failure I see (but correct me if I'm wrong) is that as the length of the trajectory grows the reward is expected to grow linearly, so short ways to get some reward will be less explored than long ways of getting the same reward, creating an imbalance unless the reward is shaped such that shorter trajectories get more reward (which is only the case in task 6).
- It might have been good to also compare with methods explicitly trying to explore better with value-functions (e.g. prioritized experience replay, Schaul et al 2015)
- At the risk of repeating myself, tau plays a major role in this method, but there is little analysis on its effect on experiments.


The methodology and reasoning is clearly explained and I think this paper communicates its message very well.
That message is novel, albeit a minor modification to a well-known algorithm, it is well motivated and, I think, a welcome addition to literature concerning exploration in RL.
The experiments are chosen accordingly, and results seem to reflect the hypothesis of the authors.

I realize the tyranny of extensive experimentation and the scarcity of time, but I do think that this paper would benefit from more (or cleverer) experimentation, as well as demonstrating more explicitly the impact of the method on exploration.
Reading this paper convinced me that measuring mismatch between a trajectory's observed reward and its probability given the current policy is a clever (and well motivated) thing to do. Yet, I think that the paper could have a more convincing empirical argument, even if it is for toy tasks.

---

The paper proposes a new algorithm based on REINFORCE which aims at exploring under-appreciate action sequences. The idea is to compare the probability of a sequence of actions under the current policy with the estimated reward. Actions where the current policy under-estimate the reward will provide a higher feedback, thus encouraging exploration of particular sequences of actions. The UREX model is tested on 6 algortihmic RL problems and show interesting properties in comparison to the standard regularized REINFORCE (MENT) model and to Q-Learning.

The model is interesting, well defined and well explained. As far as I know, the UREX model is an original model which will certainly be useful for the RL community. The only drawback of the paper is to restrict the evaluation of this algortihm to algorithmic problems that are specific while it would be easy to test the proposed model onto other standard RL problems. This would clearly help to make the article stronger and I greatly encourage the authors to add some other tasks in their paper.

---

Hi,

I feel the weight ""w_\tau(a^k|h)"" proposed in Equation (10) is quite crucial to model the *under appreciation*. Equation (10) is linked to the optimal policy distribution proposed by Equation (5). The weight should be proportional  to π*/π_\theta, and π* is proposed in Equation (5) based on the rewards given optimal policy. But apparently, in Equation (10), the reward term ""r"" used is from the underlying policy, which is not optimal. Otherwise, it's not quite possible to compute that for gradient update. 

Hope you could clarify on this. Thank you!",,4.0,3.0,,,7.333333333333333,,,4.0,,2.3333333333333335
383,"DESIGNING NEURAL NETWORK ARCHITECTURES
Authors: REINFORCEMENT LEARNING, Bowen Baker, Otkrist Gupta, Nikhil Naik, Ramesh Raskar
Source file: 383.pdf

ABSTRACT
At present, designing convolutional neural network (CNN) architectures requires both human expertise and labor. New architectures are handcrafted by careful experimentation or modified from a handful of existing networks. We introduce MetaQNN, a meta-modeling algorithm based on reinforcement learning to automatically generate high-performing CNN architectures for a given learning task. The learning agent is trained to sequentially choose CNN layers using Qlearning with an -greedy exploration strategy and experience replay. The agent explores a large but finite space of possible architectures and iteratively discovers designs with improved performance on the learning task. On image classification benchmarks, the agent-designed networks (consisting of only standard convolution, pooling, and fully-connected layers) beat existing networks designed with the same layer types and are competitive against the state-of-the-art methods that use more complex layer types. We also outperform existing meta-modeling approaches for network design on image classification tasks.

1 INTRODUCTION
Deep convolutional neural networks (CNNs) have seen great success in the past few years on a variety of machine learning problems (LeCun et al., 2015). A typical CNN architecture consists of several convolution, pooling, and fully connected layers. While constructing a CNN, a network designer has to make numerous design choices: the number of layers of each type, the ordering of layers, and the hyperparameters for each type of layer, e.g., the receptive field size, stride, and number of receptive fields for a convolution layer. The number of possible choices makes the design space of CNN architectures extremely large and hence, infeasible for an exhaustive manual search. While there has been some work (Pinto et al., 2009; Bergstra et al., 2013; Domhan et al., 2015) on automated or computer-aided neural network design, new CNN architectures or network design elements are still primarily developed by researchers using new theoretical insights or intuition gained from experimentation.
In this paper, we seek to automate the process of CNN architecture selection through a metamodeling procedure based on reinforcement learning. We construct a novel Q-learning agent whose goal is to discover CNN architectures that perform well on a given machine learning task with no human intervention. The learning agent is given the task of sequentially picking layers of a CNN model. By discretizing and limiting the layer parameters to choose from, the agent is left with a finite but large space of model architectures to search from. The agent learns through random exploration and slowly begins to exploit its findings to select higher performing models using the - greedy strategy (Mnih et al., 2015). The agent receives the validation accuracy on the given machine learning task as the reward for selecting an architecture. We expedite the learning process through repeated memory sampling using experience replay (Lin, 1993). We refer to this Q-learning based meta-modeling method as MetaQNN, which is summarized in Figure 1.1
We conduct experiments with a space of model architectures consisting of only standard convolution, pooling, and fully connected layers using three standard image classification datasets: CIFAR-10,
1For more information, model files, and code, please visit https://bowenbaker.github.io/metaqnn/
SVHN, and MNIST. The learning agent discovers CNN architectures that beat all existing networks designed only with the same layer types (e.g., Springenberg et al. (2014); Srivastava et al. (2015)). In addition, their performance is competitive against network designs that include complex layer types and training procedures (e.g., Clevert et al. (2015); Lee et al. (2016)). Finally, the MetaQNN selected models comfortably outperform previous automated network design methods (Stanley & Miikkulainen, 2002; Bergstra et al., 2013). The top network designs discovered by the agent on one dataset are also competitive when trained on other datasets, indicating that they are suited for transfer learning tasks. Moreover, we can generate not just one, but several varied, well-performing network designs, which can be ensembled to further boost the prediction performance.

2 RELATED WORK
Designing neural network architectures: Research on automating neural network design goes back to the 1980s when genetic algorithm-based approaches were proposed to find both architectures and weights (Schaffer et al., 1992). However, to the best of our knowledge, networks designed with genetic algorithms, such as those generated with the NEAT algorithm (Stanley & Miikkulainen, 2002), have been unable to match the performance of hand-crafted networks on standard benchmarks (Verbancsics & Harguess, 2013). Other biologically inspired ideas have also been explored; motivated by screening methods in genetics, Pinto et al. (2009) proposed a high-throughput network selection approach where they randomly sample thousands of architectures and choose promising ones for further training. In recent work, Saxena & Verbeek (2016) propose to sidestep the architecture selection process through densely connected networks of layers, which come closer to the performance of hand-crafted networks.
Bayesian optimization has also been used (Shahriari et al., 2016) for automatic selection of network architectures (Bergstra et al., 2013; Domhan et al., 2015) and hyperparameters (Snoek et al., 2012; Swersky et al., 2013). Notably, Bergstra et al. (2013) proposed a meta-modeling approach based on Tree of Parzen Estimators (TPE) (Bergstra et al., 2011) to choose both the type of layers and hyperparameters of feed-forward networks; however, they fail to match the performance of handcrafted networks.
Reinforcement Learning: Recently there has been much work at the intersection of reinforcement learning and deep learning. For instance, methods using CNNs to approximate theQ-learning utility function (Watkins, 1989) have been successful in game-playing agents (Mnih et al., 2015; Silver et al., 2016) and robotic control (Lillicrap et al., 2015; Levine et al., 2016). These methods rely on phases of exploration, where the agent tries to learn about its environment through sampling, and exploitation, where the agent uses what it learned about the environment to find better paths. In traditional reinforcement learning settings, over-exploration can lead to slow convergence times, yet over-exploitation can lead to convergence to local minima (Kaelbling et al., 1996). However, in the case of large or continuous state spaces, the -greedy strategy of learning has been empirically shown to converge (Vermorel & Mohri, 2005). Finally, when the state space is large or exploration is costly,
the experience replay technique (Lin, 1993) has proved useful in experimental settings (Adam et al., 2012; Mnih et al., 2015). We incorporate these techniques—Q-learning, the -greedy strategy and experience replay—in our algorithm design.

3 BACKGROUND
Our method relies on Q-learning, a type of reinforcement learning. We now summarize the theoretical formulation of Q-learning, as adopted to our problem. Consider the task of teaching an agent to find optimal paths as a Markov Decision Process (MDP) in a finite-horizon environment. Constraining the environment to be finite-horizon ensures that the agent will deterministically terminate in a finite number of time steps. In addition, we restrict the environment to have a discrete and finite state space S as well as action space U . For any state si ∈ S , there is a finite set of actions, U(si) ⊆ U , that the agent can choose from. In an environment with stochastic transitions, an agent in state si taking some action u ∈ U(si) will transition to state sj with probability ps′|s,u(sj |si, u), which may be unknown to the agent. At each time step t, the agent is given a reward rt, dependent on the transition from state s to s′ and action u. rt may also be stochastic according to a distribution pr|s′,s,u. The agent’s goal is to maximize the total expected reward over all possible trajectories, i.e., maxTi∈T RTi , where the total expected reward for a trajectory Ti is
RTi = ∑ (s,u,s′)∈Ti Er|s,u,s′ [r|s, u, s ′]. (1)
Though we limit the agent to a finite state and action space, there are still a combinatorially large number of trajectories, which motivates the use of reinforcement learning. We define the maximization problem recursively in terms of subproblems as follows. For any state si ∈ S and subsequent action u ∈ U(si), we define the maximum total expected reward to be Q∗(si, u). Q∗(·) is known as the action-value function and individual Q∗(si, u) are know as Q-values. The recursive maximization equation, which is known as Bellman’s Equation, can be written as
Q∗(si, u) = Esj |si,u [ Er|si,u,sj [r|si, u, sj ] + γmaxu′∈U(sj)Q∗(sj , u′) ] . (2)
In many cases, it is impossible to analytically solve Bellman’s Equation (Bertsekas, 2015), but it can be formulated as an iterative update
Qt+1(si, u) = (1− α)Qt(si, u) + α [ rt + γmaxu′∈U(sj)Qt(sj , u ′) ] . (3)
Equation 3 is the simplest form of Q-learning proposed by Watkins (1989). For well formulated problems, limt→∞Qt(s, u) = Q∗(s, u), as long as each transition is sampled infinitely many times (Bertsekas, 2015). The update equation has two parameters: (i) α is a Q-learning rate which determines the weight given to new information over old information, and (ii) γ is the discount factor which determines the weight given to short-term rewards over future rewards. The Q-learning algorithm is model-free, in that the learning agent can solve the task without ever explicitly constructing an estimate of environmental dynamics. In addition, Q-learning is off policy, meaning it can learn about optimal policies while exploring via a non-optimal behavioral distribution, i.e. the distribution by which the agent explores its environment.
We choose the behavior distribution using an -greedy strategy (Mnih et al., 2015). With this strategy, a random action is taken with probability and the greedy action, maxu∈U(si)Qt(si, u), is chosen with probability 1− . We anneal from 1→ 0 such that the agent begins in an exploration phase and slowly starts moving towards the exploitation phase. In addition, when the exploration cost is large (which is true for our problem setting), it is beneficial to use the experience replay technique for faster convergence (Lin, 1992). In experience replay, the learning agent is provided with a memory of its past explored paths and rewards. At a given interval, the agent samples from the memory and updates its Q-values via Equation 3.
4 DESIGNING NEURAL NETWORK ARCHITECTURES WITH Q-LEARNING
We consider the task of training a learning agent to sequentially choose neural network layers. Figure 2 shows feasible state and action spaces (a) and a potential trajectory the agent may take along with the CNN architecture defined by this trajectory (b). We model the layer selection process as a Markov Decision Process with the assumption that a well-performing layer in one network should
also perform well in another network. We make this assumption based on the hierarchical nature of the feature representations learned by neural networks with many hidden layers (LeCun et al., 2015). The agent sequentially selects layers via the -greedy strategy until it reaches a termination state. The CNN architecture defined by the agent’s path is trained on the chosen learning problem, and the agent is given a reward equal to the validation accuracy. The validation accuracy and architecture description are stored in a replay memory, and experiences are sampled periodically from the replay memory to update Q-values via Equation 3. The agent follows an schedule which determines its shift from exploration to exploitation.
Our method requires three main design choices: (i) reducing CNN layer definitions to simple state tuples, (ii) defining a set of actions the agent may take, i.e., the set of layers the agent may pick next given its current state, and (iii) balancing the size of the state-action space—and correspondingly, the model capacity—with the amount of exploration needed by the agent to converge. We now describe the design choices and the learning process in detail.

4.1 THE STATE SPACE
Each state is defined as a tuple of all relevant layer parameters. We allow five different types of layers: convolution (C), pooling (P), fully connected (FC), global average pooling (GAP), and softmax (SM), though the general method is not limited to this set. Table 1 shows the relevant parameters for each layer type and also the discretization we chose for each parameter. Each layer has a parameter layer depth (shown as Layer 1, 2, ... in Figure 2). Adding layer depth to the state space allows us to constrict the action space such that the state-action graph is directed and acyclic (DAG) and also allows us to specify a maximum number of layers the agent may select before terminating.
Each layer type also has a parameter called representation size (R-size). Convolutional nets progressively compress the representation of the original signal through pooling and convolution. The presence of these layers in our state space may lead the agent on a trajectory where the intermediate signal representation gets reduced to a size that is too small for further processing. For example, five 2× 2 pooling layers each with stride 2 will reduce an image of initial size 32× 32 to size 1× 1. At this stage, further pooling, or convolution with receptive field size greater than 1, would be meaningless and degenerate. To avoid such scenarios, we add the R-size parameter to the state tuple s, which allows us to restrict actions from states with R-size n to those that have a receptive field size less than or equal to n. To further constrict the state space, we chose to bin the representation sizes into three discrete buckets. However, binning adds uncertainty to the state transitions: depending on the true underlying representation size, a pooling layer may or may not change the R-size bin. As a result, the action of pooling can lead to two different states, which we model as stochasticity in state transitions. Please see Figure A1 in appendix for an illustrated example.

4.2 THE ACTION SPACE
We restrict the agent from taking certain actions to both limit the state-action space and make learning tractable. First, we allow the agent to terminate a path at any point, i.e. it may choose a termination state from any non-termination state. In addition, we only allow transitions for a state with layer depth i to a state with layer depth i + 1, which ensures that there are no loops in the graph. This constraint ensures that the state-action graph is always a DAG. Any state at the maximum layer depth, as prescribed in Table 1, may only transition to a termination layer.
Next, we limit the number of fully connected (FC) layers to be at maximum two, because a large number of FC layers can lead to too may learnable parameters. The agent at a state with type FC may transition to another state with type FC if and only if the number of consecutive FC states is less than the maximum allowed. Furthermore, a state s of type FC with number of neurons d may only transition to either a termination state or a state s′ of type FC with number of neurons d′ ≤ d. An agent at a state of type convolution (C) may transition to a state with any other layer type. An agent at a state with layer type pooling (P) may transition to a state with any other layer type other than another P state because consecutive pooling layers are equivalent to a single, larger pooling layer which could lie outside of our chosen state space. Furthermore, only states with representation size in bins (8, 4] and (4, 1] may transition to an FC layer, which ensures that the number of weights does not become unreasonably huge. Note that a majority of these constraints are in place to enable faster convergence on our limited hardware (see Section 5) and not a limitation of the method in itself.
4.3 Q-LEARNING TRAINING PROCEDURE
For the iterativeQ-learning updates (Equation 3), we set theQ-learning rate (α) to 0.01. In addition, we set the discount factor (γ) to 1 to not over-prioritize short-term rewards. We decrease from 1.0 to 0.1 in steps, where the step-size is defined by the number of unique models trained (Table 2). At = 1.0, the agent samples CNN architecture with a random walk along a uniformly weighted Markov chain. Every topology sampled by the agent is trained using the procedure described in Section 5, and the prediction performance of this network topology on the validation set is recorded. We train a larger number of models at = 1.0 as compared to other values of to ensure that the agent has adequate time to explore before it begins to exploit. We stop the agent at = 0.1 (and not at = 0) to obtain a stochastic final policy, which generates perturbations of the global minimum.2 Ideally, we want to identify several well-performing model topologies, which can then be ensembled to improve prediction performance.
During the entire training process (starting at = 1.0), we maintain a replay dictionary which stores (i) the network topology and (ii) prediction performance on a validation set, for all of the sampled
2 = 0 indicates a completely deterministic policy. Because we would like to generate several good models for ensembling and analysis, we stop at = 0.1, which represents a stochastic final policy.
models. If a model that has already been trained is re-sampled, it is not re-trained, but instead the previously found validation accuracy is presented to the agent. After each model is sampled and trained, the agent randomly samples 100 models from the replay dictionary and applies the Q-value update defined in Equation 3 for all transitions in each sampled sequence. The Q-value update is applied to the transitions in temporally reversed order, which has been shown to speed up Q-values convergence (Lin, 1993).

5 EXPERIMENT DETAILS
During the model exploration phase, we trained each network topology with a quick and aggressive training scheme. For each experiment, we created a validation set by randomly taking 5,000 samples from the training set such that the resulting class distributions were unchanged. For every network, a dropout layer was added after every two layers. The ith dropout layer, out of a total n dropout layers, had a dropout probability of i2n . Each model was trained for a total of 20 epochs with the Adam optimizer (Kingma & Ba, 2014) with β1 = 0.9, β2 = 0.999, ε = 10−8. The batch size was set to 128, and the initial learning rate was set to 0.001. If the model failed to perform better than a random predictor after the first epoch, we reduced the learning rate by a factor of 0.4 and restarted training, for a maximum of 5 restarts. For models that started learning (i.e., performed better than a random predictor), we reduced the learning rate by a factor of 0.2 every 5 epochs. All weights were initialized with Xavier initialization (Glorot & Bengio, 2010). Our experiments using Caffe (Jia et al., 2014) took 8-10 days to complete for each dataset with a hardware setup consisting of 10 NVIDIA GPUs.
After the agent completed the schedule (Table 2), we selected the top ten models that were found over the course of exploration. These models were then finetuned using a much longer training schedule, and only the top five were used for ensembling. We now provide details of the datasets and the finetuning process.
The Street View House Numbers (SVHN) dataset has 10 classes with a total of 73,257 samples in the original training set, 26,032 samples in the test set, and 531,131 additional samples in the extended training set. During the exploration phase, we only trained with the original training set, using 5,000 random samples as validation. We finetuned the top ten models with the original plus extended training set, by creating preprocessed training and validation sets as described by Lee et al. (2016). Our final learning rate schedule after tuning on validation set was 0.025 for 5 epochs, 0.0125 for 5 epochs, 0.0001 for 20 epochs, and 0.00001 for 10 epochs.
CIFAR-10, the 10 class tiny image dataset, has 50,000 training samples and 10,000 testing samples. During the exploration phase, we took 5,000 random samples from the training set for validation. The maximum layer depth was increased to 18. After the experiment completed, we used the same validation set to tune hyperparameters, resulting in a final training scheme which we ran on the entire training set. In the final training scheme, we set a learning rate of 0.025 for 40 epochs, 0.0125 for 40 epochs, 0.0001 for 160 epochs, and 0.00001 for 60 epochs, with all other parameters unchanged. During this phase, we preprocess using global contrast normalization and use moderate data augmentation, which consists of random mirroring and random translation by up to 5 pixels.
MNIST, the 10 class handwritten digits dataset, has 60,000 training samples and 10,000 testing samples. We preprocessed each image with global mean subtraction. In the final training scheme, we trained each model for 40 epochs and decreased learning rate every 5 epochs by a factor of 0.2. For further tuning details please see Appendix C.

6 RESULTS
Model Selection Analysis: From Q-learning principles, we expect the learning agent to improve in its ability to pick network topologies as reduces and the agent enters the exploitation phase. In
Figure 3, we plot the rolling mean of prediction accuracy over 100 models and the mean accuracy of models sampled at different values, for the CIFAR-10 and SVHN experiments. The plots show that, while the prediction accuracy remains flat during the exploration phase ( = 1) as expected, the agent consistently improves in its ability to pick better-performing models as reduces from 1 to 0.1. For example, the mean accuracy of models in the SVHN experiment increases from 52.25% at = 1 to 88.02% at = 0.1. Furthermore, we demonstrate the stability of the Q-learning procedure with 10 independent runs on a subset of the SVHN dataset in Section D.1 of the Appendix. Additional analysis of Q-learning results can be found in Section D.2.
The top models selected by the Q-learning agent vary in the number of parameters but all demonstrate high performance (see Appendix Tables 1-3). For example, the number of parameters for the top five CIFAR-10 models range from 11.26 million to 1.10 million, with only a 2.32% decrease in test error. We find design motifs common to the top hand-crafted network architectures as well. For example, the agent often chooses a layer of type C(N, 1, 1) as the first layer in the network. These layers generate N learnable linear transformations of the input data, which is similar in spirit to preprocessing of input data from RGB to a different color spaces such as YUV, as found in prior work (Sermanet et al., 2012; 2013).
Prediction Performance: We compare the prediction performance of the MetaQNN networks discovered by theQ-learning agent with state-of-the-art methods on three datasets. We report the accuracy of our best model, along with an ensemble of top five models. First, we compare MetaQNN with six existing architectures that are designed with standard convolution, pooling, and fully-connected layers alone, similar to our designs. As seen in Table 3, our top model alone, as well as the committee ensemble of five models, outperforms all similar models. Next, we compare our results with six top networks overall, which contain complex layer types and design ideas, including generalized pooling functions, residual connections, and recurrent modules. Our results are competitive with these methods as well (Table 4). Finally, our method outperforms existing automated network de-
sign methods. MetaQNN obtains an error of 6.92% as compared to 21.2% reported by Bergstra et al. (2011) on CIFAR-10; and it obtains an error of 0.32% as compared to 7.9% reported by Verbancsics & Harguess (2013) on MNIST.
The difference in validation error between the top 10 models for MNIST was very small, so we also created an ensemble with all 10 models. This ensemble achieved a test error of 0.28%—which beats the current state-of-the-art on MNIST without data augmentation.
The best CIFAR-10 model performs 1-2% better than the four next best models, which is why the ensemble accuracy is lower than the best model’s accuracy. We posit that the CIFAR-10 MetaQNN did not have adequate exploration time given the larger state space compared to that of the SVHN experiment, causing it to not find more models with performance similar to the best model. Furthermore, the coarse training scheme could have been not as well suited for CIFAR-10 as it was for SVHN, causing some models to under perform.
Transfer Learning Ability: Network designs such as VGGnet (Simonyan & Zisserman, 2014) can be adopted to solve a variety of computer vision problems. To check if the MetaQNN networks provide similar transfer learning ability, we use the best MetaQNN model on the CIFAR-10 dataset for training other computer vision tasks. The model performs well (Table 5) both when training from random initializations, and finetuning from existing weights.

7 CONCLUDING REMARKS
Neural networks are being used in an increasingly wide variety of domains, which calls for scalable solutions to produce problem-specific model architectures. We take a step towards this goal and show that a meta-modeling approach using reinforcement learning is able to generate tailored CNN designs for different image classification tasks. Our MetaQNN networks outperform previous metamodeling methods as well as hand-crafted networks which use the same types of layers.
While we report results for image classification problems, our method could be applied to different problem settings, including supervised (e.g., classification, regression) and unsupervised (e.g., autoencoders). The MetaQNN method could also aid constraint-based network design, by optimizing parameters such as size, speed, and accuracy. For instance, one could add a threshold in the state-action space barring the agent from creating models larger than the desired limit. In addition,
∗Results in this column obtained with the top MetaQNN architecture for CIFAR-10, trained from random initialization with CIFAR-100 data.
one could modify the reward function to penalize large models for constraining memory or penalize slow forward passes to incentivize quick inference.
There are several future avenues for research in reinforcement learning-driven network design as well. In our current implementation, we use the same set of hyperparameters to train all network topologies during the Q-learning phase and further finetune the hyperparameters for top models selected by the MetaQNN agent. However, our approach could be combined with hyperparameter optimization methods to further automate the network design process. Moreover, we constrict the state-action space using coarse, discrete bins to accelerate convergence. It would be possible to move to larger state-action spaces using methods for Q-function approximation (Bertsekas, 2015; Mnih et al., 2015).

ACKNOWLEDGMENTS
We thank Peter Downs for creating the project website and contributing to illustrations. We acknowledge Center for Bits and Atoms at MIT for their help with computing resources. Finally, we thank members of Camera Culture group at MIT Media Lab for their help and support.

A ALGORITHM
We first describe the main components of the MetaQNN algorithm. Algorithm 1 shows the main loop, where the parameter M would determine how many models to run for a given and the parameter K would determine how many times to sample the replay database to update Q-values on each iteration. The function TRAIN refers to training the specified network and returns a validation accuracy. Algorithm 2 details the method for sampling a new network using the -greedy strategy, where we assume we have a function TRANSITION that returns the next state given a state and action. Finally, Algorithm 3 implements theQ-value update detailed in Equation 3, with discounting factor set to 1, for an entire state sequence in temporally reversed order.
Algorithm 1 Q-learning For CNN Topologies Initialize:
replay memory← [ ] Q← {(s, u) ∀s ∈ S, u ∈ U(s) : 0.5}
for episode = 1 to M do S, U ← SAMPLE NEW NETWORK( , Q) accuracy← TRAIN(S) replay memory.append((S, U, accuracy)) for memory = 1 to K do
SSAMPLE , USAMPLE , accuracySAMPLE ← Uniform{replay memory} Q← UPDATE Q VALUES(Q, SSAMPLE , USAMPLE , accuracySAMPLE)
end for end for
Algorithm 2 SAMPLE NEW NETWORK( , Q) Initialize:
state sequence S = [sSTART] action sequence U = [ ]
while U [−1] 6= terminate do α ∼ Uniform[0, 1) if α > then
u = argmaxu∈U(S[−1])Q[(S[−1], u)] s′ = TRANSITION(S[−1], u)
else u ∼ Uniform{U(S[−1])} s′ = TRANSITION(S[−1], u) end if U.append(u) if u != terminate then
S.append(s′) end if
end while return S, U
Algorithm 3 UPDATE Q VALUES(Q, S, U , accuracy) Q[S[−1], U [−1]] = (1− α)Q[S[−1], U [−1]] + α · accuracy for i = length(S)− 2 to 0 do
Q[S[i], U [i]] = (1− α)Q[S[i], U [i]] + αmaxu∈U(S[i+1])Q[S[i+ 1], u] end for return Q

B REPRESENTATION SIZE BINNING
As mentioned in Section 4.1 of the main text, we introduce a parameter called representation size to prohibit the agent from taking actions that can reduce the intermediate signal representation to a size that is too small for further processing. However, this process leads to uncertainties in state transitions, as illustrated in Figure A1, which is handled by the standard Q-learning formulation.
P(2,2)
R-size: 18 R-size bin: 1
R-size: 9 R-size bin: 1
(a)
P(2,2)
R-size: 7 R-size bin: 2
R-size: 14 R-size bin: 1
(b)
States Actions
p 1 2 p
R-size bin: 1
R-size bin: 1 R-size bin: 2
P(2,2)
(c)
Figure A1: Representation size binning: In this figure, we show three example state transitions. The true representation size (R-size) parameter is included in the figure to show the true underlying state. Assuming there are two R-size bins, R-size Bin1: [8,∞) and R-size Bin2: (0, 7], Figure A1a shows the case where the initial state is in R-size Bin1 and true representation size is 18. After the agent chooses to pool with a 2×2 filter with stride 2, the true representation size reduces to 9 but the R-size bin does not change. In Figure A1b, the same 2 × 2 pooling layer with stride 2 reduces the actual representation size of 14 to 7, but the bin changes to R-size Bin2. Therefore, in figures A1a and A1b, the agent ends up in different final states, despite originating in the same initial state and choosing the same action. Figure A1c shows that in our state-action space, when the agent takes an action that reduces the representation size, it will have uncertainty in which state it will transition to.

C MNIST EXPERIMENT
We noticed that the final MNIST models were prone to overfitting, so we increased dropout and did a small grid search for the weight regularization parameter. For both tuning and final training, we warmed the model with the learned weights from after the first epoch of initial training. The final models and solvers can be found on our project website https://bowenbaker.github.io/metaqnn/ . Figure A2 shows the Q-Learning performance for the MNIST experiment.
D FURTHER ANALYSIS OF Q-LEARNING
Figure 3 of the main text and Figure A2 show that as the agent begins to exploit, it improves in architecture selection. It is also informative to look at the distribution of models chosen at each . Figure A4 gives further insight into the performance achieved at each for both experiments.
D.1 Q-LEARNING STABILITY
Because the Q-learning agent explores via a random or semi-random distribution, it is natural to ask whether the agent can consistently improve architecture performance. While the success of the three independent experiments described in the main text allude to stability, here we present further evidence. We conduct 10 independent runs of the Q-learning procedure on 10% of the SVHN dataset (which corresponds to ∼7,000 training examples). We use a smaller dataset to reduce the computation time of each independent run to 10GPU-days, as opposed to the 100GPU-days it would take on the full dataset. As can be seen in Figure A3, the Q-learning procedure with the exploration schedule detailed in Table 2 is fairly stable. The standard deviation at = 1 is notably smaller than at other stages, which we attribute to the large difference in number of samples at each stage.
0 500 1000 1500 2000 2500 3000 3500 Iterations
0.00
0.10
0.20
0.30
0.40
0.50
0.60
0.70
0.80
0.90
1.00
A cc
u ra
cy
Epsilon = 1.0 .9.8.7 .6 .5 .4 .3 .2 .1
MNIST Q-Learning Performance
Average Accuracy Per Epsilon Rolling Mean Model Accuracy
Figure A2: MNIST Q-Learning Performance. The blue line shows a rolling mean of model accuracy versus iteration, where in each iteration of the algorithm the agent is sampling a model. Each bar (in light blue) marks the average accuracy over all models that were sampled during the exploration phase with the labeled . As decreases, the average accuracy goes up, demonstrating that the agent learns to select better-performing CNN architectures.
0.10.20.30.40.50.60.70.80.91.0 Epsilon
0.45
0.50
0.55
0.60
0.65
0.70
0.75
0.80
M ea
n Ac
cu ra
cy
Q-Learning Stability (Across 10 Runs)
(a)
0.10.20.30.40.50.60.70.80.91.0 Epsilon
0.45
0.50
0.55
0.60
0.65
0.70
0.75 0.80 M ea n Ac cu ra cy
Q-Learning Individual Runs
(b)
Figure A3: Figure A3a shows the mean model accuracy and standard deviation at each over 10 independent runs of the Q-learning procedure on 10% of the SVHN dataset. Figure A3b shows the mean model accuracy at each for each independent experiment. Despite some variance due to a randomized exploration strategy, each independent run successfully improves architecture performance.
Furthermore, the best model found during each run had remarkably similar performance with a mean accuracy of 88.25% and standard deviation of 0.58%, which shows that each run successfully found at least one very high performing model. Note that we did not use an extended training schedule to improve performance in this experiment.
D.2 Q-VALUE ANALYSIS
We now analyze the actualQ-values generated by the agent during the training process. The learning agent iteratively updates the Q-values of each path during the -greedy exploration. Each Q-value is initialized at 0.5. After the -schedule is complete, we can analyze the final Q-value associated with each path to gain insights into the layer selection process. In the left column of Figure A5, we plot the average Q-value for each layer type at different layer depths (for both SVHN and CIFAR10) datasets. Roughly speaking, a higher Q-value associated with a layer type indicates a higher probability that the agent will pick that layer type. In Figure A5, we observe that, while the average Q-value is higher for convolution and pooling layers at lower layer depths, the Q-values for fullyconnected and termination layers (softmax and global average pooling) increase as we go deeper into the network. This observation matches with traditional network designs.
We can also plot the averageQ-values associated with different layer parameters for further analysis. In the right column of Figure A5, we plot the averageQ-values for convolution layers with receptive
field sizes 1, 3, and 5 at different layer depths. The plots show that layers with receptive field size of 5 have a higher Q-value as compared to sizes 1 and 3 as we go deeper into the networks. This indicates that it might be beneficial to use larger receptive field sizes in deeper networks.
In summary, the Q-learning method enables us to perform analysis on the relative benefits of different design parameters of our state space, and possibly gain insights for new CNN designs.

E TOP TOPOLOGIES SELECTED BY ALGORITHM
In Tables A1 through A3, we present the top five model architectures selected with Q-learning for each dataset, along with their prediction error reported on the test set, and their total number of parameters. To download the Caffe solver and prototext files, please visit https://bowenbaker.github.io/metaqnn/ .
Model Architecture Test Error (%) # Params (106) [C(512,5,1), C(256,3,1), C(256,5,1), C(256,3,1), P(5,3), C(512,3,1), C(512,5,1), P(2,2), SM(10)] 6.92 11.18 [C(128,1,1), C(512,3,1), C(64,1,1), C(128,3,1), P(2,2), C(256,3,1), P(2,2), C(512,3,1), P(3,2), SM(10)] 8.78 2.17 [C(128,3,1), C(128,1,1), C(512,5,1), P(2,2), C(128,3,1), P(2,2), C(64,3,1), C(64,5,1), SM(10)] 8.88 2.42 [C(256,3,1), C(256,3,1), P(5,3), C(256,1,1), C(128,3,1), P(2,2), C(128,3,1), SM(10)] 9.24 1.10 [C(128,5,1), C(512,3,1), P(2,2), C(128,1,1), C(128,5,1), P(3,2), C(512,3,1), SM(10)] 11.63 1.66
Table A1: Top 5 model architectures: CIFAR-10.
Model Architecture Test Error (%) # Params (106) [C(128,3,1), P(2,2), C(64,5,1), C(512,5,1), C(256,3,1), C(512,3,1), P(2,2), C(512,3,1), C(256,5,1), C(256,3,1), C(128,5,1), C(64,3,1), SM(10)] 2.24 9.81 [C(128,1,1), C(256,5,1), C(128,5,1), P(2,2), C(256,5,1), C(256,1,1), C(256,3,1), C(256,3,1), C(256,5,1), C(512,5,1), C(256,3,1), C(128,3,1), SM(10)] 2.28 10.38 [C(128,5,1), C(128,3,1), C(64,5,1), P(5,3), C(128,3,1), C(512,5,1), C(256,5,1), C(128,5,1), C(128,5,1), C(128,3,1), SM(10)] 2.32 6.83 [C(128,1,1), C(256,5,1), C(128,5,1), C(256,3,1), C(256,5,1), P(2,2), C(128,1,1), C(512,3,1), C(256,5,1), P(2,2), C(64,5,1), C(64,1,1), SM(10)] 2.35 6.99 [C(128,1,1), C(256,5,1), C(128,5,1), C(256,5,1), C(256,5,1), C(256,1,1), P(3,2), C(128,1,1), C(256,5,1), C(512,5,1), C(256,3,1), C(128,3,1), SM(10)] 2.36 10.05
Table A2: Top 5 model architectures: SVHN. Note that we do not report the best accuracy on test set from the above models in Tables 3 and 4 from the main text. This is because the model that achieved 2.28% on the test set performed the best on the validation set.
Model Architecture Test Error (%) # Params (106) [C(64,1,1), C(256,3,1), P(2,2), C(512,3,1), C(256,1,1), P(5,3), C(256,3,1), C(512,3,1), FC(512), SM(10)] 0.35 5.59 [C(128,3,1), C(64,1,1), C(64,3,1), C(64,5,1), P(2,2), C(128,3,1), P(3,2), C(512,3,1), FC(512), FC(128), SM(10)] 0.38 7.43 [C(512,1,1), C(128,3,1), C(128,5,1), C(64,1,1), C(256,5,1), C(64,1,1), P(5,3), C(512,1,1), C(512,3,1), C(256,3,1), C(256,5,1), C(256,5,1), SM(10)] 0.40 8.28 [C(64,3,1), C(128,3,1), C(512,1,1), C(256,1,1), C(256,5,1), C(128,3,1), P(5,3), C(512,1,1), C(512,3,1), C(128,5,1), SM(10)] 0.41 6.27 [C(64,3,1), C(128,1,1), P(2,2), C(256,3,1), C(128,5,1), C(64,1,1), C(512,5,1), C(128,5,1), C(64,1,1), C(512,5,1), C(256,5,1), C(64,5,1), SM(10)] 0.43 8.10 [C(64,1,1), C(256,5,1), C(256,5,1), C(512,1,1), C(64,3,1), P(5,3), C(256,5,1), C(256,5,1), C(512,5,1), C(64,1,1), C(128,5,1), C(512,5,1), SM(10)] 0.44 9.67 [C(128,3,1), C(512,3,1), P(2,2), C(256,3,1), C(128,5,1), C(64,1,1), C(64,5,1), C(512,5,1), GAP(10), SM(10)] 0.44 3.52 [C(256,3,1), C(256,5,1), C(512,3,1), C(256,5,1), C(512,1,1), P(5,3), C(256,3,1), C(64,3,1), C(256,5,1), C(512,3,1), C(128,5,1), C(512,5,1), SM(10)] 0.46 12.42 [C(512,5,1), C(128,5,1), C(128,5,1), C(128,3,1), C(256,3,1), C(512,5,1), C(256,3,1), C(128,3,1), SM(10)] 0.55 7.25 [C(64,5,1), C(512,5,1), P(3,2), C(256,5,1), C(256,3,1), C(256,3,1), C(128,1,1), C(256,3,1), C(256,5,1), C(64,1,1), C(256,3,1), C(64,3,1), SM(10)] 0.56 7.55
Table A3: Top 10 model architectures: MNIST. We report the top 10 models for MNIST because we included all 10 in our final ensemble. Note that we do not report the best accuracy on test set from the above models in Tables 3 and 4 from the main text. This is because the model that achieved 0.44% on the test set performed the best on the validation set.
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
Validation Accuracy
0
10
20
30
40
50
60
% M
o d e ls
Model Accuracy Distribution (SVHN)
epsilon
0.1 0.2 0.3 0.4 0.5
0.6 0.7 0.8 0.9 1.0
(a)
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
Validation Accuracy
0
10
20
30
40
50
60
% M
o d e ls
Model Accuracy Distribution (SVHN)
epsilon
0.1 1.0
(b)
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Validation Accuracy
0
5
10
15
20
% M
o d e ls
Model Accuracy Distribution (CIFAR-10)
epsilon
0.1 0.2 0.3 0.4 0.5
0.6 0.7 0.8 0.9 1.0
(c)
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Validation Accuracy
0
5
10
15 20 % M o d e ls
Model Accuracy Distribution (CIFAR-10)
epsilon
0.1 1.0
(d)
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Validation Accuracy
0
20
40
60
80
100
% M
od el
s
Model Accuracy Distribution (MNIST)
epsilon 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
(e)
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Validation Accuracy
0
20
40
60
80
100
% M
od el
s
Model Accuracy Distribution (MNIST)
epsilon 0.1 1.0
(f)
Figure A4: Accuracy Distribution versus : Figures A4a, A4c, and A4e show the accuracy distribution for each for the SVHN, CIFAR-10, and MNIST experiments, respectively. Figures A4b, A4d, and A4f show the accuracy distributions for the initial = 1 and the final = 0.1. One can see that the accuracy distribution becomes much more peaked in the high accuracy ranges at small for each experiment.
0 2 4 6 8 10 12 14 Layer Depth
0.0
0.2
0.4
0.6
0.8
1.0
A ve
ra ge
Q -V
al ue
Average Q-Value vs. Layer Depth (SVHN)
Convolution Fully Connected Pooling Global Average Pooling Softmax
(a)
0 2 4 6 8 10 12 Layer Depth
0.5
0.6
0.7
0.8
0.9
1.0
A ve
ra ge
Q -V
al ue
Average Q-Value vs. Layer Depth for Convolution Layers (SVHN)
Receptive Field Size 1 Receptive Field Size 3 Receptive Field Size 5
(b)
0 5 10 15 20 Layer Depth
0.0
0.2
0.4
0.6
0.8
1.0
A ve
ra ge
Q -V
al ue
Average Q-Value vs. Layer Depth (CIFAR10)
Convolution Fully Connected Pooling Global Average Pooling Softmax
(c)
0 2 4 6 8 10 12 14 16 18 Layer Depth
0.5
0.6
0.7
0.8
0.9 1.0 A ve ra ge Q -V al ue
Average Q-Value vs. Layer Depth for Convolution Layers (CIFAR10)
Receptive Field Size 1 Receptive Field Size 3 Receptive Field Size 5
(d)
0 2 4 6 8 10 12 14 Layer Depth
0.0
0.2
0.4
0.6
0.8
1.0
A ve
ra ge
Q -V
al ue
Average Q-Value vs. Layer Depth (MNIST)
Convolution Fully Connected Pooling Global Average Pooling Softmax
(e)
0 2 4 6 8 10 12 Layer Depth
0.5
0.6
0.7
0.8
0.9
1.0
A ve
ra ge
Q -V
al ue
Average Q-Value vs. Layer Depth for Convolution Layers (MNIST)
Receptive Field Size 1 Receptive Field Size 3 Receptive Field Size 5
(f)
Figure A5: Average Q-Value versus Layer Depth for different layer types are shown in the left column. Average Q-Value versus Layer Depth for different receptive field sizes of the convolution layer are shown in the right column.
","At present, designing convolutional neural network (CNN) architectures requires both human expertise and labor. New architectures are handcrafted by careful experimentation or modified from a handful of existing networks. We introduce MetaQNN, a meta-modeling algorithm based on reinforcement learning to automatically generate high-performing CNN architectures for a given learning task. The learning agent is trained to sequentially choose CNN layers using Qlearning with an -greedy exploration strategy and experience replay. The agent explores a large but finite space of possible architectures and iteratively discovers designs with improved performance on the learning task. On image classification benchmarks, the agent-designed networks (consisting of only standard convolution, pooling, and fully-connected layers) beat existing networks designed with the same layer types and are competitive against the state-of-the-art methods that use more complex layer types. We also outperform existing meta-modeling approaches for network design on image classification tasks.",ICLR 2017 conference submission,True,,"This paper comes up with a novel approach to searching the space of architectures for deep neural networks using reinforcement learning. The idea is straightforward and sensible: use a reinforcement learning strategy to iteratively grow a deep net graph (the space of actions is e.g. adding different layer types) via Q-learning. The reviewers agree that the idea is interesting, novel and promising but are underwhelmed with the execution of the experiments and the empirical results. 
 
 The idea behind the paper and the formulation of the problem are quite similar to a concurrent submission (

---

We have added the results of the stability experiment (as suggested by AnonReviewer1) into the Appendix section D.1. We have also included a citation to the CNF paper and results from the latest ResNet paper.

---

This paper introduces a reinforcement learning framework for designing a neural network architecture. For each time-step, the agent picks a new layer type with corresponding layer parameters (e.g., #filters). In order to reduce the size of state-action space, they used a small set of design choices.

Strengths:
- A novel approach for automatic design of neural network architectures.
- Shows quite promising results on several datasets (MNIST, CIFAR-10).

Weakness:
- Limited architecture design choices due to many prior assumptions (e.g., a set of possible number of convolution filters, at most 2 fully-connected layers, maximum depth, hard-coded dropout, etc.)
- The method is demonstrated in tabular Q-learning setting, but it is unclear whether the proposed method would work in a large state-action space.

Overall, this is an interesting and novel approach for neural network architecture design, and it seems to be worth publication despite some weaknesses.

---

The paper looks solid and the idea is natural. Results seem promising as well.

I am mostly concerned about the computational cost of the method. 8-10 days on 10 GPUs for relatively tiny datasets is quite prohibitive for most applications I would ever encounter.
 I think the main question is how this approach scales to larger images and also when applied to more exotic and possibly tiny datasets. Can you run an experiment on Caltech-101 for instance? I would be very curious to see if your approach is suitable for the low-data regime and areas where we all do not know right away how a suitable architecture looks like. For Cifar-10/100, MNIST and SVHN, everyone knows very well what a reasonable model initialization looks like.

If you show proof that you can discover a competitive architecture for something like Caltech-101, I would recommend the paper for publication.

Minor: 
- ResNets should be mentioned in Table

---

Authors learn deep architectures on a few small vision problems using Q-learning and obtain solid results, SOTA results when limiting to certain types of layers and competitive against everything else. It would be good to know how well this performs when allowing more complex structures. Paper would be much more convincing on a real-size task such as ImageNet.

---

This paper comes up with a novel approach to searching the space of architectures for deep neural networks using reinforcement learning. The idea is straightforward and sensible: use a reinforcement learning strategy to iteratively grow a deep net graph (the space of actions is e.g. adding different layer types) via Q-learning. The reviewers agree that the idea is interesting, novel and promising but are underwhelmed with the execution of the experiments and the empirical results. 
 
 The idea behind the paper and the formulation of the problem are quite similar to a concurrent submission (

---

We have added the results of the stability experiment (as suggested by AnonReviewer1) into the Appendix section D.1. We have also included a citation to the CNF paper and results from the latest ResNet paper.

---

This paper introduces a reinforcement learning framework for designing a neural network architecture. For each time-step, the agent picks a new layer type with corresponding layer parameters (e.g., #filters). In order to reduce the size of state-action space, they used a small set of design choices.

Strengths:
- A novel approach for automatic design of neural network architectures.
- Shows quite promising results on several datasets (MNIST, CIFAR-10).

Weakness:
- Limited architecture design choices due to many prior assumptions (e.g., a set of possible number of convolution filters, at most 2 fully-connected layers, maximum depth, hard-coded dropout, etc.)
- The method is demonstrated in tabular Q-learning setting, but it is unclear whether the proposed method would work in a large state-action space.

Overall, this is an interesting and novel approach for neural network architecture design, and it seems to be worth publication despite some weaknesses.

---

The paper looks solid and the idea is natural. Results seem promising as well.

I am mostly concerned about the computational cost of the method. 8-10 days on 10 GPUs for relatively tiny datasets is quite prohibitive for most applications I would ever encounter.
 I think the main question is how this approach scales to larger images and also when applied to more exotic and possibly tiny datasets. Can you run an experiment on Caltech-101 for instance? I would be very curious to see if your approach is suitable for the low-data regime and areas where we all do not know right away how a suitable architecture looks like. For Cifar-10/100, MNIST and SVHN, everyone knows very well what a reasonable model initialization looks like.

If you show proof that you can discover a competitive architecture for something like Caltech-101, I would recommend the paper for publication.

Minor: 
- ResNets should be mentioned in Table

---

Authors learn deep architectures on a few small vision problems using Q-learning and obtain solid results, SOTA results when limiting to certain types of layers and competitive against everything else. It would be good to know how well this performs when allowing more complex structures. Paper would be much more convincing on a real-size task such as ImageNet.",3.0,4.5,3.0,3.0,3.6666666666666665,6.0,,,3.6666666666666665,,
419,"TOPICRNN: A RECURRENT NEURAL NETWORK WITH LONG-RANGE SEMANTIC DEPENDENCY
Authors: Adji B. Dieng, Chong Wang, Jianfeng Gao
Source file: 419.pdf

ABSTRACT
In this paper, we propose TopicRNN, a recurrent neural network (RNN)-based language model designed to directly capture the global semantic meaning relating words in a document via latent topics. Because of their sequential nature, RNNs are good at capturing the local structure of a word sequence – both semantic and syntactic – but might face difficulty remembering long-range dependencies. Intuitively, these long-range dependencies are of semantic nature. In contrast, latent topic models are able to capture the global semantic structure of a document but do not account for word ordering. The proposed TopicRNN model integrates the merits of RNNs and latent topic models: it captures local (syntactic) dependencies using an RNN and global (semantic) dependencies using latent topics. Unlike previous work on contextual RNN language modeling, our model is learned endto-end. Empirical results on word prediction show that TopicRNN outperforms existing contextual RNN baselines. In addition, TopicRNN can be used as an unsupervised feature extractor for documents. We do this for sentiment analysis on the IMDB movie review dataset and report an error rate of 6.28%. This is comparable to the state-of-the-art 5.91% resulting from a semi-supervised approach. Finally, TopicRNN also yields sensible topics, making it a useful alternative to document models such as latent Dirichlet allocation.

1 INTRODUCTION
When reading a document, short or long, humans have a mechanism that somehow allows them to remember the gist of what they have read so far. Consider the following example:
“The U.S.presidential race isn’t only drawing attention and controversy in the United States – it’s being closely watched across the globe. But what does the rest of the world think about a campaign that has already thrown up one surprise after another? CNN asked 10 journalists for their take on the race so far, and what their country might be hoping for in America’s next —”
The missing word in the text above is easily predicted by any human to be either President or Commander in Chief or their synonyms. There have been various language models – from simple ngrams to the most recent RNN-based language models – that aim to solve this problem of predicting correctly the subsequent word in an observed sequence of words.
A good language model should capture at least two important properties of natural language. The first one is correct syntax. In order to do prediction that enjoys this property, we often only need to consider a few preceding words. Therefore, correct syntax is more of a local property. Word order matters in this case. The second property is the semantic coherence of the prediction. To achieve
∗Work was done while at Microsoft Research.
this, we often need to consider many preceding words to understand the global semantic meaning of the sentence or document. The ordering of the words usually matters much less in this case.
Because they only consider a fixed-size context window of preceding words, traditional n-gram and neural probabilistic language models (Bengio et al., 2003) have difficulties in capturing global semantic information. To overcome this, RNN-based language models (Mikolov et al., 2010; 2011) use hidden states to “remember” the history of a word sequence. However, none of these approaches explicitly model the two main properties of language mentioned above, correct syntax and semantic coherence. Previous work by Chelba and Jelinek (2000) and Gao et al. (2004) exploit syntactic or semantic parsers to capture long-range dependencies in language.
In this paper, we propose TopicRNN, a RNN-based language model that is designed to directly capture long-range semantic dependencies via latent topics. These topics provide context to the RNN. Contextual RNNs have received a lot of attention (Mikolov and Zweig, 2012; Mikolov et al., 2014; Ji et al., 2015; Lin et al., 2015; Ji et al., 2016; Ghosh et al., 2016). However, the models closest to ours are the contextual RNN model proposed by Mikolov and Zweig (2012) and its most recent extension to the long-short term memory (LSTM) architecture (Ghosh et al., 2016). These models use pre-trained topic model features as an additional input to the hidden states and/or the output of the RNN. In contrast, TopicRNN does not require pre-trained topic model features and can be learned in an end-to-end fashion. We introduce an automatic way for handling stop words that topic models usually have difficulty dealing with. Under a comparable model size set up, TopicRNN achieves better perplexity scores than the contextual RNN model of Mikolov and Zweig (2012) on the Penn TreeBank dataset 1. Moreover, TopicRNN can be used as an unsupervised feature extractor for downstream applications. For example, we derive document features of the IMDB movie review dataset using TopicRNN for sentiment classification. We reported an error rate of 6.28%. This is close to the state-of-the-art 5.91% (Miyato et al., 2016) despite that we do not use the labels and adversarial training in the feature extraction stage.
The remainder of the paper is organized as follows: Section 2 provides background on RNN-based language models and probabilistic topic models. Section 3 describes the TopicRNN network architecture, its generative process and how to perform inference for it. Section 4 presents per-word perplexity results on the Penn TreeBank dataset and the classification error rate on the IMDB 100K dataset. Finally, we conclude and provide future research directions in Section 5.

2 BACKGROUND
We present the background necessary for building the TopicRNN model. We first review RNN-based language modeling, followed by a discussion on the construction of latent topic models.

2.1 RECURRENT NEURAL NETWORK-BASED LANGUAGE MODELS
Language modeling is fundamental to many applications. Examples include speech recognition and machine translation. A language model is a probability distribution over a sequence of words in a predefined vocabulary. More formally, let V be a vocabulary set and y1, ..., yT a sequence of T words with each yt ∈ V . A language model measures the likelihood of a sequence through a joint probability distribution,
p(y1, ..., yT ) = p(y1) T∏ t=2 p(yt|y1:t−1).
Traditional n-gram and feed-forward neural network language models (Bengio et al., 2003) typically make Markov assumptions about the sequential dependencies between words, where the chain rule shown above limits conditioning to a fixed-size context window.
RNN-based language models (Mikolov et al., 2011) sidestep this Markov assumption by defining the conditional probability of each word yt given all the previous words y1:t−1 through a hidden
1Ghosh et al. (2016) did not publish results on the PTB and we did not find the code online.
state ht (typically via a softmax function):
p(yt|y1:t−1) , p(yt|ht), ht = f(ht−1, xt).
The function f(·) can either be a standard RNN cell or a more complex cell such as GRU (Cho et al., 2014) or LSTM (Hochreiter and Schmidhuber, 1997). The input and target words are related via the relation xt ≡ yt−1. These RNN-based language models have been quite successful (Mikolov et al., 2011; Chelba et al., 2013; Jozefowicz et al., 2016).
While in principle RNN-based models can “remember” arbitrarily long histories if provided enough capacity, in practice such large-scale neural networks can easily encounter difficulties during optimization (Bengio et al., 1994; Pascanu et al., 2013; Sutskever, 2013) or overfitting issues (Srivastava et al., 2014). Finding better ways to model long-range dependencies in language modeling is therefore an open research challenge. As motivated in the introduction, much of the long-range dependency in language comes from semantic coherence, not from syntactic structure which is more of a local phenomenon. Therefore, models that can capture long-range semantic dependencies in language are complementary to RNNs. In the following section, we describe a family of such models called probabilistic topic models.

2.2 PROBABILISTIC TOPIC MODELS
Probabilistic topic models are a family of models that can be used to capture global semantic coherency (Blei and Lafferty, 2009). They provide a powerful tool for summarizing, organizing, and navigating document collections. One basic goal of such models is to find groups of words that tend to co-occur together in the same document. These groups of words are called topics and represent a probability distribution that puts most of its mass on this subset of the vocabulary. Documents are then represented as mixtures over these latent topics. Through posterior inference, the learned topics capture the semantic coherence of the words they cluster together (Mimno et al., 2011).
The simplest topic model is latent Dirichlet allocation (LDA) (Blei et al., 2003). It assumes K underlying topics β = {β1, . . . , βK} , each of which is a distribution over a fixed vocabulary. The generative process of LDA is as follows: First generate the K topics, βk ∼iid Dirichlet(τ). Then for each document containing words y1:T , independently generate document-level variables and data:
1. Draw a document-specific topic proportion vector θ ∼ Dirichlet(α).
2. For the tth word in the document,
(a) Draw topic assignment zt ∼ Discrete(θ). (b) Draw word yt ∼ Discrete(βzt).
Marginalizing each zt, we obtain the probability of y1:T via a matrix factorization followed by an integration over the latent variable θ,
p(y1:T |β) = ∫ p(θ) T∏ t=1 ∑ zt p(zt|θ)p(yt|zt, β)dθ = ∫ p(θ) T∏ t=1 (βθ)ytdθ. (1)
In LDA the prior distribution on the topic proportions is a Dirichlet distribution; it can be replaced by many other distributions. For example, the correlated topic model (Blei and Lafferty, 2006) uses a log-normal distribution. Most topic models are “bag of words” models in that word order is ignored. This makes it easier for topic models to capture global semantic information. However, this is also one of the reasons why topic models do not perform well on general-purpose language modeling applications such as word prediction. While bi-gram topic models have been proposed (Wallach, 2006), higher order models quickly become intractable.
Another issue encountered by topic models is that they do not model stop words well. This is because stop words usually do not carry semantic meaning; their appearance is mainly to make the sentence more readable according to the grammar of the language. They also appear frequently in
almost every document and can co-occur with almost any word2. In practice, these stop words are chosen using tf-idf (Blei and Lafferty, 2009).

3 THE TOPICRNN MODEL
We next describe the proposed TopicRNN model. In TopicRNN, latent topic models are used to capture global semantic dependencies so that the RNN can focus its modeling capacity on the local dynamics of the sequences. With this joint modeling, we hope to achieve better overall performance on downstream applications.
The model. TopicRNN is a generative model. For a document containing the words y1:T ,
1. Draw a topic vector3 θ ∼ N(0, I). 2. Given word y1:t−1, for the tth word yt in the document,
(a) Compute hidden state ht = fW (xt, ht−1), where we let xt , yt−1. (b) Draw stop word indicator lt ∼ Bernoulli(σ(Γ>ht)), with σ the sigmoid function. (c) Draw word yt ∼ p(yt|ht, θ, lt, B), where
p(yt = i|ht, θ, lt, B) ∝ exp ( v>i ht + (1− lt)b>i θ ) .
The stop word indicator lt controls how the topic vector θ affects the output. If lt = 1 (indicating yt is a stop word), the topic vector θ has no contribution to the output. Otherwise, we add a bias to favor those words that are more likely to appear when mixing with θ, as measured by the dot product between θ and the latent word vector bi for the ith vocabulary word. As we can see, the longrange semantic information captured by θ directly affects the output through an additive procedure. Unlike Mikolov and Zweig (2012), the contextual information is not passed to the hidden layer of the RNN. The main reason behind our choice of using the topic vector as bias instead of passing it into the hidden states of the RNN is because it enables us to have a clear separation of the contributions of global semantics and those of local dynamics. The global semantics come from the topics which are meaningful when stop words are excluded. However these stop words are needed for the local dynamics of the language model. We hence achieve this separation of global vs local via a binary decision model for the stop words. It is unclear how to achieve this if we pass the topics to the
2Wallach et al. (2009) described using asymmetric priors to alleviate this issue. Although it is not clear how to use this idea in TopicRNN, we plan to investigate such priors in future work.
3Instead of using the Dirichlet distribution, we choose the Gaussian distribution. This allows for more flexibility in the sequence prediction problem and also has advantages during inference.
hidden states of the RNN. This is because the hidden states of the RNN will account for all words (including stop words) whereas the topics exclude stop words.
We show the unrolled graphical representation of TopicRNN in Figure 1(a). We denote all model parameters as Θ = {Γ, V, B,W,Wc} (see Appendix A.1 for more details). Parameter Wc is for the inference network, which we will introduce below. The observations are the word sequences y1:T and stop word indicators l1:T .4 The log marginal likelihood of the sequence y1:T is
log p(y1:T , l1:T |ht) = log ∫ p(θ) T∏ t=1 p(yt|ht, lt, θ)p(lt|ht)dθ. (2)
Model inference. Direct optimization of Equation 2 is intractable so we use variational inference for approximating this marginal (Jordan et al., 1999). Let q(θ) be the variational distribution on the marginalized variable θ. We construct the variational objective function, also called the evidence lower bound (ELBO), as follows:
L(y1:T , l1:T |q(θ),Θ) , Eq(θ) [ T∑ t=1 log p(yt|ht, lt, θ) + log p(lt|ht) + log p(θ)− log q(θ) ] ≤ log p(y1:T , l1:T |ht,Θ).
Following the proposed variational autoencoder technique, we choose the form of q(θ) to be an inference network using a feed-forward neural network (Kingma and Welling, 2013; Miao et al., 2015). Let Xc ∈ N |Vc|+ be the term-frequency representation of y1:T excluding stop words (with Vc the vocabulary size without the stop words). The variational autoencoder inference network q(θ|Xc,Wc) with parameter Wc is a feed-forward neural network with ReLU activation units that projects Xc into a K-dimensional latent space. Specifically, we have
q(θ|Xc,Wc) = N(θ;µ(Xc), diag(σ2(Xc))), µ(Xc) = W1g(Xc) + a1,
log σ(Xc) = W2g(Xc) + a2,
where g(·) denotes the feed-forward neural network. The weight matrices W1, W2 and biases a1, a2 are shared across documents. Each document has its own µ(Xc) and σ(Xc) resulting in a unique distribution q(θ|Xc) for each document. The output of the inference network is a distribution on θ, which we regard as the summarization of the semantic information, similar to the topic proportions in latent topic models. We show the role of the inference network in Figure 1(b). During training, the parameters of the inference network and the model are jointly learned and updated via truncated backpropagation through time using the Adam algorithm (Kingma and Ba, 2014). We use stochastic samples from q(θ|Xc) and the reparameterization trick towards this end (Kingma and Welling, 2013; Rezende et al., 2014).
Generating sequential text and computing perplexity. Suppose we are given a word sequence y1:t−1, from which we have an initial estimation of q(θ|Xc). To generate the next word yt, we compute the probability distribution of yt given y1:t−1 in an online fashion. We choose θ to be a point estimate θ̂, the mean of its current distribution q(θ|Xc). Marginalizing over the stop word indicator lt which is unknown prior to observing yt, the approximate distribution of yt is
p(yt|y1:t−1) ≈ ∑ lt p(yt|ht, θ̂, lt)p(lt|ht).
The predicted word yt is a sample from this predictive distribution. We update q(θ|Xc) by including yt toXc if yt is not a stop word. However, updating q(θ|Xc) after each word prediction is expensive, so we use a sliding window as was done in Mikolov and Zweig (2012). To compute the perplexity, we use the approximate predictive distribution above.
Model Complexity. TopicRNN has a complexity of O(H × H + H × (C + K) + Wc), where H is the size of the hidden layer of the RNN, C is the vocabulary size, K is the dimension of the topic vector, and Wc is the number of parameters of the inference network. The contextual RNN of Mikolov and Zweig (2012) accounts forO(H×H+H×(C+K)), not including the pre-training process, which might require more parameters than the additional Wc in our complexity.
4Stop words can be determined using one of the several lists available online. For example, http://www. lextek.com/manuals/onix/stopwords2.html

4 EXPERIMENTS
We assess the performance of our proposed TopicRNN model on word prediction and sentiment analysis5. For word prediction we use the Penn TreeBank dataset, a standard benchmark for assessing new language models (Marcus et al., 1993). For sentiment analysis we use the IMDB 100k dataset (Maas et al., 2011), also a common benchmark dataset for this application6. We use RNN, LSTM, and GRU cells in our experiments leading to TopicRNN, TopicLSTM, and TopicGRU.

4.1 WORD PREDICTION
We first tested TopicRNN on the word prediction task using the Penn Treebank (PTB) portion of the Wall Street Journal. We use the standard split, where sections 0-20 (930K tokens) are used for training, sections 21-22 (74K tokens) for validation, and sections 23-24 (82K tokens) for testing (Mikolov et al., 2010). We use a vocabulary of size 10K that includes the special token unk for rare words and eos that indicates the end of a sentence. TopicRNN takes documents as inputs. We split the PTB data into blocks of 10 sentences to constitute documents as done by (Mikolov and Zweig, 2012). The inference network takes as input the bag-of-words representation of the input document. For that reason, the vocabulary size of the inference network is reduced to 9551 after excluding 449 pre-defined stop words.
In order to compare with previous work on contextual RNNs we trained TopicRNN using different network sizes. We performed word prediction using a recurrent neural network with 10 neurons,
5Our code will be made publicly available for reproducibility. 6These datasets are publicly available at http://www.fit.vutbr.cz/~imikolov/rnnlm/
simple-examples.tgz and http://ai.stanford.edu/~amaas/data/sentiment/.
100 neurons and 300 neurons. For these experiments, we used a multilayer perceptron with 2 hidden layers and 200 hidden units per layer for the inference network. The number of topics was tuned depending on the size of the RNN. For 10 neurons we used 18 topics. For 100 and 300 neurons we found 50 topics to be optimal. We used the validation set to tune the hyperparameters of the model. We used a maximum of 15 epochs for the experiments and performed early stopping using the validation set. For comparison purposes we did not apply dropout and used 1 layer for the RNN and its counterparts in all the word prediction experiments as reported in Table 2. One epoch for 10 neurons takes 2.5 minutes. For 100 neurons, one epoch is completed in less than 4 minutes. Finally, for 300 neurons one epoch takes less than 6 minutes. These experiments were ran on Microsoft Azure NC12 that has 12 cores, 2 Tesla K80 GPUs, and 112 GB memory. First, we show five randomly drawn topics in Table 1. These results correspond to a network with 100 neurons. We also illustrate some inferred topic distributions for several documents from TopicGRU in Figure 2. Similar to standard topic models, these distributions are also relatively peaky.
Next, we compare the performance of TopicRNN to our baseline contextual RNN using perplexity. Perplexity can be thought of as a measure of surprise for a language model. It is defined as the exponential of the average negative log likelihood. Table 2 summarizes the results for different network sizes. We learn three things from these tables. First, the perplexity is reduced the larger the network size. Second, RNNs with context features perform better than RNNs without context features. Third, we see that TopicRNN gives lower perplexity than the previous baseline result reported by Mikolov and Zweig (2012). Note that to compute these perplexity scores for word prediction we use a sliding window to compute θ as we move along the sequences. The topic vector θ that is used from the current batch of words is estimated from the previous batch of words. This enables fair comparison to previously reported results (Mikolov and Zweig, 2012).7
Another aspect of the TopicRNN model we studied is its capacity to generate coherent text. To do this, we randomly drew a document from the test set and used this document as seed input to the inference network to compute θ. Our expectation is that the topics contained in this seed document are reflected in the generated text. Table 3 shows generated text from models learned on the PTB and IMDB datasets. See Appendix A.3 for more examples.
7We adjusted the scores in Table 2 from what was previously reported after correcting a bug in the computation of the ELBO.

4.2 SENTIMENT ANALYSIS
We performed sentiment analysis using TopicRNN as a feature extractor on the IMDB 100K dataset. This data consists of 100,000 movie reviews from the Internet Movie Database (IMDB) website. The data is split into 75% for training and 25% for testing. Among the 75K training reviews, 50K are unlabelled and 25K are labelled as carrying either a positive or a negative sentiment. All 25K test reviews are labelled. We trained TopicRNN on 65K random training reviews and used the remaining 10K reviews for validation. To learn a classifier, we passed the 25K labelled training reviews through the learned TopicRNN model. We then concatenated the output of the inference network and the last state of the RNN for each of these 25K reviews to compute the feature vectors. We then used these feature vectors to train a neural network with one hidden layer, 50 hidden units, and a sigmoid activation function to predict sentiment, exactly as done in Le and Mikolov (2014).
To train the TopicRNN model, we used a vocabulary of size 5,000 and mapped all other words to the unk token. We took out 439 stop words to create the input of the inference network. We used 500 units and 2 layers for the inference network, and used 2 layers and 300 units per-layer for the
RNN. We chose a step size of 5 and defined 200 topics. We did not use any regularization such as dropout. We trained the model for 13 epochs and used the validation set to tune the hyperparameters of the model and track perplexity for early stopping. This experiment took close to 78 hours on a MacBook pro quad-core with 16GHz of RAM. See Appendix A.4 for the visualization of some of the topics learned from this data.
Table 4 summarizes sentiment classification results from TopicRNN and other methods. Our error rate is 6.28%.8 This is close to the state-of-the-art 5.91% (Miyato et al., 2016) despite that we do not use the labels and adversarial training in the feature extraction stage. Our approach is most similar to Le and Mikolov (2014), where the features were extracted in a unsupervised way and then a one-layer neural net was trained for classification.
Figure 3 shows the ability of TopicRNN to cluster documents using the feature vectors as created during the sentiment analysis task. Reviews with positive sentiment are coloured in green while reviews carrying negative sentiment are shown in red. This shows that TopicRNN can be used as an unsupervised feature extractor for downstream applications. Table 3 shows generated text from models learned on the PTB and IMDB datasets. See Appendix A.3 for more examples. The overall generated text from IMDB encodes a negative sentiment.

5 DISCUSSION AND FUTURE WORK
In this paper we introduced TopicRNN, a RNN-based language model that combines RNNs and latent topics to capture local (syntactic) and global (semantic) dependencies between words. The global dependencies as captured by the latent topics serve as contextual bias to an RNN-based language model. This contextual information is learned jointly with the RNN parameters by maximizing the evidence lower bound of variational inference. TopicRNN yields competitive per-word perplexity on the Penn Treebank dataset compared to previous contextual RNN models. We have reported a competitive classification error rate for sentiment analysis on the IMDB 100K dataset. We have also illustrated the capacity of TopicRNN to generate sensible topics and text. In future work, we will study the performance of TopicRNN when stop words are dynamically discovered during training. We will also extend TopicRNN to other applications where capturing context is important such as in dialog modeling. If successful, this will allow us to have a model that performs well across different natural language processing applications.
8The experiments were solely based on TopicRNN. Experiments using TopicGRU/TopicLSTM are being carried out and will be added as an extended version of this paper.

A APPENDIX
A.1 DIMENSION OF THE PARAMETERS OF THE MODEL:
We use the following notation: C is the vocabulary size (including stop words), H is the number of hidden units of the RNN, K is the number of topics, and E is the dimension of the inference network hidden layer. Table 5 gives the dimension of each of the parameters of the TopicRNN model (ignoring the biases).
A.2 DOCUMENTS USED TO INFER THE DISTRIBUTIONS ON FIGURE 2
Figure on the left: ’the’, ’market’, ’has’, ’grown’, ’relatively’, ’quiet’, ’since’, ’the’, ’china’, ’crisis’, ’but’, ’if’, ’the’, ’japanese’, ’return’, ’in’, ’force’, ’their’, ’financial’, ’might’, ’could’, ’compensate’, ’to’, ’some’, ’extent’, ’for’, ’local’, ’investors’, ""’"", ’<unk>’, ’commitment’, ’another’, ’and’, ’critical’, ’factor’, ’is’, ’the’, ’u.s.’, ’hong’, ’kong’, ""’s"", ’biggest’, ’export’, ’market’, ’even’, ’before’, ’the’, ’china’, ’crisis’, ’weak’, ’u.s.’, ’demand’, ’was’, ’slowing’, ’local’, ’economic’, ’growth’, ’<unk>’, ’strong’, ’consumer’, ’spending’, ’in’, ’the’, ’u.s.’, ’two’, ’years’, ’ago’, ’helped’, ’<unk>’, ’the’, ’local’, ’economy’, ’at’, ’more’, ’than’, ’twice’, ’its’, ’current’, ’rate’, ’indeed’, ’a’, ’few’, ’economists’, ’maintain’, ’that’, ’global’, ’forces’, ’will’, ’continue’, ’to’, ’govern’, ’hong’, ’kong’, ""’s"", ’economic’, ’<unk>’, ’once’, ’external’, ’conditions’, ’such’, ’as’, ’u.s.’, ’demand’, ’swing’, ’in’, ’the’, ’territory’, ""’s"", ’favor’, ’they’, ’argue’, ’local’, ’businessmen’, ’will’, ’probably’, ’overcome’, ’their’, ’N’, ’worries’, ’and’, ’continue’, ’doing’, ’business’, ’as’, ’usual’, ’but’, ’economic’, ’arguments’, ’however’, ’solid’, ’wo’, ""n’t"", ’necessarily’, ’<unk>’, ’hong’, ’kong’, ""’s"", ’N’, ’million’, ’people’, ’many’, ’are’, ’refugees’, ’having’, ’fled’, ’china’, ""’s"", ’<unk>’, ’cycles’, ’of’, ’political’, ’repression’, ’and’, ’poverty’, ’since’, ’the’, ’communist’, ’party’, ’took’, ’power’, ’in’, ’N’, ’as’, ’a’, ’result’, ’many’, ’of’, ’those’, ’now’, ’planning’, ’to’, ’leave’, ’hong’, ’kong’, ’ca’, ""n’t"", ’easily’, ’be’, ’<unk>’, ’by’, ’<unk>’, ’improvements’, ’in’, ’the’, ’colony’, ""’s"", ’political’, ’and’, ’economic’, ’climate’
Figure on the middle: ’it’, ’said’, ’the’, ’man’, ’whom’, ’it’, ’did’, ’not’, ’name’, ’had’, ’been’, ’found’, ’to’, ’have’, ’the’, ’disease’, ’after’, ’hospital’, ’tests’, ’once’, ’the’, ’disease’, ’was’, ’confirmed’, ’all’, ’the’, ’man’, ""’s"", ’associates’, ’and’, ’family’, ’were’, ’tested’, ’but’, ’none’, ’have’, ’so’, ’far’, ’been’, ’found’, ’to’, ’have’, ’aids’, ’the’, ’newspaper’, ’said’, ’the’, ’man’, ’had’, ’for’, ’a’, ’long’, ’time’, ’had’, ’a’, ’chaotic’, ’sex’, ’life’, ’including’, ’relations’, ’with’, ’foreign’, ’men’, ’the’, ’newspaper’, ’said’, ’the’, ’polish’, ’government’, ’increased’, ’home’, ’electricity’, ’charges’, ’by’, ’N’, ’N’, ’and’, ’doubled’, ’gas’, ’prices’, ’the’, ’official’, ’news’, ’agency’, ’<unk>’, ’said’, ’the’, ’increases’, ’were’, ’intended’, ’to’, ’bring’, ’<unk>’, ’low’, ’energy’, ’charges’, ’into’, ’line’, ’with’, ’production’, ’costs’, ’and’, ’compensate’, ’for’, ’a’, ’rise’, ’in’, ’coal’, ’prices’, ’in’, ’<unk>’, ’news’, ’south’, ’korea’, ’in’, ’establishing’, ’diplomatic’, ’ties’, ’with’, ’poland’, ’yesterday’, ’announced’, ’$’, ’N’, ’million’, ’in’, ’loans’, ’to’, ’the’, ’financially’, ’strapped’, ’warsaw’, ’government’, ’in’, ’a’, ’victory’, ’for’, ’environmentalists’, ’hungary’, ""’s"", ’parliament’, ’terminated’, ’a’, ’multibillion-dollar’, ’river’, ’<unk>’, ’dam’, ’being’, ’built’, ’by’, ’<unk>’, ’firms’, ’the’, ’<unk>’, ’dam’, ’was’, ’designed’, ’to’, ’be’, ’<unk>’, ’with’, ’another’, ’dam’, ’now’, ’nearly’, ’complete’, ’N’, ’miles’, ’<unk>’, ’in’, ’czechoslovakia’, ’in’, ’ending’, ’hungary’, ""’s"", ’part’, ’of’, ’the’, ’project’, ’parliament’, ’authorized’, ’prime’, ’minister’, ’<unk>’, ’<unk>’, ’to’, ’modify’, ’a’, ’N’, ’agreement’, ’with’, ’czechoslovakia’, ’which’, ’still’, ’wants’, ’the’, ’dam’, ’to’, ’be’, ’built’, ’mr.’, ’<unk>’, ’said’, ’in’, ’parliament’, ’that’, ’czechoslovakia’, ’and’, ’hungary’, ’would’, ’suffer’, ’environmental’, ’damage’, ’if’, ’the’, ’<unk>’, ’<unk>’, ’were’, ’built’, ’as’, ’planned’
Figure on the right: ’in’, ’hartford’, ’conn.’, ’the’, ’charter’, ’oak’, ’bridge’, ’will’, ’soon’, ’be’, ’replaced’, ’the’, ’<unk>’, ’<unk>’, ’from’, ’its’, ’<unk>’, ’<unk>’, ’to’, ’a’, ’park’, ’<unk>’, ’are’, ’possible’, ’citizens’, ’in’, ’peninsula’, ’ohio’, ’upset’, ’over’, ’changes’, ’to’, ’a’, ’bridge’, ’negotiated’, ’a’, ’deal’, ’the’, ’bottom’, ’half’, ’of’, ’the’, ’<unk>’, ’will’, ’be’, ’type’, ’f’, ’while’, ’the’, ’top’, ’half’, ’will’, ’have’, ’the’, ’old’, ’bridge’, ""’s"", ’<unk>’, ’pattern’, ’similarly’, ’highway’, ’engineers’, ’agreed’, ’to’, ’keep’, ’the’, ’old’, ’<unk>’, ’on’, ’the’, ’key’, ’bridge’, ’in’, ’washington’, ’d.c.’, ’as’, ’long’, ’as’, ’they’, ’could’, ’install’, ’a’, ’crash’, ’barrier’, ’between’, ’the’, ’sidewalk’, ’and’, ’the’, ’road’, ’<unk>’, ’<unk>’, ’drink’, ’carrier’, ’competes’, ’with’, ’<unk>’, ’<unk>’, ’<unk>’, ’just’, ’got’, ’easier’, ’or’, ’so’, ’claims’, ’<unk>’, ’corp.’, ’the’, ’maker’, ’of’, ’the’, ’<unk>’, ’the’, ’chicago’, ’company’, ""’s"", ’beverage’, ’carrier’, ’meant’, ’to’, ’replace’, ’<unk>’, ’<unk>’, ’at’, ’<unk>’, ’stands’, ’and’, ’fast-food’, ’outlets’, ’resembles’, ’the’, ’plastic’, ’<unk>’, ’used’, ’on’, ’<unk>’, ’of’, ’beer’, ’only’, ’the’, ’<unk>’, ’hang’, ’from’, ’a’, ’<unk>’, ’of’, ’<unk>’, ’the’, ’new’, ’carrier’, ’can’, ’<unk>’, ’as’, ’many’, ’as’, ’four’, ’<unk>’, ’at’, ’once’, ’inventor’, ’<unk>’, ’marvin’, ’says’, ’his’, ’design’, ’virtually’, ’<unk>’, ’<unk>’
A.3 MORE GENERATED TEXT FROM THE MODEL:
We illustrate below some generated text resulting from training TopicRNN on the PTB dataset. Here we used 50 neurons and 100 topics:
Text1: but the refcorp bond fund might have been unk and unk of the point rate eos house in national unk wall restraint in the property pension fund sold willing to zenith was guaranteed by $ N million at short-term rates maturities around unk products eos deposit posted yields slightly
Text2: it had happened by the treasury ’s clinical fund month were under national disappear institutions but secretary nicholas instruments succeed eos and investors age far compound average new york stock exchange bonds typically sold $ N shares in the N but paying yields further an average rate of long-term funds
We illustrate below some generated text resulting from training TopicRNN on the IMDB dataset. The settings are the same as for the sentiment analysis experiment:
the film ’s greatest unk unk and it will likely very nice movies to go to unk why various david proves eos the story were always well scary friend high can be a very strange unk unk is in love with it lacks even perfect for unk for some of the worst movies come on a unk gave a rock unk eos whatever let ’s possible eos that kyle can ’t different reasons about the unk and was not what you ’re not a fan of unk unk us rock which unk still in unk ’s music unk one as
A.4 TOPICS FROM IMDB:
Below we show some topics resulting from the sentiment analysis on the IMDB dataset. The total number of topics is 200. Note here all the topics turn around movies which is expected since all reviews are about movies.
","In this paper, we propose TopicRNN, a recurrent neural network (RNN)-based language model designed to directly capture the global semantic meaning relating words in a document via latent topics. Because of their sequential nature, RNNs are good at capturing the local structure of a word sequence – both semantic and syntactic – but might face difficulty remembering long-range dependencies. Intuitively, these long-range dependencies are of semantic nature. In contrast, latent topic models are able to capture the global semantic structure of a document but do not account for word ordering. The proposed TopicRNN model integrates the merits of RNNs and latent topic models: it captures local (syntactic) dependencies using an RNN and global (semantic) dependencies using latent topics. Unlike previous work on contextual RNN language modeling, our model is learned endto-end. Empirical results on word prediction show that TopicRNN outperforms existing contextual RNN baselines. In addition, TopicRNN can be used as an unsupervised feature extractor for documents. We do this for sentiment analysis on the IMDB movie review dataset and report an error rate of 6.28%. This is comparable to the state-of-the-art 5.91% resulting from a semi-supervised approach. Finally, TopicRNN also yields sensible topics, making it a useful alternative to document models such as latent Dirichlet allocation.",ICLR 2017 conference submission,True,,"This work combines a LDA-type topic model with a RNN and models this by having an additive effect on the predictive distribution via the topic parameters. A variational auto-encoder is used to infer the topic distribution for a given piece of text and the RNN is trained as a RNNLM. The last hidden state of the RNNLM and the topic parameters are then concatenated to use as a feature representation.

The paper is well written and easy to understand. Using the topic as an additive effect on the vocabulary allows for easy inference but intuitively I would expect the topic to affect the dynamics too, e.g. the state of the RNN. The results on using this model as a feature extractor for IMDB are quite strong. Is the RNN fine-tuned on the labelled IMDB data? However, the results for PTB are weaker. From the original paper, an ensemble of 2 LSTMs is able to match the topicRNN score. This method of jointly modelling topics and a language model seems effective and relatively easy to implement.

Finally, the IMDB result is no longer state of the art since this result appeared in May (Miyato et al., Adversarial Training Methods for Semi-Supervised Text Classification).

Some questions:
How important is the stop word modelling? What do the results look like if l_t = 0.5 for all t?

It seems surprising that the RNN was more effective than the LSTM. Was gradient clipping tried in the topicLSTM case? Do GRUs also fail to work?

It is also unfortunate that the model requires a stop-word list. Is the link in footnote 4 the one that is used in the experiments?

Does factoring out the topics in this way allow the RNN to scale better with more neurons? How reasonable does the topic distribution look for individual documents? How peaked do they tend to be? Can you show some examples of the inferred distribution? The topics look odd for IMDB with the top word of two of the topics being the same: 'campbell'. It would be interesting to compare these topics with those inferred by LDA on the same datasets.

Minor comments:
Below figure 2: GHz -> GB
\Gamma is not defined.

---

Though the have been attempts to incorporate both ""topic-like"" and ""sequence-like"" methods in the past (e.g, the work of Hanna Wallach, Amit Gruber and other), they were quite computationally expensive, especially when high-order ngrams are incorporated. This is a modern take on this challenge: using RNNs and the VAE / inference network framework. The results are quite convincing, and the paper is well written.
 
 Pros:
 -- clean and simple model
 -- sufficiently convincing experimentation
 
 Cons:
 -- other ways to model interaction between RNN and topic representation could be considered (see comments of R2 and R1)

---

We thank the reviewers and the anonymous commenters for the helpful feedback and questions!
 
We first summarize the main idea of this paper below:
 
Neural network-based language models have achieved state of the art results on many NLP tasks. One difficult problem is to capture long-range dependencies as motivated in the introduction of this paper. We propose to solve this by integrating latent topics as context and jointly training these contextual features with the parameters of an RNN language model. We provide a natural way of doing this integration by modeling stop words that are excluded by topic models but needed for sequential language models. This is done via binary classification where the probability of being a stop word is dictated by the hidden layer of the RNN. This modeling approach is possible when the contextual features as provided by the topics are passed directly to the softmax output layer of the RNN as additional bias. We illustrate the performance of this approach on two tasks and two datasets: word prediction on PTB and sentiment analysis on IMDB.  We provide competitive perplexity scores on PTB showing more generalization capabilities (for example we only need a TopicGRU with 100 neurons to achieve a better perplexity than stacking 2 LSTMs with 200 neurons each ---112.4 vs 115.9). ""This method of jointly modeling topics and a language model seems effective and relatively easy to implement."" quoted from AnonReviewer1.
 
We have revised the paper and added the following changes:
1- we added a line on the middle of page 7 to clarify even more how we compute the topic vector \theta using a sliding window for word prediction.
2- we added the test perplexity scores for TopicRNN, TopicLSTM, and TopicGRU as required by AnonReviewer3.
3- we added the inferred distributions from some documents as required by AnonReviewer1.
4- we added an explanation of why we passed the topics directly to the output layer at the bottom of page 4. 

We answer each reviewer individually. See below.

---

I have a question regrading on the language modeling part. I believe it seems unfair to get a global word distribution(i.e. document topic) first and then use it to do word prediction. The RNN model would never do this and would perform not very good on the very beginning of this article. So does the ppl performance increase comes from this? 

What if the RNN model gets a global embedding first and then do the word prediction?

---

This paper introduces a model that blends ideas from generative topic models with those from recurrent neural network language models. The authors evaluate the proposed approach on a document level classification benchmark as well as a language modeling benchmark and it seems to work well. There is also some analysis as to topics learned by the model and its ability to generate text. Overall the paper is clearly written and with the code promised by the authors others should be able to re-implement the approach. I have 2 potentially major questions I would ask the authors to address:

1 - LDA topic models make an exchangability (bag of words) assumption. The discussion of the generative story for TopicRNN should explicitly discuss whether this assumption is also made. On the surface it appears it is since y_t is sampled using only the document topic vector and h_t but we know that in practice h_t comes from a recurrent model that observes y_t-1. Not clear how this clean exposition of the generative model relates to what is actually done. In the Generating sequential text section it’s clear the topic model can’t generate words without using y_1 - t-1 but this seems inconsistent with the generative model specification. This needs to be shown in the paper and made clear to have a complete paper.


2 -  The topic model only allows for linear interactions of the topic vector theta. It seems like this might be required to keep the generative model tractable but seems like a very poor assumption. We would expect the topic representation to have rich interactions with a language model to create nonlinear adjustments to word probabilities for a document. Please add discussion as to why this modeling choice exists and if possible how future work could modify that assumption (or explain why it’s not such a bad assumption as one might imagine)




Figure 2 colors very difficult to distinguish.

---

This paper presents TopicRNN, a combination of LDA and RNN that augments traditional RNN with latent topics by having a switching variable that includes/excludes additive effects from latent topics when generating a word. 
Experiments on two tasks are performed: language modeling on PTB, and sentiment analysis on IMBD. 
The authors show that TopicRNN outperforms vanilla RNN on PTB and achieves SOTA result on IMDB.

Some questions and comments:
- In Table 2, how do you use LDA features for RNN (RNN LDA features)? 
- I would like to see results from LSTM included here, even though it is lower perplexity than TopicRNN. I think it's still useful to see how much adding latent topics close the gap between RNN and LSTM.
- The generated text in Table 3 are not meaningful to me. What is this supposed to highlight? Is this generated text for topic ""trading""? What about the IMDB one?
- How scalable is the proposed method for large vocabulary size (>10K)?
- What is the accuracy on IMDB if the extracted features is used directly to perform classification? (instead of being passed to a neural network with one hidden state). I think this is a fairer comparison to BoW, LDA, and SVM methods presented as baselines.

---

This work combines a LDA-type topic model with a RNN and models this by having an additive effect on the predictive distribution via the topic parameters. A variational auto-encoder is used to infer the topic distribution for a given piece of text and the RNN is trained as a RNNLM. The last hidden state of the RNNLM and the topic parameters are then concatenated to use as a feature representation.

The paper is well written and easy to understand. Using the topic as an additive effect on the vocabulary allows for easy inference but intuitively I would expect the topic to affect the dynamics too, e.g. the state of the RNN. The results on using this model as a feature extractor for IMDB are quite strong. Is the RNN fine-tuned on the labelled IMDB data? However, the results for PTB are weaker. From the original paper, an ensemble of 2 LSTMs is able to match the topicRNN score. This method of jointly modelling topics and a language model seems effective and relatively easy to implement.

Finally, the IMDB result is no longer state of the art since this result appeared in May (Miyato et al., Adversarial Training Methods for Semi-Supervised Text Classification).

Some questions:
How important is the stop word modelling? What do the results look like if l_t = 0.5 for all t?

It seems surprising that the RNN was more effective than the LSTM. Was gradient clipping tried in the topicLSTM case? Do GRUs also fail to work?

It is also unfortunate that the model requires a stop-word list. Is the link in footnote 4 the one that is used in the experiments?

Does factoring out the topics in this way allow the RNN to scale better with more neurons? How reasonable does the topic distribution look for individual documents? How peaked do they tend to be? Can you show some examples of the inferred distribution? The topics look odd for IMDB with the top word of two of the topics being the same: 'campbell'. It would be interesting to compare these topics with those inferred by LDA on the same datasets.

Minor comments:
Below figure 2: GHz -> GB
\Gamma is not defined.

---

This work combines a LDA-type topic model with a RNN and models this by having an additive effect on the predictive distribution via the topic parameters. A variational auto-encoder is used to infer the topic distribution for a given piece of text and the RNN is trained as a RNNLM. The last hidden state of the RNNLM and the topic parameters are then concatenated to use as a feature representation.

The paper is well written and easy to understand. Using the topic as an additive effect on the vocabulary allows for easy inference but intuitively I would expect the topic to affect the dynamics too, e.g. the state of the RNN. The results on using this model as a feature extractor for IMDB are quite strong. Is the RNN fine-tuned on the labelled IMDB data? However, the results for PTB are weaker. From the original paper, an ensemble of 2 LSTMs is able to match the topicRNN score. This method of jointly modelling topics and a language model seems effective and relatively easy to implement.

Finally, the IMDB result is no longer state of the art since this result appeared in May (Miyato et al., Adversarial Training Methods for Semi-Supervised Text Classification).

Some questions:
How important is the stop word modelling? What do the results look like if l_t = 0.5 for all t?

It seems surprising that the RNN was more effective than the LSTM. Was gradient clipping tried in the topicLSTM case? Do GRUs also fail to work?

It is also unfortunate that the model requires a stop-word list. Is the link in footnote 4 the one that is used in the experiments?

Does factoring out the topics in this way allow the RNN to scale better with more neurons? How reasonable does the topic distribution look for individual documents? How peaked do they tend to be? Can you show some examples of the inferred distribution? The topics look odd for IMDB with the top word of two of the topics being the same: 'campbell'. It would be interesting to compare these topics with those inferred by LDA on the same datasets.

Minor comments:
Below figure 2: GHz -> GB
\Gamma is not defined.

---

Though the have been attempts to incorporate both ""topic-like"" and ""sequence-like"" methods in the past (e.g, the work of Hanna Wallach, Amit Gruber and other), they were quite computationally expensive, especially when high-order ngrams are incorporated. This is a modern take on this challenge: using RNNs and the VAE / inference network framework. The results are quite convincing, and the paper is well written.
 
 Pros:
 -- clean and simple model
 -- sufficiently convincing experimentation
 
 Cons:
 -- other ways to model interaction between RNN and topic representation could be considered (see comments of R2 and R1)

---

We thank the reviewers and the anonymous commenters for the helpful feedback and questions!
 
We first summarize the main idea of this paper below:
 
Neural network-based language models have achieved state of the art results on many NLP tasks. One difficult problem is to capture long-range dependencies as motivated in the introduction of this paper. We propose to solve this by integrating latent topics as context and jointly training these contextual features with the parameters of an RNN language model. We provide a natural way of doing this integration by modeling stop words that are excluded by topic models but needed for sequential language models. This is done via binary classification where the probability of being a stop word is dictated by the hidden layer of the RNN. This modeling approach is possible when the contextual features as provided by the topics are passed directly to the softmax output layer of the RNN as additional bias. We illustrate the performance of this approach on two tasks and two datasets: word prediction on PTB and sentiment analysis on IMDB.  We provide competitive perplexity scores on PTB showing more generalization capabilities (for example we only need a TopicGRU with 100 neurons to achieve a better perplexity than stacking 2 LSTMs with 200 neurons each ---112.4 vs 115.9). ""This method of jointly modeling topics and a language model seems effective and relatively easy to implement."" quoted from AnonReviewer1.
 
We have revised the paper and added the following changes:
1- we added a line on the middle of page 7 to clarify even more how we compute the topic vector \theta using a sliding window for word prediction.
2- we added the test perplexity scores for TopicRNN, TopicLSTM, and TopicGRU as required by AnonReviewer3.
3- we added the inferred distributions from some documents as required by AnonReviewer1.
4- we added an explanation of why we passed the topics directly to the output layer at the bottom of page 4. 

We answer each reviewer individually. See below.

---

I have a question regrading on the language modeling part. I believe it seems unfair to get a global word distribution(i.e. document topic) first and then use it to do word prediction. The RNN model would never do this and would perform not very good on the very beginning of this article. So does the ppl performance increase comes from this? 

What if the RNN model gets a global embedding first and then do the word prediction?

---

This paper introduces a model that blends ideas from generative topic models with those from recurrent neural network language models. The authors evaluate the proposed approach on a document level classification benchmark as well as a language modeling benchmark and it seems to work well. There is also some analysis as to topics learned by the model and its ability to generate text. Overall the paper is clearly written and with the code promised by the authors others should be able to re-implement the approach. I have 2 potentially major questions I would ask the authors to address:

1 - LDA topic models make an exchangability (bag of words) assumption. The discussion of the generative story for TopicRNN should explicitly discuss whether this assumption is also made. On the surface it appears it is since y_t is sampled using only the document topic vector and h_t but we know that in practice h_t comes from a recurrent model that observes y_t-1. Not clear how this clean exposition of the generative model relates to what is actually done. In the Generating sequential text section it’s clear the topic model can’t generate words without using y_1 - t-1 but this seems inconsistent with the generative model specification. This needs to be shown in the paper and made clear to have a complete paper.


2 -  The topic model only allows for linear interactions of the topic vector theta. It seems like this might be required to keep the generative model tractable but seems like a very poor assumption. We would expect the topic representation to have rich interactions with a language model to create nonlinear adjustments to word probabilities for a document. Please add discussion as to why this modeling choice exists and if possible how future work could modify that assumption (or explain why it’s not such a bad assumption as one might imagine)




Figure 2 colors very difficult to distinguish.

---

This paper presents TopicRNN, a combination of LDA and RNN that augments traditional RNN with latent topics by having a switching variable that includes/excludes additive effects from latent topics when generating a word. 
Experiments on two tasks are performed: language modeling on PTB, and sentiment analysis on IMBD. 
The authors show that TopicRNN outperforms vanilla RNN on PTB and achieves SOTA result on IMDB.

Some questions and comments:
- In Table 2, how do you use LDA features for RNN (RNN LDA features)? 
- I would like to see results from LSTM included here, even though it is lower perplexity than TopicRNN. I think it's still useful to see how much adding latent topics close the gap between RNN and LSTM.
- The generated text in Table 3 are not meaningful to me. What is this supposed to highlight? Is this generated text for topic ""trading""? What about the IMDB one?
- How scalable is the proposed method for large vocabulary size (>10K)?
- What is the accuracy on IMDB if the extracted features is used directly to perform classification? (instead of being passed to a neural network with one hidden state). I think this is a fairer comparison to BoW, LDA, and SVM methods presented as baselines.

---

This work combines a LDA-type topic model with a RNN and models this by having an additive effect on the predictive distribution via the topic parameters. A variational auto-encoder is used to infer the topic distribution for a given piece of text and the RNN is trained as a RNNLM. The last hidden state of the RNNLM and the topic parameters are then concatenated to use as a feature representation.

The paper is well written and easy to understand. Using the topic as an additive effect on the vocabulary allows for easy inference but intuitively I would expect the topic to affect the dynamics too, e.g. the state of the RNN. The results on using this model as a feature extractor for IMDB are quite strong. Is the RNN fine-tuned on the labelled IMDB data? However, the results for PTB are weaker. From the original paper, an ensemble of 2 LSTMs is able to match the topicRNN score. This method of jointly modelling topics and a language model seems effective and relatively easy to implement.

Finally, the IMDB result is no longer state of the art since this result appeared in May (Miyato et al., Adversarial Training Methods for Semi-Supervised Text Classification).

Some questions:
How important is the stop word modelling? What do the results look like if l_t = 0.5 for all t?

It seems surprising that the RNN was more effective than the LSTM. Was gradient clipping tried in the topicLSTM case? Do GRUs also fail to work?

It is also unfortunate that the model requires a stop-word list. Is the link in footnote 4 the one that is used in the experiments?

Does factoring out the topics in this way allow the RNN to scale better with more neurons? How reasonable does the topic distribution look for individual documents? How peaked do they tend to be? Can you show some examples of the inferred distribution? The topics look odd for IMDB with the top word of two of the topics being the same: 'campbell'. It would be interesting to compare these topics with those inferred by LDA on the same datasets.

Minor comments:
Below figure 2: GHz -> GB
\Gamma is not defined.",,3.0,,3.0,2.0,7.0,,,3.6666666666666665,,3.0
448,"DEEP INFORMATION PROPAGATION
Authors: Samuel S. Schoenholz, Justin Gilmer, Surya Ganguli
Source file: 448.pdf

ABSTRACT
We study the behavior of untrained neural networks whose weights and biases are randomly distributed using mean field theory. We show the existence of depth scales that naturally limit the maximum depth of signal propagation through these random networks. Our main practical result is to show that random networks may be trained precisely when information can travel through them. Thus, the depth scales that we identify provide bounds on how deep a network may be trained for a specific choice of hyperparameters. As a corollary to this, we argue that in networks at the edge of chaos, one of these depth scales diverges. Thus arbitrarily deep networks may be trained only sufficiently close to criticality. We show that the presence of dropout destroys the order-to-chaos critical point and therefore strongly limits the maximum trainable depth for random networks. Finally, we develop a mean field theory for backpropagation and we show that the ordered and chaotic phases correspond to regions of vanishing and exploding gradient respectively.

1 INTRODUCTION
Deep neural network architectures have become ubiquitous in machine learning. The success of deep networks is due to the fact that they are highly expressive (Montufar et al., 2014) while simultaneously being relatively easy to optimize (Choromanska et al., 2015; Goodfellow et al., 2014) with strong generalization properties (Recht et al., 2015). Consequently, developments in machine learning often accompany improvements in our ability to train increasingly deep networks. Despite this, designing novel network architectures is frequently equal parts art and science. This is, in part, because a general theory for neural networks that might inform design decisions has lagged behind the feverish pace of design.
A pair of recent papers (Poole et al., 2016; Raghu et al., 2016) demonstrated that random neural networks are exponentially expressive in their depth. Central to their approach was the consideration of networks after random initialization, whose weights and biases were i.i.d. Gaussian distributed. In particular the paper by Poole et al. (2016) developed a “mean field” formalism for treating wide, untrained, neural networks. They showed that these mean field networks exhibit an order-to-chaos transition as a function of the weight and bias variances. Notably the mean field formalism is not closely tied to a specific choice of activation function or loss.
In this paper, we demonstrate the existence of several characteristic “depth” scales that emerge naturally and control signal propagation in these random networks. We then show that one of these depth scales, ξc, diverges at the boundary between order and chaos. This result is insensitive to many architectural decisions (such as choice of activation function) and will generically be true at any order-to-chaos transition. We then extend these results to include dropout and we show that even small amounts of dropout destroys the order-to-chaos critical point and consequently removes the divergence in ξc. Together these results bound the depth to which signal may propagate through random neural networks.
We then develop a corresponding mean field model for gradients and we show that a duality exists between the forward propagation of signals and the backpropagation of gradients. The ordered and chaotic phases that Poole et al. (2016) identified correspond to regions of vanishing and exploding gradients, respectively. We demonstrate the validity of this mean field theory by computing gradients of random networks on MNIST. This provides a formal explanation of the ‘vanishing gradients’
∗Work done as a member of the Google Brain Residency program (g.co/brainresidency)
phenomenon that has long been observed in neural networks (Bengio et al., 1993). We continue to show that the covariance between two gradients is controlled by the same depth scale that limits correlated signal propagation in the forward direction.
Finally, we hypothesize that a necessary condition for a random neural network to be trainable is that information should be able to pass through it. Thus, the depth-scales identified here bound the set of hyperparameters that will lead to successful training. To test this ansatz we train ensembles of deep, fully connected, feed-forward neural networks of varying depth on MNIST and CIFAR10, with and without dropout. Our results confirm that neural networks are trainable precisely when their depth is not much larger than ξc. This result is dataset independent and is, therefore, a universal function of network architecture.
A corollary of these result is that asymptotically deep neural networks should be trainable provided they are initialized sufficiently close to the order-to-chaos transition. The notion of “edge of chaos” initialization has been explored previously. Such investigations have been both direct as in Bertschinger et al. (2005); Glorot & Bengio (2010) or indirect, through initialization schemes that favor deep signal propagation such as batch normalization (Ioffe & Szegedy, 2015), orthogonal matrix initialization (Saxe et al., 2014), random walk initialization (Sussillo & Abbott, 2014), composition kernels (Daniely et al., 2016), or residual network architectures (He et al., 2015). The novelty of the work presented here is two-fold. First, our framework predicts the depth at which networks may be trained even far from the order-to-chaos transition. While a skeptic might ask when it would be profitable to initialize a network far from criticality, we respond by noting that there are architectures (such as neural networks with dropout) where no critical point exists and so this more general framework is needed. Second, our work provides a formal, as opposed to intuitive, explanation for why very deep networks can only be trained near the edge of chaos.

2 BACKGROUND
We begin by recapitulating the mean-field formalism developed in Poole et al. (2016). Consider a fully-connected, untrained, feed-forward, neural network of depth L with layer width Nl and some nonlinearity φ : R → R. Since this is an untrained neural network we suppose that its weights and biases are respectively i.i.d. as W lij ∼ N(0, σ2w/Nl) and bli ∼ N(0, σ2b ). Notationally we set zli to be the pre-activations of the lth layer and yl+1i to be the activations of that layer. Finally, we take the input to the network to be y0i = xi. The propagation of a signal through the network is described by the pair of equations,
zli = ∑ j W lijy l j + b l i y l+1 i = φ(z l i). (1)
Since the weights and biases are randomly distributed, these equations define a probability distribution on the activations and pre-activations over an ensemble of untrained neural networks. The “mean-field” approximation is then to replace zli by a Gaussian whose first two moments match those of zli. For the remainder of the paper we will take the mean field approximation as given.
Consider first the evolution of a single input, xi;a, as it evolves through the network (as quantified by yli;a and z l i;a). Since the weights and biases are independent with zero mean, the first two moments of the pre-activations in the same layer will be,
E[zli;a] = 0 E[zli;azlj;a] = qlaaδij (2)
where δij is the Kronecker delta. Here qlaa is the variance of the pre-activations in the lth layer due to an input xi;a and it is described by the recursion relation,
qlaa = σ 2 w
∫ Dzφ2 (√ ql−1aa z ) + σ2b (3)
where ∫ Dz = 1√
2π
∫ dze− 1 2 z 2
is the measure for a standard Gaussian distribution. Together these equations completely describe the evolution of a single input through a mean field neural network. For any choice of σ2w and σ 2 b with bounded φ, eq. 3 has a fixed point at q ∗ = liml→∞ qlaa.
The propagation of a pair of signals, x0i;a and x 0 i;b, through this network can be understood similarly. Here the mean pre-activations are trivially the same as in the single-input case. The independence
of the weights and biases implies that the covariance between different pre-activations in the same layer will be given by, E[zli;azlj;b] = qlabδij . The covariance, qlab, will be given by the recurrence relation,
qlab = σ 2 w ∫ Dz1Dz2φ(u1)φ(u2) + σ2b (4)
where u1 = √ ql−1aa z1 and u2 = √ ql−1bb ( cl−1ab z1 + √ 1− (cl−1ab )2z2 ) , with clab = q l ab/ √ qlaaq l bb,
are Gaussian approximations to the pre-activations in the preceding layer with the correct covariance matrix. Moreover clab is the correlation between the two inputs after l layers.
Examining eq. 4 it is clear that c∗ = 1 is a fixed point of the recurrence relation. To determine whether or not the c∗ = 1 is an attractive fixed point the quantity,
χ1 = ∂clab ∂cl−1ab = σ2w
∫ Dz [ φ′ (√ q∗z )]2
(5)
is introduced. Poole et al. (2016) note that the c∗ = 1 fixed point is stable if χ1 < 1 and is unstable otherwise. Thus, χ1 = 1 represents a critical line separating an ordered phase (in which c∗ = 1 and all inputs end up asymptotically correlated) and a chaotic phase (in which c∗ < 1 and all inputs end up asymptotically decorrelated). For the case of φ = tanh, the phase diagram in fig. 1 (a) is observed.

3 ASYMPTOTIC EXPANSIONS AND DEPTH SCALES
Our first contribution is to demonstrate the existence of two depth-scales that arise naturally within the framework of mean field neural networks. Motivating the existence of these depth-scales, we iterate eq. 3 and 4 until convergence for many values of σ2w between 0.1 and 3.0 and with σ 2 b = 0.05 starting with q0aa = q 0 bb = 0.8 and c 0 ab = 0.6. We see, in fig. 1 (b) and (c), that the manner in which both qlaa approaches q ∗ and clab approaches c
∗ is exponential over many orders of magnitude. We therefore anticipate that asymptotically |qlaa − q∗| ∼ e−l/ξq and |clab − c∗| ∼ e−l/ξc for sufficiently large l. Here, ξq and ξc define depth-scales over which information may propagate about the magnitude of a single input and the correlation between two inputs respectively.
We will presently prove that qlaa and c l ab are asymptotically exponential. In both cases we will use the same fundamental strategy wherein we expand one of the recurrence relations (either eq. 3 or eq. 4) about its fixed point to get an approximate “asymptotic” recurrence relation. We find that this asymptotic recurrence relation in turn implies exponential decay towards the fixed point over a depth-scale, ξx.
We first analyze eq. 3 and identify a depth-scale at which information about a single input may propagate. Let qlaa = q ∗ + l. By construction so long as liml→∞ qlaa = q ∗ exists it follows that
l → 0 as l→∞. Eq. 3 may be expanded to lowest order in l to arrive at an asymptotic recurrence relation (see Appendix 7.1),
l+1 = l [ χ1 + σ 2 w ∫ Dzφ′′ (√ q∗z ) φ (√ q∗z )] +O ( ( l)2 ) . (6)
Notably, the term multiplying l is a constant. It follows that for large l the asymptotic recurrence relation has an exponential solution, l ∼ e−l/ξq , with ξq given by
ξ−1q = − log [ χ1 + σ 2 w ∫ Dzφ′′ (√ q∗z ) φ (√ q∗z )] . (7)
This establishes ξq as a depth scale that controls how deep information from a single input may penetrate into a random neural network.
Next, we consider eq. 4. Using a similar argument (detailed in Appendix 7.2) we can expand about clab = c ∗ + l to find an asymptotic recurrence relation,
l+1 = l [ σ2w ∫ Dz1Dz2φ′(u∗1)φ′(u∗2) ] +O(( l)2). (8)
Here u∗1 = √ q∗z1 and u∗2 = √ q∗(c∗z1 + √ 1− (c∗)2z2). Thus, once again, we expect that for large l this recurrence will have an exponential solution, l ∼ e−l/ξc , with ξc given by
ξ−1c = − log [ σ2w ∫ Dz1Dz2φ′(u∗1)φ′(u∗2) ] . (9)
In the ordered phase c∗ = 1 and so ξ−1c = − logχ1. Since the transition between order and chaos occurs when χ1 = 1 it follows that ξc diverges at any order-to-chaos transition so long as q∗ and c∗ exist.
These results can be investigated intuitively by plotting cl+1ab vs c l ab in fig. 2 (a). In the ordered phase there is only a single fixed point, clab = 1. In the chaotic regime we see that a second fixed point develops and the clab = 1 point becomes unstable. We see that the linearization about the fixed points becomes significantly closer to the trivial map near the order-to-chaos transition.
To test these claims we measure ξq and ξc directly by iterating the recurrence relations for qlaa and clab as before with q 0 aa = q 0 bb = 0.8 and c 0 ab = 0.6. In this case we consider values of σ 2 w between
0.1 and 3.0 and σ2b between 0.01 and 0.3. For each hyperparameter settings we fit the resulting residuals, |qlaa − q∗| and |clab − c∗|, to exponential functions and infer the depth-scale. We then compare this measured depth-scale to that predicted by the asymptotic expansion. The result of this measurement is shown in fig. 2. In general we see that the agreement is quite good. As expected we see that ξc diverges at the critical point.
As observed in Poole et al. (2016) we see that the depth scale for the propagation of information in a single input, ξq , is consistently finite and significantly shorter than ξc. To understand why this is the case consider eq. 6 and note that for tanh nonlinearities the second term is always negative. Thus, even as χ1 approaches 1 we expect χ1 + σ2w ∫ Dzφ′′(√q∗z)φ(√q∗z) to be substantially smaller than 1.

3.1 DROPOUT
The mean field formalism can be extended to include dropout. The main contribution here will be to argue that even infinitesimal amounts of dropout destroys the mean field critical point, and therefore limits the trainable network depth. In the presence of dropout the propagation equation, eq. 1, becomes,
zli = 1
ρ ∑ j W lijp l jy l j + b l i (10)
where pj ∼ Bernoulli(ρ) and ρ is the dropout rate. As is typically the case we have re-scaled the sum by ρ−1 so that the mean of the pre-activation is invariant with respect to our choice of dropout rate.
Following a similar procedure to the original mean field calculation consider the fate of two inputs, x0i;a and x 0 i;b, as they are propagated through such a random network. We take the dropout masks to be chosen independently for the two inputs mimicking the manner in which dropout is employed in practice. With dropout the diagonal term in the covariance matrix will be (see Appendix 7.3),
q̄laa = σ2w ρ
∫ Dzφ2 (√ q̄l−1aa z ) + σ2b . (11)
The variance of a single input with dropout will therefore propagate in an identical fashion to the vanilla case with a re-scaling σ2w → σ2w/ρ. Intuitively, this result implies that, for the case of a single input, the presence of dropout simply increases the effective variance of the weights.
Computing the off-diagonal term of the covariance matrix similarly (see Appendix 7.4),
q̄lab = σ 2 w ∫ Dz1Dz2φ(ū1)φ(ū2) + σ2b (12)
with ū1, ū2, and c̄lab defined by analogy to the mean field equations without dropout. Here, unlike in the case of a single input, the recurrence relation is identical to the recurrence relation without dropout. To see that c̄∗ = 1 is no longer a fixed point of these dynamics consider what happens to eq. 12 when we input c̄l = 1. For simplicity, we leverage the short range of ξq to replace q̄laa = q̄ l bb = q̄ ∗. We find (see Appendix 7.5),
c̄l+1ab = 1− 1− ρ ρq̄∗ σ2w
∫ Dzφ2 (√ q̄∗z ) . (13)
The second term is positive for any ρ < 1. This implies that if c̄lab = 1 for any l then c̄ l+1 ab < 1. Thus, c∗ = 1 is not a fixed point of eq. 12 for any ρ < 1. Since eq. 12 is identical in form to eq. 4 it follows that the depth scale for signal propagation with dropout will likewise be given by eq. 9 with the substitutions q∗ → q̄∗ and c∗ → c̄∗ computed using eq. 11 and eq. 12 respectively. Importantly, since there is no longer a sharp critical point with dropout we do not expect a diverging depth scale.
As in networks without dropout we plot, in fig. 3 (a), the iterative map c̄l+1ab as a function of c̄ l ab. Most significantly, we see that the c̄lab = 1 is no longer a fixed point of the dynamics. Instead, as the dropout rate increases c̄lab gets mapped to decreasing values and the fixed point monotonically decreases.
To test these results we plot in fig. 3 (b) the asymptotic correlation, c∗, as a function of σ2w for different values of dropout from ρ = 0.8 to ρ = 1.0. As expected, we see that for all ρ < 1 there is no sharp transition between c∗ = 1 and c∗ < 1. Moreover as the dropout rate increases the correlation c∗ monotonically decreases. Intuitively this makes sense. Identical inputs passed through two different dropout masks will become increasingly dissimilar as the dropout rate increases. In fig. 3 (c) we show the depth scale, ξc, as a function of σ2w for the same range of dropout probabilities. We find that, as predicted, the depth of signal propagation with dropout is drastically reduced and, importantly, there is no longer a divergence in ξc. Increasing the dropout rate continues to decrease the correlation depth for constant σ2w.

4 GRADIENT BACKPROPAGATION
There is a duality between the forward propagation of signals and the backpropagation of gradients. To elucidate this connection consider the backpropagation equations given a loss E,
∂E
∂W lij = δliφ(z l−1 j ) δ l i = φ ′(zli) ∑ j δl+1j W l+1 ji (14)
with the identification δli = ∂E/∂z l i. Within mean field theory, it is clear that the scale of fluctuations of the gradient of weights in a layer will be proportional to E[(δli)2] (see appendix 7.6). In contrast to the pre-activations in forward propagation (eq. 1), the δli will typically not be Gaussian distributed even in the large layer width limit.
Nonetheless, we can work out a recurrence relation for the variance of the error, q̃ laa = E[(δli)2], leveraging the Gaussian ansatz on the pre-activations. In order to do this, however, we must first make an additional approximation that the weights used during forward propagation are drawn independently from the weights used in backpropagation. This approximation is similar in spirit to the vanilla mean field approximation and is reminiscent of work on feedback alignment (Lillicrap et al., 2014). With this in mind we arrive at the recurrence (see appendix 7.7),
q̃ laa = q̃ l+1 aa Nl+1 Nl χ1. (15)
The presence of χ1 in the above equation should perhaps not be surprising. In Poole et al. (2016) they show that χ1 is intimately related to the tangent space of a given layer in mean field neural
networks. We note that the backpropagation recurrence features an explicit dependence on the ratio of widths of adjacent layers of the network, Nl+1/Nl. Here we will consider exclusively constant width networks where this factor is unity. For a discussion of the case of unequal layer widths see Glorot & Bengio (2010).
Since χ1 depends only on the asymptotic q∗ it follows that for constant width networks we expect eq. 15 to again have an exponential solution with,
q̃ laa = q̃ L aae −(L−l)/ξ∇ ξ−1 ∇ = − logχ1. (16)
Note that here ξ−1 ∇ = − logχ1 both above and below the transition. It follows that ξ∇ can be both positive and negative. We conclude that there should be three distinct regimes for the gradients.
1. In the ordered phase, χ1 < 1 and so ξ∇ > 0. We therefore expect gradients to vanish over a depth |ξ∇ |. 2. At criticality, χ1 → 1 and so ξ∇ →∞. Here gradients should be stable regardless of depth. 3. In the chaotic phase, χ1 > 1 and so ξ∇ < 0. It follows that in this regime gradients should
explode over a depth |ξ∇ |.
Intuitively these three regimes make sense. To see this, recall that perturbations to a weight in layer l can alternatively be viewed as perturbations to the pre-activations in the same layer. In the ordered phase both the perturbed signal and the unperturbed signal will be asymptotically mapped to the same point and the derivative will be small. In the chaotic phase the perturbed and unperturbed signals will become asymptotically decorrelated and the gradient will be large.
To investigate these predictions we construct deep random networks of depth L = 240 and layerwidth Nl = 300. We then consider the cross-entropy loss of these networks on MNIST. In fig. 4 (a) we plot the layer-by-layer 2-norm of the gradient, ||∇W labE|| 2 2, as a function of layer, l, for different values of σ2w. We see that ||∇W labE|| 2 2 behaves exponentially over many orders of magnitude. Moreover, we see that the gradient vanishes in the ordered phase and explodes in the chaotic phase. We test the quantitative predictions of eq. 16 in fig. 4 (b) where we compare |ξ∇ | as predicted from theory with the measured depth-scale constructed from exponential fits to the gradient data. Here we see good quantitative agreement between the theoretical predictions from mean field random networks and experimentally realized networks. Together these results suggest that the approximations on the backpropagation equations were representative of deep, wide, random networks.
Finally, we show that the depth scale for correlated signal propagation likewise controls the depth at which information stored in the covariance between gradients can survive. The existence of
consistent gradients across similar samples from a training set ought to be especially important for determining whether or not a given neural network architecture can be trained. To establish this depth-scale first note (see Appendix 7.8) that the covariance between gradients of two different inputs, xi;1 and xi;2, will be proportional to (∇W lijEa) · (∇W lijEb) ∼ E[δ l i;aδ l i;b] = q̃ l ab where Ea is the loss evaluated on xi;a and δi;a = ∂Ea/∂zli;a are appropriately defined errors.
It can be shown (see Appendix 7.9) that q̃ lab features the recurrence relation,
q̃ lab = q̃ l+1 ab Nl+1 Nl+2 σ2w
∫ Dz1Dz2φ′(u1)φ′(u2) (17)
where u1 and u2 are defined similarly as for the forward pass. Expanding asymptotically it is clear that to zeroth order in l, q̃lab will have an exponential solution with q̃ l ab = q̃ L abe −(L−l)/ξc with ξc as defined in the forward pass.

5 EXPERIMENTAL RESULTS
Taken together, the results of this paper lead us to the following hypothesis: a necessary condition for a random network to be trained is that information about the inputs should be able to propagate forward through the network, and information about the gradients should be able to propagate backwards through the network. The preceding analysis shows that networks will have this property precisely when the network depth, L, is not much larger than the depth-scale ξc. This criterion is data independent and therefore offers a “universal” constraint on the hyperparameters that depends on network architecture alone. We now explore this relationship between depth of signal propagation and network trainability empirically.
To investigate this prediction, we consider random networks of depth 10 ≤ L ≤ 300 and 1 ≤ σ2w ≤ 4 with σ2b = 0.05. We train these networks using Stochastic Gradient Descent (SGD) and RMSProp
on MNIST and CIFAR10. We use a learning rate of 10−3 for SGD when L . 200, 10−4 for larger L, and 10−5 for RMSProp. These learning rates were selected by grid search between 10−6 and 10−2 in exponentially spaced steps of size 10. We note that the depth dependence of learning rate was explored in detail in Saxe et al. (2014). In fig. 5 (a)-(d) we color in red the training accuracy that neural networks achieved as a function of σ2w and L for different datasets, training time, and choice of minimizer (see Appendix 7.10 for more comparisons). In all cases the neural networks over-fit the data to give a training accuracy of 100% and test accuracies of 98% on MNIST and 55% on CIFAR10. We emphasize that the purpose of this study is to demonstrate trainability as opposed to optimizing test accuracy.
We now make the connection between the depth scale, ξc, and the maximum trainable depth more precise. Given the arguments in the preceding sections we note that if L = nξc then signal through the network will be attenuated by a factor of en. To understand how much signal can be lost while still allowing for training, we overlay in fig. 5 (a) curves corresponding to nξc from n = 1 to 6. We find that networks appear to be trainable when L . 6ξc. It would be interesting to understand why this is the case.
Motivated by this argument in fig. 5 (b)-(d) in white, dashed, overlay we plot twice the predicted depth scale, 6ξc. There is clearly a relationship between the depth of correlated signal propagation and whether or not these networks are trainable. Networks closer to their critical point appear to train more quickly than those further away. Moreover, this relationship has no obvious dependence on dataset, duration of training, or minimizer. We therefore conclude that these bounds on trainable hyperparameters are universal. This in turn implies that to train increasingly deep networks, one must generically be ever closer to criticality.
Next we consider the effect of dropout. As we showed earlier, even infinitesimal amounts of dropout disrupt the order-to-chaos phase transition and cause the depth scale to become finite. However, since the effect of a single dropout mask is to simply re-scale the weight variance by σ2w → σ2w/ρ, the gradient magnitude will be stable near criticality, while the input and gradient correlations will not be. This therefore offers a unique opportunity to test whether the relevant depth-scale is |1/ logχ1| or ξc.
In fig. 6 we repeat the same experimental setup as above on MNIST with dropout rates ρ = 0.99, 0.98, and 0.94. We observe, first and foremost, that even extremely modest amounts of dropout limit the maximum trainable depth to about L = 100. We additionally notice that the depth-scale, ξc, predicts the trainable region accurately for varying amounts of dropout.

6 DISCUSSION
In this paper we have elucidated the existence of several depth-scales that control signal propagation in random neural networks. Furthermore, we have shown that the degree to which a neural network can be trained depends crucially on its ability to propagate information about inputs and gradients
through its full depth. At the transition between order and chaos, information stored in the correlation between inputs can propagate infinitely far through these random networks. This in turn implies that extremely deep neural networks may be trained sufficiently close to criticality. However, our contribution goes beyond advocating for hyperparameter selection that brings random networks to be nearly critical. Instead, we offer a general purpose framework that predicts, at the level of mean field theory, which hyperparameters should allow a network to be trained. This is especially relevant when analyzing schemes like dropout where there is no critical point and which therefore imply an upper bound on trainable network depth.
An alternative perspective as to why information stored in the covariance between inputs is crucial for training can be understood by appealing to the correspondence between infinitely wide Bayesian neural networks and Gaussian Processes (Neal, 2012). In particular the covariance, qlab, is intimately related to the kernel of the induced Gaussian Process. It follows that cases in which signal stored in the covariance between inputs may propagate through the network correspond precisely to situations in which the associated Gaussian Process is well defined.
Our work suggests that it may be fruitful to investigate pre-training schemes that attempt to perturb the weights of a neural network to favor information flow through the network. In principle this could be accomplished through a layer-by-layer local criterion for information flow or by selecting the mean and variance in schemes like batch normalization to maximize the covariance depth-scale.
These results suggest that theoretical work on random neural networks can be used to inform practical architectural decisions. However, there is still much work to be done. For instance, the framework developed here does not apply to unbounded activations, such as rectified linear units, where it can be shown that there are phases in which eq. 3 does not have a fixed point. Additionally, the analysis here applies directly only to fully connected feed-forward networks, and will need to be extended to architectures with structured weight matrices such as convolutional networks.
We close by noting that in physics it has long been known that, through renormalization, the behavior of systems near critical points can control their behavior even far from the idealized critical case. We therefore make the somewhat bold hypothesis that a broad class of neural network topologies will be controlled by the fully-connected mean field critical point.

ACKNOWLEDGMENTS
We thank Ben Poole, Jeffrey Pennington, Maithra Raghu, and George Dahl for useful discussions. We are additionally grateful to RocketAI for introducing us to Temporally Recurrent Online Learning and two-dimensional time.

7 APPENDIX
Here we present derivations of results from throughout the paper.

7.1 SINGLE INPUT DEPTH-SCALE

Result:
Consider the recurrence relation for the variance of a single input,
qlaa = σ 2 w
∫ Dzφ2 (√ ql−1aa z ) + σ2b (18)
and a fixed point of the dynamics, q∗. qlaa can be expanded about the fixed point to yield the asymptotic recurrence relation,
l+1 = l [ χ1 + σ 2 w ∫ Dzφ′′ (√ q∗z ) φ (√ q∗z )] +O ( ( l)2 ) . (19)
Derivation:
We begin by first expanding to order l,
q∗ + l+1 = σ2w
∫ Dz [ φ (√ q∗ + lz )]2
+ σ2b (20)
≈ σ2w ∫ Dz [ φ (√ q∗z + 1
2 lz√ q∗
)]2 + σ2b (21)
≈ σ2w ∫ Dz [ φ (√ q∗z ) + 1
2 lz√ q∗ φ′ (√ q∗z )]2 + σ2b +O(( l)2) (22)
≈ σ2w ∫ Dzφ2 (√ q∗z ) + σ2b + l σ
2 w√ q∗
∫ Dzzφ (√ q∗z ) φ′ (√ q∗z ) +O(( l)2) (23)
≈ q∗ + l σ 2 w√ q∗
∫ Dzzφ(√q∗z)φ′ (√ q∗z ) +O(( l)2). (24)
We therefore arrive at the approximate reccurence relation,
l+1 = l σ2w√ q∗
∫ Dzzφ(√q∗z)φ′ (√ q∗z ) +O(( l)2). (25)
Using the identity, ∫ Dzzf(z) = ∫ Dzf ′(z) we can rewrite this asymptotic recurrence relation as,
l+1 = l [ σ2w ∫ Dz [ φ′ (√ q∗z )]2 + σ2w ∫ Dzφ′′ (√ q∗z ) φ (√ q∗z )] +O(( l)2) (26)
= l [ χ1 + σ 2 w ∫ Dzφ′′ (√ q∗z ) φ (√ q∗z )] +O(( l)2) (27)
as required.

7.2 TWO INPUT DEPTH-SCALE

Result:
Consider the recurrence relation for the co-variance of two input,
qlab = σ 2 w ∫ Dz1Dz2φ(u1)φ(u2) + σ2b , (28)
a correlation between the inputs, clab = q l ab/ √ qlaaq l bb, and a fixed point of the dynamics, c
∗. clab can be expanded about the fixed point to yield the asymptotic recurrence relation,
l+1 = l [ σ2w ∫ Dz1Dz2φ′(u1)φ′(u2) ] +O ( ( l)2 ) . (29)

Derivation:
Since the relaxation of qlaa and q l bb to q ∗ occurs much more quickly than the convergence of qlab we approximate qlaa = q l bb = q
∗ as in Poole et al. (2016). We therefore consider the perturbation qlab/q ∗ = clab = c ∗ + l. It follows that we may make the approximation,
ul2 = √ q∗ ( clabz1 + √ 1− (clab)2z2 ) (30)
≈ √q∗ ( c∗z1 + √ 1− (c∗)2 − 2c∗ lz2 ) + √ q∗ lz1 +O( 2) (31)
(32)
We now consider the case where c∗ < 1 and c∗ = 1 separately; we will later show that these two results agree with one another. First we consider the case where c∗ < 1 in which case we may safely expand the above equation to get,
ul2 = √ q∗ ( c∗z1 + √ 1− (c∗)2z2 ) + √ q∗ l ( z1 −
c∗√ 1− (c∗)2 z2
) +O( 2). (33)
This allows us to in turn approximate the recurrence relation,
cl+1ab = σ2w q∗
∫ Dz1Dz2φ(u∗1)φ(ul2) + σ2b (34)
≈ σ 2 w
q∗
∫ Dz1Dz2φ(u∗1) [ φ(u∗2) + √ q∗ l ( z1 −
c∗√ 1− (c∗)2 z2
) φ′(u∗2) ] + σ2b +O( 2)
(35) = c∗ + σ2w√ q∗ l ∫ Dz1Dz2 ( z1 − c∗√ 1− (c∗)2 z2 ) φ(u∗1)φ ′(u∗2) (36)
= c∗ + σ2w√ q∗ l
[∫ Dz1Dz2z1φ(u∗1)φ′(u∗2)−
c∗√ 1− (c∗)2
∫ Dz1Dz2z2φ(u∗1)φ′(u∗2) ] (37)
= c∗ + σ2w l [∫ Dz1Dz2(φ′(u∗1)φ′(u∗2) + c∗φ(u∗1)φ′′(u∗2))− c∗ ∫ Dz1Dz2φ(u∗1)φ′′(u∗2) ] (38)
= c∗ + σ2w l ∫ Dz1Dz2φ′(u∗1)φ′(u∗2). (39)
where u∗1 and u ∗ 2 are appropriately defined asymptotic random variables. This leads to the asymptotic recurrence relation,
l+1 = σ2w l ∫ Dz1Dz2φ′(u∗1)φ′(u∗2) (40)
as required.
We now consider the case where c∗ = 1 and clab = 1 − l. In this case the expansion of ul2 will become,
ul2 = √ q∗z1 + √ 2q∗ lz2 − √ q∗ lz1 +O( 3/2) (41)
and so the lowest order correction is of orderO( √ l) as opposed toO( l). As usual we now expand the recurrence relation, noting that u∗2 = u ∗ 1 is independent of z2 when c ∗ = 1 to find,
cl+1ab = σ2w q∗
∫ Dz1Dz2φ(u∗1)φ(ul2) + σ2b (42)
≈ σ 2 w
q∗
∫ Dz1Dz2φ(u∗1) [ φ(u∗2) + (√ 2q∗ lz2 − √ q∗ lz1 ) φ′(u∗2) + q ∗ lz22φ ′′(u∗2) ] + σ2b
(43)
= c∗ + σ2w l
∫ Dzφ(√q∗z) [ φ′′( √ q∗z)− 1√
q∗ zφ′( √ q∗z)
] (44)
= c∗ + σ2w l
[∫ Dzφ(√q∗z)φ′′(√q∗z)− 1√
q∗
∫ Dzzφ(√q∗z)φ′(√q∗z) ] (45)
= c∗ − σ2w l ∫ Dz [ φ′( √ q∗z) ]2 (46)
It follows that the asymptotic recurrence relation in this case will be, l+1 = − lσ2w ∫ Dz [ φ′( √ q∗z) ]2 = − lχ1. (47)
where χ1 is the stability condition for the ordered phase. We note that although the approximations were somewhat different the asymptotic recurrence relation for c∗ < 1 reduces eq. 47 result for c∗ = 1. We may therefore use 4 for all c∗.

7.3 VARIANCE OF AN INPUT WITH DROPOUT
Result:
In the presence of dropout with rate ρ, the variance of a single input as it is passed through the network is described by the recurrence relation,
q̄laa = σ2w ρ
∫ Dzφ2 (√ q̄l−1aa z ) + σ2b . (48)

Derivation:
Recall that the recurrence relation for the pre-activations is given by,
zli = 1
ρ ∑ j W lijp l jy l j + b l i (49)
where plj ∼ Bernoulli(ρ). It follows that the variance will be given by,
q̄laa = E[(zli)2] (50)
= 1
ρ2 ∑ j E[(W lij)2]E[(ρlj)2]E[(ylj)2] + E[(bli)2] (51)
= σ2w ρ
∫ Dzφ2 (√ q̄l−1aa z ) + σ2b . (52)
where we have used the fact that E[(plj)2] = ρ.

7.4 COVARIANCE OF TWO INPUTS WITH DROPOUT

Result:
The co-variance between two signals, zli;a and z l i;b, with separate i.i.d. dropout masks p l i;a and p l i;b is given by,
q̄lab = σ 2 w ∫ Dz1Dz2φ(ū1)φ(ū2) + σ2b . (53)
where, in analogy to eq. 4, ū1 = √ q̄laaz1 and ū2 = √ q̄lbb ( c̄labz1 + √ 1− (c̄lab)2z2 ) .

Derivation:
Proceeding directly we find that,
E[zli;azli;b] = 1
ρ2 ∑ j E[(W lij)2]E[plj;a]E[plj;b]E[ylj;aylj;b] + E[bli] (54)
= σ2w ∫ Dz1Dz2φ(ū1)φ(ū2) + σ2b (55)
where we have used the fact that E[pli;a] = E[pli;b] = ρ. We have also used the same substitution for E[ylj;aylj;b] used in the original mean field calculation with the appropriate substitution.
7.5 THE LACK OF A c∗ = 1 FIXED POINT WITH DROPOUT

Result:
If clab = 1 then it follows that,
c̄l+1ab = 1− 1− ρ ρq̄∗ σ2w
∫ Dzφ2 (√ q̄∗z )
(56)
subject to the approximation, qlaa ≈ qlbb ≈ q∗. This implies that cl+1ab < 1. Derivation:
Plugging in clab = 1 with q l aa ≈ qlbb ≈ q∗ we find that ū1 = ū2 = √ q∗z1. It follows that,
cl+1ab = ql+1ab q∗
(57)
= 1
q∗
[ σ2w ∫ Dzφ2 (√ q∗z ) + σ2b ] (58)
= 1
q∗
[ σ2w(1− ρ−1 + ρ−1) ∫ Dzφ2 (√ q∗z ) + σ2b ] (59)
= 1
q∗ [ σ2w ρ ∫ Dzφ2 (√ q∗z ) + σ2b ] + σ2w q∗ (1− ρ−1) ∫ Dzφ2 (√ q∗z )
(60)
= 1− 1− ρ ρq̄∗ σ2w
∫ Dzφ2 (√ q̄∗z )
(61)
as required. Here we have integrated out z2 since nether ū1 nor ū2 depend on it.

7.6 MEAN FIELD GRADIENT SCALING

Result:
In mean field theory the expected magnitude of the gradient ||∇W lijE|| 2 will be proportional to E[(δli)2].

Derivation:
We first note that since the W lij are i.i.d. it follows that,
||∇W lijE|| 2 = ∑ ij
( ∂E
∂W lij
)2 (62)
≈ NlNl+1E ( ∂E ∂W lij )2 (63) where we have used the fact that the first line is related to the sample expectation over the different realizations of theW lij to approximate it by the analytic expectation in the second line. In mean field theory since the pre-activations in each layer are assumed to be i.i.d. Gaussian it follows that,
E ( ∂E ∂W lij )2 = E[(δli)2]E[φ2(zl−1j )] (64) and the result follows.

7.7 MEAN FIELD BACKPROPAGATION

Result:
In mean field theory the recursion relation for the variance of the errors, q̃ l = E[(δli)2] is given by,
q̃ laa = q̃ l+1 aa Nl+1 Nl+2 χ1(q l aa). (65)
Derivation:
Computing the variance directly and using mean field approximation, q̃ laa = E[(δli;a)2] = E[(φ′(zli;a))2] ∑ j E[(δl+1j;a ) 2]E[(W l+1ji ) 2] (66)
= E[(φ′(zli;a))2] σ2w Nl+1 ∑ j E[(δl+1j;a ) 2] (67)
= E[(φ′(zli;a))2] Nl+1 Nl+2 σ2w q̃ l+1 aa (68)
= σ2w q̃ l+1 aa Nl+1 Nl+2
∫ Dz [ φ′ (√
qlaaz
)]2 (69)
≈ q̃ l+1aa Nl+1 Nl+2 χ1 (70)
as required. In the last step we have made the approximation that qlaa ≈ q∗ since the depth scale for the variance is short ranged.

7.8 MEAN FIELD GRADIENT COVARIANCE SCALING

Result:
In mean field theory we expect the covariance between the gradients of two different inputs to scale as,
(∇W lijEa) · (∇W lijEb) ∼ E[δi;aδi;b]. (71)

Derivation:
We proceed in a manner analogous to Appendix 7.6. Note that in mean field theory since the weights are i.i.d. it follows that
(∇W lijEa) · (∇W lijEb) = ∑ ij ∂Ea ∂W lij ∂Eb ∂W lij
(72)
≈ NlNl+1E [ ∂Ea ∂W lij ∂Eb ∂W lij ] (73)
where, as before, the final term is approximating the sample expectation. Since the weights in the forward and backwards passes are chosen independently it follows that we can factor the expectation as,
E [ ∂Ea ∂W lij ∂Eb ∂W lij ] = E[δli;aδli;b]E[φ(zli;a)φ(zli;b)] (74)
and the result follows.

7.9 MEAN FIELD BACKPROPAGATION OF COVARIANCE

Result:
The covariance between the gradients due to two inputs scales as,
q̃ lab = q̃ l+1 ab Nl+1 Nl+2 σ2w
∫ Dz1Dz2φ′(u1)φ′(u2) (75)
under backpropagation.

Derivation
As in the analogous derivation for the variance, we compute directly, q̃ lab = E[δli;aδli;b] = E [φ′(zi;a)φ′(zi;b)] ∑ j E[δl+1j;a δ l+1 j;b ]E[(W l+1 ji ) 2] (76)
= q̃ l+1ab Nl+1 Nl+2 σ2w
∫ Dz1Dz2φ′(u1)φ′(u2) (77)
as required.

7.10 FURTHER EXPERIMENTAL RESULTS
Here we include some more experimental figures that investigate the effects of training time, minimizer, and dataset more closely.
","We study the behavior of untrained neural networks whose weights and biases are randomly distributed using mean field theory. We show the existence of depth scales that naturally limit the maximum depth of signal propagation through these random networks. Our main practical result is to show that random networks may be trained precisely when information can travel through them. Thus, the depth scales that we identify provide bounds on how deep a network may be trained for a specific choice of hyperparameters. As a corollary to this, we argue that in networks at the edge of chaos, one of these depth scales diverges. Thus arbitrarily deep networks may be trained only sufficiently close to criticality. We show that the presence of dropout destroys the order-to-chaos critical point and therefore strongly limits the maximum trainable depth for random networks. Finally, we develop a mean field theory for backpropagation and we show that the ordered and chaotic phases correspond to regions of vanishing and exploding gradient respectively.",ICLR 2017 conference submission,True,,"This paper presents a mathematical analysis of how information is propagated through deep feed-forward neural networks, with novel analysis addressing the problem of vanishing and exploding gradients in the backward pass of backpropagation and the use of the dropout algorithm. The paper is clear and well-written, the analysis is thorough, and the experimental results showing agreement with the model are very nice.

---

This is one of the two top papers in my stack. In total the reviews are a little bit on the light side in terms of level of detail and there are some concerns regarding how useful the results are from a practical point of view. However, I am confident that the paper should be accepted.

---

Just making sure, your experiments with dropout follows the convention that during evaluation, it is turned off?

---

I'm not familiar enough with mean-field techniques to judge the soundness of Eq 2, but I'm willing to roll with it.

Minor point on presentation: Speaking of the ""evolution"" of x_{i;a} as it travels through the network could give some readers helpful intuition, but for me it was confusing because x_{*;a} is the immutable input vector, and it's the just-introduced z and y variables that represent its so-called evolution, no?

In interpreting this analysis - A network may be trainable if information does not pass through it, if the training steps, by whatever reason, perturb the weights so that information starts to pass through it (without subsequently perturbing the weights to stop information from passing through it.) Perhaps this could be clarified by a definition of “training algorithm”?

Comments on central claims:
Previous work on initializing neural networks to promote information flow (e.g. Glorot & Bengio,

---

The paper expands a recent mean-field approximation of deep random neural networks to study depth-dependent information propagation, its phase-dependence and the influence of drop-out. The paper is extremely well written, the mathematical analysis is thorough and numerical experiments are included that underscore the theoretical results. Overall the paper stands out as one of the few papers that thoroughly analyses training and performance of deep nets.

---

This paper presents a mathematical analysis of how information is propagated through deep feed-forward neural networks, with novel analysis addressing the problem of vanishing and exploding gradients in the backward pass of backpropagation and the use of the dropout algorithm. The paper is clear and well-written, the analysis is thorough, and the experimental results showing agreement with the model are very nice.

---

This paper presents a mathematical analysis of how information is propagated through deep feed-forward neural networks, with novel analysis addressing the problem of vanishing and exploding gradients in the backward pass of backpropagation and the use of the dropout algorithm. The paper is clear and well-written, the analysis is thorough, and the experimental results showing agreement with the model are very nice.

---

This is one of the two top papers in my stack. In total the reviews are a little bit on the light side in terms of level of detail and there are some concerns regarding how useful the results are from a practical point of view. However, I am confident that the paper should be accepted.

---

Just making sure, your experiments with dropout follows the convention that during evaluation, it is turned off?

---

I'm not familiar enough with mean-field techniques to judge the soundness of Eq 2, but I'm willing to roll with it.

Minor point on presentation: Speaking of the ""evolution"" of x_{i;a} as it travels through the network could give some readers helpful intuition, but for me it was confusing because x_{*;a} is the immutable input vector, and it's the just-introduced z and y variables that represent its so-called evolution, no?

In interpreting this analysis - A network may be trainable if information does not pass through it, if the training steps, by whatever reason, perturb the weights so that information starts to pass through it (without subsequently perturbing the weights to stop information from passing through it.) Perhaps this could be clarified by a definition of “training algorithm”?

Comments on central claims:
Previous work on initializing neural networks to promote information flow (e.g. Glorot & Bengio,

---

The paper expands a recent mean-field approximation of deep random neural networks to study depth-dependent information propagation, its phase-dependence and the influence of drop-out. The paper is extremely well written, the mathematical analysis is thorough and numerical experiments are included that underscore the theoretical results. Overall the paper stands out as one of the few papers that thoroughly analyses training and performance of deep nets.

---

This paper presents a mathematical analysis of how information is propagated through deep feed-forward neural networks, with novel analysis addressing the problem of vanishing and exploding gradients in the backward pass of backpropagation and the use of the dropout algorithm. The paper is clear and well-written, the analysis is thorough, and the experimental results showing agreement with the model are very nice.",,,,,,8.333333333333334,,,3.0,,
462,"ON DETECTING ADVERSARIAL PERTURBATIONS
Authors: Jan Hendrik Metzen
Source file: 462.pdf

ABSTRACT
Machine learning and deep learning in particular has advanced tremendously on perceptual tasks in recent years. However, it remains vulnerable against adversarial perturbations of the input that have been crafted specifically to fool the system while being quasi-imperceptible to a human. In this work, we propose to augment deep neural networks with a small “detector” subnetwork which is trained on the binary classification task of distinguishing genuine data from data containing adversarial perturbations. Our method is orthogonal to prior work on addressing adversarial perturbations, which has mostly focused on making the classification network itself more robust. We show empirically that adversarial perturbations can be detected surprisingly well even though they are quasi-imperceptible to humans. Moreover, while the detectors have been trained to detect only a specific adversary, they generalize to similar and weaker adversaries. In addition, we propose an adversarial attack that fools both the classifier and the detector and a novel training procedure for the detector that counteracts this attack.

1 INTRODUCTION
In the last years, machine learning and in particular deep learning methods have led to impressive performance on various challenging perceptual tasks, such as image classification (Russakovsky et al., 2015; He et al., 2016) and speech recognition (Amodei et al., 2016). Despite these advances, perceptual systems of humans and machines still differ significantly. As Szegedy et al. (2014) have shown, small but carefully directed perturbations of images can lead to incorrect classification with high confidence on artificial systems. Yet, for humans these perturbations are often visually imperceptible and do not stir any doubt about the correct classification. In fact, so called adversarial examples are crucially characterized by requiring minimal perturbations that are quasi-imperceptible to a human observer. For computer vision tasks, multiple techniques to create such adversarial examples have been developed recently. Perhaps most strikingly, adversarial examples have been shown to transfer between different network architectures, and networks trained on disjoint subsets of data (Szegedy et al., 2014). Adversarial examples have also been shown to translate to the real world (Kurakin et al., 2016), e.g., adversarial images can remain adversarial even after being printed and recaptured with a cell phone camera. Moreover, Papernot et al. (2016a) have shown that a potential attacker can construct adversarial examples for a network of unknown architecture by training an auxiliary network on similar data and exploiting the transferability of adversarial inputs.
The vulnerability to adversarial inputs can be problematic and even prevent the application of deep learning methods in safety- and security-critical applications. The problem is particularly severe when human safety is involved, for example in the case of perceptual tasks for autonomous driving. Methods to increase robustness against adversarial attacks have been proposed and range from augmenting the training data (Goodfellow et al., 2015) over applying JPEG compression to the input (Dziugaite et al., 2016) to distilling a hardened network from the original classifier network (Papernot et al., 2016b). However, for some recently published attacks (Carlini & Wagner, 2016), no effective counter-measures are known yet.
In this paper, we propose to train a binary detector network, which obtains inputs from intermediate feature representations of a classifier, to discriminate between samples from the original data set and adversarial examples. Being able to detect adversarial perturbations might help in safety- and security-critical semi-autonomous systems as it would allow disabling autonomous operation and
requesting human intervention (along with a warning that someone might be manipulating the system). However, it might intuitively seem very difficult to train such a detector since adversarial inputs are generated by tiny, sometimes visually imperceptible, perturbations of genuine examples. Despite this intuition, our results on CIFAR10 and a 10-class subset of ImageNet show that a detector network that achieves high accuracy in detection of adversarial inputs can be trained successfully. Moreover, while we train a detector network to detect perturbations of a specific adversary, our experiments show that detectors generalize to similar and weaker adversaries. An obvious attack against our approach would be to develop adversaries that take into account both networks, the classification and the adversarial detection network. We present one such adversary and show that we can harden the detector against such an adversary using a novel training procedure.

2 BACKGROUND
Since their discovery by Szegedy et al. (2014), several methods to generate adversarial examples have been proposed. Most of these methods generate adversarial examples by optimizing an image w.r.t. the linearized classification cost function of the classification network by maximizing the probability for all but the true class or minimizing the probability of the true class (e.g., (Goodfellow et al., 2015), (Kurakin et al., 2016)). The method introduced by Moosavi-Dezfooli et al. (2016b) estimates a linearization of decision boundaries between classes in image space and iteratively shifts an image towards the closest of these linearized boundaries. For more details about these methods, please refer to Section 3.1.
Several approaches exist to increase a model’s robustness against adversarial attacks. Goodfellow et al. (2015) propose to augment the training set with adversarial examples. At training time, they minimize the loss for real and adversarial examples, while adversarial examples are chosen to fool the current version of the model. In contrast, Zheng et al. (2016) propose to append a stability term to the objective function, which forces the model to have similar outputs for samples of the training set and their perturbed versions. This differs from data augmentation since it encourages smoothness of the model output between original and distorted samples instead of minimizing the original objective on the adversarial examples directly. Another defense-measure against certain adversarial attack methods is defensive distillation (Papernot et al., 2016b), a special form of network distillation, to train a network that becomes almost completely resistant against attacks such as the L-BFGS attack (Szegedy et al., 2014) and the fast gradient sign attack (Goodfellow et al., 2015). However, Carlini & Wagner (2016) recently introduced a novel method for constructing adversarial examples that manages to (very successfully) break many defense methods, including defensive distillation. In fact, the authors find that previous attacks were very fragile and could easily fail to find adversarial examples even when they existed. An experiment on the cross-model adversarial portability (Rozsa et al., 2016) has shown that models with higher accuracies tend to be more robust against adversarial examples, while examples that fool them are more portable to less accurate models.
Even though the existence of adversarial examples has been demonstrated several times on many different classification tasks, the question of why adversarial examples exist in the first place and whether they are sufficiently regular to be detectable, which is studied in this paper, has remained open. Szegedy et al. (2014) speculated that the data-manifold is filled with “pockets” of adversarial inputs that occur with very low probability and thus are almost never observed in the test set. Yet, these pockets are dense and so an adversarial example is found virtually near every test case. The authors further speculated that the high non-linearity of deep networks might be the cause for the existence of these low-probability pockets. Later, Goodfellow et al. (2015) introduced the linear explanation: Given an input and some adversarial noise η (subject to: ||η||∞ < ), the dot product between a weight vector w and an adversarial input xadv = x+ η is given by wTxadv = wTx+ wTη. The adversarial noise η causes a neuron’s activation to grow by wTη. The max-norm constraint on η does not allow for large values in one dimension, but if x and thus η are high-dimensional, many small changes in each dimension of η can accumulate to a large change in a neuron’s activation. The conclusion was that “linear behavior in high-dimensional spaces is sufficient to cause adversarial examples”.
Tanay & Griffin (2016) challenged the linear-explanation hypothesis by constructing classes of images that do not suffer from adversarial examples under a linear classifier. They also point out that if the change in activation wTη grows linearly with the dimensionality of the problem, so does the activation
wTx. Instead of the linear explanation, Tanay et al. provide a different explanation for the existence of adversarial examples, including a strict condition for the non-existence of adversarial inputs, a novel measure for the strength of adversarial examples and a taxonomy of different classes of adversarial inputs. Their main argument is that if a learned class boundary lies close to the data manifold, but the boundary is (slightly) tilted with respect to the manifold1, then adversarial examples can be found by perturbing points from the data manifold towards the classification boundary until the perturbed input crosses the boundary. If the boundary is only slightly tilted, the distance required by the perturbation to cross the decision-boundary is very small, leading to strong adversarial examples that are visually almost imperceptibly close to the data. Tanay et. al further argue that such situations are particularly likely to occur along directions of low variance in the data and thus speculate that adversarial examples can be considered an effect of an over-fitting phenomenon that could be alleviated by proper regularization, though it is completely unclear how to regularize neural networks accordingly.
Recently, Moosavi-Dezfooli et al. (2016a) demonstrated that there even exist universal, imageagnostic perturbations which, when added to all data points, fool deep nets on a large fraction of ImageNet validation images. Moreover, they showed that these universal perturbations are to a certain extent also transferable between different network architectures. While this observation raises interesting questions about geometric properties and correlations of different parts of the decision boundary of deep nets, potential regularities in adversarial perturbations may also help detecting them. However, the existence of universal perturbations does not necessarily imply that the adversarial examples generated by data-dependent adversaries will be regular. Actually, Moosavi-Dezfooli et al. (2016a) show that universal perturbations are not unique and that there even exist many different universal perturbations which have little in common. This paper studies if data-dependent adversarial perturbations can nevertheless be detected reliably and answers this question affirmatively.

3 METHODS
In this section, we introduce the adversarial attacks used in the experiments, propose an approach for detecting adversarial perturbations, introduce a novel adversary that aims at fooling both the classification network and the detector, and propose a training method for the detector that aims at counteracting this novel adversary.

3.1 GENERATING ADVERSARIAL EXAMPLES
Let x be an input image x ∈ R3×width×height, ytrue(x) be a one-hot encoding of the true class of image x, and Jcls(x, y(x)) be the cost function of the classifier (typically cross-entropy). We briefly introduce different adversarial attacks used in the remainder of the paper.
Fast method: One simple approach to compute adversarial examples was described by Goodfellow et al. (2015). The applied perturbation is the direction in image space which yields the highest increase of the linearized cost function under `∞-norm. This can be achieved by performing one step in the direction of the gradient’s sign with step-width ε:
xadv = x+ ε sgn(∇xJcls(x, ytrue(x)))
Here, ε is a hyper-parameter governing the distance between adversarial and original image. As suggested in Kurakin et al. (2016) we also refer to this as the fast method due to its non-iterative and hence fast computation.
Basic Iterative method (`∞ and `2): As an extension, Kurakin et al. (2016) introduced an iterative version of the fast method, by applying it several times with a smaller step size α and clipping all pixels after each iteration to ensure results stay in the ε-neighborhood of the original image:
xadv0 = x, x adv n+1 = Clip ε x { xadvn + α sgn(∇xJcls(xadvn , ytrue(x))) } 1It is easier to imagine a linear decision-boundary - for neural networks this argument must be translated into
a non-linear equivalent of boundary tilting.
Following Kurakin et al. (2016), we refer to this method as the basic iterative method and use α = 1, i.e., we change each pixel maximally by 1. The number of iterations is set to 10. In addition to this method, which is based on the `∞-norm, we propose an analogous method based on the `2-norm: in each step this method moves in the direction of the (normalized) gradient and projects the adversarial examples back on the ε-ball around x (points with `2 distance ε to x) if the `2 distance exceeds ε:
xadv0 = x, x adv n+1 = Project ε x { xadvn + α
∇xJcls(xadvn , ytrue(x)) ||∇xJcls(xadvn , ytrue(x))||2 } DeepFool method: Moosavi-Dezfooli et al. (2016b) introduced the DeepFool adversary which iteratively perturbs an image xadv0 . Therefore, in each step the classifier is linearized around x adv n and the closest class boundary is determined. The minimal step according to the `p distance from xadvn to traverse this class boundary is determined and the resulting point is used as xadvn+1. The algorithm stops once xadvn+1 changes the class of the actual (not linearized) classifier. Arbitrary `p-norms can be used within DeepFool, and here we focus on the `2- and `∞-norm. The technical details can be found in (Moosavi-Dezfooli et al., 2016b). We would like to note that we use the variant of DeepFool presented in the first version of the paper (https://arxiv.org/abs/1511.04599v1) since we found it to be more stable compared to the variant reported in the final version.

3.2 DETECTING ADVERSARIAL EXAMPLES
We augment classification networks by (relatively small) subnetworks, which branch off the main network at some layer and produce an output padv ∈ [0, 1] which is interpreted as the probability of the input being adversarial. We call this subnetwork “adversary detection network” (or “detector” for short) and train it to classify network inputs into being regular examples or examples generated by a specific adversary. For this, we first train the classification networks on the regular (non-adversarial) dataset as usual and subsequently generate adversarial examples for each data point of the train set using one of the methods discussed in Section 3.1. We thus obtain a balanced, binary classification dataset of twice the size of the original dataset consisting of the original data (label zero) and the corresponding adversarial examples (label one). Thereupon, we freeze the weights of the classification network and train the detector such that it minimizes the cross-entropy of padv and the labels. The details of the adversary detection subnetwork and how it is attached to the classification network are specific for datasets and classification networks. Thus, evaluation and discussion of various design choices of the detector network are provided in the respective section of the experimental results.

3.3 DYNAMIC ADVERSARIES AND DETECTORS
In the worst case, an adversary might not only have access to the classification network and its gradient but also to the adversary detector and its gradient2. In this case, the adversary might potentially generate inputs to the network that fool both the classifier (i.e., get classified wrongly) and fool the detector (i.e., look innocuous). In principle, this can be achieved by replacing the cost Jcls(x, ytrue(x)) by (1 − σ)Jcls(x, ytrue(x)) + σJdet(x, 1), where σ ∈ [0, 1] is a hyperparameter and Jdet(x, 1) is the cost (cross-entropy) of the detector for the generated x and the label one, i.e., being adversarial. An adversary maximizing this cost would thus aim at letting the classifier mis-label the input x and making the detectors output padv as small as possible. The parameter σ allows trading off these two objectives. For generating x, we propose the following extension of the basic iterative (`∞) method:
xadv0 = x; x adv n+1 = Clip ε x { xadvn + α [ (1− σ) sgn(∇xJcls(xadvn , ytrue(x))) + σ sgn(∇xJdet(xadvn , 1)) ]} Note that we found a smaller α to be essential for this method to work; more specifically, we use α = 0.25. Since such an adversary can adapt to the detector, we call it a dynamic adversary. To
2We would like to emphasize that is a stronger assumption than granting the adversary access to only the original classifier’s predictions and gradients since the classifier’s predictions need often be presented to a user (and thus also to an adversary). The same is typically not true for the predictions of the adversary detector as they will only be used internally.
counteract dynamic adversaries, we propose dynamic adversary training, a method for hardening detectors against dynamic adversaries. Based on the approach proposed by Goodfellow et al. (2015), instead of precomputing a dataset of adversarial examples, we compute the adversarial examples on-the-fly for each mini-batch and let the adversary modify each data point with probability 0.5. Note that a dynamic adversary will modify a data point differently every time it encounters the data point since it depends on the detector’s gradient and the detector changes over time. We extend this approach to dynamic adversaries by employing a dynamic adversary, whose parameter σ is selected uniform randomly from [0, 1], for generating the adversarial data points during training. By training the detector in this way, we implicitly train it to resist dynamic adversaries for various values of σ. In principle, this approach bears the risk of oscillation and unlearning for σ > 0 since both, the detector and adversary, adapt to each other (i.e., there is no fixed data distribution). In practice, however, we found this approach to converge stably without requiring careful tuning of hyperparameters.

4 EXPERIMENTAL RESULTS
In this section, we present results on the detectability of adversarial perturbations on the CIFAR10 dataset (Krizhevsky, 2009), both for static and dynamic adversaries. Moreover, we investigate whether adversarial perturbations are also detectable in higher-resolution images based on a subset of the ImageNet dataset (Russakovsky et al., 2015).

4.1 CIFAR10
We use a 32-layer Residual Network (He et al., 2016, ResNet) as classifier. The structure of the network is shown in Figure 1. The network has been trained for 100 epochs with stochastic gradient descent and momentum on 45000 data points from the train set. The momentum term was set to 0.9 and the initial learning rate was set to 0.1, reduced to 0.01 after 41 epochs, and further reduced to 0.001 after 61 epochs. After each epoch, the network’s performance on the validation data (the remaining 5000 data points from the train set) was determined. The network with maximal performance on the validation data was used in the subsequent experiments (with all tunable weights being fixed). This network’s accuracy on non-adversarial test data is 91.3%. We attach an adversary detection subnetwork (called “detector” below) to the ResNet. The detector is a convolutional neural network using batch normalization (Ioffe & Szegedy, 2015) and rectified linear units. In the experiments, we investigate different positions where the detector can be attached (see also Figure 1).

4.1.1 STATIC ADVERSARIES
In this subsection, we investigate a static adversary, i.e., an adversary that only has access to the classification network but not to the detector. The detector was trained for 20 epochs on 45000 data points from the train set and their corresponding adversarial examples using the Adam optimizer (Kingma & Ba, 2015) with a learning rate of 0.0001 and β1 = 0.99, β2 = 0.999. The remaining 5000 data points from the CIFAR10 train set are used as validation data and used for model selection. The detector was attached to position AD(2) (see Figure 1) except for the DeepFool-based adversaries where the detector was attached to AD(4); see below for a discussion. For the “Fast” and “Iterative” adversaries, the parameter ε from Section 3.1 was chosen from [1, 2, 3, 4] for `∞-based methods and from [20, 40, 60, 80] for `2-based methods; larger values of ε generally result in reduced accuracy of the classifier but increased detectability. For the “Iterative” method with `2-norm, we used α = 20, i.e., in each iteration we make a step of `2 distance 20. Please note that these values of ε are based on assuming a range of [0, 255] per color channel of the input.
Figure 2 (left) compares the detectability3 of different adversaries. In general, points in the lower left of the plot correspond to stronger adversaries because their adversarial examples are harder to detect and at the same time fool the classifier on most of the images. Detecting adversarial examples works surprisingly well given that no differences are perceivable to humans for all shown settings: the detectability is above 80% for all adversaries which decrease classification accuracy below 30% and above 90% for adversaries which decrease classification accuracy below 10%. Comparing the different adversaries, the “Fast” adversary can generally be considered as a weak adversary, the DeepFool based methods as relatively strong adversaries, and the “Iterative” method being somewhere in-between. Moreover, the methods based on the `2-norm are generally slightly stronger than their `∞-norm counter-parts.
Figure 2 (right) compares the detectability of different adversaries for detectors attached at different points to the classification network. ε was chosen minimal under the constraint that the classification accuracy is below 30%. For the “Fast” and “Iterative” adversaries, the attachment position AD(2) works best, i.e., attaching to a middle layer where more abstract features are already extracted but still the full spatial resolution is maintained. For the DeepFool methods, the general pattern is similar except for AD(4), which works best for these adversaries.
Figure 3 illustrates the generalizability of trained detectors for the same adversary with different choices of ε: while a detector trained for large ε does not generalize well to small ε, the other direction works reasonably well. Figure 4 shows the generalizability of detectors trained for one adversary when tested on data from other adversaries (ε was chosen again minimal under the constraint that the
3Detectability refers to the accuracy of the detector. The detectability on the test data is calculated as follows: for every test sample, a corresponding adversarial example is generated. The original and the corresponding adversarial examples form a joint test set (twice the size of the original test set). This test set is shuffled and the detector is evaluated on this dataset. Original and corresponding adversarial example are thus processed independently.
classification accuracy is below 30%): we can see that detectors generalize well between `∞- and `2-norm based variants of the same approach. Moreover, detectors trained on the stronger “Iterative” adversary generalize well to the weaker “Fast” adversary but not vice versa. Detectors trained for the DeepFool-based methods do not generalize well to other adversaries; however, detectors trained for the “Iterative” adversaries generalize relatively well to the DeepFool adversaries.

4.1.2 DYNAMIC ADVERSARIES
In this section, we evaluate the robustness of detector networks to dynamic adversaries (see Section 3.3). For this, we evaluate the detectability of dynamic adversaries for σ ∈ {0.0, 0.1, . . . , 1.0}. We use the same optimizer and detector network as in Section 4.1.1. When evaluating the detectability of dynamic adversaries with σ close to 1, we need to take into account that the adversary might choose to solely focus on fooling the detector, which is trivially achieved by leaving the input unmodified. Thus, we ignore adversarial examples that do not cause a misclassification in the evaluation of the detector and evaluate the detector’s accuracy on regular data versus the successful adversarial examples. Figure 5 shows the results of a dynamic adversary with ε = 1 against a static detector, which was trained to only detect static adversaries, and a dynamic detector, which was explicitly trained to resist dynamic adversaries. As can be seen, the static detector is not robust to dynamic adversaries since for certain values of σ, namely σ = 0.3 and σ = 0.4, the detectability is close to
chance level while the predictive performance of the classifier is severely reduced to less than 30% accuracy. A dynamic detector is considerably more robust and achieves a detectability of more than 70% for any choice of σ.

4.2 10-CLASS IMAGENET
In this section, we report results for static adversaries on a subset of ImageNet consisting of all data from ten randomly selected classes4. The motivation for this section is to investigate whether adversarial perturbations can be detected in higher-resolution images and for other network architectures than residual networks. We limit the experiment to ten classes in order to keep the computational resources required for computing the adversarial examples small and avoid having too similar classes which would oversimplify the task for the adversary. We use a pretrained VGG16 (Simonyan & Zisserman, 2015) as classification network and add a layer before the softmax which selects only the 10 relevant class entries from the logits vector. Based on preliminary experiments, we attach the detector network after the fourth max-pooling layer. The detector network consists of a sequence of five 3x3 convolutions with 196 feature maps each using batch-normalization and rectified linear units, followed by a 1x1 convolution which maps onto the 10 classes, global-average pooling, and a softmax layer. An additional 2x2 max-pooling layer is added after the first convolution. Note that we did not tune the specific details of the detector network; other topologies might perform better than the results reported below. When applicable, we vary ε ∈ [2, 4, 6] for `∞-based methods and ε ∈ [400, 800, 1200] for `2. Moreover, we limit changes of the DeepFool adversaries to an `∞ distance of 6 since the adversary would otherwise sometimes generate distortions which are clearly perceptible. We train the detector for 500 epochs using the Adam optimizer with a learning rate of 0.0001 and β1 = 0.99, β2 = 0.999.
Figure 6 compares the detectability of different static adversaries. All adversaries fail to decrease predictive accuracy of the classifier below the chance level of 0.1 (note that predictive accuracy refers to the accuracy on the 10-class problem not on the full 1000-class problem) for the given values of ε. Nevertheless, detectability is 85% percent or more with the exception of the “Iterative” `2-based adversary with ε = 400. For this adversary, the detector only reaches chance level. Other choices of the detector’s attachment depth, internal structure, or hyperparameters of the optimizer might achieve
4The synsets of the selected classes are: palace; joystick; bee; dugong, Dugong dugon; cardigan; modem; confectionery, confectionary, candy store; valley, vale; Persian cat; stone wall. Classes were selected by randomly drawing 10 ILSVRC2012 Synset-IDs (i.e. integers from [1, 1000]), using the randint function of the python-package numpy after initializing numpy’s random number generator seed with 0. This results in a train set of 10000 images, a validation set of 2848 images, and a test set (from ImageNet’s validation data) of 500 images.
better results; however, this failure case emphasizes that the detector has to detect very subtle patterns and the optimizer might get stuck in bad local optima or plateaus.
Figure 7 illustrates the transferability of the detector between different values of ε. The results are roughly analogous to the results on CIFAR10 in Section 4.1.1: detectors trained for an adversary for a small value of ε work well for the same adversary with larger ε but not vice versa. Note that a detector trained for the “Iterative” `2-based adversary with ε = 1200 can detect the changes of the same adversary with ε = 400 with 78% accuracy; this emphasizes that this adversary is not principally undetectable but that rather the optimization of a detector for this setting is difficult. Figure 8 shows the transferability between adversaries: transferring the detector works well between similar adversaries such as between the two DeepFool adversaries and between the Fast and Iterative adversary based on the `∞ distance. Moreover, detectors trained for DeepFool adversaries work well on all other adversaries. In summary, transferability is not symmetric and typically works best between similar adversaries and from stronger to weaker adversary.

5 DISCUSSION
Why can tiny adversarial perturbations be detected that well? Adopting the boundary tilting perspective of Tanay & Griffin (2016), strong adversarial examples occur in situations in which classification boundaries are tilted against the data manifold such that they lie close and nearly parallel to the data manifold. A detector could (potentially) identify adversarial examples by detecting inputs which are slightly off the data manifold’s center in the direction of a nearby class boundary. Thus, the detector can focus on detecting inputs which move away from the data manifold in a certain direction, namely one of the directions to a nearby class boundary (the detector does not have explicit
knowledge of class boundaries but it might learn about their direction implicitly from the adversarial training data). However, training a detector which captures these directions in a model with small capacity and generalizes to unseen data requires certain regularities in adversarial perturbations. The results of Moosavi-Dezfooli et al. (2016a) suggest that there may exist regularities in the adversarial perturbations since universal perturbations exist. However, these perturbations are not unique and data-dependent adversaries might potentially choose among many different possible perturbations in a non-regular way, which would be hard to detect. Our positive results on detectability suggest that this is not the case for the tested adversaries. Thus, our results are somewhat complementary to Moosavi-Dezfooli et al. (2016a): while they show that universal, image-agnostic perturbations exist, we show that image-dependent perturbations are sufficiently regular to be detectable. Whether a detector generalizes over different adversaries depends mainly on whether the adversaries choose among many different possible perturbations in a consistent way.
Why is the joint classifier/detector system harder to fool? For a static detector, there might be areas which are adversarial to both classifier and detector; however, this will be a (small) subset of the areas which are adversarial to the classifier alone. Nevertheless, results in Section 4.1.2 show that such a static detector can be fooled along with the classifier. However, a dynamic detector is considerably harder to fool: on the one hand, it might further reduce the number of areas which are both adversarial to classifier and detector. On the other hand, the areas which are adversarial to the detector might become increasingly non-regular and difficult to find by gradient descent-based adversaries.

6 CONCLUSION AND OUTLOOK
In this paper, we have shown empirically that adversarial examples can be detected surprisingly well using a detector subnetwork attached to the main classification network. While this does not directly allow classifying adversarial examples correctly, it allows mitigating adversarial attacks against machine learning systems by resorting to fallback solutions, e.g., a face recognition might request human intervention when verifying a person’s identity and detecting a potential adversarial attack. Moreover, being able to detect adversarial perturbations may in the future enable a better understanding of adversarial examples by applying network introspection to the detector network. Furthermore, the gradient propagated back through the detector may be used as a source of regularization of the classifier against adversarial examples. We leave this to future work. Additional future work will be developing stronger adversaries that are harder to detect by adding effective randomization which would make selection of adversarial perturbations less regular. Finally, developing methods for training detectors explicitly such that they can detect many different kinds of attacks reliably at the same time would be essential for safety- and security-related applications.

ACKNOWLEDGMENTS
We would like to thank Michael Herman and Michael Pfeiffer for helpful discussions and their feedback on drafts of this article. Moreover, we would like to thank the developers of Theano (The Theano Development Team, 2016), keras (https://keras.io), and seaborn (http:// seaborn.pydata.org/).
","Machine learning and deep learning in particular has advanced tremendously on perceptual tasks in recent years. However, it remains vulnerable against adversarial perturbations of the input that have been crafted specifically to fool the system while being quasi-imperceptible to a human. In this work, we propose to augment deep neural networks with a small “detector” subnetwork which is trained on the binary classification task of distinguishing genuine data from data containing adversarial perturbations. Our method is orthogonal to prior work on addressing adversarial perturbations, which has mostly focused on making the classification network itself more robust. We show empirically that adversarial perturbations can be detected surprisingly well even though they are quasi-imperceptible to humans. Moreover, while the detectors have been trained to detect only a specific adversary, they generalize to similar and weaker adversaries. In addition, we propose an adversarial attack that fools both the classifier and the detector and a novel training procedure for the detector that counteracts this attack.",ICLR 2017 conference submission,True,,"I reviewed the manuscript on December 5th.

Summary:
The authors investigate the phenomenon of adversarial perturbations and ask whether one may build a system to independently detect an adversarial data point -- if one could detect an adversarial example, then might prevent a machine from automatically processing it. Importantly, the authors investigate whether it is possible to build an adversarial detector which is resilient to adversarial examples built against *both* the classifier and the detector. Their results suggest that training a detector in this more difficult setting still yields gains but does not entirely resolve the problem of detecting adversarial examples.

Major comments:

The authors describe a novel approach for dealing with adversarial examples from a security standpoint -- namely, build an independent system to detect the adversary so a human might intervene in those cases. 

A potential confound of this approach is that an adversary might respond by constructing adversarial examples to fool *both* the original classifier and the new detector. If that were possible, then this approach is moot since an attacker could always outwit the original system. To their credit, the authors show that building a 'dynamic' detector to detect adversarial examples but also be resilient to an adversary mitigates this potential escalation (worse case from 55% to 70% detection rate). Even though the 'dynamic' detector  demonstrates positive gains, I am concerned about overall scores. Detecting adversarial examples at this rate would not be a reliable security procedure.

My second comment is about 'model transferability'. My definition of 'model transferability' is different then the one used in the paper. My definition means that one constructs an adversarial example on one network and measures how well the adversarial examples attack a second trained model -- where the second model has been trained with different initial conditions. (The author's definition of 'transferability' is based on seeing how well the detector generalizes across training methods). 'Model transferability' (per my definition) is quite important because it measures how general an adversarial example is across all models -- and not specific to a given trained model. Different methods have different levels of 'model transferability' (Kurakin et al, 2016) and I am concerned how well the detector they built would be able to detect adversarial examples across *all models* and not just the trained model in question. In other words, a good detector would be able to detect adversarial examples from any network and not just one particularly trained network. This question seems largely unaddressed in this paper but perhaps I missed some subtle point in their descriptions.

Minor comments:

If there were any points in the bottom-left of the Figure 2 left, then this would be very important to see -- perhaps move the legend to highlight if the area contains no points.

- X-axis label is wrong in Figure 2 right.

Measure the transferability of the detector?

- How is \sigma labeled on Figure 5?

- Whenever an image is constructed to be an 'adversary', has the image actually been tested to see if it is adversarial? In other words, does the adversarial image actually result in a misclassification by the original network?

---

The paper explores the automatic detection of adversarial examples by training a classifier to recognize them. This is an interesting direction, even though they are obviously concerns about training an adversary to circumvent this model. Nonetheless, the experimental results presented in the paper are of interest to the ICLR audience. Many of the initial reviewer comments appear to be appropriately addressed in the revision of the paper.

---

We have uploaded a new revision of the paper in which we have tried to address the reviewer comments. Here is a more detailed changelog:

* Fixed a bug in the ImageNet experiment: we originally applied the softmax operator twice (once before and once after selecting the ten target classes). This did not affect the accuracy of the classification network but made the network harder to fool by adversaries for similar reasons as in the ``defensive distillation'' approach. We have corrected the issue in the updated version of the paper by applying softmax only after selection the ten target classes. To briefly summarize the corrected results: adversaries remain detectable with an accuracy of at least 85% (with the same exception as before, the basic iterative l2-based adversary for epsilon=400). More details are contained in the updated Section 4.2. Sorry for this error in the first revision.
* Fixed wrong resolution in Figure 1 (16x16 instead of 8x8). Thanks to AnonReviewer3 for noting this.
* Input range specified to be [0, 255] (Section 4.1.1). Thanks to AnonReviewer1 for requesting clarification on this.
* Clarified computation of adversarial detectability (footnote in Section 4.1.1).
* We discuss briefly that dynamic adversaries are based on stronger assumptions than static adversaries (footnote in Section 3.3)
* Clarified that we did use version 1 of DeepFool (Section 3.1)
* Fixed x-axis label in Figure 2 (right). Thanks to AnonReviewer2 for noting this.
* Moved legend in Figure 2 (left) to upper right corner based on suggestion of AnonReviewer2.
* Clarified choices of \sigma in Figure 5
* Adding more details about the dynamic adversary training method.

---

This paper proposes a new idea to help defending adversarial examples by training a complementary classifier to detect them. The results of the paper show that adversarial examples in fact can be easily detected. Moreover, such detector generalizes well to other similar or weaker adversarial examples. The idea of this paper is simple but non-trivial. While no final scheme is proposed in the paper how this idea can help in building defensive systems, it actually provides a potential new direction. Based on its novelty, I suggest an acceptance.

My main concern of this paper is about its completeness. No effective method is reported in the paper to defend the dynamic adversaries. It could be difficult to do so, but rather the paper doesn’t seem to put much effort to investigate this part. How difficult it is to defend the dynamic adversaries is an important and interesting question following the conclusions of this paper. Such investigation may essentially help improve our understanding of adversarial examples.
That being said, the novelty of this paper is still significant.

Minor comment:
The paper needs to improve its clarity. Some important details are skipped in the paper. For example, the paper should provide more details about the dynamic adversaries and the dynamic adversary training method.

---

This paper explores an important angle to adversarial examples: the detection of adversarial images and their utilization for trainig more robust networks.

This takes the competition between adversaries and models to a new level. The paper presents appealing evidence for the feasibility of robustifying networks by employing the a detector subnetwork that is trained particularly for the purpose of detecting the adversaries in a terget manner rather than just making the networks themselves robust to adversarial examples.

The jointly trained primary/detector system is evaluated in various scenarios including the cases when the adversary generator has access to the model and those where they are generated in a generic way.

The results of the paper show good improvements with the approach and present well motived thorough analyses to back the main message. The writing is clear and concise.

---

I reviewed the manuscript on December 5th.

Summary:
The authors investigate the phenomenon of adversarial perturbations and ask whether one may build a system to independently detect an adversarial data point -- if one could detect an adversarial example, then might prevent a machine from automatically processing it. Importantly, the authors investigate whether it is possible to build an adversarial detector which is resilient to adversarial examples built against *both* the classifier and the detector. Their results suggest that training a detector in this more difficult setting still yields gains but does not entirely resolve the problem of detecting adversarial examples.

Major comments:

The authors describe a novel approach for dealing with adversarial examples from a security standpoint -- namely, build an independent system to detect the adversary so a human might intervene in those cases. 

A potential confound of this approach is that an adversary might respond by constructing adversarial examples to fool *both* the original classifier and the new detector. If that were possible, then this approach is moot since an attacker could always outwit the original system. To their credit, the authors show that building a 'dynamic' detector to detect adversarial examples but also be resilient to an adversary mitigates this potential escalation (worse case from 55% to 70% detection rate). Even though the 'dynamic' detector  demonstrates positive gains, I am concerned about overall scores. Detecting adversarial examples at this rate would not be a reliable security procedure.

My second comment is about 'model transferability'. My definition of 'model transferability' is different then the one used in the paper. My definition means that one constructs an adversarial example on one network and measures how well the adversarial examples attack a second trained model -- where the second model has been trained with different initial conditions. (The author's definition of 'transferability' is based on seeing how well the detector generalizes across training methods). 'Model transferability' (per my definition) is quite important because it measures how general an adversarial example is across all models -- and not specific to a given trained model. Different methods have different levels of 'model transferability' (Kurakin et al, 2016) and I am concerned how well the detector they built would be able to detect adversarial examples across *all models* and not just the trained model in question. In other words, a good detector would be able to detect adversarial examples from any network and not just one particularly trained network. This question seems largely unaddressed in this paper but perhaps I missed some subtle point in their descriptions.

Minor comments:

If there were any points in the bottom-left of the Figure 2 left, then this would be very important to see -- perhaps move the legend to highlight if the area contains no points.

- X-axis label is wrong in Figure 2 right.

Measure the transferability of the detector?

- How is \sigma labeled on Figure 5?

- Whenever an image is constructed to be an 'adversary', has the image actually been tested to see if it is adversarial? In other words, does the adversarial image actually result in a misclassification by the original network?

---

I reviewed the manuscript on December 5th.

Summary:
The authors investigate the phenomenon of adversarial perturbations and ask whether one may build a system to independently detect an adversarial data point -- if one could detect an adversarial example, then might prevent a machine from automatically processing it. Importantly, the authors investigate whether it is possible to build an adversarial detector which is resilient to adversarial examples built against *both* the classifier and the detector. Their results suggest that training a detector in this more difficult setting still yields gains but does not entirely resolve the problem of detecting adversarial examples.

Major comments:

The authors describe a novel approach for dealing with adversarial examples from a security standpoint -- namely, build an independent system to detect the adversary so a human might intervene in those cases. 

A potential confound of this approach is that an adversary might respond by constructing adversarial examples to fool *both* the original classifier and the new detector. If that were possible, then this approach is moot since an attacker could always outwit the original system. To their credit, the authors show that building a 'dynamic' detector to detect adversarial examples but also be resilient to an adversary mitigates this potential escalation (worse case from 55% to 70% detection rate). Even though the 'dynamic' detector  demonstrates positive gains, I am concerned about overall scores. Detecting adversarial examples at this rate would not be a reliable security procedure.

My second comment is about 'model transferability'. My definition of 'model transferability' is different then the one used in the paper. My definition means that one constructs an adversarial example on one network and measures how well the adversarial examples attack a second trained model -- where the second model has been trained with different initial conditions. (The author's definition of 'transferability' is based on seeing how well the detector generalizes across training methods). 'Model transferability' (per my definition) is quite important because it measures how general an adversarial example is across all models -- and not specific to a given trained model. Different methods have different levels of 'model transferability' (Kurakin et al, 2016) and I am concerned how well the detector they built would be able to detect adversarial examples across *all models* and not just the trained model in question. In other words, a good detector would be able to detect adversarial examples from any network and not just one particularly trained network. This question seems largely unaddressed in this paper but perhaps I missed some subtle point in their descriptions.

Minor comments:

If there were any points in the bottom-left of the Figure 2 left, then this would be very important to see -- perhaps move the legend to highlight if the area contains no points.

- X-axis label is wrong in Figure 2 right.

Measure the transferability of the detector?

- How is \sigma labeled on Figure 5?

- Whenever an image is constructed to be an 'adversary', has the image actually been tested to see if it is adversarial? In other words, does the adversarial image actually result in a misclassification by the original network?

---

The paper explores the automatic detection of adversarial examples by training a classifier to recognize them. This is an interesting direction, even though they are obviously concerns about training an adversary to circumvent this model. Nonetheless, the experimental results presented in the paper are of interest to the ICLR audience. Many of the initial reviewer comments appear to be appropriately addressed in the revision of the paper.

---

We have uploaded a new revision of the paper in which we have tried to address the reviewer comments. Here is a more detailed changelog:

* Fixed a bug in the ImageNet experiment: we originally applied the softmax operator twice (once before and once after selecting the ten target classes). This did not affect the accuracy of the classification network but made the network harder to fool by adversaries for similar reasons as in the ``defensive distillation'' approach. We have corrected the issue in the updated version of the paper by applying softmax only after selection the ten target classes. To briefly summarize the corrected results: adversaries remain detectable with an accuracy of at least 85% (with the same exception as before, the basic iterative l2-based adversary for epsilon=400). More details are contained in the updated Section 4.2. Sorry for this error in the first revision.
* Fixed wrong resolution in Figure 1 (16x16 instead of 8x8). Thanks to AnonReviewer3 for noting this.
* Input range specified to be [0, 255] (Section 4.1.1). Thanks to AnonReviewer1 for requesting clarification on this.
* Clarified computation of adversarial detectability (footnote in Section 4.1.1).
* We discuss briefly that dynamic adversaries are based on stronger assumptions than static adversaries (footnote in Section 3.3)
* Clarified that we did use version 1 of DeepFool (Section 3.1)
* Fixed x-axis label in Figure 2 (right). Thanks to AnonReviewer2 for noting this.
* Moved legend in Figure 2 (left) to upper right corner based on suggestion of AnonReviewer2.
* Clarified choices of \sigma in Figure 5
* Adding more details about the dynamic adversary training method.

---

This paper proposes a new idea to help defending adversarial examples by training a complementary classifier to detect them. The results of the paper show that adversarial examples in fact can be easily detected. Moreover, such detector generalizes well to other similar or weaker adversarial examples. The idea of this paper is simple but non-trivial. While no final scheme is proposed in the paper how this idea can help in building defensive systems, it actually provides a potential new direction. Based on its novelty, I suggest an acceptance.

My main concern of this paper is about its completeness. No effective method is reported in the paper to defend the dynamic adversaries. It could be difficult to do so, but rather the paper doesn’t seem to put much effort to investigate this part. How difficult it is to defend the dynamic adversaries is an important and interesting question following the conclusions of this paper. Such investigation may essentially help improve our understanding of adversarial examples.
That being said, the novelty of this paper is still significant.

Minor comment:
The paper needs to improve its clarity. Some important details are skipped in the paper. For example, the paper should provide more details about the dynamic adversaries and the dynamic adversary training method.

---

This paper explores an important angle to adversarial examples: the detection of adversarial images and their utilization for trainig more robust networks.

This takes the competition between adversaries and models to a new level. The paper presents appealing evidence for the feasibility of robustifying networks by employing the a detector subnetwork that is trained particularly for the purpose of detecting the adversaries in a terget manner rather than just making the networks themselves robust to adversarial examples.

The jointly trained primary/detector system is evaluated in various scenarios including the cases when the adversary generator has access to the model and those where they are generated in a generic way.

The results of the paper show good improvements with the approach and present well motived thorough analyses to back the main message. The writing is clear and concise.

---

I reviewed the manuscript on December 5th.

Summary:
The authors investigate the phenomenon of adversarial perturbations and ask whether one may build a system to independently detect an adversarial data point -- if one could detect an adversarial example, then might prevent a machine from automatically processing it. Importantly, the authors investigate whether it is possible to build an adversarial detector which is resilient to adversarial examples built against *both* the classifier and the detector. Their results suggest that training a detector in this more difficult setting still yields gains but does not entirely resolve the problem of detecting adversarial examples.

Major comments:

The authors describe a novel approach for dealing with adversarial examples from a security standpoint -- namely, build an independent system to detect the adversary so a human might intervene in those cases. 

A potential confound of this approach is that an adversary might respond by constructing adversarial examples to fool *both* the original classifier and the new detector. If that were possible, then this approach is moot since an attacker could always outwit the original system. To their credit, the authors show that building a 'dynamic' detector to detect adversarial examples but also be resilient to an adversary mitigates this potential escalation (worse case from 55% to 70% detection rate). Even though the 'dynamic' detector  demonstrates positive gains, I am concerned about overall scores. Detecting adversarial examples at this rate would not be a reliable security procedure.

My second comment is about 'model transferability'. My definition of 'model transferability' is different then the one used in the paper. My definition means that one constructs an adversarial example on one network and measures how well the adversarial examples attack a second trained model -- where the second model has been trained with different initial conditions. (The author's definition of 'transferability' is based on seeing how well the detector generalizes across training methods). 'Model transferability' (per my definition) is quite important because it measures how general an adversarial example is across all models -- and not specific to a given trained model. Different methods have different levels of 'model transferability' (Kurakin et al, 2016) and I am concerned how well the detector they built would be able to detect adversarial examples across *all models* and not just the trained model in question. In other words, a good detector would be able to detect adversarial examples from any network and not just one particularly trained network. This question seems largely unaddressed in this paper but perhaps I missed some subtle point in their descriptions.

Minor comments:

If there were any points in the bottom-left of the Figure 2 left, then this would be very important to see -- perhaps move the legend to highlight if the area contains no points.

- X-axis label is wrong in Figure 2 right.

Measure the transferability of the detector?

- How is \sigma labeled on Figure 5?

- Whenever an image is constructed to be an 'adversary', has the image actually been tested to see if it is adversarial? In other words, does the adversarial image actually result in a misclassification by the original network?",,,,,,6.333333333333333,,,3.6666666666666665,,
478,"LEARNING INVARIANT REPRESENTATIONS OF PLANAR CURVES
Authors: Gautam Pai, Aaron Wetzler
Source file: 478.pdf

ABSTRACT
We propose a metric learning framework for the construction of invariant geometric functions of planar curves for the Euclidean and Similarity group of transformations. We leverage on the representational power of convolutional neural networks to compute these geometric quantities. In comparison with axiomatic constructions, we show that the invariants approximated by the learning architectures have better numerical qualities such as robustness to noise, resiliency to sampling, as well as the ability to adapt to occlusion and partiality. Finally, we develop a novel multi-scale representation in a similarity metric learning paradigm.

1 INTRODUCTION
The discussion on invariance is a strong component of the solutions to many classical problems in numerical differential geometry. A typical example is that of planar shape analysis where one desires to have a local function of the contour which is invariant to rotations, translations and reflections like the Euclidean curvature. This representation can be used to obtain correspondence between the shapes and also to compare and classify them. However, the numerical construction of such functions from discrete sampled data is non-trivial and requires robust numerical techniques for their stable and efficient computation.
Convolutional neural networks have been very successful in recent years in solving problems in image processing, recognition and classification. Efficient architectures have been studied and developed to extract semantic features from images invariant to a certain class or category of transformations. Coupled with efficient optimization routines and more importantly, a large amount of data, a convolutional neural network can be trained to construct invariant representations and semantically significant features of images as well as other types of data such as speech and language. It is widely acknowledged that such networks have superior representational power compared to more principled methods with more handcrafted features such as wavelets, Fourier methods, kernels etc. which are not optimal for more semantic data processing tasks.
In this paper we connect two seemingly different fields: convolutional neural network based metric learning methods and numerical differential geometry. The results we present are the outcome of investigating the question: ”Can metric learning methods be used to construct invariant geometric quantities?” By training with a Siamese configuration involving only positive and negative examples of Euclidean transformations, we show that the network is able to train for an invariant geometric function of the curve which can be contrasted with a theoretical quantity: Euclidean curvature. An example of each can be seen Figure 1. We compare the learned invariant functions with axiomatic counterparts and provide a discussion on their relationship. Analogous to principled constructions like curvature-scale space methods and integral invariants, we develop a multi-scale representation using a data-dependent learning based approach. We show that network models are able to construct geometric invariants that are numerically more stable and robust than these more principled approaches. We contrast the computational work-flow of a typical numerical geometry pipeline with that of the convolutional neural network model and develop a relationship among them highlighting important geometric ideas.
In Section 2 we begin by giving a brief summary of the theory and history of invariant curve representations. In Section 3 we explain our main contribution of casting the problem into the form which
enables training a convolutional neural network for generating invariant signatures to the Euclidean and Similarity group transformations. Section 4 provides a discussion on developing a multi-scale representation followed by the experiments and discussion in Section 5.

2 BACKGROUND
An invariant representation of a curve is the set of signature functions assigned to every point of the curve which does not change despite the action of a certain type of transformation. A powerful theorem from E. Cartan (Cartan (1983)) and Sophus Lie (Ackerman (1976)) characterizes the space of these invariant signatures. It begins with the concept of arc-length which is a generalized notion of the length along a curve. Given a type of transformation, one can construct an intrinsic arclength that is independent of the parameterization of the curve, and compute the curvature with respect to this arc-length. The fundamental invariants of the curve, known as differential invariants (Bruckstein & Netravali (1995), Calabi et al. (1998)) are the set of functions comprising of the curvature and its successive derivatives with respect to the invariant arc-length. These differential invariants are unique in a sense that two curves are related by the group transformation if and only if their differential invariant signatures are identical. Moreover, every invariant of the curve is a
function of these fundamental differential invariants. Consider C(p) = [ x(p) y(p) ] : a planar curve with
coordinates x and y parameterized by some parameter p. The Euclidean arc-length, is given by
s(p) = ∫ p 0 |Cp| dp = ∫ p 0 √ x2p + y 2 p dp, (1)
where xp = dxdp , and yp = dy dp and the principal invariant signature, that is the Euclidean curvature is given by
κ(p) = det(Cp, Cpp) |Cp|3 = xpypp − ypxpp
(x2p + y 2 p)
3 2
. (2)
Thus, we have the Euclidean differential invariant signatures given by the set {κ, κs, κss ...} for every point on the curve. Cartan’s theorem provides an axiomatic construction of invariant signatures and the uniqueness property of the theorem guarantees their theoretical validity. Their importance is highlighted from the fact that any invariant is a function of the fundamental differential invariants.
The difficulty with differential invariants is their stable numerical computation. Equations 1 and 2, involve non-linear functions of derivatives of the curve and this poses serious numerical issues for their practical implementation where noise and poor sampling techniques are involved. Apart from methods like Pajdla & Van Gool (1995) and Weiss (1993), numerical considerations motivated the development of multi-scale representations. These methods used alternative constructions of invariant signatures which were robust to noise. More importantly, they allowed a hierarchical representation, in which the strongest and the most global components of variation in the contour of the curve are encoded in signatures of higher scale, and as we go lower, the more localized and rapid changes get injected into the representation. Two principal methods in this category are scale-space methods and integral invariants. In scale-space methods (Mokhtarian & Mackworth (1992); Sapiro & Tannenbaum (1995); Bruckstein et al. (1996)), the curve is subjected to an invariant evolution process where it can be evolved to different levels of abstraction. See Figure 5. The curvature function
at each evolved time t is then recorded as an invariant. For example, {κ(s, t), κs(s, t), κss(s, t)...} would be the Euclidean-invariant representations at scale t.
Integral invariants (Manay et al. (2004); Fidler et al. (2008); Pottmann et al. (2009); Hong & Soatto (2015)) are invariant signatures which compute integral measures along the curve. For example, for each point on the contour, the integral area invariant computes the area of the region obtained from the intersection of a ball of radius r placed at that point and the interior of the contour. The integral nature of the computation gives the signature robustness to noise and by adjusting different radii of the ball r one can associate a scale-space of responses for this invariant. Fidler et al. (2008) and Pottmann et al. (2009) provide a detailed treatise on different types of integral invariants and their properties.
It is easy to observe that differential and integral invariants can be thought of as being obtained from non-linear operations of convolution filters. The construction of differential invariants employ filters for which the action is equivalent to numerical differentiation (high pass filtering) whereas integral invariants use filters which act like numerical integrators (low pass filtering) for stabilizing the invariant. This provides a motivation to adopt a learning based approach and we demonstrate that the process of estimating these filters and functions can be outsourced to a learning framework. We use the Siamese configuration for implementing this idea. Such configurations have been used in signature verification (Bromley et al. (1993)), face-verification and recognition(Sun et al. (2014); Taigman et al. (2014); Hu et al. (2014)), metric learning (Chopra et al. (2005)), image descriptors (Carlevaris-Bianco & Eustice (2014)), dimensionality reduction (Hadsell et al. (2006)) and also for generating 3D shape descriptors for correspondence and retrieval (Masci et al. (2015); Xie et al. (2015)). In these papers, the goal was to learn the descriptor and hence the similarity metric from data using notions of only positive and negative examples. We use the same framework for estimation of geometric invariants. However, in contrast to these methods, we contribute an analysis of the output descriptor and provide a geometric context to the learning process. The contrastive loss function driving the training ensures that the network chooses filters which push and pull different features of the curve into the invariant by balancing a mix of robustness and fidelity.

3 TRAINING FOR INVARIANCE
A planar curve can be represented either explicitly by sampling points on the curve or using an implicit representation such as level sets (Kimmel (2012)). We work with an explicit representation of simple curves (open or closed) with random variable sampling of the points along the curve. Thus, every curve is a N × 2 array denoting the X and Y coordinates of the N points. We build a convolutional neural network which inputs a curve and outputs a representation or signature for every point on the curve. We can interpret this architecture as an algorithmic scheme of representing a function over the curve. However feeding in a single curve is insufficient and instead we run this convolutional architecture in a Siamese configuration (Figure 2) that accepts a curve and a
transformed version (positive) of the curve or an unrelated curve (negative). By using two identical copies of the same network sharing weights to process these two curves we are able to extract geometric invariance by using a loss function to require that the two arms of the Siamese configuration must produce values that are minimally different for curves which are related by Euclidean transformations representing positive examples and maximally different for carefully constructed negative examples. To fully enable training of our network we build a large dataset comprising of positive and negative examples of the relevant transformations from a database of curves. We choose to minimize the contrastive loss between the two outputs of the Siamese network as this directs the network architecture to model a function over the curve which is invariant to the transformation.

3.1 LOSS FUNCTION
We employ the contrastive loss function (Chopra et al. (2005); LeCun et al. (2006)) for training our network. The Siamese configuration comprises of two identical networks of Figure 3 computing signatures for two separate inputs of data. Associated to each input pair is a label which indicates whether or not that pair is a positive (λ = 1) or a negative (λ = 0) example (Figure 2). Let C1i and C2i be the curves imputed to first and second arm of the configuration for the ith example of the data with label λi. Let SΘ(C) denote the output of the network for a given set of weights Θ for input curve C. The contrastive loss function is given by:
C(Θ) = 1 N { i=N∑ i=1 λi || SΘ(C1i)−SΘ(C2i) || + (1−λi) max( 0, µ − || SΘ(C1i)−SΘ(C2i) || ) } , (3) where µ is a cross validated hyper-parameter known as margin which defines the metric threshold beyond which negative examples are penalized.

3.2 ARCHITECTURE
The network inputs a N × 2 array representing the coordinates of N points along the curve. The sequential nature of the curves and the mostly 1D-convolution operations can also be looked at from the point of view of temporal signals using recurrent neural network architectures. Here however we choose instead to use a multistage CNN pipeline. The network, given by one arm of the Siamese configuration, comprises of three stages that use layer units which are typically considered the basic building blocks of modern CNN architectures. Each stage contains two sequential batches of convolutions appended with rectified linear units (ReLU) and ending with a max unit. The convolutional unit comprises of convolutions with 15 filters of width 5 as depicted in Figure 3. The max unit computes the maximum of 15 responses per point to yield an intermediate output after each stage. The final stage is followed by a linear layer which linearly combines the responses to yield the final output. Since, every iteration of convolution results in a reduction of the sequence length, sufficient padding is provided on both ends of the curve. This ensures that the value of the signature at a point is the result of the response of the computation resulting from the filter centered around that point.

3.3 BUILDING REPRESENTATIVE DATASETS AND IMPLEMENTATION
In order to train for invariance, we need to build a dataset with two major attributes: First, it needs to contain a large number of examples of the transformation and second, the curves involved in the training need to have sufficient richness in terms of different patterns of sharp edges, corners, smoothness, noise and sampling factors to ensure sufficient generalizability of the model. To sufficiently span the space of Euclidean transformations, we generate random two dimensional rotations by uniformly sampling angles from [−π, π]. The curves are normalized by removing the mean and dividing by the standard deviation thereby achieving invariance to translations and uniform scaling. The contours are extracted from the shapes of the MPEG7 Database (Latecki et al. (2000)) as shown in first part of Figure 4. It comprises a total of 1400 shapes containing 70 different categories of objects. 700 of the total were used for training and 350 each for testing and validation. The positive examples are constructed by taking a curve and randomly transforming it by a rotation, translation and reflection and pairing them together. The negative examples are obtained by pairing curves which are deemed dissimilar as explained in Section 4. The contours are extracted and each contour is sub-sampled to 500 points. We build the training dataset of 10, 000 examples with approximately 50% each for the positive and negative examples. The network and training is performed using the Torch library Collobert et al. (2002). We trained using Adagrad Duchi et al. (2011) at a learning rate of 5 × 10−4 and a batch size of 10. We set the contrastive loss hyperparameter margin µ = 1 and Figure 4 shows the error plot for training and the convergence of the loss to a minimum. The rest of this work describes how we can observe and extend the efficacy of the trained network on new data.

4 MULTI-SCALE REPRESENTATIONS
Invariant representations at varying levels of abstraction have a theoretical interest as well as practical importance to them. Enumeration at different scales enables a hierarchical method of analysis which is useful when there is noise and hence stability is desired in the invariant. As mentioned in Section 2, the invariants constructed from scale-space methods and integral invariants, naturally allow for such a decomposition by construction.
A valuable insight for multi-scale representations is provided in the theorems of Gage, Hamilton and Grayson (Gage & Hamilton (1986); Grayson (1987)). It says that if we evolve any smooth nonintersecting planar curve with mean curvature flow, which is invariant to Euclidean transformations, it will ultimately converge into a circle before vanishing into a point. The curvature corresponding to this evolution follows a profile as shown in Figure 5, going from a possibly noisy descriptive feature to a constant function. In our framework, we observe an analogous behavior in a data-dependent setting. The positive part of the loss function (λ = 1) forces the network to push the outputs of the positive examples closer, whereas the negative part (λ = 0) forces the weights of network to push the outputs of the negative examples apart, beyond the distance barrier of µ. If the training data does not contain any negative example, it is easy to see that the weights of the network will converge to a point which will yield a constant output that trivially minimizes the loss function in Equation 3.
Curvature: κ
Figure 5: Curve evolution and the corresponding curvature profile.
1 2 3 4 5
1 2 3 4 5
Figure 6: Experiments with multi-scale representations. Each signature is the output of a network trained on a dataset with training examples formed as per the rows of Table 1. Index1 indicates low and 5 indicates a higher level of abstraction.
This is analogous to that point in curvature flow which yields a circle and therefore has a constant curvature.
Designing the negative examples of the training data provides the means to obtain a multi-scale representation. Since we are training for a local descriptor of a curve, that is, a function whose value at a point depends only on its local neighborhood, a negative example must pair curves such that corresponding points on each curve must have different local neighborhoods. One such possibility is to construct negative examples which pair curves with their smoothed or evolved versions as in Table 1. Minimizing the loss function in equation 3 would lead to an action which pushes apart the signatures of the curve and its evolved or smoothed counterpart, thereby injecting the signature with fidelity and descriptiveness. We construct separate data-sets where the negative examples are drawn as shown in the rows of Table1 and train a network model for each of them using the loss function 3. In our experiments we perform smoothing by using a local polynomial regression with weighted linear least squares for obtaining the evolved contour. Figure 6 shows the outputs of these different networks which demonstrate a scale-space like behavior.

5 EXPERIMENTS AND DISCUSSION
Ability to handle low signal to noise ratios and efficiency of computation are typical qualities desired in a geometric invariant. To test the numerical stability and robustness of the invariant signatures
we designed two experiments. In the first experiment, we add increasing levels of zero-mean Gaussian noise to the curve and compare the three types of signatures: differential (Euclidean curvature), integral (integral area invariant) and the output of our network (henceforth termed as network invariant) as shown in Figure 7. Apart from adding noise, we also rotate the curve to obtain a better assessment of the Euclidean invariance property. In Figure 8, we test descriptiveness of the signature under noisy conditions in a shape retrieval task for a set of 30 shapes with 6 different categories. For every curve, we generate 5 signatures at different scales for the integral and the network invariant and use them as a representation for that shape. We use the Hausdorff distance as a distance measure (Bronstein et al. (2008)) between the two sets of signatures to rank the shapes for retrieval. Figure 7 and 8 demonstrate the robustness of the network especially at high noise levels.
In the second experiment, we decimate a high resolution contour at successive resolutions by randomly sub-sampling and redistributing a set of its points (marked blue in Figure 9) and observe the signatures at certain fixed points (marked red in Figure 9) on the curve. Figure 9 shows that the network is able to handle these changes in sampling and compares well with the integral invariant. Figures 7 and Figure 9 represent behavior of geometric signatures for two different tests: large noise for a moderate strength of signal and low signal for a moderate level of noise.

6 CONCLUSION
We have demonstrated a method to learn geometric invariants of planar curves. Using just positive and negative examples of Euclidean transformations, we showed that a convolutional neural network
70% 50% 30%
20% 10% 5%
0 10 20 30 40 50 60
is able to effectively discover and encode transform-invariant properties of curves while remaining numerically robust in the face of noise. By using a geometric context to the training process we were able to develop novel multi-scale representations from a learning based approach without explicitly
enforcing such behavior. As compared to a more axiomatic framework of modeling with differential geometry and engineering with numerical analysis, we demonstrated a way of replacing this pipeline with a deep learning framework which combines both these aspects. The non-specific nature of this framework can be seen as providing the groundwork for future deep learning data based problems in differential geometry.

ACKNOWLEDGMENTS
This project has received funding from the European Research Council (ERC) under the European Unions Horizon 2020 research and innovation program (grant agreement No 664800)
","We propose a metric learning framework for the construction of invariant geometric functions of planar curves for the Euclidean and Similarity group of transformations. We leverage on the representational power of convolutional neural networks to compute these geometric quantities. In comparison with axiomatic constructions, we show that the invariants approximated by the learning architectures have better numerical qualities such as robustness to noise, resiliency to sampling, as well as the ability to adapt to occlusion and partiality. Finally, we develop a novel multi-scale representation in a similarity metric learning paradigm.",ICLR 2017 conference submission,True,,"Pros : 
- New representation with nice properties that are derived and compared with a mathematical baseline and background
- A simple algorithm to obtain the representation

Cons :
- The paper sounds like an applied maths paper, but further analysis on the nature of the representation could be done, for instance, by understanding the nature of each layer, or at least, the first.

---

This work proposes learning of local representations of planar curves using convolutional neural networks.
 Invariance to rigid transformations and discriminability are enforced with a metric learning framework using a siamese architecture. Preliminary experiments on toy datasets compare favorably with predefined geometric invariants (both differential and integral). 
 
 The reviewers found value on the problem set-up and the proposed model, and were generally satisfied with the author's response. They also expressed concern that the experimental section is currently a bit weak and does not include any real data. Also, the paper does not offer theoretical insights that inform us about the design of the representation or about the provable invariance guarantees. All things considered, the AC recommends acceptance in the form of a poster, but strongly encourages the authors to strengthen the work both in the experimental and the theoretical aspects.

---

Based on the constructive suggestions of the reviewers, we have updated the paper with two minor changes:

(1.) We have added a figure in the appendix section, showing the learned filters from the first layer of the network.

(2.) We have added an additional line in Section 4:  ""Since we are training for a local descriptor of a curve, that is, a function whose value at a point depends only on its local neighborhood, a negative example must pair curves such that corresponding points on each curve must have different local neighborhoods"",  in order to highlight the locality property of our framework.

---

I'm torn on this one. Seeing the MPEG-7 dataset and references to curvature scale space brought to mind the old saying that ""if it's not worth doing, it's not worth doing well."" There is no question that the MPEG-7 dataset/benchmark got saturated long ago, and it's quite surprising to see it in a submission to a modern ML conference. I brought up the question of ""why use this representation"" with the authors and they said their ""main purpose was to connect the theory of differential geometry of curves with the computational engine of a convolutional neural network."" Fair enough. I agree these are seemingly different fields, and the authors deserve some credit for connecting them. If we give them the benefit of the doubt that this was worth doing, then the approach they pursue using a Siamese configuration makes sense, and their adaptation of deep convnet frameworks to 1D signals is reasonable. To the extent that the old invariant based methods made use of smoothed/filtered representations coupled with nonlinearities, it's sensible to revisit this problem using convnets. I wouldn't mind seeing this paper accepted, since it's different from the mainstream, but I worry about there being too narrow an audience at ICLR that still cares about this type of shape representation.

---

Authors show that a contrastive loss for a Siamese architecture can be used for learning representations for planar curves. With the proposed framework, authors are able to learn a representation which is comparable to traditional differential or integral invariants, as evaluated on few toy examples.

The paper is generally well written and shows an interesting application of the Siamese architecture. However, the experimental evaluation and the results show that these are rather preliminary results as not many of the choices are validated. My biggest concern is in the choice of the negative samples, as the network basically learns only to distinguish between shapes at different scales, instead of recognizing different shapes. It is well known fact that in order to achieve a good performance with the contrastive loss, one has to be careful about the hard negative sampling, as using too easy negatives may lead to inferior results. Thus, this may be the underlying reason for such choice of the negatives? Unfortunately, this is not discussed in the paper.

Furthermore the paper misses a more thorough quantitative evaluation and concentrates more on showing particular examples, instead of measuring more robust statistics over multiple curves (invariance to noise and sampling artifacts).

In general, the paper shows interesting first steps in this direction, however it is not clear whether the experimental section is strong and thorough enough for the ICLR conference. Also the novelty of the proposed idea is limited as Siamese networks are used for many years and this work only shows that they can be applied to a different task.

---

Pros : 
- New representation with nice properties that are derived and compared with a mathematical baseline and background
- A simple algorithm to obtain the representation

Cons :
- The paper sounds like an applied maths paper, but further analysis on the nature of the representation could be done, for instance, by understanding the nature of each layer, or at least, the first.

---

Pros : 
- New representation with nice properties that are derived and compared with a mathematical baseline and background
- A simple algorithm to obtain the representation

Cons :
- The paper sounds like an applied maths paper, but further analysis on the nature of the representation could be done, for instance, by understanding the nature of each layer, or at least, the first.

---

This work proposes learning of local representations of planar curves using convolutional neural networks.
 Invariance to rigid transformations and discriminability are enforced with a metric learning framework using a siamese architecture. Preliminary experiments on toy datasets compare favorably with predefined geometric invariants (both differential and integral). 
 
 The reviewers found value on the problem set-up and the proposed model, and were generally satisfied with the author's response. They also expressed concern that the experimental section is currently a bit weak and does not include any real data. Also, the paper does not offer theoretical insights that inform us about the design of the representation or about the provable invariance guarantees. All things considered, the AC recommends acceptance in the form of a poster, but strongly encourages the authors to strengthen the work both in the experimental and the theoretical aspects.

---

Based on the constructive suggestions of the reviewers, we have updated the paper with two minor changes:

(1.) We have added a figure in the appendix section, showing the learned filters from the first layer of the network.

(2.) We have added an additional line in Section 4:  ""Since we are training for a local descriptor of a curve, that is, a function whose value at a point depends only on its local neighborhood, a negative example must pair curves such that corresponding points on each curve must have different local neighborhoods"",  in order to highlight the locality property of our framework.

---

I'm torn on this one. Seeing the MPEG-7 dataset and references to curvature scale space brought to mind the old saying that ""if it's not worth doing, it's not worth doing well."" There is no question that the MPEG-7 dataset/benchmark got saturated long ago, and it's quite surprising to see it in a submission to a modern ML conference. I brought up the question of ""why use this representation"" with the authors and they said their ""main purpose was to connect the theory of differential geometry of curves with the computational engine of a convolutional neural network."" Fair enough. I agree these are seemingly different fields, and the authors deserve some credit for connecting them. If we give them the benefit of the doubt that this was worth doing, then the approach they pursue using a Siamese configuration makes sense, and their adaptation of deep convnet frameworks to 1D signals is reasonable. To the extent that the old invariant based methods made use of smoothed/filtered representations coupled with nonlinearities, it's sensible to revisit this problem using convnets. I wouldn't mind seeing this paper accepted, since it's different from the mainstream, but I worry about there being too narrow an audience at ICLR that still cares about this type of shape representation.

---

Authors show that a contrastive loss for a Siamese architecture can be used for learning representations for planar curves. With the proposed framework, authors are able to learn a representation which is comparable to traditional differential or integral invariants, as evaluated on few toy examples.

The paper is generally well written and shows an interesting application of the Siamese architecture. However, the experimental evaluation and the results show that these are rather preliminary results as not many of the choices are validated. My biggest concern is in the choice of the negative samples, as the network basically learns only to distinguish between shapes at different scales, instead of recognizing different shapes. It is well known fact that in order to achieve a good performance with the contrastive loss, one has to be careful about the hard negative sampling, as using too easy negatives may lead to inferior results. Thus, this may be the underlying reason for such choice of the negatives? Unfortunately, this is not discussed in the paper.

Furthermore the paper misses a more thorough quantitative evaluation and concentrates more on showing particular examples, instead of measuring more robust statistics over multiple curves (invariance to noise and sampling artifacts).

In general, the paper shows interesting first steps in this direction, however it is not clear whether the experimental section is strong and thorough enough for the ICLR conference. Also the novelty of the proposed idea is limited as Siamese networks are used for many years and this work only shows that they can be applied to a different task.

---

Pros : 
- New representation with nice properties that are derived and compared with a mathematical baseline and background
- A simple algorithm to obtain the representation

Cons :
- The paper sounds like an applied maths paper, but further analysis on the nature of the representation could be done, for instance, by understanding the nature of each layer, or at least, the first.",,,,,,6.333333333333333,,,3.3333333333333335,,
484,"Authors: POOLING GEOMETRY, Nadav Cohen, Amnon Shashua
Source file: 484.pdf

ABSTRACT
Our formal understanding of the inductive bias that drives the success of convolutional networks on computer vision tasks is limited. In particular, it is unclear what makes hypotheses spaces born from convolution and pooling operations so suitable for natural images. In this paper we study the ability of convolutional networks to model correlations among regions of their input. We theoretically analyze convolutional arithmetic circuits, and empirically validate our findings on other types of convolutional networks as well. Correlations are formalized through the notion of separation rank, which for a given partition of the input, measures how far a function is from being separable. We show that a polynomially sized deep network supports exponentially high separation ranks for certain input partitions, while being limited to polynomial separation ranks for others. The network’s pooling geometry effectively determines which input partitions are favored, thus serves as a means for controlling the inductive bias. Contiguous pooling windows as commonly employed in practice favor interleaved partitions over coarse ones, orienting the inductive bias towards the statistics of natural images. Other pooling schemes lead to different preferences, and this allows tailoring the network to data that departs from the usual domain of natural imagery. In addition to analyzing deep networks, we show that shallow ones support only linear separation ranks, and by this gain insight into the benefit of functions brought forth by depth – they are able to efficiently model strong correlation under favored partitions of the input.

1 INTRODUCTION
A central factor in the application of machine learning to a given task is the inductive bias, i.e. the choice of hypotheses space from which learned functions are taken. The restriction posed by the inductive bias is necessary for practical learning, and reflects prior knowledge regarding the task at hand. Perhaps the most successful exemplar of inductive bias to date manifests itself in the use of convolutional networks (LeCun and Bengio (1995)) for computer vision tasks. These hypotheses spaces are delivering unprecedented visual recognition results (e.g. Krizhevsky et al. (2012); Szegedy et al. (2015); Simonyan and Zisserman (2014); He et al. (2015)), largely responsible for the resurgence of deep learning (LeCun et al. (2015)). Unfortunately, our formal understanding of the inductive bias behind convolutional networks is limited – the assumptions encoded into these models, which seem to form an excellent prior knowledge for imagery data, are for the most part a mystery.
Existing works studying the inductive bias of deep networks (not necessarily convolutional) do so in the context of depth efficiency, essentially arguing that for a given amount of resources, more layers result in higher expressiveness. More precisely, depth efficiency refers to a situation where a function realized by a deep network of polynomial size, requires super-polynomial size in order to be realized (or approximated) by a shallower network. In recent years, a large body of research was devoted to proving existence of depth efficiency under different types of architectures (see for example Delalleau and Bengio (2011); Pascanu et al. (2013); Montufar et al. (2014); Telgarsky (2015); Eldan and Shamir (2015); Poggio et al. (2015); Mhaskar et al. (2016)). Nonetheless, despite the wide attention it is receiving, depth efficiency does not convey the complete story behind the inductive bias of deep networks. While it does suggest that depth brings forth functions that are otherwise unattainable, it does not explain why these functions are useful. Loosely speaking, the
hypotheses space of a polynomially sized deep network covers a small fraction of the space of all functions. We would like to understand why this small fraction is so successful in practice.
A specific family of convolutional networks gaining increased attention is that of convolutional arithmetic circuits. These models follow the standard paradigm of locality, weight sharing and pooling, yet differ from the most conventional convolutional networks in that their point-wise activations are linear, with non-linearity originating from product pooling. Recently, Cohen et al. (2016b) analyzed the depth efficiency of convolutional arithmetic circuits, showing that besides a negligible (zero measure) set, all functions realizable by a deep network require exponential size in order to be realized (or approximated) by a shallow one. This result, termed complete depth efficiency, stands in contrast to previous depth efficiency results, which merely showed existence of functions efficiently realizable by deep networks but not by shallow ones. Besides their analytic advantage, convolutional arithmetic circuits are also showing promising empirical performance. In particular, they are equivalent to SimNets – a deep learning architecture that excels in computationally constrained settings (Cohen and Shashua (2014); Cohen et al. (2016a)), and in addition, have recently been utilized for classification with missing data (Sharir et al. (2016)). Motivated by these theoretical and practical merits, we focus our analysis in this paper on convolutional arithmetic circuits, viewing them as representative of the class of convolutional networks. We empirically validate our conclusions with both convolutional arithmetic circuits and convolutional rectifier networks – convolutional networks with rectified linear (ReLU, Nair and Hinton (2010)) activation and max or average pooling. Adaptation of the formal analysis to networks of the latter type, similarly to the adaptation of the analysis in Cohen et al. (2016b) carried out by Cohen and Shashua (2016), is left for future work.
Our analysis approaches the study of inductive bias from the direction of function inputs. Specifically, we study the ability of convolutional arithmetic circuits to model correlation between regions of their input. To analyze the correlations of a function, we consider different partitions of input regions into disjoint sets, and ask how far the function is from being separable w.r.t. these partitions. Distance from separability is measured through the notion of separation rank (Beylkin and Mohlenkamp (2002)), which can be viewed as a surrogate of the L2 distance from the closest separable function. For a given function and partition of its input, high separation rank implies that the function induces strong correlation between sides of the partition, and vice versa.
We show that a deep network supports exponentially high separation ranks for certain input partitions, while being limited to polynomial or linear (in network size) separation ranks for others. The network’s pooling geometry effectively determines which input partitions are favored in terms of separation rank, i.e. which partitions enjoy the possibility of exponentially high separation rank with polynomial network size, and which require network to be exponentially large. The standard choice of square contiguous pooling windows favors interleaved (entangled) partitions over coarse ones that divide the input into large distinct areas. Other choices lead to different preferences, for example pooling windows that join together nodes with their spatial reflections lead to favoring partitions that split the input symmetrically. We conclude that in terms of modeled correlations, pooling geometry controls the inductive bias, and the particular design commonly employed in practice orients it towards the statistics of natural images (nearby pixels more correlated than ones that are far apart). Moreover, when processing data that departs from the usual domain of natural imagery, prior knowledge regarding its statistics can be used to derive respective pooling schemes, and accordingly tailor the inductive bias.
With regards to depth efficiency, we show that separation ranks under favored input partitions are exponentially high for all but a negligible set of the functions realizable by a deep network. Shallow networks on the other hand, treat all partitions equally, and support only linear (in network size) separation ranks. Therefore, almost all functions that may be realized by a deep network require a replicating shallow network to have exponential size. By this we return to the complete depth efficiency result of Cohen et al. (2016b), but with an added important insight into the benefit of functions brought forth by depth – they are able to efficiently model strong correlation under favored partitions of the input.
The remainder of the paper is organized as follows. Sec. 2 provides a brief presentation of necessary background material from the field of tensor analysis. Sec. 3 describes the convolutional arithmetic circuits we analyze, and their relation to tensor decompositions. In sec. 4 we convey the concept of separation rank, on which we base our analyses in sec. 5 and 6. The conclusions from our analyses are empirically validated in sec. 7. Finally, sec. 8 concludes.

2 PRELIMINARIES
The analyses carried out in this paper rely on concepts and results from the field of tensor analysis. In this section we establish the minimal background required in order to follow our arguments 1 , referring the interested reader to Hackbusch (2012) for a broad and comprehensive introduction to the field.
The core concept in tensor analysis is a tensor, which for our purposes may simply be thought of as a multi-dimensional array. The order of a tensor is defined to be the number of indexing entries in the array, which are referred to as modes. The dimension of a tensor in a particular mode is defined as the number of values that may be taken by the index in that mode. For example, a 4-by-3 matrix is a tensor of order 2, i.e. it has two modes, with dimension 4 in mode 1 and dimension 3 in mode 2. If A is a tensor of order N and dimension Mi in each mode i ∈ [N ] := {1, . . . , N}, the space of all configurations it can take is denoted, quite naturally, by RM1×···×MN .
A fundamental operator in tensor analysis is the tensor product, which we denote by ⊗. It is an operator that intakes two tensors A ∈ RM1×···×MP and B ∈ RMP+1×···×MP+Q (orders P and Q respectively), and returns a tensor A ⊗ B ∈ RM1×···×MP+Q (order P + Q) defined by: (A ⊗ B)d1...dP+Q = Ad1...dP · BdP+1...dP+Q . Notice that in the case P = Q = 1, the tensor product reduces to the standard outer product between vectors, i.e. if u ∈ RM1 and v ∈ RM2 , then u⊗ v is no other than the rank-1 matrix uv> ∈ RM1×M2 . We now introduce the important concept of matricization, which is essentially the rearrangement of a tensor as a matrix. SupposeA is a tensor of order N and dimension Mi in each mode i ∈ [N ], and let (I, J) be a partition of [N ], i.e. I and J are disjoint subsets of [N ] whose union gives [N ]. We may write I = {i1, . . . , i|I|} where i1 < · · · < i|I|, and similarly J = {j1, . . . , j|J|} where j1 < · · · < j|J|. The matricization of A w.r.t. the partition (I, J), denoted JAKI,J , is the
∏|I| t=1Mit -by-∏|J|
t=1Mjt matrix holding the entries ofA such thatAd1...dN is placed in row index 1+ ∑|I| t=1(dit −
1) ∏|I| t′=t+1Mit′ and column index 1 + ∑|J| t=1(djt − 1) ∏|J| t′=t+1Mjt′ . If I = ∅ or J = ∅, then by
definition JAKI,J is a row or column (respectively) vector of dimension ∏N t=1Mt holding Ad1...dN
in entry 1 + ∑N t=1(dt − 1) ∏N t′=t+1Mt′ .
A well known matrix operator is the Kronecker product, which we denote by . For two matrices A ∈ RM1×M2 and B ∈ RN1×N2 , A B is the matrix in RM1N1×M2N2 holding AijBkl in row index (i − 1)N1 + k and column index (j − 1)N2 + l. Let A and B be tensors of orders P and Q respectively, and let (I, J) be a partition of [P +Q]. The basic relation that binds together the tensor product, the matricization operator, and the Kronecker product, is:
JA⊗ BKI,J = JAKI∩[P ],J∩[P ] JBK(I−P )∩[Q],(J−P )∩[Q] (1) where I − P and J − P are simply the sets obtained by subtracting P from each of the elements in I and J respectively. In words, eq. 1 implies that the matricization of the tensor product between A and B w.r.t. the partition (I, J) of [P + Q], is equal to the Kronecker product between two matricizations: that of A w.r.t. the partition of [P ] induced by the lower values of (I, J), and that of B w.r.t. the partition of [Q] induced by the higher values of (I, J).

3 CONVOLUTIONAL ARITHMETIC CIRCUITS
The convolutional arithmetic circuit architecture on which we focus in this paper is the one considered in Cohen et al. (2016b), portrayed in fig. 1(a). Instances processed by a network are represented as N -tuples of s-dimensional vectors. They are generally thought of as images, with the s-dimensional vectors corresponding to local patches. For example, instances could be 32-by-32 RGB images, with local patches being 5 × 5 regions crossing the three color bands. In this case, assuming a patch is taken around every pixel in an image (boundaries padded), we have N = 1024 and s = 75. Throughout the paper, we denote a general instance by X = (x1, . . . ,xN ), with x1 . . .xN ∈ Rs standing for its patches.
1 The definitions we give are actually concrete special cases of more abstract algebraic definitions as given in Hackbusch (2012). We limit the discussion to these special cases since they suffice for our needs and are easier to grasp.
The first layer in a network is referred to as representation. It consists of applying M representation functions fθ1 . . .fθM : Rs → R to all patches, thereby creating M feature maps. In the case where representation functions are chosen as fθd(x) = σ(w > d x + bd), with parameters θd = (wd, bd) ∈ Rs × R and some point-wise activation σ(·), the representation layer reduces to a standard convolutional layer. More elaborate settings are also possible, for example modeling the representation as a cascade of convolutional layers with pooling in-between. Following the representation, a network includes L hidden layers indexed by l = 0. . .L− 1. Each hidden layer l begins with a 1 × 1 conv operator, which is simply a three-dimensional convolution with rl channels and filters of spatial dimensions 1-by-1. 2 This is followed by spatial pooling, that decimates feature maps by taking products of non-overlapping two-dimensional windows that cover the spatial extent. The last of the L hidden layers (l = L−1) reduces feature maps to singletons (its pooling operator is global), creating a vector of dimension rL−1. This vector is mapped into Y network outputs through a final dense linear layer.
Altogether, the architectural parameters of a network are the type of representation functions (fθd ), the pooling window shapes and sizes (which in turn determine the number of hidden layers L), and the number of channels in each layer (M for representation, r0. . .rL−1 for hidden layers, Y for output). Given these architectural parameters, the learnable parameters of a network are the representation weights (θd for channel d), the conv weights (al,γ for channel γ of hidden layer l), and the output weights (aL,y for output node y).
For a particular setting of weights, every node (neuron) in a given network realizes a function from (Rs)N to R. The receptive field of a node refers to the indexes of input patches on which its function may depend. For example, the receptive field of node j in channel γ of conv oper-
2 Cohen et al. (2016b) consider two settings for the 1 × 1 conv operator. The first, referred to as weight sharing, is the one described above, and corresponds to standard convolution. The second is more general, allowing filters that slide across the previous layer to have different weights at different spatial locations. It is shown in Cohen et al. (2016b) that without weight sharing, a convolutional arithmetic circuit with one hidden layer (or more) is universal, i.e. can realize any function if its size (width) is unbounded. This property is imperative for the study of depth efficiency, as that requires shallow networks to ultimately be able to replicate any function realized by a deep network. In this paper we limit the presentation to networks with weight sharing, which are not universal. We do so because they are more conventional, and since our entire analysis is oblivious to whether or not weights are shared (applies as is to both settings). The only exception is where we reproduce the depth efficiency result of Cohen et al. (2016b). There, we momentarily consider networks without weight sharing.
ator at hidden layer 0 is {j}, and that of an output node is [N ], corresponding to the entire input. Denote by h(l,γ,j) the function realized by node j of channel γ in conv operator at hidden layer l, and let I(l,γ,j) ⊂ [N ] be its receptive field. By the structure of the network it is evident that I(l,γ,j) does not depend on γ, so we may write I(l,j) instead. Moreover, assuming pooling windows are uniform across channels (as customary with convolutional networks), and taking into account the fact that they do not overlap, we conclude that I(l,j1) and I(l,j2) are necessarily disjoint if j1 6=j2. A simple induction over l = 0. . .L − 1 then shows that h(l,γ,j) may be expressed as h(l,γ,j)(xi1 , . . . ,xiT ) = ∑M d1...dT=1 A(l,γ,j)d1...dT ∏T t=1 fθdt (xit), where {i1, . . . , iT } stands for the receptive field I(l,j), and A(l,γ,j) is a tensor of order T = |I(l,j)| and dimension M in each mode, with entries given by polynomials in the network’s conv weights {al,γ}l,γ . Taking the induction one step further (from last hidden layer to network output), we obtain the following expression for functions realized by network outputs:
hy (x1, . . . ,xN ) = ∑M
d1...dN=1 Ayd1...dN ∏N i=1 fθdi (xi) (2)
y ∈ [Y ] here is an output node index, and hy is the function realized by that node. Ay is a tensor of order N and dimension M in each mode, with entries given by polynomials in the network’s conv weights {al,γ}l,γ and output weights aL,y . Hereafter, terms such as function realized by a network or coefficient tensor realized by a network, are to be understood as referring to hy orAy respectively. Next, we present explicit expressions for Ay under two canonical networks – deep and shallow.
Deep network. Consider a network as in fig. 1(a), with pooling windows set to cover four entries each, resulting in L = log4N hidden layers. The linear weights of such a network are {a0,γ ∈ RM}γ∈[r0] for conv operator in hidden layer 0, {al,γ ∈ Rrl−1}γ∈[rl] for conv operator in hidden layer l = 1. . .L − 1, and {aL,y ∈ RrL−1}y∈[Y ] for dense output operator. They determine the coefficient tensor Ay (eq. 2) through the following recursive decomposition:
φ1,γ︸︷︷︸ order 4
= ∑r0
α=1 a1,γα · ⊗4a0,α , γ ∈ [r1]
· · · φl,γ︸︷︷︸
order 4l
= ∑rl−1
α=1 al,γα · ⊗4φl−1,α , l ∈ {2. . .L− 1}, γ ∈ [rl]
· · · Ay︸︷︷︸
order 4L N
= ∑rL−1
α=1 aL,yα · ⊗4φL−1,α (3)
al,γα and a L,y α here are scalars representing entry α in the vectors a l,γ and aL,y respectively, and the symbol⊗with a superscript stands for a repeated tensor product, e.g.⊗4a0,α := a0,α⊗a0,α⊗a0,α⊗ a0,α. To verify that under pooling windows of size four Ay is indeed given by eq. 3, simply plug the rows of the decomposition into eq. 2, starting from bottom and continuing upwards. For context, eq. 3 describes what is known as a hierarchical tensor decomposition (see chapter 11 in Hackbusch (2012)), with underlying tree over modes being a full quad-tree (corresponding to the fact that the network’s pooling windows cover four entries each).
Shallow network. The second network we pay special attention to is shallow, comprising a single hidden layer with global pooling – see illustration in fig. 1(b). The linear weights of such a network are {a0,γ ∈ RM}γ∈[r0] for hidden conv operator and {a1,y ∈ Rr0}y∈[Y ] for dense output operator. They determine the coefficient tensor Ay (eq. 2) as follows:
Ay = ∑r0
γ=1 a1,yγ · ⊗Na0,γ (4)
where a1,yγ stands for entry γ of a 1,y , and again, the symbol ⊗ with a superscript represents a repeated tensor product. The tensor decomposition in eq. 4 is an instance of the classic CP decomposition, also known as rank-1 decomposition (see Kolda and Bader (2009) for a historic survey).
To conclude this section, we relate the background material above, as well as our contribution described in the upcoming sections, to the work of Cohen et al. (2016b). The latter shows that with
arbitrary coefficient tensorsAy , functions hy as in eq. 2 form a universal hypotheses space. It is then shown that convolutional arithmetic circuits as in fig. 1(a) realize such functions by applying tensor decompositions to Ay , with the type of decomposition determined by the structure of a network (number of layers, number of channels in each layer etc.). The deep network (fig. 1(a) with size-4 pooling windows and L = log4N hidden layers) and the shallow network (fig. 1(b)) presented hereinabove are two special cases, whose corresponding tensor decompositions are given in eq. 3 and 4 respectively. The central result in Cohen et al. (2016b) relates to inductive bias through the notion of depth efficiency – it is shown that in the parameter space of a deep network, all weight settings but a set of (Lebesgue) measure zero give rise to functions that can only be realized (or approximated) by a shallow network if the latter has exponential size. This result does not relate to the characteristics of instances X = (x1, . . . ,xN ), it only treats the ability of shallow networks to replicate functions realized by deep networks.
In this paper we draw a line connecting the inductive bias to the nature of X , by studying the relation between a network’s architecture and its ability to model correlation among patches xi. Specifically, in sec. 4 we consider partitions (I, J) of [N ] (I ·∪J = [N ], where ·∪ stands for disjoint union), and present the notion of separation rank as a measure of the correlation modeled between the patches indexed by I and those indexed by J . In sec. 5.1 the separation rank of a network’s function hy w.r.t. a partition (I, J) is proven to be equal to the rank of JAyKI,J – the matricization of the coefficient tensor Ay w.r.t. (I, J). Sec. 5.2 derives lower and upper bounds on this rank for a deep network, showing that it supports exponential separation ranks with polynomial size for certain partitions, whereas for others it is required to be exponentially large. Subsequently, sec. 5.3 establishes an upper bound on rankJAyKI,J for shallow networks, implying that these must be exponentially large in order to model exponential separation rank under any partition, and thus cannot efficiently replicate a deep network’s correlations. Our analysis concludes in sec. 6, where we discuss the pooling geometry of a deep network as a means for controlling the inductive bias by determining a correspondence between partitions (I, J) and spatial partitions of the input. Finally, we demonstrate experimentally in sec. 7 how different pooling geometries lead to superior performance in different tasks. Our experiments include not only convolutional arithmetic circuits, but also convolutional rectifier networks, i.e. convolutional networks with ReLU activation and max or average pooling.

4 SEPARATION RANK
In this section we define the concept of separation rank for functions realized by convolutional arithmetic circuits (sec. 3), i.e. real functions that take as input X = (x1, . . . ,xN ) ∈ (Rs)N . The separation rank serves as a measure of the correlations such functions induce between different sets of input patches, i.e. different subsets of the variable set {x1, . . . ,xN}. Let (I, J) be a partition of input indexes, i.e. I and J are disjoint subsets of [N ] whose union gives [N ]. We may write I = {i1, . . . , i|I|} where i1 < · · · < i|I|, and similarly J = {j1, . . . , j|J|} where j1 < · · · < j|J|. For a function h : (Rs)N → R, the separation rank w.r.t. the partition (I, J) is defined as follows: 3
sep(h; I, J) := min { R ∈ N ∪ {0} : ∃g1. . .gR : (Rs)|I| → R, g′1. . .g′R : (Rs)|J| → R s.t. (5)
h(x1, . . . ,xN ) = ∑R
ν=1 gν(xi1 , . . . ,xi|I|)g
′ ν(xj1 , . . . ,xj|J|) } In words, it is the minimal number of summands that together give h, where each summand is separable w.r.t. (I, J), i.e. is equal to a product of two functions – one that intakes only patches indexed by I , and another that intakes only patches indexed by J . One may wonder if it is at all possible to express h through such summands, i.e. if the separation rank of h is finite. From the theory of tensor products between L2 spaces (see Hackbusch (2012) for a comprehensive coverage), we know that any h∈L2((Rs)N ), i.e. any h that is measurable and square-integrable, may be approximated arbitrarily well by summations of the form ∑R ν=1 gν(xi1 , . . . ,xi|I|)g ′ ν(xj1 , . . . ,xj|J|). Exact realization however is only guaranteed at the limit R → ∞, thus in general the separation rank of h
3 If I = ∅ or J = ∅ then by definition sep(h; I, J) = 1 (unless h ≡ 0, in which case sep(h; I, J) = 0).
need not be finite. Nonetheless, as we show in sec. 5, for the class of functions we are interested in, namely functions realizable by convolutional arithmetic circuits, separation ranks are always finite.
The concept of separation rank was introduced in Beylkin and Mohlenkamp (2002) for numerical treatment of high-dimensional functions, and has since been employed for various applications, e.g. quantum chemistry (Harrison et al. (2003)), particle engineering (Hackbusch (2006)) and machine learning (Beylkin et al. (2009)). If the separation rank of a function w.r.t. a partition of its input is equal to 1, the function is separable, meaning it does not model any interaction between the sets of variables. Specifically, if sep(h; I, J) = 1 then there exist g : (Rs)|I| → R and g′ : (Rs)|J| → R such that h(x1, . . . ,xN ) = g(xi1 , . . . ,xi|I|)g′(xj1 , . . . ,xj|J|), and the function h cannot take into account consistency between the values of {xi1 , . . . ,xi|I|} and those of {xj1 , . . . ,xj|J|}. In a statistical setting, if h is a probability density function, this would mean that {xi1 , . . . ,xi|I|} and {xj1 , . . . ,xj|J|} are statistically independent. The higher sep(h; I, J) is, the farther h is from this situation, i.e. the more it models dependency between {xi1 , . . . ,xi|I|} and {xj1 , . . . ,xj|J|}, or equivalently, the stronger the correlation it induces between the patches indexed by I and those indexed by J .
The interpretation of separation rank as a measure of deviation from separability is formalized in app. B, where it is shown that sep(h; I, J) is closely related to the L2 distance of h from the set of separable functions w.r.t. (I, J). Specifically, we define D(h; I, J) as the latter distance divided by the L2 norm of h 4 , and show that sep(h; I, J) provides an upper bound on D(h; I, J). While it is not possible to lay out a general lower bound onD(h; I, J) in terms of sep(h; I, J), we show that the specific lower bounds on sep(h; I, J) underlying our analyses can be translated into lower bounds on D(h; I, J). This implies that our results, facilitated by upper and lower bounds on separation ranks of convolutional arithmetic circuits, may equivalently be framed in terms of L2 distances from separable functions.

5 CORRELATION ANALYSIS
In this section we analyze convolutional arithmetic circuits (sec. 3) in terms of the correlations they can model between sides of different input partitions, i.e. in terms of the separation ranks (sec. 4) they support under different partitions (I, J) of [N ]. We begin in sec. 5.1, establishing a correspondence between separation ranks and coefficient tensor matricization ranks. This correspondence is then used in sec. 5.2 and 5.3 to analyze the deep and shallow networks (respectively) presented in sec. 3. We note that we focus on these particular networks merely for simplicity of presentation – the analysis can easily be adapted to account for alternative networks with different depths and pooling schemes.

5.1 FROM SEPARATION RANK TO MATRICIZATION RANK
Let hy be a function realized by a convolutional arithmetic circuit, with corresponding coefficient tensor Ay (eq. 2). Denote by (I, J) an arbitrary partition of [N ], i.e. I ·∪J = [N ]. We are interested in studying sep(hy; I, J) – the separation rank of hy w.r.t. (I, J) (eq. 5). As claim 1 below states, assuming representation functions {fθd}d∈[M ] are linearly independent (if they are not, we drop dependent functions and modify Ay accordingly 5 ), this separation rank is equal to the rank of JAyKI,J – the matricization of the coefficient tensor Ay w.r.t. the partition (I, J). Our problem thus translates to studying ranks of matricized coefficient tensors.
Claim 1. Let hy be a function realized by a convolutional arithmetic circuit (fig. 1(a)), with corresponding coefficient tensor Ay (eq. 2). Assume that the network’s representation functions fθd are linearly independent, and that they, as well as the functions gν , g′ν in the definition of separation
4 The normalization (division by norm) is of critical importance – without it rescaling h would accordingly rescale D(h; I, J), rendering the latter uninformative in terms of deviation from separability. 5 Suppose for example that fθM is dependent, i.e. there exist α1 . . . αM−1 ∈ R such that fθM (x) =∑M−1 d=1 αd·fθd(x). We may then plug this into eq. 2, and obtain an expression for hy that has fθ1 . . .fθM−1 as representation functions, and a coefficient tensor with dimension M − 1 in each mode. Continuing in this fashion, one arrives at an expression for hy whose representation functions are linearly independent.
rank (eq. 5), are measurable and square-integrable. 6 Then, for any partition (I, J) of [N ], it holds that sep(hy; I, J) = rankJAyKI,J .
Proof. See app. A.1.
As the linear weights of a network vary, so do the coefficient tensors (Ay) it gives rise to. Accordingly, for a particular partition (I, J), a network does not correspond to a single value of rankJAyKI,J , but rather supports a range of values. We analyze this range by quantifying its maximum, which reflects the strongest correlation that the network can model between the input patches indexed by I and those indexed by J . One may wonder if the maximal value of rankJAyKI,J is the appropriate statistic to measure, as a-priori, it may be that rankJAyKI,J is maximal for very few of the network’s weight settings, and much lower for all the rest. Apparently, as claim 2 below states, this is not the case, and in fact rankJAyKI,J is maximal under almost all of the network’s weight settings. Claim 2. Consider a convolutional arithmetic circuit (fig. 1(a)) with corresponding coefficient tensor Ay (eq. 2). Ay depends on the network’s linear weights – {al,γ}l,γ and aL,y , thus for a given partition (I, J) of [N ], rankJAyKI,J is a function of these weights. This function obtains its maximum almost everywhere (w.r.t. Lebesgue measure).
Proof. See app. A.2.

5.2 DEEP NETWORK
In this subsection we study correlations modeled by the deep network presented in sec. 3 (fig. 1(a) with size-4 pooling windows and L = log4N hidden layers). In accordance with sec. 5.1, we do so by characterizing the maximal ranks of coefficient tensor matricizations under different partitions.
Recall from eq. 3 the hierarchical decomposition expressing a coefficient tensor Ay realized by the deep network. We are interested in matricizations of this tensor under different partitions of [N ]. Let (I, J) be an arbitrary partition, i.e. I ·∪J = [N ]. Matricizing the last level of eq. 3 w.r.t. (I, J), while applying the relation in eq. 1, gives:
JAyKI,J = ∑rL−1
α=1 aL,yα ·
q φL−1,α ⊗ φL−1,α ⊗ φL−1,α ⊗ φL−1,α y I,J
= ∑rL−1
α=1 aL,yα ·
q φL−1,α ⊗ φL−1,α y I∩[2·4L−1],J∩[2·4L−1]
q φL−1,α ⊗ φL−1,α y (I−2·4L−1)∩[2·4L−1],(J−2·4L−1)∩[2·4L−1]
Applying eq. 1 again, this time to matricizations of the tensor φL−1,α ⊗ φL−1,α, we obtain: JAyKI,J = ∑rL−1
α=1 aL,yα ·
q φL−1,α y I∩[4L−1],J∩[4L−1]
q φL−1,α y (I−4L−1)∩[4L−1],(J−4L−1)∩[4L−1] q φL−1,α
y (I−2·4L−1)∩[4L−1],(J−2·4L−1)∩[4L−1]
q φL−1,α y (I−3·4L−1)∩[4L−1],(J−3·4L−1)∩[4L−1]
For every k ∈ [4] define IL−1,k := (I−(k−1)·4L−1)∩[4L−1] and JL−1,k := (J−(k−1)·4L−1)∩ [4L−1]. In words, (IL−1,k, JL−1,k) represents the partition induced by (I, J) on the k’th quadrant of [N ], i.e. on the k’th size-4L−1 group of input patches. We now have the following matricized version of the last level in eq. 3:
JAyKI,J = ∑rL−1
α=1 aL,yα · 4 t=1 JφL−1,αKIL−1,t,JL−1,t 6 Square-integrability of representation functions fθd may seem as a limitation at first glance, as for example neurons fθd(x) = σ(w > d x + bd), with parameters θd = (wd, bd) ∈ Rs × R and sigmoid or ReLU activation σ(·), do not meet this condition. However, since in practice our inputs are bounded (e.g. they represent image pixels by holding intensity values), we may view functions as having compact support, which, as long as they are continuous (holds in all cases of interest), ensures square-integrability.
where the symbol with a running index stands for an iterative Kronecker product. To derive analogous matricized versions for the upper levels of eq. 3, we define for l ∈ {0. . .L − 1}, k ∈ [N/4l]: Il,k := (I − (k − 1) · 4l) ∩ [4l] Jl,k := (J − (k − 1) · 4l) ∩ [4l] (6) That is to say, (Il,k, Jl,k) represents the partition induced by (I, J) on the set of indexes {(k − 1) · 4l + 1, . . . , k · 4l}, i.e. on the k’th size-4l group of input patches. With this notation in hand, traversing upwards through the levels of eq. 3, with repeated application of the relation in eq. 1, one arrives at the following matrix decomposition for JAyKI,J :
Jφ1,γKI1,k,J1,k︸ ︷︷ ︸ M |I1,k|-by-M |J1,k|
= ∑r0
α=1 a1,γα · 4 t=1 Ja0,αKI0,4(k−1)+t,J0,4(k−1)+t , γ ∈ [r1]
· · ·
Jφl,γKIl,k,Jl,k︸ ︷︷ ︸ M |Il,k|-by-M |Jl,k|
= ∑rl−1
α=1 al,γα · 4 t=1 Jφl−1,αKIl−1,4(k−1)+t,Jl−1,4(k−1)+t , l ∈ {2. . .L− 1}, γ ∈ [rl]
· · ·
JAyKI,J︸ ︷︷ ︸ M |I|-by-M |J|
= ∑rL−1
α=1 aL,yα · 4 t=1 JφL−1,αKIL−1,t,JL−1,t (7)
Eq. 7 expresses JAyKI,J – the matricization w.r.t. the partition (I, J) of a coefficient tensorAy realized by the deep network, in terms of the network’s conv weights {al,γ}l,γ and output weights aL,y . As discussed above, our interest lies in the maximal rank that this matricization can take. Theorem 1 below provides lower and upper bounds on this maximal rank, by making use of eq. 7, and of the rank-multiplicative property of the Kronecker product (rank(A B) = rank(A)·rank(B)). Theorem 1. Let (I, J) be a partition of [N ], and JAyKI,J be the matricization w.r.t. (I, J) of a coefficient tensor Ay (eq. 2) realized by the deep network (fig. 1(a) with size-4 pooling windows). For every l ∈ {0. . .L− 1} and k ∈ [N/4l], define Il,k and Jl,k as in eq. 6. Then, the maximal rank that JAyKI,J can take (when network weights vary) is:
• No smaller than min{r0,M}S , where S := |{k ∈ [N/4] : I1,k 6= ∅ ∧ J1,k 6= ∅}|.
• No greater than min{Mmin{|I|,|J|}, rL−1 ∏4 t=1 c
L−1,t}, where c0,k := 1 for k ∈ [N ], and cl,k := min{Mmin{|Il,k|,|Jl,k|}, rl−1 ∏4 t=1 c l−1,4(k−1)+t} for l ∈ [L− 1], k ∈ [N/4l].
Proof. See app. A.3.
The lower bound in theorem 1 is exponential in S, the latter defined to be the number of size-4 patch groups that are split by the partition (I, J), i.e. whose indexes are divided between I and J . Partitions that split many of the size-4 patch groups will thus lead to a large lower bound. For example, consider the partition (Iodd, Jeven) defined as follows:
Iodd = {1, 3, . . . , N − 1} Jeven = {2, 4, . . . , N} (8) This partition splits all size-4 patch groups (S = N/4), leading to a lower bound that is exponential in the number of patches (N ).
The upper bound in theorem 1 is expressed via constants cl,k, defined recursively over levels l = 0. . .L − 1, with k ranging over 1. . .N/4l for each level l. What prevents cl,k from growing double-exponentially fast (w.r.t. l) is the minimization with Mmin{|Il,k|,|Jl,k|}. Specifically, if min{|Il,k| , |Jl,k|} is small, i.e. if the partition induced by (I, J) on the k’th size-4l group of patches is unbalanced (most of the patches belong to one side of the partition, and only a few belong to the other), cl,k will be of reasonable size. The higher this takes place in the hierarchy (i.e. the larger l is), the lower our eventual upper bound will be. In other words, if partitions induced by (I, J) on size-4l patch groups are unbalanced for large values of l, the upper bound in theorem 1 will be small. For example, consider the partition (I low, Jhigh) defined by:
I low = {1, . . . , N/2} Jhigh = {N/2 + 1, . . . , N} (9)
Under (I low, Jhigh), all partitions induced on size-4L−1 patch groups (quadrants of [N ]) are completely one-sided (min{|IL−1,k|, |JL−1,k|} = 0 for all k ∈ [4]), resulting in the upper bound being no greater than rL−1 – linear in network size.
To summarize this discussion, theorem 1 states that with the deep network, the maximal rank of a coefficient tensor matricization w.r.t. (I, J), highly depends on the nature of the partition (I, J) – it will be exponentially high for partitions such as (Iodd, Jeven), that split many size-4 patch groups, while being only polynomial (or linear) for partitions like (I low, Jhigh), under which size-4l patch groups are unevenly divided for large values of l. Since the rank of a coefficient tensor matricization w.r.t. (I, J) corresponds to the strength of correlation modeled between input patches indexed by I and those indexed by J (sec. 5.1), we conclude that the ability of a polynomially sized deep network to model correlation between sets of input patches highly depends on the nature of these sets.

5.3 SHALLOW NETWORK
We now turn to study correlations modeled by the shallow network presented in sec. 3 (fig. 1(b)). In line with sec. 5.1, this is achieved by characterizing the maximal ranks of coefficient tensor matricizations under different partitions.
Recall from eq. 4 the CP decomposition expressing a coefficient tensor Ay realized by the shallow network. For an arbitrary partition (I, J) of [N ], i.e. I ·∪J = [N ], matricizing this decomposition with repeated application of the relation in eq. 1, gives the following expression for JAyKI,J – the matricization w.r.t. (I, J) of a coefficient tensor realized by the shallow network:
JAyKI,J = ∑r0
γ=1 a1,yγ ·
( |I|a0,γ )( |J|a0,γ )> (10)
|I|a0,γ and |J|a0,γ here are column vectors of dimensions M |I| and M |J| respectively, standing for the Kronecker products of a0,γ ∈ RM with itself |I| and |J | times (respectively). Eq. 10 immediately leads to two observations regarding the ranks that may be taken by JAyKI,J . First, they depend on the partition (I, J) only through its division size, i.e. through |I| and |J |. Second, they are no greater than min{Mmin{|I|,|J|}, r0}, meaning that the maximal rank is linear (or less) in network size. In light of sec. 5.1 and 5.2, these findings imply that in contrast to the deep network, which with polynomial size supports exponential separation ranks under favored partitions, the shallow network treats all partitions (of a given division size) equally, and can only give rise to an exponential separation rank if its size is exponential.
Suppose now that we would like to use the shallow network to replicate a function realized by a polynomially sized deep network. So long as the deep network’s function admits an exponential separation rank under at least one of the favored partitions (e.g. (Iodd, Jeven) – eq. 8), the shallow network would have to be exponentially large in order to replicate it, i.e. depth efficiency takes place. 7 Since all but a negligible set of the functions realizable by the deep network give rise to maximal separation ranks (sec 5.1), we obtain the complete depth efficiency result of Cohen et al. (2016b). However, unlike Cohen et al. (2016b), which did not provide any explanation for the usefulness of functions brought forth by depth, we obtain an insight into their utility – they are able to efficiently model strong correlation under favored partitions of the input.

6 INDUCTIVE BIAS THROUGH POOLING GEOMETRY
The deep network presented in sec. 3, whose correlations we analyzed in sec. 5.2, was defined as having size-4 pooling windows, i.e. pooling windows covering four entries each. We have yet
7 Convolutional arithmetic circuits as we have defined them (sec. 3) are not universal. In particular, it may very well be that a function realized by a polynomially sized deep network cannot be replicated by the shallow network, no matter how large (wide) we allow it to be. In such scenarios depth efficiency does not provide insight into the complexity of functions brought forth by depth. To obtain a shallow network that is universal, thus an appropriate gauge for depth efficiency, we may remove the constraint of weight sharing, i.e. allow the filters in the hidden conv operator to hold different weights at different spatial locations (see Cohen et al. (2016b) for proof that this indeed leads to universality). All results we have established for the original shallow network remain valid when weight sharing is removed. In particular, the separation ranks of the network are still linear in its size. This implies that as suggested, depth efficiency indeed holds.
to specify the shapes of these windows, or equivalently, the spatial (two-dimensional) locations of nodes grouped together in the process of pooling. In compliance with standard convolutional network design, we now assume that the network’s (size-4) pooling windows are contiguous square blocks, i.e. have shape 2 × 2. Under this configuration, the network’s functional description (eq. 2 with Ay given by eq. 3) induces a spatial ordering of input patches 8 , which may be described by the following recursive process:
• Set the index of the top-left patch to 1.
• For l = 1, . . ., L = log4N : Replicate the already-assigned top-left 2l−1-by-2l−1 block of indexes, and place copies on its right, bottom-right and bottom. Then, add a 4l−1 offset to all indexes in the right copy, a 2 · 4l−1 offset to all indexes in the bottom-right copy, and a 3 · 4l−1 offset to all indexes in the bottom copy.
With this spatial ordering (illustrated in fig. 1(c)), partitions (I, J) of [N ] convey a spatial pattern. For example, the partition (Iodd, Jeven) (eq. 8) corresponds to the pattern illustrated on the left of fig. 1(c), whereas (I low, Jhigh) (eq. 9) corresponds to the pattern illustrated on the right. Our analysis (sec. 5.2) shows that the deep network is able to model strong correlation under (Iodd, Jeven), while being inefficient for modeling correlation under (I low, Jhigh). More generally, partitions for which S, defined in theorem 1, is high, convey patterns that split many 2 × 2 patch blocks, i.e. are highly entangled. These partitions enjoy the possibility of strong correlation. On the other hand, partitions for which min{|Il,k| , |Jl,k|} is small for large values of l (see eq. 6 for definition of Il,k and Jl,k) convey patterns that divide large 2l × 2l patch blocks unevenly, i.e. separate the input to distinct contiguous regions. These partitions, as we have seen, suffer from limited low correlations.
We conclude that with 2× 2 pooling, the deep network is able to model strong correlation between input regions that are highly entangled, at the expense of being inefficient for modeling correlation between input regions that are far apart. Had we selected a different pooling regime, the preference of input partition patterns in terms of modeled correlation would change. For example, if pooling windows were set to group nodes with their spatial reflections (horizontal, vertical and horizontalvertical), coarse patterns that divide the input symmetrically, such as the one illustrated on the right of fig. 1(c), would enjoy the possibility of strong correlation, whereas many entangled patterns would now suffer from limited low correlation. The choice of pooling shapes thus serves as a means for controlling the inductive bias in terms of correlations modeled between input regions. Square contiguous windows, as commonly employed in practice, lead to a preference that complies with our intuition regarding the statistics of natural images (nearby pixels more correlated than distant ones). Other pooling schemes lead to different preferences, and this allows tailoring a network to data that departs from the usual domain of natural imagery. We demonstrate this experimentally in the next section, where it is shown how different pooling geometries lead to superior performance in different tasks.

7 EXPERIMENTS
The main conclusion from our analyses (sec. 5 and 6) is that the pooling geometry of a deep convolutional network controls its inductive bias by determining which correlations between input regions can be modeled efficiently. We have also seen that shallow networks cannot model correlations efficiently, regardless of the considered input regions. In this section we validate these assertions empirically, not only with convolutional arithmetic circuits (subject of our analyses), but also with convolutional rectifier networks – convolutional networks with ReLU activation and max or average pooling. For conciseness, we defer to app. C some details regarding our implementation. The latter is fully available online at https://github.com/HUJI-Deep/inductive-pooling.
8 The network’s functional description assumes a one-dimensional full quad-tree grouping of input patch indexes. That is to say, it assumes that in the first pooling operation (hidden layer 0), the nodes corresponding to patches x1,x2,x3,x4 are pooled into one group, those corresponding to x5,x6,x7,x8 are pooled into another, and so forth. Similar assumptions hold for the deeper layers. For example, in the second pooling operation (hidden layer 1), the node with receptive field {1, 2, 3, 4}, i.e. the one corresponding to the quadruple of patches {x1,x2,x3,x4}, is assumed to be pooled together with the nodes whose receptive fields are {5, 6, 7, 8}, {9, 10, 11, 12} and {13, 14, 15, 16}.
Our experiments are based on a synthetic classification benchmark inspired by medical imaging tasks. Instances to be classified are 32-by-32 binary images, each displaying a random distorted oval shape (blob) with missing pixels in its interior (holes). For each image, two continuous scores in range [0, 1] are computed. The first, referred to as closedness, reflects how morphologically closed a blob is, and is defined to be the ratio between the number of pixels in the blob, and the number of pixels in its closure (see app. D for exact definition of the latter). The second score, named symmetry, reflects the degree to which a blob is left-right symmetric about its center. It is measured by cropping the bounding box around a blob, applying a left-right flip to the latter, and computing the ratio between the number of pixels in the intersection of the blob and its reflection, and the number of pixels in the blob. To generate labeled sets for classification (train and test), we render multiple images, sort them according to their closedness and symmetry, and for each of the two scores, assign the label “high” to the top 40% and the label “low” to the bottom 40% (the mid 20% are considered ill-defined). This creates two binary (two-class) classification tasks – one for closedness and one for symmetry (see fig. 2 for a sample of images participating in both tasks). Given that closedness is a property of a local nature, we expect its classification task to require a predictor to be able to model strong correlations between neighboring pixels. Symmetry on the other hand is a property that relates pixels to their reflections, thus we expect its classification task to demand that a predictor be able to model correlations across distances.
We evaluated the deep convolutional arithmetic circuit considered throughout the paper (fig. 1(a) with size-4 pooling windows) under two different pooling geometries. The first, referred to as square, comprises standard 2 × 2 pooling windows. The second, dubbed mirror, pools together nodes with their horizontal, vertical and horizontal-vertical reflections. In both cases, input patches (xi) were set as individual pixels, resulting in N = 1024 patches and L = log4N = 5 hidden layers. M = 2 representation functions (fθd ) were fixed, the first realizing the identity on binary inputs (fθ1(b) = b for b ∈ {0, 1}), and the second realizing negation (fθ2(b) = 1− b for b ∈ {0, 1}). Classification was realized through Y = 2 network outputs, with prediction following the stronger activation. The number of channels across all hidden layers was uniform, and varied between 8 and 128. Fig. 3 shows the results of applying the deep network with both square and mirror pooling, to both closedness and symmetry tasks, where each of the latter has 20000 images for training and 4000 images for testing. As can be seen in the figure, square pooling significantly outperforms mirror pooling in closedness classification, whereas the opposite occurs in symmetry classification. This complies with our discussion in sec. 6, according to which square pooling supports modeling correlations between entangled (neighboring) regions of the input, whereas mirror pooling puts focus on correlations between input regions that are symmetric w.r.t. one another. We thus obtain a demonstration of how prior knowledge regarding a task at hand may be used to tailor the inductive bias of a deep convolutional network by designing an appropriate pooling geometry.
In addition to the deep network, we also evaluated the shallow convolutional arithmetic circuit analyzed in the paper (fig. 1(b)). The architectural choices for this network were the same as those
Deep convolutional arithmetic circuit
described above for the deep network besides the number of hidden channels, which in this case applied to the network’s single hidden layer, and varied between 64 and 4096. The highest train and test accuracies delivered by this network (with 4096 hidden channels) were roughly 62% on closedness task, and 77% on symmetry task. The fact that these accuracies are inferior to those of the deep network, even when the latter’s pooling geometry is not optimal for the task at hand, complies with our analysis in sec. 5. Namely, it complies with the observation that separation ranks (correlations) are sometimes exponential and sometimes polynomial with the deep network, whereas with the shallow one they are never more than linear in network size.
Finally, to assess the validity of our findings for convolutional networks in general, not just convolutional arithmetic circuits, we repeated the above experiments with convolutional rectifier networks. Namely, we placed ReLU activations after every conv operator, switched the pooling operation from product to average, and re-evaluated the deep (square and mirror pooling geometries) and shallow networks. We then reiterated this process once more, with pooling operation set to max instead of average. The results obtained by the deep networks are presented in fig. 4. The shallow network with average pooling reached train/test accuracies of roughly 58% on closedness task, and 55% on symmetry task. With max pooling, performance of the shallow network did not exceed chance. Altogether, convolutional rectifier networks exhibit the same phenomena observed with convolutional arithmetic circuits, indicating that the conclusions from our analyses likely apply to such networks as well. Formal adaptation of the analyses to convolutional rectifier networks, similarly to the adaptation of Cohen et al. (2016b) carried out in Cohen and Shashua (2016), is left for future work.

8 DISCUSSION
Through the notion of separation rank, we studied the relation between the architecture of a convolutional network, and its ability to model correlations among input regions. For a given input partition, the separation rank quantifies how far a function is from separability, which in a probabilistic setting, corresponds to statistical independence between sides of the partition.
Our analysis shows that a polynomially sized deep convolutional arithmetic circuit supports exponentially high separation ranks for certain input partitions, while being limited to polynomial or linear (in network size) separation ranks for others. The network’s pooling window shapes effectively determine which input partitions are favored in terms of separation rank, i.e. which partitions enjoy the possibility of exponentially high separation ranks with polynomial network size, and which require network to be exponentially large. Pooling geometry thus serves as a means for controlling the inductive bias. The particular pooling scheme commonly employed in practice – square contiguous windows, favors interleaved partitions over ones that divide the input to distinct areas, thus orients the inductive bias towards the statistics of natural images (nearby pixels more correlated than distant
0 20 40 60 80 100 120 140
breadth (# of channels in each hidden layer)
70
75
80
85
90
95
100
a cc
u ra
cy [
% ]
closedness task
0 20 40 60 80 100 120 140
breadth (# of channels in each hidden layer)
70
75
80
85
90
95
100
a cc
u ra
cy [
% ]
symmetry task
square pool - train square pool - test mirror pool - train mirror pool - test
Deep convolutional rectifier network (average pooling)
Deep convolutional rectifier network (max pooling)
ones). Other pooling schemes lead to different preferences, and this allows tailoring the network to data that departs from the usual domain of natural imagery.
As opposed to deep convolutional arithmetic circuits, shallow ones support only linear (in network size) separation ranks. Therefore, in order to replicate a function realized by a deep network (exponential separation rank), a shallow network must be exponentially large. By this we derive the depth efficiency result of Cohen et al. (2016b), but in addition, provide an insight into the benefit of functions brought forth by depth – they are able to efficiently model strong correlation under favored partitions of the input.
We validated our conclusions empirically, with convolutional arithmetic circuits as well as convolutional rectifier networks – convolutional networks with ReLU activation and max or average pooling. Our experiments demonstrate how different pooling geometries lead to superior performance in different tasks. Specifically, we evaluate deep networks in the measurement of shape continuity, a task of a local nature, and show that standard square pooling windows outperform ones that join together nodes with their spatial reflections. In contrast, when measuring shape symmetry, modeling correlations across distances is of vital importance, and the latter pooling geometry is superior to the conventional one. Shallow networks are inefficient at modeling correlations of any kind, and indeed lead to poor performance on both tasks.
Finally, our analyses and results bring forth the possibility of expanding the coverage of correlations efficiently modeled by a deep convolutional network. Specifically, by blending together multiple pooling geometries in the hidden layers of a network, it is possible to facilitate simultaneous support for a wide variety of correlations suiting data of different types. Investigation of this direction, from both theoretical and empirical perspectives, is viewed as a promising avenue for future research.

ACKNOWLEDGMENTS
This work is supported by Intel grant ICRI-CI #9-2012-6133, by ISF Center grant 1790/12 and by the European Research Council (TheoryDL project). Nadav Cohen is supported by a Google Doctoral Fellowship in Machine Learning.

A DEFERRED PROOFS
A.1 PROOF OF CLAIM 1
We prove the equality in two steps, first showing that sep(hy; I, J)≤rankJAyKI,J , and then establishing the converse. The first step is elementary, and does not make use of the representation functions’ (fθd ) linear independence, or of measurability/square-integrability. The second step does rely on these assumptions, and employs slightly more advanced mathematical machinery. Throughout the proof, we assume without loss of generality that the partition (I, J) of [N ] is such that I takes on lower values, while J takes on higher ones. That is to say, we assume that I = {1, . . . , |I|} and J = {|I|+ 1, . . . , N}. 9
To prove that sep(hy; I, J)≤rankJAyKI,J , denote by R the rank of JAyKI,J . The latter is an M |I|-by-M |J| matrix, thus there exist vectors u1. . .uR ∈ RM |I| and v1. . .vR ∈ RM |J| such that JAyKI,J = ∑R ν=1 uνv > ν . For every ν ∈ [R], let Bν be the tensor of order |I| and dimension M in each mode whose arrangement as a column vector gives uν , i.e. whose matricization w.r.t. the partition ([|I|], ∅) is equal to uν . Similarly, let Cν , ν ∈ [R], be the tensor of order |J | = N − |I| and dimension M in each mode whose matricization w.r.t. the partition (∅, [|J |]) (arrangement as a row vector) is equal to v>ν . It holds that:
JAyKI,J = ∑R
ν=1 uνv
> ν
= ∑R
ν=1 JBνK[|I|],∅ JCνK∅,[|J|] = ∑R
ν=1 JBνKI∩[|I|],J∩[|I|] JCνK(I−|I|)∩[|J|],(J−|I|)∩[|J|] = ∑R
ν=1 JBν ⊗ CνKI,J = r∑R
ν=1 Bν ⊗ Cν z I,J
where the third equality relies on the assumption I = {1, . . . , |I|}, J = {|I|+ 1, . . . , N}, the fourth equality makes use of the relation in eq. 1, and the last equality is based on the linearity of the matricization operator. Since matricizations are merely rearrangements of tensors, the fact that JAyKI,J = J ∑R ν=1 B
ν⊗CνKI,J implies Ay = ∑R ν=1 B ν ⊗ Cν , or equivalently, Ayd1...dN = ∑R ν=1 B ν d1...d|I|
· Cνd|I|+1...dN for every d1. . .dN ∈ [M ]. Plugging this into eq. 2 gives:
hy (x1, . . . ,xN ) = ∑M
d1...dN=1 Ayd1...dN ∏N i=1 fθdi (xi)
= ∑M
d1...dN=1
∑R ν=1 Bνd1...d|I| · C ν d|I|+1...dN ∏N i=1 fθdi (xi)
= ∑R
ν=1 (∑M d1...d|I|=1 Bνd1...d|I| ∏|I| i=1 fθdi (xi) ) · (∑M
d|I|+1...dN=1 Cνd|I|+1...dN ∏N i=|I|+1 fθdi (xi) ) (11)
For every ν ∈ [R], define the functions gν : (Rs)|I| → R and g′ν : (Rs)|J| → R as follows:
gν(x1, . . . ,x|I|) := ∑M
d1...d|I|=1 Bνd1...d|I| ∏|I| i=1 fθdi (xi)
g′ν(x1, . . . ,x|J|) := ∑M
d1...d|J|=1 Cνd1...d|J| ∏|J| i=1 fθdi (xi)
9 To see that this does not limit generality, denote I = {i1, . . . , i|I|} and J = {j1, . . . , j|J|}, and define an auxiliary function h′y by permuting the entries of hy such that those indexed by I are on the left and those indexed by J on the right, i.e. h′y(xi1 , . . . ,xi|I| ,xj1 , . . . ,xj|J|) = hy(x1, . . . ,xN ). Obviously sep(hy; I, J) = sep(h′y; I ′, J ′), where the partition (I ′, J ′) is defined by I ′ = {1, . . . , |I|} and J ′ = {|I| + 1, . . . , N}. Analogously to the definition of h′y , let A′y be the tensor obtained by permuting the modes of Ay such that those indexed by I are on the left and those indexed by J on the right, i.e. A′ydi1 ...di|I|dj1 ...dj|J| = A y d1...dN
. It is not difficult to see that matricizing A′y w.r.t. (I ′, J ′) is equivalent to matricizing Ay w.r.t. (I, J), i.e. JA′yKI′,J′ = JAyKI,J , and in particular rankJA′yKI′,J′ = rankJAyKI,J . Moreover, since by definition Ay is a coefficient tensor corresponding to hy (eq. 2), A′y will be a coefficient tensor that corresponds to h′y . Now, our proof will show that sep(h′y; I ′, J ′) = rankJA′yKI′,J′ , which, in light of the equalities above, implies sep(hy; I, J) = rankJAyKI,J , as required.
Substituting these into eq. 11 leads to:
hy (x1, . . . ,xN ) = ∑R
ν=1 gν(x1, . . . ,x|I|)g
′ ν(x|I|+1, . . . ,xN )
which by definition of the separation rank (eq. 5), implies sep(hy; I, J)≤R. By this we have shown that sep(hy; I, J)≤rankJAyKI,J , as required.
For proving the converse inequality, i.e. sep(hy; I, J)≥rankJAyKI,J , we rely on basic concepts and results from functional analysis, or more specifically, from the topic of L2 spaces. While a full introduction to this topic is beyond our scope (the interested reader is referred to Rudin (1991)), we briefly lay out here the minimal background required in order to follow our proof. For any n ∈ N, L2(Rn) is formally defined as the Hilbert space of Lebesgue measurable square-integrable real functions over Rn 10 , equipped with standard (pointwise) addition and scalar multiplication, as well as the inner product defined by integration over point-wise multiplication. For our purposes, L2(Rn) may simply be thought of as the (infinite-dimensional) vector space of functions g : Rn → R satisfying ∫ g2 < ∞, with inner product defined by 〈g1, g2〉 := ∫ g1·g2. Our proof will make use of the following basic facts related to L2 spaces:
Fact 1. If V is a finite-dimensional subspace of L2(Rn), then any g∈L2(Rn) may be expressed as g = p+ δ, with p∈V and δ∈V ⊥ (i.e. δ is orthogonal to all elements in V ). Moreover, such a representation is unique, so in the case where g∈V , we necessarily have p = g and δ ≡ 0.
Fact 2. If g∈L2(Rn), g′∈L2(Rn′), then the function (x1,x2)7→g(x1)·g′(x2) belongs to L2(Rn × Rn′).
Fact 3. Let V and V ′ be finite-dimensional subspaces of L2(Rn) and L2(Rn′) respectively, and define U⊂L2(Rn × Rn′) to be the subspace spanned by {(x1,x2) 7→p(x1)·p′(x2) : p∈V, p′∈V ′}. Given g∈L2(Rn), g′∈L2(Rn′), consider the function (x1,x2) 7→g(x1)·g′(x2) in L2(Rn × Rn′). This function belongs to U⊥ if g∈V ⊥ or g′∈V ′⊥.
Fact 4. If g1. . .gm∈L2(Rn) are linearly independent, then for any k ∈ N, the set of functions {(x1, . . . ,xk) 7→ ∏k i=1 gdi(xi)}d1...dk∈[m] is linearly independent in L 2((Rn)k).
To facilitate application of the theory of L2 spaces, we now make use of the assumption that the network’s representation functions fθd , as well as the functions gν , g ′ ν in the definition of separation rank (eq. 5), are measurable and square-integrable. Taking into account the expression given in eq. 2 for hy , as well as fact 2 above, one readily sees that fθ1 . . .fθM∈L
2(Rs) implies hy∈L2((Rs)N ). The separation rank sep(hy; I, J) will be the minimal non-negative integer R such that there exist g1. . .gR∈L2((Rs)|I|) and g′1. . .g′R∈L2((Rs)|J|) for which:
hy(x1, . . . ,xN ) = ∑R
ν=1 gν(x1, . . . ,x|I|)g
′ ν(x|I|+1, . . . ,xN ) (12)
We would like to show that sep(hy; I, J)≥rankJAyKI,J . Our strategy for achieving this will be to start from eq. 12, and derive an expression for JAyKI,J comprising a sum of R rank-1 matrices. As an initial step along this path, define the following finite-dimensional subspaces:
V := span { (x1, . . . ,x|I|) 7→ ∏|I| i=1 fθdi (xi) } d1...d|I|∈[M ] ⊂ L2 ( (Rs)|I| )
(13)
V ′ := span { (x1, . . . ,x|J|) 7→ ∏|J| i=1 fθdi (xi) } d1...d|J|∈[M ] ⊂ L2 ( (Rs)|J| )
(14)
U := span { (x1, . . . ,xN ) 7→ ∏N
i=1 fθdi (xi) } d1...dN∈[M ] ⊂ L2 ( (Rs)N )
(15)
Notice that hy∈U (eq. 2), and that U is the span of products from V and V ′, i.e.:
U = span { (x1, . . . ,xN ) 7→p(x1, . . . ,x|I|)·p′(x|I|+1, . . . ,xN ) : p∈V, p′∈V ′ }
(16)
Returning to eq. 12, we apply fact 1 to obtain orthogonal decompositions of g1. . .gR w.r.t. V , and of g′1. . .g′R w.r.t. V ′. This gives p1. . .pR∈V , δ1. . .δR∈V ⊥, p′1. . .p′R∈V ′ and δ′1. . .δ′R∈V ′⊥, such that gν = pν + δν and
10 More precisely, elements of the space are equivalence classes of functions, where two functions are considered equivalent if the set in Rn on which they differ has measure zero.
g′ν = p ′ ν + δ ′ ν for every ν ∈ [R]. Plug this into eq. 12:
hy(x1, . . . ,xN ) = ∑R
ν=1 gν(x1, . . . ,x|I|)·g′ν(x|I|+1, . . . ,xN )
= ∑R
ν=1
( pν(x1, . . . ,x|I|) + δν(x1, . . . ,x|I|) ) · ( p′ν(x|I|+1, . . . ,xN ) + δ ′ ν(x|I|+1, . . . ,xN )
) = ∑R ν=1 pν(x1, . . . ,x|I|)·p′ν(x|I|+1, . . . ,xN )
+ ∑R
ν=1 pν(x1, . . . ,x|I|)·δ′ν(x|I|+1, . . . ,xN )
+ ∑R
ν=1 δν(x1, . . . ,x|I|)·p′ν(x|I|+1, . . . ,xN )
+ ∑R
ν=1 δν(x1, . . . ,x|I|)·δ′ν(x|I|+1, . . . ,xN )
Given that U is the span of products from V and V ′ (eq. 16), and that pν∈V, δν∈V ⊥, p′ν∈V ′, δ′ν∈V ′⊥, one readily sees that the first term in the latter expression belongs to U , while, according to fact 3, the second, third and fourth terms are orthogonal to U . We thus obtained an orthogonal decomposition of hy w.r.t. U . Since hy is contained in U , the orthogonal component must vanish (fact 1), and we amount at:
hy(x1, . . . ,xN ) = ∑R
ν=1 pν(x1, . . . ,x|I|)·p′ν(x|I|+1, . . . ,xN ) (17)
For every ν ∈ [R], let Bν and Cν be coefficient tensors of pν and p′ν w.r.t. the functions that span V and V ′ (eq. 13 and 14), respectively. Put formally, Bν and Cν are tensors of orders |I| and |J | (respectively), with dimension M in each mode, meeting:
pν(x1, . . . ,x|I|) = ∑M
d1...d|I|=1 Bνd1...d|I| ∏|I| i=1 fθdi (xi)
p′ν(x1, . . . ,x|J|) = ∑M
d1...d|J|=1 Cνd1...d|J| ∏|J| i=1 fθdi (xi)
Substitute into eq. 17:
hy (x1, . . . ,xN ) = ∑R
ν=1 (∑M d1...d|I|=1 Bνd1...d|I| ∏|I| i=1 fθdi (xi) ) · (∑M
d|I|+1...dN=1 Cνd|I|+1...dN ∏N i=|I|+1 fθdi (xi) ) = ∑R ν=1 ∑M d1...dN=1 Bνd1...d|I| · C ν d|I|+1...dN ∏N i=1 fθdi (xi)
= ∑M
d1...dN=1
(∑R ν=1 Bνd1...d|I| · C ν d|I|+1...dN )∏N i=1 fθdi (xi)
Compare this expression for hy to that given in eq. 2:∑M d1...dN=1 (∑R ν=1 Bνd1...d|I| · C ν d|I|+1...dN )∏N i=1 fθdi (xi) = ∑M d1...dN=1 Ayd1...dN ∏N i=1 fθdi (xi) (18) At this point we utilize the given linear independence of fθ1 . . .fθM∈L
2(Rs), from which it follows (fact 4) that the functions spanning U (eq. 15) are linearly independent in L2((Rs)N ). Both sides of eq. 18 are linear combinations of these functions, thus their coefficients must coincide:
Ayd1...dN = ∑R ν=1 Bνd1...d|I| · C ν d|I|+1...dN ,∀d1. . .dN ∈ [M ] ⇐⇒ A y = ∑R ν=1 Bν ⊗ Cν
Matricizing the tensor equation on the right w.r.t. (I, J) gives: JAyKI,J = r∑R
ν=1 Bν ⊗ Cν z I,J
= ∑R
ν=1 JBν ⊗ CνKI,J = ∑R
ν=1 JBνKI∩[|I|],J∩[|I|] JCνK(I−|I|)∩[|J|],(J−|I|)∩[|J|] = ∑R
ν=1 JBνK[|I|],∅ JCνK∅,[|J|]
where the second equality is based on the linearity of the matricization operator, the third equality relies on the relation in eq. 1, and the last equality makes use of the assumption I = {1, . . . , |I|}, J = {|I| + 1, . . . , N}.
For every ν ∈ [R], JBνK[|I|],∅ is a column vector of dimension M |I| and JCνK∅,[|J|] is a row vector of dimension M |J|. Denoting these by uν and v>ν respectively, we may write:
JAyKI,J = ∑R
ν=1 uνv
> ν
This shows that rankJAyKI,J≤R. Since R is a general non-negative integer that admits eq. 12, we may take it to be minimal, i.e. to be equal to sep(hy; I, J) – the separation rank of hy w.r.t. (I, J). By this we obtain rankJAyKI,J≤sep(hy; I, J), which is what we set out to prove.
A.2 PROOF OF CLAIM 2
The claim is framed in measure theoretical terms, and in accordance, so will its proof be. While a complete introduction to measure theory is beyond our scope (the interested reader is referred to Jones (2001)), we briefly convey here the intuition behind the concepts we will be using, as well as facts we rely upon. The Lebesgue measure is defined over sets in a Euclidean space, and may be interpreted as quantifying their “volume”. For example, the Lebesgue measure of a unit hypercube is one, of the entire space is infinity, and of a finite set of points is zero. In this context, when a phenomenon is said to occur almost everywhere, it means that the set of points in which it does not occur has Lebesgue measure zero, i.e. is negligible. An important result we will make use of (proven in Caron and Traynor (2005) for example) is the following. Given a polynomial defined over n real variables, the set of points in Rn on which it vanishes is either the entire space (when the polynomial in question is the zero polynomial), or it must have Lebesgue measure zero. In other words, if a polynomial is not identically zero, it must be different from zero almost everywhere.
Heading on to the proof, we recall from sec. 3 that the entries of the coefficient tensor Ay (eq. 2) are given by polynomials in the network’s conv weights {al,γ}l,γ and output weights aL,y . Since JAyKI,J – the matricization of Ay w.r.t. the partition (I, J), is merely a rearrangement of the tensor as a matrix, this matrix too has entries given by polynomials in the network’s linear weights. Now, denote by r the maximal rank taken by JAyKI,J as network weights vary, and consider a specific setting of weights for which this rank is attained. We may assume without loss of generality that under this setting, the top-left r-by-r block of JAyKI,J is non-singular. The corresponding minor, i.e. the determinant of the sub-matrix (JAyKI,J)1:r,1:r , is thus a polynomial defined over {al,γ}l,γ and aL,y which is not identically zero. In light of the above, this polynomial is different from zero almost everywhere, implying that rank(JAyKI,J)1:r,1:r = r almost everywhere. Since rankJAyKI,J≥rank(JAyKI,J)1:r,1:r , and since by definition r is the maximal rank that JAyKI,J can take, we have that rankJAyKI,J is maximal almost everywhere.
A.3 PROOF OF THEOREM 1
The matrix decomposition in eq. 7 expresses JAKI,J in terms of the network’s linear weights – {a0,γ ∈ RM}γ∈[r0] for conv operator in hidden layer 0, {a
l,γ ∈ Rrl−1}γ∈[rl] for conv operator in hidden layer l = 1. . .L−1, and aL,y ∈ RrL−1 for node y of dense output operator. We prove lower and upper bounds on the maximal rank that JAKI,J can take as these weights vary. Our proof relies on the rank-multiplicative property of the Kronecker product (rank(A B) = rank(A)·rank(B) for any real matrices A and B – see Bellman (1970) for proof), but is otherwise elementary.
Beginning with the lower bound, consider the following weight setting (eγ here stands for a vector holding 1 in entry γ and 0 at all other entries, 0 stands for a vector holding 0 at all entries, and 1 stands for a vector holding 1 at all entries, with the dimension of a vector to be understood by context):
a0,γ = { eγ , γ≤min{r0,M} 0 , otherwise (19)
a1,γ = { 1 , γ = 1 0 , otherwise
al,γ = { e1 , γ = 1 0 , otherwise for l = 2. . .L− 1
aL,y = e1
Let n ∈ [N/4]. Recalling the definition of Il,k and Jl,k from eq. 6, consider the sets I1,n and J1,n, as well as I0,4(n−1)+t and J0,4(n−1)+t for t ∈ [4]. (I1,n, J1,n) is a partition of [4], i.e. I1,n ·∪J1,n = [4], and for every t ∈ [4] we have I0,4(n−1)+t = {1} and J0,4(n−1)+t = ∅ if t belongs to I1,n, and otherwise I0,4(n−1)+t = ∅
and J0,4(n−1)+t = {1} if t belongs to J1,n. This implies that for an arbitrary vector v, the matricization JvKI0,4(n−1)+t,J0,4(n−1)+t is equal to v if t∈I1,n, and to v> if t∈J1,n. Accordingly, for any γ ∈ [r0]:
4 t=1 Ja0,γKI0,4(n−1)+t,J0,4(n−1)+t =  (a0,γ a0,γ a0,γ a0,γ) , |I1,n| = 4 |J1,n| = 0 (a0,γ a0,γ a0,γ)(a0,γ)> , |I1,n| = 3 |J1,n| = 1 (a0,γ a0,γ)(a0,γ a0,γ)> , |I1,n| = 2 |J1,n| = 2 (a0,γ)(a0,γ a0,γ a0,γ)> , |I1,n| = 1 |J1,n| = 3 (a0,γ a0,γ a0,γ a0,γ)> , |I1,n| = 0 |J1,n| = 4
Assume that γ ≤ min{r0,M}. By our setting a0,γ = eγ , so the above matrix holds 1 in a single entry and 0 in all the rest. Moreover, if the matrix is not a row or column vector, i.e. if both I1,n and J1,n are non-empty, the column index and row index of the entry holding 1 are both unique w.r.t. γ, i.e. they do not repeat as γ ranges over 1 . . .min{r0,M}. We thus have:
rank (∑min{r0,M} γ=1 4 t=1 Ja0,γKI0,4(n−1)+t,J0,4(n−1)+t ) = { min{r0,M} , I1,n 6= ∅ ∧ J1,n 6= ∅ 1 , I1,n = ∅ ∨ J1,n = ∅
Since we set a1,1 = 1 and a0,γ = 0 for γ > min{r0,M}, we may write:
rank (∑r0 γ=1 a1,1γ · 4 t=1 Ja0,γKI0,4(n−1)+t,J0,4(n−1)+t ) = { min{r0,M} , I1,n 6= ∅ ∧ J1,n 6= ∅ 1 , I1,n = ∅ ∨ J1,n = ∅
The latter matrix is by definition equal to Jφ1,1KI1,n,J1,n (see top row of eq. 7), and so for every n ∈ [N/4]:
rank q φ1,1 y I1,n,J1,n = { min{r0,M} , I1,n 6= ∅ ∧ J1,n 6= ∅ 1 , I1,n = ∅ ∨ J1,n = ∅ (20)
Now, the fact that we set aL,y = e1 and al,1 = e1 for l = 2. . .L− 1, implies that the second to last levels of the decomposition in eq. 7 collapse to:
JAyKI,J = N/4
t=1 Jφ1,1KI1,t,J1,t Applying the rank-multiplicative property of the Kronecker product, and plugging in eq. 20, we obtain:
rankJAyKI,J = ∏N/4
t=1 rankJφ1,1KI1,t,J1,t = min{r0,M}S
where S := |{t ∈ [N/4] : I1,t 6= ∅ ∧ J1,t 6= ∅}|. This equality holds for the specific weight setting we defined in eq. 19. Maximizing over all weight settings gives the sought after lower bound:
max {al,γ}l,γ ,aL,y
rankJAyKI,J ≥ min{r0,M}S
Moving on to the upper bound, we show by induction over l = 1. . .L−1 that for any k ∈ [N/4l] and γ ∈ [rl], the rank of Jφl,γKIl,k,Jl,k is no greater than cl,k, regardless of the chosen weight setting. For the base case l = 1 we have:
Jφ1,γKI1,k,J1,k = ∑r0
α=1 a1,γα · 4 t=1 Ja0,αKI0,4(k−1)+t,J0,4(k−1)+t The M |I1,k|-by-M |J1,k| matrix Jφ1,γKI1,k,J1,k is given here as a sum of r0 rank-1 terms, thus obviously its rank is no greater than min{Mmin{|I1,k|,|J1,k|}, r0}. Since by definition c0,t = 1 for all t ∈ [N ], we may write:
rankJφ1,γKI1,k,J1,k ≤ min { Mmin{|I1,k|,|J1,k|}, r0 ∏4 t=1 c0,4(k−1)+t }
c1,k is defined by the right hand side of this inequality, so our inductive hypotheses holds for l = 1. For l > 1: Jφl,γKIl,k,Jl,k = ∑rl−1
α=1 al,γα · 4 t=1 Jφl−1,αKIl−1,4(k−1)+t,Jl−1,4(k−1)+t Taking ranks:
rankJφl,γKIl,k,Jl,k = rank (∑rl−1
α=1 al,γα · 4 t=1
Jφl−1,αKIl−1,4(k−1)+t,Jl−1,4(k−1)+t )
≤ ∑rl−1
α=1 rank ( 4 t=1 Jφl−1,αKIl−1,4(k−1)+t,Jl−1,4(k−1)+t )
= ∑rl−1
α=1 ∏4 t=1 rankJφl−1,αKIl−1,4(k−1)+t,Jl−1,4(k−1)+t
≤ ∑rl−1
α=1 ∏4 t=1 cl−1,4(k−1)+t
= rl−1 ∏4
t=1 cl−1,4(k−1)+t
where we used rank sub-additivity in the second line, the rank-multiplicative property of the Kronecker product in the third line, and our inductive hypotheses for l − 1 in the fourth line. Since the number rows and columns in Jφl,γKIl,k,Jl,k is M |Il,k| and M |Jl,k| respectively, we may incorporate these terms into the inequality, obtaining:
rankJφl,γKIl,k,Jl,k ≤ min { Mmin{|Il,k|,|Jl,k|}, rl−1 ∏4 t=1 cl−1,4(k−1)+t }
The right hand side here is equal to cl,k by definition, so our inductive hypotheses indeed holds for all l = 1. . .L − 1. To establish the sought after upper bound on the rank of JAyKI,J , we recall that the latter is given by:
JAyKI,J = ∑rL−1
α=1 aL,yα · 4 t=1 JφL−1,αKIL−1,t,JL−1,t Carry out a series of steps similar to before, while making use of our inductive hypotheses for l = L− 1:
rankJAyKI,J = rank (∑rL−1
α=1 aL,yα · 4 t=1
JφL−1,αKIL−1,t,JL−1,t )
≤ ∑rL−1
α=1 rank ( 4 t=1 JφL−1,αKIL−1,t,JL−1,t )
= ∑rL−1
α=1 ∏4 t=1 rankJφL−1,αKIL−1,t,JL−1,t
≤ ∑rL−1
α=1 ∏4 t=1 cL−1,t
= rL−1 ∏4
t=1 cL−1,t
Since JAyKI,J has M |I| rows and M |J| columns, we may include these terms in the inequality, thus reaching the upper bound we set out to prove.

B SEPARATION RANK AND THE L2 DISTANCE FROM SEPARABLE FUNCTIONS
Our analysis of correlations modeled by convolutional networks is based on the concept of separation rank, conveyed in sec. 4. When the separation rank of a function w.r.t. a partition of its input is equal to 1, the function is separable, meaning it does not model any interaction between sides of the partition. We argued that the higher the separation rank, the farther the function is from this situation, i.e. the stronger the correlation it induces between sides of the partition. In the current appendix we formalize this argument, by relating separation rank to the L2 distance from the set of separable functions. We begin by defining and characterizing a normalized (scale invariant) version of this distance (app. B.1). It is then shown (app. B.2) that separation rank provides an upper bound on the normalized distance. Finally, a lower bound that applies to deep convolutional arithmetic circuits is derived (app. B.3), based on the lower bound for their separation ranks established in sec. 5.2. Together, these steps imply that our entire analysis, facilitated by upper and lower bounds on separation ranks of convolutional arithmetic circuits, can be interpreted as based on upper and lower bounds on (normalized) L2 distances from separable functions.
In the text hereafter, we assume familiarity of the reader with the contents of sec. 2, 3, 4, 5 and the proofs given in app. A. We also rely on basic knowledge in the topic of L2 spaces (see discussion in app. A.1 for minimal background required in order to follow our arguments), as well as several results concerning singular values of matrices. In line with sec. 5, an assumption throughout this appendix is that all functions in question are measurable and square-integrable (i.e. belong to L2 over the respective Euclidean space), and in app. B.3, we also make use of the fact that representation functions (fθd ) of a convolutional arithmetic circuit can be regarded as linearly independent (see sec. 5.1). Finally, for convenience, we now fix (I, J) – an arbitrary partition of [N ]. Specifically, I and J are disjoint subsets of [N ] whose union gives [N ], denoted by I = {i1, . . . , i|I|} with i1 < · · · < i|I|, and J = {j1, . . . , j|J|} with j1 < · · · < j|J|.
B.1 NORMALIZED L2 DISTANCE FROM SEPARABLE FUNCTIONS
For a function h∈L2((Rs)N ) (which is not identically zero), the normalized L2 distance from the set of separable functions w.r.t. (I, J), is defined as follows:
D(h; I, J) := 1
‖h‖ · infg∈L2((Rs)|I|) g′∈L2((Rs)|J|) ∥∥∥h(x1, . . . ,xN )− g(xi1 , . . . ,xi|I|)g′(xj1 , . . . ,xj|J|)∥∥∥ (21) where ‖·‖ refers to the norm of L2 space, e.g. ‖h‖ := ( ∫ (Rs)N h
2)1/2. In words, D(h; I, J) is defined as the minimal L2 distance between h and a function that is separable w.r.t. (I, J), divided by the norm of h. The
normalization (division by ‖h‖) admits scale invariance to D(h; I, J), and is of critical importance – without it, rescaling h would accordingly rescale the distance measure, rendering the latter uninformative in terms of deviation from separability.
It is worthwhile noting the resemblance between D(h; I, J) and the concept of mutual information (see Cover and Thomas (2012) for a comprehensive introduction). Both measures quantify the interaction that a normalized function 11 induces between input variables, by measuring distance from separable functions. The difference between the measures is threefold. First, mutual information considers probability density functions (non-negative and in L1), while D(h; I, J) applies to functions in L2. Second, the notion of distance in mutual information is quantified through the Kullback-Leibler divergence, whereas in D(h; I, J) it is simply the L2 metric. Third, while mutual information evaluates the distance from a specific separable function – product of marginal distributions, D(h; I, J) evaluates the minimal distance across all separable functions.
We now turn to establish a spectral characterization of D(h; I, J), which will be used in app. B.2 and B.3 for deriving upper and lower bounds (respectively). Assume we have the following expression for h:
h(x1, . . . ,xN ) = ∑m
µ=1 ∑m′ µ′=1 Aµ,µ′ · φµ(xi1 , . . . ,xi|I|)φ ′ µ′(xj1 , . . . ,xj|J|) (22)
wherem andm′ are positive integers, A is anm-by-m′ real matrix, and {φµ}mµ=1, {φ′µ′}m ′
µ′=1 are orthonormal sets of functions in L2((Rs)|I|), L2((Rs)|J|) respectively. We refer to such expression as an orthonormal separable decomposition of h, with A being its coefficient matrix. We will show that for any orthonormal separable decomposition, D(h; I, J) is given by the following formula:
D(h; I, J) = √ 1− σ 2 1(A)
σ21(A) + · · ·+ σ2min{m,m′}(A) (23)
where σ1(A) ≥ · · · ≥ σmin{m,m′}(A) ≥ 0 are the singular values of the coefficient matrix A. This implies that if the largest singular value of A accounts for a significant portion of the spectral energy, the normalized L2 distance of h from separable functions is small. On the other hand, if all but a fraction of the spectral energy is attributed to trailing singular values, h is far from being separable (D(h; I, J) is close to 1).
As a first step in deriving eq. 23, we show that ‖h‖2 = σ21(A) + · · ·+ σ2min{m,m′}(A):
‖h‖2 = (1)
∫ h2(x1, . . . ,xN )dx1· · ·dxN
= (2) ∫ (∑m µ=1 ∑m′ µ′=1 Aµ,µ′ · φµ(xi1 , . . . ,xi|I|)φ ′ µ′(xj1 , . . . ,xj|J|) )2 dx1· · ·dxN
= (3) ∫ ∑m µ,µ̄=1 ∑m′ µ′,µ̄′=1 Aµ,µ′Aµ̄,µ̄′ · φµ(xi1 , . . . ,xi|I|)φ ′ µ′(xj1 , . . . ,xj|J|)
·φµ̄(xi1 , . . . ,xi|I|)φ ′ µ̄′(xj1 , . . . ,xj|J|)dx1· · ·dxN
= (4) ∑m µ,µ̄=1 ∑m′ µ′,µ̄′=1 Aµ,µ′Aµ̄,µ̄′ ∫ φµ(xi1 , . . . ,xi|I|)φ ′ µ′(xj1 , . . . ,xj|J|)
·φµ̄(xi1 , . . . ,xi|I|)φ ′ µ̄′(xj1 , . . . ,xj|J|)dx1· · ·dxN
= (5) ∑m µ,µ̄=1 ∑m′ µ′,µ̄′=1 Aµ,µ′Aµ̄,µ̄′ ∫ φµ(xi1 , . . . ,xi|I|)φµ̄(xi1 , . . . ,xi|I|)dxi1 · · ·dxi|I|
· ∫ φ′µ′(xj1 , . . . ,xj|J|)φ ′ µ̄′(xj1 , . . . ,xj|J|)dxj1 · · ·dxj|J|
= (6) ∑m µ,µ̄=1 ∑m′ µ′,µ̄′=1 Aµ,µ′Aµ̄,µ̄′ · { 1 , µ = µ̄ 0 , otherwise } · { 1 , µ′ = µ̄′ 0 , otherwise } = (7) ∑m µ=1 ∑m′ µ′=1 A2µ,µ′
= (8)
σ21(A) + · · ·+ σ2min{m,m′}(A) (24)
Equality (1) here originates from the definition of L2 norm. (2) is obtained by plugging in the expression in eq. 22. (3) is merely an arithmetic manipulation. (4) follows from the linearity of integration. (5) makes use
11 An equivalent definition ofD(h; I, J) is the minimalL2 distance between h/ ‖h‖ and a function separable w.r.t. (I, J). Accordingly, we may view D(h; I, J) as operating on normalized functions.
of Fubini’s theorem (see Jones (2001)). (6) results from the orthonormality of {φµ}mµ=1 and {φ′µ′}m ′
µ′=1. (7) is a trivial computation. Finally, (8) is an outcome of the fact that the squared Frobenius norm of a matrix, i.e. the sum of squares over its entries, is equal to the sum of squares over its singular values (see Golub and Van Loan (2013) for proof).
Let g∈L2((Rs)|I|). By fact 1 in app. A.1, there exist scalars α1 . . . αm ∈ R, and a function δ∈L2((Rs)|I|) orthogonal to span{φ1 . . . φm}, such that g = ∑m µ=1 αµ ·φµ+δ. Similarly, for any g
′∈L2((Rs)|J|) there exist α′1 . . . α ′ m′ ∈ R and δ′∈span{φ′1 . . . φ′m′}⊥ such that g′ = ∑m′ µ′=1 α ′ µ′ ·φ′µ′ + δ′. Fact 2 in app. A.1 indicates that the function given by (x1, . . . ,xN ) 7→g(xi1 , . . . ,xi|I|)g ′(xj1 , . . . ,xj|J|) belongs toL
2((Rs)N ). We may express it as follows:
g(xi1 , . . . ,xi|I|)g ′(xj1 , . . . ,xj|J|) = ∑m µ=1 ∑m′ µ′=1 αµα ′ µ′ · φµ(xi1 , . . . ,xi|I|)φ ′ µ′(xj1 , . . . ,xj|J|)
+ (∑m
µ=1 αµ · φµ(xi1 , . . . ,xi|I|)
) · δ′(xj1 , . . . ,xj|J|)
+δ(xi1 , . . . ,xi|I|) · (∑m′ µ′=1 α′µ′ · φ′µ′(xj1 , . . . ,xj|J|) ) +δ(xi1 , . . . ,xi|I|)δ ′(xj1 , . . . ,xj|J|)
According to fact 3 in app. A.1, the second, third and fourth terms on the right hand side of the above are orthogonal to span{(x1, . . . ,xN ) 7→φµ(xi1 , . . . ,xi|I|)φ ′ µ′(xj1 , . . . ,xj|J|)}µ∈[m],µ′∈[m′]. Denote their summation by E(x1, . . . ,xN ), and subtract the overall function from h (given by eq. 22): h(x1, . . . ,xN )− g(xi1 , . . . ,xi|I|)g ′(xj1 , . . . ,xj|J|)
= ∑m
µ=1 ∑m′ µ′=1 Aµ,µ′ · φµ(xi1 , . . . ,xi|I|)φ ′ µ′(xj1 , . . . ,xj|J|)
− ∑m
µ=1 ∑m′ µ′=1 αµα ′ µ′ · φµ(xi1 , . . . ,xi|I|)φ ′ µ′(xj1 , . . . ,xj|J|)− E(x1, . . . ,xN )
= ∑m
µ=1 ∑m′ µ′=1 (Aµ,µ′ − αµα′µ′) · φµ(xi1 , . . . ,xi|I|)φ ′ µ′(xj1 , . . . ,xj|J|)− E(x1, . . . ,xN )
Since the two terms in the latter expression are orthogonal to one another, we have:∥∥∥h(x1, . . . ,xN )− g(xi1 , . . . ,xi|I|)g′(xj1 , . . . ,xj|J|)∥∥∥2 = ∥∥∥∥∑mµ=1∑m′µ′=1(Aµ,µ′ − αµα′µ′) · φµ(xi1 , . . . ,xi|I|)φ′µ′(xj1 , . . . ,xj|J|) ∥∥∥∥2 + ‖E(x1, . . . ,xN )‖2
Applying a sequence of steps as in eq. 24 to the first term in the second line of the above, we obtain:∥∥∥h(x1, . . . ,xN )− g(xi1 , . . . ,xi|I|)g′(xj1 , . . . ,xj|J|)∥∥∥2 = m∑ µ=1 m′∑ µ′=1 (Aµ,µ′−αµα′µ′)2+‖E(x1, . . . ,xN )‖2
E(x1, . . . ,xN ) = 0 if δ and δ′ are the zero functions, implying that:∥∥∥h(x1, . . . ,xN )− g(xi1 , . . . ,xi|I|)g′(xj1 , . . . ,xj|J|)∥∥∥2 ≥∑mµ=1∑m′µ′=1(Aµ,µ′ − αµα′µ′)2 with equality holding if g = ∑m µ=1 αµ ·φµ and g ′ = ∑m′ µ′=1 α ′ µ′ ·φ′µ′ . Now, ∑m µ=1 ∑m′ µ′=1(Aµ,µ′ −αµα ′ µ′) 2 is the squared Frobenius distance between the matrix A and the rank-1 matrix αα′>, where α and α′ are column vectors holding α1 . . . αm and α′1 . . . α′m′ respectively. This squared distance is greater than or equal to the sum of squares over the second to last singular values of A, and moreover, the inequality holds with equality for proper choices of α and α′ (Eckart and Young (1936)). From this we conclude that:∥∥∥h(x1, . . . ,xN )− g(xi1 , . . . ,xi|I|)g′(xj1 , . . . ,xj|J|)∥∥∥2 ≥ σ22(A) + · · ·+ σ2min{m,m′}(A) with equality holding if g and g′ are set to ∑m µ=1 αµ ·φµ and ∑m′ µ′=1 α ′ µ′ ·φ′µ′ (respectively) for proper choices of α1 . . . αm and α′1 . . . α′m′ . We thus have the infimum over all possible g, g ′:
inf g∈L2((Rs)|I|) g′∈L2((Rs)|J|) ∥∥∥h(x1, . . . ,xN )− g(xi1 , . . . ,xi|I|)g′(xj1 , . . . ,xj|J|)∥∥∥2 = σ22(A) + · · ·+ σ2min{m,m′}(A) (25)
Recall that we would like to derive the formula in eq. 23 forD(h; I, J), assuming h is given by the orthonormal separable decomposition in eq. 22. Taking square root of the equalities established in eq. 24 and 25, and plugging them into the definition of D(h; I, J) (eq. 21), we obtain the sought after result.
B.2 UPPER BOUND THROUGH SEPARATION RANK
We now relate D(h; I, J) – the normalized L2 distance of h∈L2((Rs)N ) from the set of separable functions w.r.t. (I, J) (eq. 21), to sep(h; I, J) – the separation rank of h w.r.t. (I, J) (eq. 5). Specifically, we make use of the formula in eq. 23 to derive an upper bound on D(h; I, J) in terms of sep(h; I, J).
Assuming h has finite separation rank (otherwise the bound we derive is trivial), we may express it as:
h(x1, . . . ,xN ) = ∑R
ν=1 gν(xi1 , . . . ,xi|I|)g
′ ν(xj1 , . . . ,xj|J|) (26)
whereR is some positive integer (necessarily greater than or equal to sep(h; I, J)), and g1. . .gR∈L2((Rs)|I|), g′1. . .g ′ R∈L2((Rs)|J|). Let {φ1, . . . , φm}⊂L2((Rs)|I|) and {φ′1, . . . , φ′m′}⊂L2((Rs)|J|) be two sets of orthonormal functions spanning span{g1. . .gR} and span{g′1. . .g′R} respectively. By definition, for every ν∈R there exist αν,1 . . . αν,m ∈ R and α′ν,1 . . . α′ν,m′ ∈ R such that gν = ∑m µ=1 αν,µ · φµ and
g′ν = ∑m′ µ′=1 α ′ ν,µ′ · φ′µ′ . Plugging this into eq. 26, we obtain:
h(x1, . . . ,xN ) = ∑m
µ=1 ∑m′ µ′=1 (∑R ν=1 αν,µα ′ ν,µ′ ) · φµ(xi1 , . . . ,xi|I|)φ ′ µ′(xj1 , . . . ,xj|J|)
This is an orthonormal separable decomposition of h (eq. 22), with coefficient matrix A = ∑R ν=1 αν(α ′ ν) >, where αν := [αν,1 . . . αν,m]> and α′ν := [α′ν,1 . . . α′ν,m′ ] > for every ν∈R. Obviously the rank of A is no greater than R, implying: σ21(A)
σ21(A) + · · ·+ σ2min{m,m′}(A) ≥ 1 R
where as in app. B.1, σ1(A) ≥ · · · ≥ σmin{m,m′}(A) ≥ 0 stand for the singular values of A. Introducing this inequality into eq. 23 gives:
D(h; I, J) = √ 1− σ 2 1(A) σ21(A) + · · ·+ σ2min{m,m′}(A) ≤ √ 1− 1 R
The latter holds for any R ∈ N that admits eq. 26, so in particular we may take it to be minimal, i.e. to be equal to sep(h; I, J) 12 , bringing forth the sought after upper bound:
D(h; I, J) ≤ √ 1− 1
sep(h; I, J) (27)
By eq. 27, low separation rank implies proximity (in normalized L2 sense) to a separable function. We may use the inequality to translate the upper bounds on separation ranks established for deep and shallow convolutional arithmetic circuits (sec. 5.2 and 5.3 respectively), into upper bounds on normalized L2 distances from separable functions. To completely frame our analysis in terms of the latter measure, a translation of the lower bound on separation ranks of deep convolutional arithmetic circuits (sec. 5.2) is also required. Eq. 27 does not facilitate such translation, and in fact, it is easy to construct functions hwhose separation ranks are high yet are very close (in normalized L2 sense) to separable functions. 13 However, as we show in app. B.3 below, the specific lower bound of interest can indeed be translated, and our analysis may entirely be framed in terms of normalized L2 distance from separable functions.
B.3 LOWER BOUND FOR DEEP CONVOLUTIONAL ARITHMETIC CIRCUITS
Let hy∈L2((Rs)N ) be a function realized by a deep convolutional arithmetic circuit (fig. 1(a) with size-4 pooling windows and L = log4 N hidden layers), i.e. hy is given by eq. 2, where fθ1 . . .fθM∈L
2(Rs) are linearly independent representation functions, and Ay is a coefficient tensor of order N and dimension M in each mode, determined by the linear weights of the network ({al,γ}l,γ ,aL,y) through the hierarchical decomposition in eq. 3. Rearrange eq. 2 by grouping indexes d1. . .dN in accordance with the partition (I, J):
hy (x1, . . . ,xN ) = ∑M
di1 ...di|I|=1 ∑M dj1 ...dj|J|=1 Ayd1...dN · (∏|I| t=1 fθdit (xit) )(∏|J| t=1 fθdjt (xjt) ) (28)
12 We disregard the trivial case where sep(h; I, J) = 0 (h is identically zero). 13 This will be the case, for example, if h is given by an orthonormal separable decomposition (eq. 22), with coefficient matrix A that has high rank but whose spectral energy is highly concentrated on one singular value.
Let m = M |I|, and define the following mapping:
µ : [M ]|I| → [m] , µ(di1 , . . . , di|I|) = 1 + ∑|I|
t=1 (dit − 1)·M
|I|−t
µ is a one-to-one correspondence between the index sets [M ]|I| and [m]. We slightly abuse notation, and denote by (di1(µ), . . . , di|I|(µ)) the tuple in [M ]
|I| that maps to µ ∈ [m]. Additionally, we denote the function∏|I| t=1 fθdit (µ) (xit), which according to fact 2 in app. A.1 belongs to L 2((Rs)|I|), by φµ(xi1 , . . . ,xi|I|). In the exact same manner, we let m′ = M |J|, and define the bijective mapping:
µ′ : [M ]|J| → [m′] , µ′(dj1 , . . . , dj|J|) = 1 + ∑|J|
t=1 (djt − 1)·M
|J|−t
As before, (dj1(µ ′), . . . , dj|J|(µ ′)) stands for the tuple in [M ]|J| that maps to µ′ ∈ [m′], and the function∏|J| t=1 fθdjt (µ′)
(xjt)∈L2((Rs)|J|) is denoted by φ′µ′(xj1 , . . . ,xj|J|). Now, recall the definition of matricization given in sec. 2, and consider JAyKI,J – the matricization of the coefficient tensorAy w.r.t. (I, J). This is a matrix of sizem-by-m′, holdingAd1...dN in row index µ(di1 , . . . , di|I|) and column index µ
′(dj1 , . . . , dj|J|). Rewriting eq. 28 with the indexes µ and µ′ instead of (di1 , . . . , di|I|) and (dj1 , . . . , dj|J|), we obtain:
hy (x1, . . . ,xN ) = ∑m
µ=1 ∑m′ µ′=1 (JAyKI,J)µ,µ′ · φµ(xi1 , . . . ,xi|I|)φ ′ µ′(xj1 , . . . ,xj|J|) (29)
This equation has the form of eq. 22. However, for it to qualify as an orthonormal separable decomposition, the sets of functions {φ1, . . . , φm}⊂L2((Rs)|I|) and {φ′1, . . . , φ′m′}⊂L2((Rs)|J|) must be orthonormal. If the latter holds eq. 23 may be applied, giving an expression for D(hy; I, J) – the normalized L2 distance of hy from the set of separable functions w.r.t. (I, J), in terms of the singular values of JAyKI,J .
We now direct our attention to the special case where fθ1 . . .fθM∈L 2(Rs) – the network’s representation functions, are known to be orthonormal. The general setting, in which only linear independence is known, will be treated thereafter. Orthonormality of representation functions implies that φ1 . . . φm∈L2((Rs)|I|) are orthonormal as well:
〈φµ, φµ̄〉 = (1)
∫ φµ(xi1 , . . . ,xi|I|)φµ̄(xi1 , . . . ,xi|I|)dxi1 · · ·dxi|I|
= (2) ∫ ∏|I| t=1 fθdit (µ) (xit) ∏|I| t=1 fθdit (µ̄) (xit)dxi1 · · ·dxi|I|
= (3) ∏|I| t=1 ∫ fθdit (µ) (xit)fθdit (µ̄) (xit)dxit
= (4) ∏|I| t=1 〈 fθdit (µ) , fθdit (µ̄) 〉 = (5) ∏|I| t=1 { 1 , dit(µ) = dit(µ̄) 0 , otherwise
} = (6) { 1 , dit(µ) = dit(µ̄) ∀t ∈ [|I|] 0 , otherwise
= (7) { 1 , µ = µ̄ 0 , otherwise
(1) and (4) here follow from the definition of inner product in L2 space, (2) replaces φµ and φµ̄ by their definitions, (3) makes use of Fubini’s theorem (see Jones (2001)), (5) relies on the (temporary) assumption that representation functions are orthonormal, (6) is a trivial step, and (7) owes to the fact that µ 7→ (di1(µ), . . . , di|I|(µ)) is an injective mapping. A similar sequence of steps (applied to 〈φ ′ µ′ , φ ′ µ̄′〉) shows that in addition to φ1 . . . φm, the functions φ′1 . . . φ′m′∈L2((Rs)|J|) will also be orthonormal if fθ1 . . .fθM are. We conclude that if representation functions are orthonormal, eq. 29 indeed provides an orthonormal separable decomposition of hy , and the formula in eq. 23 may be applied:
D(hy; I, J) = √ 1− σ 2 1(JAyKI,J)
σ21(JAyKI,J) + · · ·+ σ2min{m,m′}(JAyKI,J) (30)
where σ1(JAyKI,J) ≥ · · · ≥ σmin{m,m′}(JAyKI,J) ≥ 0 are the singular values of the coefficient tensor matricization JAyKI,J . In sec. 5.2 we showed that the maximal separation rank realizable by a deep network is greater than or equal to min{r0,M}S , whereM, r0 are the number of channels in the representation and first hidden layers (respectively), and S stands for the number of index quadruplets (sets of the form {4k-3, 4k-2, 4k-1, 4k} for some k ∈
[N/4]) that are split by the partition (I, J). To prove this lower bound, we presented in app. A.3 a specific setting for the linear weights of the network ({al,γ}l,γ ,aL,y) under which rankJAyKI,J = min{r0,M}S . Careful examination of the proof shows that with this particular weight setting, not only is the rank of JAyKI,J equal to min{r0,M}S , but also, all of its non-zero singular values are equal to one another. 14 This implies that σ21(JAyKI,J)/(σ21(JAyKI,J) + · · · + σ2min{m,m′}(JAyKI,J)) = min{r0,M}−S , and since we currently assume that fθ1 . . .fθM are orthonormal, eq. 30 applies and we obtain D(hy; I, J) = √ 1−min{r0,M}−S . Maximizing over all possible weight settings, we arrive at the following lower bound for the normalized L2 distance from separable functions brought forth by a deep convolutional arithmetic circuit:
sup {al,γ}l,γ , aL,y
D ( hy|{al,γ}l,γ ,aL,y ; I, J ) ≥ √ 1− 1
min{r0,M}S (31)
Turning to the general case, we omit the assumption that representation functions fθ1 . . .fθM∈L 2(Rs) are orthonormal, and merely rely on their linear independence. The latter implies that the dimension of span{fθ1 . . .fθM } is M , thus there exist orthonormal functions ϕ1. . .ϕM∈L
2(Rs) that span it. Let F ∈ RM×M be a transition matrix between the bases – the matrix defined byϕc = ∑M d=1 Fc,d·fθd , ∀c ∈ [M ]. Suppose now that we replace the original representation functions fθ1 . . .fθM by the orthonormal ones ϕ1. . .ϕM . Using the latter, the lower bound in eq. 31 applies, and there exists a setting for the linear weights of the network – {al,γ}l,γ ,aL,y , such that D(hy; I, J)≥ √ 1−min{r0,M}−S . Recalling the structure of convolutional arithmetic circuits (fig. 1(a)), one readily sees that if we return to the original representation functions fθ1 . . .fθM , while multiplying conv weights in hidden layer 0 by F
> (i.e. mapping a0,γ 7→F>a0,γ), the overall function hy remains unchanged, and in particular D(hy; I, J)≥ √ 1−min{r0,M}−S still holds. We conclude that the lower bound in eq. 31 applies, even if representation functions are not orthonormal.
To summarize, we translated the lower bound from sec. 5.2 on the maximal separation rank realized by a deep convolutional arithmetic circuit, into a lower bound on the maximal normalized L2 distance from separable functions (eq. 31). This, along with the translation of upper bounds facilitated in app. B.2, implies that the analysis carried out in the paper, which studies correlations modeled by convolutional networks through the notion of separation rank, may equivalently be framed in terms of normalized L2 distance from separable functions. We note however that there is one particular aspect in our original analysis that does not carry through the translation. Namely, in sec. 5.1 it was shown that separation ranks realized by convolutional arithmetic circuits are maximal almost always, i.e. for all linear weight settings but a set of (Lebesgue) measure zero. Put differently, for a given partition (I, J), the maximal separation rank brought forth by a network characterizes almost all functions realized by it. An equivalent statement does not hold with the continuous measure of normalized L2 distance from separable functions. The behavior of this measure across the hypotheses space of a network is non-trivial, and forms a subject for future research.
C IMPLEMENTATION DETAILS
In this appendix we provide implementation details omitted from the description of our experiments in sec. 7. Our implementation, available online at https://github.com/HUJI-Deep/inductive-pooling, is based on the SimNets branch (Cohen et al. (2016a)) of Caffe toolbox (Jia et al. (2014)). The latter realizes convolutional arithmetic circuits in log-space for numerical stability.
When training convolutional arithmetic circuits, we followed the hyper-parameter choices made by Sharir et al. (2016). In particular, our objective function was the cross-entropy loss with no L2 regularization (i.e. with weight decay set to 0), optimized using Adam (Kingma and Ba (2014)) with step-size α = 0.003 and moment decay rates β1 = β2 = 0.9. 15000 iterations with batch size 64 (48 epochs) were run, with the step-size α decreasing by a factor of 10 after 12000 iterations (38.4 epochs). We did not use dropout (Srivastava et al. (2014)), as the limiting factor in terms of accuracies was the difficulty of fitting training data (as opposed to overfitting) – see fig. 3.
For training the conventional convolutional rectifier networks, we merely switched the hyper-parameters of Adam to the recommended settings specified in Kingma and Ba (2014) (α = 0.001, β1 = 0.9, β2 = 0.999), and set weight decay to the standard value of 0.0001.
14 To see this, note that with the specified weight setting, for every n ∈ [N/4], Jφ1,1KI1,n,J1,n has one of two forms: it is either a non-zero (row/column) vector, or it is a matrix holding 1 in several entries and 0 in all the rest, where any two entries holding 1 reside in different rows and different columns. The first of the two forms admits a single non-zero singular value. The second brings forth several singular values equal to 1, possibly accompanied by null singular values. In both cases, all non-zero singular values of Jφ1,1KI1,n,J1,n are equal to one another. Now, since JAyKI,J = N/4n=1Jφ1,1KI1,n,J1,n , and since the Kronecker product multiplies singular values (see Bellman (1970)), we have that all non-zero singular values of JAyKI,J are equal, as required.

D MORPHOLOGICAL CLOSURE
The synthetic dataset used in our experiments (sec. 7) consists of binary images displaying different shapes (blobs). One of the tasks facilitated by this dataset is the detection of morphologically closed blobs, i.e. of images that are relatively similar to their morphological closure. The procedure we followed for computing the morphological closure of a binary image is:
1. Pad the given image with background (0 value) pixels
2. Morphological dilation: simultaneously turn on (set to 1) all pixels that have a (left, right, top or bottom) neighbor originally active (holding 1)
3. Morphological erosion: simultaneously turn off (set to 0) all pixels that have a (left, right, top or bottom) neighbor currently inactive (holding 0)
4. Remove pixels introduced in padding
It is not difficult to see that any pixel active in the original image is necessarily active in its closure. Moreover, pixels that are originally inactive yet are surrounded by active ones will also be turned on in the closure, hence the effect of “gap filling”. Finally, we note that the particular sequence of steps described above represents the most basic form of morphological closure. The interested reader is referred to Haralick et al. (1987) for a much more comprehensive introduction.
","Our formal understanding of the inductive bias that drives the success of convolutional networks on computer vision tasks is limited. In particular, it is unclear what makes hypotheses spaces born from convolution and pooling operations so suitable for natural images. In this paper we study the ability of convolutional networks to model correlations among regions of their input. We theoretically analyze convolutional arithmetic circuits, and empirically validate our findings on other types of convolutional networks as well. Correlations are formalized through the notion of separation rank, which for a given partition of the input, measures how far a function is from being separable. We show that a polynomially sized deep network supports exponentially high separation ranks for certain input partitions, while being limited to polynomial separation ranks for others. The network’s pooling geometry effectively determines which input partitions are favored, thus serves as a means for controlling the inductive bias. Contiguous pooling windows as commonly employed in practice favor interleaved partitions over coarse ones, orienting the inductive bias towards the statistics of natural images. Other pooling schemes lead to different preferences, and this allows tailoring the network to data that departs from the usual domain of natural imagery. In addition to analyzing deep networks, we show that shallow ones support only linear separation ranks, and by this gain insight into the benefit of functions brought forth by depth – they are able to efficiently model strong correlation under favored partitions of the input.",ICLR 2017 conference submission,True,,"The paper provides a highly complex algebraic machinery to analyze the type of functions covered by convolutional network. As in most attempts  in this direction in the literature, the ideal networks described in paper, which have to be interpretable as polynomials over tensors, do not match the type of CNNs used in practice: for instance the Relu non-linearity is replaced with a product of linear functions (or a sum of logs).

While the paper is very technical to read, every concept is clearly stated and mathematical terminology properly introduced. Still, I think some the authors could make some effort to make the key concepts more accessible, and give a more intuitive understanding of what the separation rank means rather before piling up different mathematical interpretation.
My SVM-era algebra is quite rusted, and I am not familiar with the separation rank framework: it would have been much easier for me to first fully understand a simple and gentle case (shallow network in section 5.3), than the general deep case.

To summarize my understanding of the key theorem 1 result:
- The upper bound of the separation rank is used to show that in the shallow case, this rank grows AT MOST linearly with the network size (as measured by the only hidden layer). So exponential network sizes are caused by this rank needing to grow exponentially, as required by the partition.
- In the deep case, one also uses the case that the upper bound is linear in the size of the network (as measured by the last hidden layer), however, this situation is caused by the selection of a partition (I^low, J^high), and the maximal rank induced by this partition is only linear anyway, hence the network size can remain linear.

If tried my best to summarize the key point of this paper and still probably failed at it, which shows how complex is this notion of partition rank, and that its linear growth with network size can either be a good or bad thing depending on the setting. Hopefully, someone will come one day with an explanation that holds in a single slide.



While this is worth publishing as conference paper in its present form, I have two suggestions that, IMHO, would make this work more significant:

On the theory side, we are still very far from the completeness of the PAC bound papers of the ""shallow era"". In particular, the non-probabilistic lower and upper bound in theorem 1 are probably loose, and there is no PAC-like theory to tell us which one to use and what is the predicted impact on performance (not just the intuition). Also, in the prediction of the inductive bias, the other half is missing. This paper attempts to predict the maximal representation capacity of a DNN under bounded network size constraints, but one of the reason why this size has to be bounded is overfitting (justified by PAC or VC-dim like bounds). If we consider the expected risk as  the sum of the empirical risk and the structural risk, this paper only seems to address fully the empirical risk minimization part, freezing the structural risk. 

On the practice side, an issue is that experiments in this paper mostly confirm what is obvious through intuition, or some simpler form of reasonings. For instance to use convolutions that join pixels which are symmetrical in images to detect symmetry. Basic hand-crafted pattern detectors, as they have been used in computer vision for decades, would just do the job. What would be a great motivation for using this framework is if it answered questions that simple human intuition cannot, and for which we are still in the dark: one example I could think of in the recent use of gated convolutions 'a trous' for 1D speech signal, popularized in Google WaveNet (

---

The paper uses the notion of separation rank from tensor algebra to analyze the correlations induced through convolution and pooling operations. They show that deep networks have exponentially larger separation ranks compared to shallow ones, and thus, can induce a much richer correlation structure compared to shallow networks. It is argued that this rich inductive bias is crucial for empirical success.
 
 The paper is technically solid. The reviewers note this, and also make a few suggestions on how to make the paper more accessible. The authors have taken this into account. In order to bridge the gap between theory and practice, it is essential for theory papers to be accessible.
 
 The paper covers related work pretty well. One aspect is misses is the recent geometric analysis of deep learning. Can the algebraic analysis be connected to geometric analysis of deep learning, e.g. in the following paper?

---

This paper investigates the fact why deep networks perform well in practice and how modifying the geometry of pooling can make the polynomially
sized deep network to provide a function with exponentially high separation rank (for certain partitioning.)

In the authors' previous works, they showed the superiority of deep networks over shallows when the activation function is ReLu and the pooling is max/mean pooling but in the current paper there is no activation function after conv and the pooling is just a multiplication of the node values. Although for the experimental results they've considered both scenarios. 

Actually, the general reasoning for this problem is hard, therefore, this drawback is not significant and the current contribution adds a reasonable amount of knowledge to the literature. 

This paper studies the convolutional arithmetic circuits and shows how this model can address the inductive biases and how pooling can adjust these biases. 

This interesting contribution gives an intuition about how deep network can capture the correlation between the input variables when its size is polynomial but and correlation is exponential.

It worth to note that although the authors tried to express their notation and definitions carefully where they were very successful, it would be helpful if they elaborate a bit more on their definitions, expressions, and conclusions in the sense to make them more accessible.

---

The paper provides a highly complex algebraic machinery to analyze the type of functions covered by convolutional network. As in most attempts  in this direction in the literature, the ideal networks described in paper, which have to be interpretable as polynomials over tensors, do not match the type of CNNs used in practice: for instance the Relu non-linearity is replaced with a product of linear functions (or a sum of logs).

While the paper is very technical to read, every concept is clearly stated and mathematical terminology properly introduced. Still, I think some the authors could make some effort to make the key concepts more accessible, and give a more intuitive understanding of what the separation rank means rather before piling up different mathematical interpretation.
My SVM-era algebra is quite rusted, and I am not familiar with the separation rank framework: it would have been much easier for me to first fully understand a simple and gentle case (shallow network in section 5.3), than the general deep case.

To summarize my understanding of the key theorem 1 result:
- The upper bound of the separation rank is used to show that in the shallow case, this rank grows AT MOST linearly with the network size (as measured by the only hidden layer). So exponential network sizes are caused by this rank needing to grow exponentially, as required by the partition.
- In the deep case, one also uses the case that the upper bound is linear in the size of the network (as measured by the last hidden layer), however, this situation is caused by the selection of a partition (I^low, J^high), and the maximal rank induced by this partition is only linear anyway, hence the network size can remain linear.

If tried my best to summarize the key point of this paper and still probably failed at it, which shows how complex is this notion of partition rank, and that its linear growth with network size can either be a good or bad thing depending on the setting. Hopefully, someone will come one day with an explanation that holds in a single slide.



While this is worth publishing as conference paper in its present form, I have two suggestions that, IMHO, would make this work more significant:

On the theory side, we are still very far from the completeness of the PAC bound papers of the ""shallow era"". In particular, the non-probabilistic lower and upper bound in theorem 1 are probably loose, and there is no PAC-like theory to tell us which one to use and what is the predicted impact on performance (not just the intuition). Also, in the prediction of the inductive bias, the other half is missing. This paper attempts to predict the maximal representation capacity of a DNN under bounded network size constraints, but one of the reason why this size has to be bounded is overfitting (justified by PAC or VC-dim like bounds). If we consider the expected risk as  the sum of the empirical risk and the structural risk, this paper only seems to address fully the empirical risk minimization part, freezing the structural risk. 

On the practice side, an issue is that experiments in this paper mostly confirm what is obvious through intuition, or some simpler form of reasonings. For instance to use convolutions that join pixels which are symmetrical in images to detect symmetry. Basic hand-crafted pattern detectors, as they have been used in computer vision for decades, would just do the job. What would be a great motivation for using this framework is if it answered questions that simple human intuition cannot, and for which we are still in the dark: one example I could think of in the recent use of gated convolutions 'a trous' for 1D speech signal, popularized in Google WaveNet (

---

This paper addresses the question of which functions are well suited to deep networks, as opposed to shallow networks.  The basic intuition is convincing and fairly straightforward.  Pooling operations bring together information.  When information is correlated, it can be more efficiently used if the geometry of pooling regions matches the correlations so that it can be brought together more efficiently.  Shallow networks without layers of localized pooling lack this mechanism to combine correlated information efficiently.

The theoretical results are focused on convolutional arithmetic circuits, building on prior theoretical results of the authors.  The results make use of the interesting technical notion of separability, which in some sense measures the degree to which a function can be represented as the composition of independent functions.  Because separability is measured relative to a partition of the input, it is an appropriate mechanism for measuring the complexity of functions relative to a particular geometry of pooling operations.  Many of the technical notions are pretty intuitive, although the tensor analysis is pretty terse and not easy to follow without knowledge of the authors’ prior work.

In some sense the comparison between deep and shallow networks is somewhat misleading, since the shallow networks lack a hierarchical pooling structure.  For example, a shallow convolutional network with RELU and max pooling does not really make sense, since the max occurs over the whole image.  So it seems that the paper is really more of an analysis of the effect of pooling vs. not having pooling.  For example, it is not clear that a deep CNN without pooling would be any more efficient than a shallow network, from this work.

It is not clear how much the theoretical results depend on the use of a model with product pooling, and how they might be extended to the more common max pooling.  Even if theoretical results are difficult to derive in this case, simple illustrative examples might be helpful.  In fact, if the authors prepare a longer version of the paper for a journal I think the results could be made more intuitive if they could add a simple toy example of a function that can be efficiently represented with a convolutional arithmetic circuit when the pooling structure fits the correlations, and perhaps showing also how this could be represented with a convolutional network with RELU and max pooling.

I would also appreciate a more explicit discussion of how the depth of a deep network affects the separability of functions that can be represented.  A shallow network doesn’t have local pooling, so the difference between deep and shallow if perhaps mostly one of pooling vs. not pooling.  However, practitioners find that very deep networks seem to be more effective than “deep” networks with only a few convolutional layers and pooling.  The paper does not explicitly discuss whether their results provide insight into this behavior.

Overall, I think that the paper attacks an important problem in an interesting way.  It is not so convincing that this really gets to the heart of why depth is so important, because of the theoretical limitation to arithmetic circuits, and because the comparison is to shallow networks that are without localized pooling.

---

The paper provides a highly complex algebraic machinery to analyze the type of functions covered by convolutional network. As in most attempts  in this direction in the literature, the ideal networks described in paper, which have to be interpretable as polynomials over tensors, do not match the type of CNNs used in practice: for instance the Relu non-linearity is replaced with a product of linear functions (or a sum of logs).

While the paper is very technical to read, every concept is clearly stated and mathematical terminology properly introduced. Still, I think some the authors could make some effort to make the key concepts more accessible, and give a more intuitive understanding of what the separation rank means rather before piling up different mathematical interpretation.
My SVM-era algebra is quite rusted, and I am not familiar with the separation rank framework: it would have been much easier for me to first fully understand a simple and gentle case (shallow network in section 5.3), than the general deep case.

To summarize my understanding of the key theorem 1 result:
- The upper bound of the separation rank is used to show that in the shallow case, this rank grows AT MOST linearly with the network size (as measured by the only hidden layer). So exponential network sizes are caused by this rank needing to grow exponentially, as required by the partition.
- In the deep case, one also uses the case that the upper bound is linear in the size of the network (as measured by the last hidden layer), however, this situation is caused by the selection of a partition (I^low, J^high), and the maximal rank induced by this partition is only linear anyway, hence the network size can remain linear.

If tried my best to summarize the key point of this paper and still probably failed at it, which shows how complex is this notion of partition rank, and that its linear growth with network size can either be a good or bad thing depending on the setting. Hopefully, someone will come one day with an explanation that holds in a single slide.



While this is worth publishing as conference paper in its present form, I have two suggestions that, IMHO, would make this work more significant:

On the theory side, we are still very far from the completeness of the PAC bound papers of the ""shallow era"". In particular, the non-probabilistic lower and upper bound in theorem 1 are probably loose, and there is no PAC-like theory to tell us which one to use and what is the predicted impact on performance (not just the intuition). Also, in the prediction of the inductive bias, the other half is missing. This paper attempts to predict the maximal representation capacity of a DNN under bounded network size constraints, but one of the reason why this size has to be bounded is overfitting (justified by PAC or VC-dim like bounds). If we consider the expected risk as  the sum of the empirical risk and the structural risk, this paper only seems to address fully the empirical risk minimization part, freezing the structural risk. 

On the practice side, an issue is that experiments in this paper mostly confirm what is obvious through intuition, or some simpler form of reasonings. For instance to use convolutions that join pixels which are symmetrical in images to detect symmetry. Basic hand-crafted pattern detectors, as they have been used in computer vision for decades, would just do the job. What would be a great motivation for using this framework is if it answered questions that simple human intuition cannot, and for which we are still in the dark: one example I could think of in the recent use of gated convolutions 'a trous' for 1D speech signal, popularized in Google WaveNet (

---

The paper uses the notion of separation rank from tensor algebra to analyze the correlations induced through convolution and pooling operations. They show that deep networks have exponentially larger separation ranks compared to shallow ones, and thus, can induce a much richer correlation structure compared to shallow networks. It is argued that this rich inductive bias is crucial for empirical success.
 
 The paper is technically solid. The reviewers note this, and also make a few suggestions on how to make the paper more accessible. The authors have taken this into account. In order to bridge the gap between theory and practice, it is essential for theory papers to be accessible.
 
 The paper covers related work pretty well. One aspect is misses is the recent geometric analysis of deep learning. Can the algebraic analysis be connected to geometric analysis of deep learning, e.g. in the following paper?

---

This paper investigates the fact why deep networks perform well in practice and how modifying the geometry of pooling can make the polynomially
sized deep network to provide a function with exponentially high separation rank (for certain partitioning.)

In the authors' previous works, they showed the superiority of deep networks over shallows when the activation function is ReLu and the pooling is max/mean pooling but in the current paper there is no activation function after conv and the pooling is just a multiplication of the node values. Although for the experimental results they've considered both scenarios. 

Actually, the general reasoning for this problem is hard, therefore, this drawback is not significant and the current contribution adds a reasonable amount of knowledge to the literature. 

This paper studies the convolutional arithmetic circuits and shows how this model can address the inductive biases and how pooling can adjust these biases. 

This interesting contribution gives an intuition about how deep network can capture the correlation between the input variables when its size is polynomial but and correlation is exponential.

It worth to note that although the authors tried to express their notation and definitions carefully where they were very successful, it would be helpful if they elaborate a bit more on their definitions, expressions, and conclusions in the sense to make them more accessible.

---

The paper provides a highly complex algebraic machinery to analyze the type of functions covered by convolutional network. As in most attempts  in this direction in the literature, the ideal networks described in paper, which have to be interpretable as polynomials over tensors, do not match the type of CNNs used in practice: for instance the Relu non-linearity is replaced with a product of linear functions (or a sum of logs).

While the paper is very technical to read, every concept is clearly stated and mathematical terminology properly introduced. Still, I think some the authors could make some effort to make the key concepts more accessible, and give a more intuitive understanding of what the separation rank means rather before piling up different mathematical interpretation.
My SVM-era algebra is quite rusted, and I am not familiar with the separation rank framework: it would have been much easier for me to first fully understand a simple and gentle case (shallow network in section 5.3), than the general deep case.

To summarize my understanding of the key theorem 1 result:
- The upper bound of the separation rank is used to show that in the shallow case, this rank grows AT MOST linearly with the network size (as measured by the only hidden layer). So exponential network sizes are caused by this rank needing to grow exponentially, as required by the partition.
- In the deep case, one also uses the case that the upper bound is linear in the size of the network (as measured by the last hidden layer), however, this situation is caused by the selection of a partition (I^low, J^high), and the maximal rank induced by this partition is only linear anyway, hence the network size can remain linear.

If tried my best to summarize the key point of this paper and still probably failed at it, which shows how complex is this notion of partition rank, and that its linear growth with network size can either be a good or bad thing depending on the setting. Hopefully, someone will come one day with an explanation that holds in a single slide.



While this is worth publishing as conference paper in its present form, I have two suggestions that, IMHO, would make this work more significant:

On the theory side, we are still very far from the completeness of the PAC bound papers of the ""shallow era"". In particular, the non-probabilistic lower and upper bound in theorem 1 are probably loose, and there is no PAC-like theory to tell us which one to use and what is the predicted impact on performance (not just the intuition). Also, in the prediction of the inductive bias, the other half is missing. This paper attempts to predict the maximal representation capacity of a DNN under bounded network size constraints, but one of the reason why this size has to be bounded is overfitting (justified by PAC or VC-dim like bounds). If we consider the expected risk as  the sum of the empirical risk and the structural risk, this paper only seems to address fully the empirical risk minimization part, freezing the structural risk. 

On the practice side, an issue is that experiments in this paper mostly confirm what is obvious through intuition, or some simpler form of reasonings. For instance to use convolutions that join pixels which are symmetrical in images to detect symmetry. Basic hand-crafted pattern detectors, as they have been used in computer vision for decades, would just do the job. What would be a great motivation for using this framework is if it answered questions that simple human intuition cannot, and for which we are still in the dark: one example I could think of in the recent use of gated convolutions 'a trous' for 1D speech signal, popularized in Google WaveNet (

---

This paper addresses the question of which functions are well suited to deep networks, as opposed to shallow networks.  The basic intuition is convincing and fairly straightforward.  Pooling operations bring together information.  When information is correlated, it can be more efficiently used if the geometry of pooling regions matches the correlations so that it can be brought together more efficiently.  Shallow networks without layers of localized pooling lack this mechanism to combine correlated information efficiently.

The theoretical results are focused on convolutional arithmetic circuits, building on prior theoretical results of the authors.  The results make use of the interesting technical notion of separability, which in some sense measures the degree to which a function can be represented as the composition of independent functions.  Because separability is measured relative to a partition of the input, it is an appropriate mechanism for measuring the complexity of functions relative to a particular geometry of pooling operations.  Many of the technical notions are pretty intuitive, although the tensor analysis is pretty terse and not easy to follow without knowledge of the authors’ prior work.

In some sense the comparison between deep and shallow networks is somewhat misleading, since the shallow networks lack a hierarchical pooling structure.  For example, a shallow convolutional network with RELU and max pooling does not really make sense, since the max occurs over the whole image.  So it seems that the paper is really more of an analysis of the effect of pooling vs. not having pooling.  For example, it is not clear that a deep CNN without pooling would be any more efficient than a shallow network, from this work.

It is not clear how much the theoretical results depend on the use of a model with product pooling, and how they might be extended to the more common max pooling.  Even if theoretical results are difficult to derive in this case, simple illustrative examples might be helpful.  In fact, if the authors prepare a longer version of the paper for a journal I think the results could be made more intuitive if they could add a simple toy example of a function that can be efficiently represented with a convolutional arithmetic circuit when the pooling structure fits the correlations, and perhaps showing also how this could be represented with a convolutional network with RELU and max pooling.

I would also appreciate a more explicit discussion of how the depth of a deep network affects the separability of functions that can be represented.  A shallow network doesn’t have local pooling, so the difference between deep and shallow if perhaps mostly one of pooling vs. not pooling.  However, practitioners find that very deep networks seem to be more effective than “deep” networks with only a few convolutional layers and pooling.  The paper does not explicitly discuss whether their results provide insight into this behavior.

Overall, I think that the paper attacks an important problem in an interesting way.  It is not so convincing that this really gets to the heart of why depth is so important, because of the theoretical limitation to arithmetic circuits, and because the comparison is to shallow networks that are without localized pooling.",,,,,,6.666666666666667,,,3.6666666666666665,,
496,"HIERARCHICAL MULTISCALE RECURRENT NEURAL NETWORKS
Authors: Junyoung Chung, Sungjin Ahn, Yoshua Bengio
Source file: 496.pdf

ABSTRACT
Learning both hierarchical and temporal representation has been among the longstanding challenges of recurrent neural networks. Multiscale recurrent neural networks have been considered as a promising approach to resolve this issue, yet there has been a lack of empirical evidence showing that this type of models can actually capture the temporal dependencies by discovering the latent hierarchical structure of the sequence. In this paper, we propose a novel multiscale approach, called the hierarchical multiscale recurrent neural network, that can capture the latent hierarchical structure in the sequence by encoding the temporal dependencies with different timescales using a novel update mechanism. We show some evidence that the proposed model can discover underlying hierarchical structure in the sequences without using explicit boundary information. We evaluate our proposed model on character-level language modelling and handwriting sequence generation.

1 INTRODUCTION
One of the key principles of learning in deep neural networks as well as in the human brain is to obtain a hierarchical representation with increasing levels of abstraction (Bengio, 2009; LeCun et al., 2015; Schmidhuber, 2015). A stack of representation layers, learned from the data in a way to optimize the target task, make deep neural networks entertain advantages such as generalization to unseen examples (Hoffman et al., 2013), sharing learned knowledge among multiple tasks, and discovering disentangling factors of variation (Kingma & Welling, 2013). The remarkable recent successes of the deep convolutional neural networks are particularly based on this ability to learn hierarchical representation for spatial data (Krizhevsky et al., 2012). For modelling temporal data, the recent resurgence of recurrent neural networks (RNN) has led to remarkable advances (Mikolov et al., 2010; Graves, 2013; Cho et al., 2014; Sutskever et al., 2014; Vinyals et al., 2015). However, unlike the spatial data, learning both hierarchical and temporal representation has been among the long-standing challenges of RNNs in spite of the fact that hierarchical multiscale structures naturally exist in many temporal data (Schmidhuber, 1991; Mozer, 1993; El Hihi & Bengio, 1995; Lin et al., 1996; Koutník et al., 2014).
A promising approach to model such hierarchical and temporal representation is the multiscale RNNs (Schmidhuber, 1992; El Hihi & Bengio, 1995; Koutník et al., 2014). Based on the observation that high-level abstraction changes slowly with temporal coherency while low-level abstraction has quickly changing features sensitive to the precise local timing (El Hihi & Bengio, 1995), the multiscale RNNs group hidden units into multiple modules of different timescales. In addition to the fact that the architecture fits naturally to the latent hierarchical structures in many temporal data, the multiscale approach provides the following advantages that resolve some inherent problems of standard RNNs: (a) computational efficiency obtained by updating the high-level layers less frequently, (b) efficiently delivering long-term dependencies with fewer updates at the high-level layers, which mitigates the vanishing gradient problem, (c) flexible resource allocation (e.g., more hidden units to the higher layers that focus on modelling long-term dependencies and less hidden units to the lower layers which are in charge of learning short-term dependencies). In addition, the learned latent hierarchical structures can provide useful information to other downstream tasks such ∗Yoshua Bengio is CIFAR Senior Fellow.
as module structures in computer program learning, sub-task structures in hierarchical reinforcement learning, and story segments in video understanding.
There have been various approaches to implementing the multiscale RNNs. The most popular approach is to set the timescales as hyperparameters (El Hihi & Bengio, 1995; Koutník et al., 2014; Bahdanau et al., 2016) instead of treating them as dynamic variables that can be learned from the data (Schmidhuber, 1991; 1992; Chung et al., 2015; 2016). However, considering the fact that non-stationarity is prevalent in temporal data, and that many entities of abstraction such as words and sentences are in variable length, we claim that it is important for an RNN to dynamically adapt its timescales to the particulars of the input entities of various length. While this is trivial if the hierarchical boundary structure is provided (Sordoni et al., 2015), it has been a challenge for an RNN to discover the latent hierarchical structure in temporal data without explicit boundary information.
In this paper, we propose a novel multiscale RNN model, which can learn the hierarchical multiscale structure from temporal data without explicit boundary information. This model, called a hierarchical multiscale recurrent neural network (HM-RNN), does not assign fixed update rates, but adaptively determines proper update times corresponding to different abstraction levels of the layers. We find that this model tends to learn fine timescales for low-level layers and coarse timescales for high-level layers. To do this, we introduce a binary boundary detector at each layer. The boundary detector is turned on only at the time steps where a segment of the corresponding abstraction level is completely processed. Otherwise, i.e., during the within segment processing, it stays turned off. Using the hierarchical boundary states, we implement three operations, UPDATE, COPY and FLUSH, and choose one of them at each time step. The UPDATE operation is similar to the usual update rule of the long short-term memory (LSTM) (Hochreiter & Schmidhuber, 1997), except that it is executed sparsely according to the detected boundaries. The COPY operation simply copies the cell and hidden states of the previous time step. Unlike the leaky integration of the LSTM or the Gated Recurrent Unit (GRU) (Cho et al., 2014), the COPY operation retains the whole states without any loss of information. The FLUSH operation is executed when a boundary is detected, where it first ejects the summarized representation of the current segment to the upper layer and then reinitializes the states to start processing the next segment. Learning to select a proper operation at each time step and to detect the boundaries, the HM-RNN discovers the latent hierarchical structure of the sequences. We find that the straight-through estimator (Hinton, 2012; Bengio et al., 2013; Courbariaux et al., 2016) is efficient for training this model containing discrete variables.
We evaluate our model on two tasks: character-level language modelling and handwriting sequence generation. For the character-level language modelling, the HM-RNN achieves the state-of-the-art results on the Text8 dataset, and comparable results to the state-of-the-art on the Penn Treebank and Hutter Prize Wikipedia datasets. The HM-RNN also outperforms the standard RNN on the handwriting sequence generation using the IAM-OnDB dataset. In addition, we demonstrate that the hierarchical structure found by the HM-RNN is indeed very similar to the intrinsic structure observed in the data. The contributions of this paper are:
• We propose for the first time an RNN model that can learn a latent hierarchical structure of a sequence without using explicit boundary information. • We show that it is beneficial to utilize the above structure through empirical evaluation. • We show that the straight-through estimator is an efficient way of training a model containing
discrete variables. • We propose the slope annealing trick to improve the training procedure based on the
straight-through estimator.

2 RELATED WORK
Two notable early attempts inspiring our model are Schmidhuber (1992) and El Hihi & Bengio (1995). In these works, it is advocated to stack multiple layers of RNNs in a decreasing order of update frequency for computational and learning efficiency. In Schmidhuber (1992), the author shows a model that can self-organize a hierarchical multiscale structure. Particularly in El Hihi & Bengio (1995), the advantages of incorporating a priori knowledge, “temporal dependencies are structured hierarchically"", into the RNN architecture is studied. The authors propose an RNN architecture that updates each layer with a fixed but different rate, called a hierarchical RNN.
LSTMs (Hochreiter & Schmidhuber, 1997) employ the multiscale update concept, where the hidden units have different forget and update rates and thus can operate with different timescales. However, unlike our model, these timescales are not organized hierarchically. Although the LSTM has a selfloop for the gradients that helps to capture the long-term dependencies by mitigating the vanishing gradient problem, in practice, it is still limited to a few hundred time steps due to the leaky integration by which the contents to memorize for a long-term is gradually diluted at every time step. Also, the model remains computationally expensive because it has to perform the update at every time step for each unit. However, our model is less prone to these problems because it learns a hierarchical structure such that, by design, high-level layers learn to perform less frequent updates than low-level layers. We hypothesize that this property mitigates the vanishing gradient problem more efficiently while also being computationally more efficient.
A more recent model, the clockwork RNN (CW-RNN) (Koutník et al., 2014) extends the hierarchical RNN (El Hihi & Bengio, 1995) and the NARX RNN (Lin et al., 1996)1. The CW-RNN tries to solve the issue of using soft timescales in the LSTM, by explicitly assigning hard timescales. In the CW-RNN, hidden units are partitioned into several modules, and different timescales are assigned to the modules such that a module i updates its hidden units at every 2(i−1)-th time step. The CW-RNN is computationally more efficient than the standard RNN including the LSTM since hidden units are updated only at the assigned clock rates. However, finding proper timescales in the CW-RNN remains as a challenge whereas our model learns the intrinsic timescales from the data. In the biscale RNNs (Chung et al., 2016), the authors proposed to model layer-wise timescales adaptively by having additional gating units, however this approach still relies on the soft gating mechanism like LSTMs.
Other forms of Hierarchical RNN (HRNN) architectures have been proposed in the cases where the explicit hierarchical boundary structure is provided. In Ling et al. (2015), after obtaining the word boundary via tokenization, the HRNN architecture is used for neural machine translation by modelling the characters and words using the first and second RNN layers, respectively. A similar HRNN architecture is also adopted in Sordoni et al. (2015) to model dialogue utterances. However, in many cases, hierarchical boundary information is not explicitly observed or expensive to obtain. Also, it is unclear how to deploy more layers than the number of boundary levels that is explicitly observed in the data.
While the above models focus on online prediction problems, where a prediction needs to be made by using only the past data, in some cases, predictions are made after observing the whole sequence. In this setting, the input sequence can be regarded as 1-D spatial data, convolutional neural networks with 1-D kernels are proposed in Kim (2014) and Kim et al. (2015) for language modelling and sentence classification. Also, in Chan et al. (2016) and Bahdanau et al. (2016), the authors proposed to obtain high-level representation of the sequences of reduced length by repeatedly merging or pooling the lower-level representation of the sequences.
Hierarchical RNN architectures have also been used to discover the segmentation structure in sequences (Fernández et al., 2007; Kong et al., 2015). It is however different to our model in the sense that they optimize the objective with explicit labels on the hierarchical segments while our model discovers the intrinsic structure only from the sequences without segment label information.
The COPY operation used in our model can be related to Zoneout (Krueger et al., 2016) which is a recurrent generalization of stochastic depth (Huang et al., 2016). In Zoneout, an identity transformation is randomly applied to each hidden unit at each time step according to a Bernoulli distribution. This results in occasional copy operations of the previous hidden states. While the focus of Zoneout is to propose a regularization technique similar to dropout (Srivastava et al., 2014) (where the regularization strength is controlled by a hyperparameter), our model learns (a) to dynamically determine when to copy from the context inputs and (b) to discover the hierarchical multiscale structure and representation. Although the main goal of our proposed model is not regularization, we found that our model also shows very good generalization performance.

3 HIERARCHICAL MULTISCALE RECURRENT NEURAL NETWORKS

3.1 MOTIVATION
To begin with, we provide an example of how a stacked RNN can model temporal data in an ideal setting, i.e., when the hierarchy of segments is provided (Sordoni et al., 2015; Ling et al., 2015). In Figure 1 (a), we depict a hierarchical RNN (HRNN) for language modelling with two layers: the first layer receives characters as inputs and generates word-level representations (C2W-RNN), and the second layer takes the word-level representations as inputs and yields phrase-level representations (W2P-RNN).
As shown, by means of the provided end-of-word labels, the C2W-RNN obtains word-level representation after processing the last character of each word and passes the word-level representation to the W2P-RNN. Then, the W2P-RNN performs an update of the phrase-level representation. Note that the hidden states of the W2P-RNN remains unchanged while all the characters of a word are processed by the C2W-RNN. When the C2W-RNN starts to process the next word, its hidden states are reinitialized using the latest hidden states of the W2P-RNN, which contain summarized representation of all the words that have been processed by that time step, in that phrase.
From this simple example, we can see the advantages of having a hierarchical multiscale structure: (1) as the W2P-RNN is updated at a much slower update rate than the C2W-RNN, a considerable amount of computation can be saved, (2) gradients are backpropagated through a much smaller number of time steps, and (3) layer-wise capacity control becomes possible (e.g., use a smaller number of hidden units in the first layer which models short-term dependencies but whose updates are invoked much more often).
Can an RNN discover such hierarchical multiscale structure without explicit hierarchical boundary information? Considering the fact that the boundary information is difficult to obtain (for example, consider languages where words are not always cleanly separated by spaces or punctuation symbols, and imperfect rules are used to separately perform segmentation) or usually not provided at all, this is a legitimate problem. It gets worse when we consider higher-level concepts which we would like the RNN to discover autonomously. In Section 2, we discussed the limitations of the existing RNN models under this setting, which either have to update all units at every time step or use fixed update frequencies (El Hihi & Bengio, 1995; Koutník et al., 2014). Unfortunately, this kind of approach is not well suited to the case where different segments in the hierarchical decomposition have different lengths: for example, different words have different lengths, so a fixed hierarchy would not update its upper-level units in synchrony with the natural boundaries in the data.

3.2 THE PROPOSED MODEL
A key element of our model is the introduction of a parametrized boundary detector, which outputs a binary value, in each layer of a stacked RNN, and learns when a segment should end in such a way to optimize the overall target objective. Whenever the boundary detector is turned on at a time step of layer ` (i.e., when the boundary state is 1), the model considers this to be the end of a
1The acronym NARX stands for Non-linear Auto-Regressive model with eXogenous inputs.
segment corresponding to the latent abstraction level of that layer (e.g., word or phrase) and feeds the summarized representation of the detected segment into the upper layer (`+ 1). Using the boundary states, at each time step, each layer selects one of the following operations: UPDATE, COPY or FLUSH. The selection is determined by (1) the boundary state of the current time step in the layer below z`−1t and (2) the boundary state of the previous time step in the same layer z ` t−1.
In the following, we describe an HM-RNN based on the LSTM update rule. We call this model a hierarchical multiscale LSTM (HM-LSTM). Consider an HM-LSTM model of L layers (` = 1, . . . , L) which, at each layer `, performs the following update at time step t:
h`t, c ` t, z ` t = f ` HM-LSTM(c ` t−1,h ` t−1,h `−1 t ,h `+1 t−1, z ` t−1, z `−1 t ). (1)
Here, h and c denote the hidden and cell states, respectively. The function f `HM-LSTM is implemented as follows. First, using the two boundary states z`t−1 and z `−1 t , the cell state is updated by:
c`t =  f `t c`t−1 + i`t g`t if z`t−1 = 0 and z`−1t = 1 (UPDATE) c`t−1 if z ` t−1 = 0 and z `−1 t = 0 (COPY)
i`t g`t if z`t−1 = 1 (FLUSH), (2)
and then the hidden state is obtained by:
h`t = { h`t−1 if COPY, o`t tanh(c`t) otherwise.
(3)
Here, (f , i,o) are forget, input, output gates, and g is a cell proposal vector. Note that unlike the LSTM, it is not necessary to compute these gates and cell proposal values at every time step. For example, in the case of the COPY operation, we do not need to compute any of these values and thus can save computations.
The COPY operation, which simply performs (c`t,h ` t)← (c`t−1,h`t−1), implements the observation that an upper layer should keep its state unchanged until it receives the summarized input from the lower layer. The UPDATE operation is performed to update the summary representation of the layer ` if the boundary z`−1t is detected from the layer below but the boundary z ` t−1 was not found at the previous time step. Hence, the UPDATE operation is executed sparsely unlike the standard RNNs where it is executed at every time step, making it computationally inefficient. If a boundary is detected, the FLUSH operation is executed. The FLUSH operation consists of two sub-operations: (a) EJECT to pass the current state to the upper layer and then (b) RESET to reinitialize the state before starting to read a new segment. This operation implicitly forces the upper layer to absorb the summary information of the lower layer segment, because otherwise it will be lost. Note that the FLUSH operation is a hard reset in the sense that it completely erases all the previous states of the same layer, which is different from the soft reset or soft forget operation in the GRU or LSTM.
Whenever needed (depending on the chosen operation), the gate values (f `t , i ` t,o ` t), the cell proposal g`t , and the pre-activation of the boundary detector z̃ ` t
2 are then obtained by: f `t i`t o`t g`t z̃`t  =  sigm sigm sigm tanh hard sigm  fslice (srecurrent(`)t + stop-down(`)t + sbottom-up(`)t + b(`)) , (4) where
s recurrent(`) t = U ` `h ` t−1, (5)
s top-down(`) t = z ` t−1U ` `+1h `+1 t−1, (6)
s bottom-up(`) t = z `−1 t W ` `−1h `−1 t . (7)
Here, we useW ji ∈ R(4dim(h `)+1)×dim(h`−1), U ji ∈ R(4dim(h `)+1)×dim(h`) to denote state transition parameters from layer i to layer j, and b ∈ R4dim(h`)+1 is a bias term. In the last layer L, the
2z̃`t can also be implemented as a function of h`t , e.g., z̃`t = hard sigm(Uh`t).
top-down connection is ignored, and we use h0t = xt. Since the input should not be omitted, we set z0t = 1 for all t. Also, we do not use the boundary detector for the last layer. The hard sigm is defined by hard sigm(x) = max ( 0,min ( 1, ax+1
2
)) with a being the slope variable.
Unlike the standard LSTM, the HM-LSTM has a top-down connection from (`+ 1) to `, which is allowed to be activated only if a boundary is detected at the previous time step of the layer ` (see Eq. 6). This makes the layer ` to be initialized with more long-term information after the boundary is detected and execute the FLUSH operation. In addition, the input from the lower layer (` − 1) becomes effective only when a boundary is detected at the current time step in the layer (`− 1) due to the binary gate z`−1t . Figure 2 (left) shows the gating mechanism of the HM-LSTM at time step t.
Finally, the binary boundary state z`t is obtained by:
z`t = fbound(z̃ ` t ). (8)
For the binarization function fbound : R→ {0, 1}, we can either use a deterministic step function:
z`t = { 1 if z̃`t > 0.5 0 otherwise,
(9)
or sample from a Bernoulli distribution z`t ∼ Bernoulli(z̃`t ). Although this binary decision is a key to our model, it is usually difficult to use stochastic gradient descent to train such model with discrete decisions as it is not differentiable.

3.3 COMPUTING GRADIENT OF BOUNDARY DETECTOR
Training neural networks with discrete variables requires more efforts since the standard backpropagation is no longer applicable due to the non-differentiability. Among a few methods for training a neural network with discrete variables such as the REINFORCE (Williams, 1992; Mnih & Gregor, 2014) and the straight-through estimator (Hinton, 2012; Bengio et al., 2013), we use the straightthrough estimator to train our model. The straight-through estimator is a biased estimator because the non-differentiable function used in the forward pass (i.e., the step function in our case) is replaced by a differentiable function during the backward pass (i.e., the hard sigmoid function in our case). The straight-through estimator, however, is much simpler and often works more efficiently in practice than other unbiased but high-variance estimators such as the REINFORCE. The straight-through estimator has also been used in Courbariaux et al. (2016) and Vezhnevets et al. (2016).
The Slope Annealing Trick. In our experiment, we use the slope annealing trick to reduce the bias of the straight-through estimator. The idea is to reduce the discrepancy between the two functions used during the forward pass and the backward pass. That is, by gradually increasing the slope a of the hard sigmoid function, we make the hard sigmoid be close to the step function. Note that starting with a high slope value from the beginning can make the training difficult while it is more applicable later when the model parameters become more stable. In our experiments, starting from slope a = 1, we slowly increase the slope until it reaches a threshold with an appropriate scheduling.

4 EXPERIMENTS
We evaluate the proposed model on two tasks, character-level language modelling and handwriting sequence generation. Character-level language modelling is a representative example of discrete
sequence modelling, where the discrete symbols form a distinct hierarchical multiscale structure. The performance on real-valued sequences is tested on the handwriting sequence generation in which a relatively clear hierarchical multiscale structure exists compared to other data such as speech signals.

4.1 CHARACTER-LEVEL LANGUAGE MODELLING
A sequence modelling task aims at learning the probability distribution over sequences by minimizing the negative log-likelihood of the training sequences:
min θ − 1 N N∑ n=1 Tn∑ t=1 log p (xnt | xn<t; θ) , (10)
where θ is the model parameter, N is the number of training sequences, and Tn is the length of the n-th sequence. A symbol at time t of sequence n is denoted by xnt , and x n <t denotes all previous symbols at time t. We evaluate our model on three benchmark text corpora: (1) Penn Treebank, (2) Text8 and (3) Hutter Prize Wikipedia. We use the bits-per-character (BPC), E[− log2 p(xt+1 | x≤t)], as the evaluation metric.
Model We use a model consisting of an input embedding layer, an RNN module and an output module. The input embedding layer maps each input symbol into 128-dimensional continuous vector without using any non-linearity. The RNN module is the HM-LSTM, described in Section 3, with three layers. The output module is a feedforward neural network with two layers, an output embedding layer and a softmax layer. Figure 2 (right) shows a diagram of the output module. At each time step, the output embedding layer receives the hidden states of the three RNN layers as input. In order to adaptively control the importance of each layer at each time step, we also introduce three scalar gating units g`t ∈ R to each of the layer outputs:
g`t = sigm(w `[h1t ; · · · ;hLt ]), (11)
where w` ∈ R ∑L
`=1 dim(h `) is the weight parameter. The output embedding het is computed by:
het = ReLU ( L∑ `=1 g`tW e `h ` t ) , (12)
where L = 3 and ReLU(x) = max(0, x) (Nair & Hinton, 2010). Finally, the probability distribution for the next target character is computed by the softmax function, softmax(xj) = e
xj∑K k=1 exk , where
each output class is a character.
Penn Treebank We process the Penn Treebank dataset (Marcus et al., 1993) by following the procedure introduced in Mikolov et al. (2012). Each update is done by using a mini-batch of 64 examples of length 100 to prevent the memory overflow problem when unfolding the RNN in time for backpropagation. The last hidden state of a sequence is used to initialize the hidden state of the next sequence to approximate the full backpropagation. We train the model using Adam (Kingma & Ba, 2014) with an initial learning rate of 0.002. We divide the learning rate by a factor of 50 when the validation negative log-likelihood stopped decreasing. The norm of the gradient is clipped with a threshold of 1 (Mikolov et al., 2010; Pascanu et al., 2012). We also apply layer normalization (Ba et al., 2016) to our models. For all of the character-level language modelling experiments, we apply the same procedure, but only change the number of hidden units, mini-batch size and the initial learning rate.
For the Penn Treebank dataset, we use 512 units in each layer of the HM-LSTM and for the output embedding layer. In Table 1 (left), we compare the test BPCs of four variants of our model to other baseline models. Note that the HM-LSTM using the step function for the hard boundary decision outperforms the others using either sampling or soft boundary decision (i.e., hard sigmoid). The test BPC is further improved with the slope annealing trick, which reduces the bias of the straight-through estimator. We increased the slope a with the following schedule a = min (5, 1 + 0.04 ·Nepoch), where Nepoch is the maximum number of epochs. The HM-LSTM achieves test BPC score of 1.24. For the remaining tasks, we fixed the hard boundary decision using the step function without slope annealing due to the difficulty of finding a good annealing schedule on large-scale datasets.
Text8 The Text8 dataset (Mahoney, 2009) consists of 100M characters extracted from the Wikipedia corpus. Text8 contains only alphabets and spaces, and thus we have total 27 symbols. In order to compare with other previous works, we follow the data splits used in Mikolov et al. (2012). We use 1024 units for each HM-LSTM layer and 2048 units for the output embedding layer. The mini-batch size and the initial learning rate are set to 128 and 0.001, respectively. The results are shown in Table 2. The HM-LSTM obtains the state-of-the-art test BPC 1.29.
Hutter Prize Wikipedia The Hutter Prize Wikipedia (enwik8) dataset (Hutter, 2012) contains 205 symbols including XML markups and special characters. We follow the data splits used in Graves (2013) where the first 90M characters are used to train the model, the next 5M characters for validation, and the remainders for the test set. We use the same model size, mini-batch size and the initial learning rate as in the Text8. In Table 1 (right), we show the HM-LSTM achieving the test BPC 1.32, which is a tie with the state-of-the-art result among the neural models. Although the neural models, show remarkable performances, their compression performance is still behind the best models such as PAQ8hp12 (Mahoney, 2005) and decomp8 (Mahoney, 2009).
Visualizing Learned Hierarchical Multiscale Structure In Figure 3 and 4, we visualize the boundaries detected by the boundary detectors of the HM-LSTM while reading a character sequence of total length 270 taken from the validation set of either the Penn Treebank or Hutter Prize Wikipedia dataset. Due to the page width limit, the figure contains the sequence partitioned into three segments of length 90. The white blocks indicate boundaries z`t = 1 while the black blocks indicate the non-boundaries z`t = 0.
Interestingly in both figures, we can observe that the boundary detector of the first layer, z1, tends to be turned on when it sees a space or after it sees a space, which is a reasonable breakpoint to separate between words. This is somewhat surprising because the model self-organizes this structure
without any explicit boundary information. In Figure 3, we observe that the z1 tends to detect the boundaries of the words but also fires within the words, where the z2 tends to fire when it sees either an end of a word or 2, 3-grams. In Figure 4, we also see flushing in the middle of a word, e.g., “tele-FLUSH-phone”. Note that “tele” is a prefix after which a various number of postfixes can follow. From these, it seems that the model uses to some extent the concept of surprise to learn the boundary. Although interpretation of the second layer boundaries is not as apparent as the first layer boundaries, it seems to segment at reasonable semantic / syntactic boundaries, e.g., “consumers may” - “want to move their telephones a” - “little closer to the tv set <unk>”, and so on.
Another remarkable point is the fact that we do not pose any constraint on the number of boundaries that the model can fire up. The model, however, learns that it is more beneficial to delay the information ejection to some extent. This is somewhat counterintuitive because it might look more beneficial to feed the fresh update to the upper layers at every time step without any delay. We conjecture the reason that the model works in this way is due to the FLUSH operation that poses an implicit constraint on the frequency of boundary detection, because it contains both a reward (feeding fresh information to upper layers) and a penalty (erasing accumulated information). The model finds an optimal balance between the reward and the penalty.
To understand the update mechanism more intuitively, in Figure 4, we also depict the heatmap of the `2-norm of the hidden states along with the states of the boundary detectors. As we expect, we can see that there is no change in the norm value within segments due to the COPY operation. Also, the color of ‖h1‖ changes quickly (at every time step) because there is no COPY operation in the first layer. The color of ‖h2‖ changes less frequently based on the states of z1t and z2t−1. The color of ‖h3‖ changes even slowly, i.e., only when z2t = 1. A notable advantage of the proposed architecture is that the internal process of the RNN becomes more interpretable. For example, we can substitute the states of z1t and z 2 t−1 into Eq. 2 and infer which operation among the UPDATE, COPY and FLUSH was applied to the second layer at time step t. We can also inspect the update frequencies of the layers simply by counting how many UPDATE and FLUSH operations were made in each layer. For example in Figure 4, we see that the first layer updates at every time step (which is 270 UPDATE operations), the second layer updates 56 times,
and only 9 updates has made in the third layer. Note that, by design, the first layer performs UPDATE operation at every time step and then the number of UPDATE operations decreases as the layer level increases. In this example, the total number of updates is 335 for the HM-LSTM which is 60% of reduction from the 810 updates of the standard RNN architecture.

4.2 HANDWRITING SEQUENCE GENERATION
We extend the evaluation of the HM-LSTM to a real-valued sequence modelling task using IAMOnDB (Liwicki & Bunke, 2005) dataset. The IAM-OnDB dataset consists of 12, 179 handwriting examples, each of which is a sequence of (x, y) coordinate and a binary indicator p for pen-tip location, giving us (x1:Tn , y1:Tn , p1:Tn), where n is an index of a sequence. At each time step, the model receives (xt, yt, pt), and the goal is to predict (xt+1, yt+1, pt+1). The pen-up (pt = 1) indicates an end of a stroke, and the pen-down (pt = 0) indicates that a stroke is in progress. There is usually a large shift in the (x, y) coordinate to start a new stroke after the pen-up happens. We remove all sequences whose length is shorter than 300. This leaves us 10, 465 sequences for training, 581 for validation, 582 for test. The average length of the sequences is 648. We normalize the range of the (x, y) coordinates separately with the mean and standard deviation obtained from the training set. We use the mini-batch size of 32, and the initial learning rate is set to 0.0003.
We use the same model architecture as used in the character-level language model, except that the output layer is modified to predict real-valued outputs. We use the mixture density network as the output layer following Graves (2013), and use 400 units for each HM-LSTM layer and for the output embedding layer. In Table 3, we compare the log-likelihood averaged over the test sequences of the IAM-OnDB dataset. We observe that the HM-LSTM outperforms the standard LSTM. The slope annealing trick further improves the test log-likelihood of the HM-LSTM into 1167 in our setting. In this experiment, we increased the slope a with the following schedule a = min (3, 1 + 0.004 ·Nepoch). In Figure 5, we let the HM-LSTM to read a randomly picked validation sequence and present the visualization of handwriting examples by segments based on either the states of z2 or the states of pen-tip location3.

5 CONCLUSION
In this paper, we proposed the HM-RNN that can capture the latent hierarchical structure of the sequences. We introduced three types of operations to the RNN, which are the COPY, UPDATE and FLUSH operations. In order to implement these operations, we introduced a set of binary variables and a novel update rule that is dependent on the states of these binary variables. Each binary variable is learned to find segments at its level, therefore, we call this binary variable, a boundary detector. On the character-level language modelling, the HM-LSTM achieved state-of-the-art result on the Text8 dataset and comparable results to the state-of-the-art results on the Penn Treebank and Hutter Prize Wikipedia datasets. Also, the HM-LSTM outperformed the standard LSTM on the handwriting sequence generation. Our results and analysis suggest that the proposed HM-RNN can discover the latent hierarchical structure of the sequences and can learn efficient hierarchical multiscale representation that leads to better generalization performance.
3The plot function could be found at blog.otoro.net/2015/12/12/handwriting-generation-demo-in-tensorflow/.

ACKNOWLEDGMENTS
The authors would like to thank Alex Graves, Tom Schaul and Hado van Hasselt for their fruitful comments and discussion. We acknowledge the support of the following agencies for research funding and computing support: Ubisoft, Samsung, IBM, Facebook, Google, Microsoft, NSERC, Calcul Québec, Compute Canada, the Canada Research Chairs and CIFAR. The authors thank the developers of Theano (Team et al., 2016). JC would like to thank Arnaud Bergenon and Frédéric Bastien for their technical support. JC would also like to thank Guillaume Alain, Kyle Kastner and David Ha for providing us useful pieces of code.
","Learning both hierarchical and temporal representation has been among the longstanding challenges of recurrent neural networks. Multiscale recurrent neural networks have been considered as a promising approach to resolve this issue, yet there has been a lack of empirical evidence showing that this type of models can actually capture the temporal dependencies by discovering the latent hierarchical structure of the sequence. In this paper, we propose a novel multiscale approach, called the hierarchical multiscale recurrent neural network, that can capture the latent hierarchical structure in the sequence by encoding the temporal dependencies with different timescales using a novel update mechanism. We show some evidence that the proposed model can discover underlying hierarchical structure in the sequences without using explicit boundary information. We evaluate our proposed model on character-level language modelling and handwriting sequence generation.",ICLR 2017 conference submission,True,,"This paper proposes a novel variant of recurrent networks that is able to learn the hierarchy of information in sequential data (e.g., character->word). Their approach does not require boundary information to segment the sequence in meaningful groups (like in Chung et al., 2016).

Their model is organized as a set of layers that aim at capturing the information form different “level of abstraction”. The lowest level activate the upper one and decide when to update it based on a controller (or state cell, called c). A key feature of their model is that c is a discrete variable, allowing potentially fast inference time. However, this makes their model more challenging to learn, leading to the use of the straight-through estimator by Hinton, 2012. 

The experiment section is thorough and their model obtain competitive performance on several challenging tasks. The qualitative results show also that their model can capture natural boundaries.

Overall this paper presents a strong and novel model with promising experimental results.



On a minor note, I have few remarks/complaints about the writing and the related work:

- In the introduction:
“One of the key principles of learning in deep neural networks as well as in the human brain” : please provide evidence for the “human brain” part of this claim.
“For modelling temporal data, the recent resurgence of recurrent neural networks (RNN) has led to remarkable advances” I believe you re missing Mikolov et al. 2010 in the references.
“in spite of the fact that hierarchical multiscale structures naturally exist in many temporal data”: missing reference to Lin et al., 1996

- in the related work:
“A more recent model, the clockwork RNN (CW-RNN) (Koutník et al., 2014) extends the hierarchicalRNN (El Hihi & Bengio, 1995)” : It extends the NARX model of Lin et al. 1996, not the El Hihi & Bengio, 1995.
While the above models focus on online prediction problems, where a prediction needs to be made…”: I believe there is a lot of missing references, in particular to Socher’s work or older recursive networks.
“The norm of the gradient is clipped with a threshold of 1 (Pascanu et al., 2012)”: this is not the first work using gradient clipping. I believe it was introduced in Mikolov et al., 2010.

Missing references:
“Recurrent neural network based language model.”, Mikolov et al. 2010
“Learning long-term dependencies in NARX recurrent neural networks”, Lin et al. 1996
“Sequence labelling in structured domains with hierarchical recurrent neural networks“, Fernandez et al. 2007
“Learning sequential tasks by incrementally adding  higher  orders”, Ring, 1993

---

This extension to RNNs is clearly motivated, and the details of the proposed method are sensible. The paper would have benefitted from more experiments such as those in Figure 5 teasing out the representations learned by this model.

---

This paper proposes a new multiscale recurrent neural network, where each layer has different time scale, and the scale is not fixed but variable and determined by a neural network. The method is elegantly formulated within a recurrent neural network framework, and shows the state-of-the-art performance on several benchmarks. The paper is well written.

Question) Can you extend it to bidirectional RNN?

---

The paper proposes a modified RNN architecture with multiple layers, where higher layers are only passed lower layer states if a FLUSH operation is predicted, consisting of passing up the state and reseting the lower layer's state. In order to select one of three operations at each time step, the authors propose using the straight-through estimator with a slope-annealing trick during training. Empirical results and visualizations illustrate that the modified architecture performs well at boundary detection.

Pros:
- Paper is well-motivated, exceptionally well-composed
- Provides promising initial results on learning hierarchical representations through visualizations and thorough experiments on language modeling and handwriting generation
- The annealing trick with the straight-through estimator also seems potentially useful for other tasks containing discrete variables, and the trade-off in the flush operation is innovative.
Cons:
- In a couple cases the paper does not fully deliver. Empirical results on computational savings are not given, and hierarchy beyond a single level (where the data contains separators such as spaces and pen up/down) does not seem to be demonstrated.
- It's unclear whether better downstream performance is due to use of hierarchical information or due to the architecture changes acting as regularization, something which could hopefully be addressed.

---

This paper proposes a novel variant of recurrent networks that is able to learn the hierarchy of information in sequential data (e.g., character->word). Their approach does not require boundary information to segment the sequence in meaningful groups (like in Chung et al., 2016).

Their model is organized as a set of layers that aim at capturing the information form different “level of abstraction”. The lowest level activate the upper one and decide when to update it based on a controller (or state cell, called c). A key feature of their model is that c is a discrete variable, allowing potentially fast inference time. However, this makes their model more challenging to learn, leading to the use of the straight-through estimator by Hinton, 2012. 

The experiment section is thorough and their model obtain competitive performance on several challenging tasks. The qualitative results show also that their model can capture natural boundaries.

Overall this paper presents a strong and novel model with promising experimental results.



On a minor note, I have few remarks/complaints about the writing and the related work:

- In the introduction:
“One of the key principles of learning in deep neural networks as well as in the human brain” : please provide evidence for the “human brain” part of this claim.
“For modelling temporal data, the recent resurgence of recurrent neural networks (RNN) has led to remarkable advances” I believe you re missing Mikolov et al. 2010 in the references.
“in spite of the fact that hierarchical multiscale structures naturally exist in many temporal data”: missing reference to Lin et al., 1996

- in the related work:
“A more recent model, the clockwork RNN (CW-RNN) (Koutník et al., 2014) extends the hierarchicalRNN (El Hihi & Bengio, 1995)” : It extends the NARX model of Lin et al. 1996, not the El Hihi & Bengio, 1995.
While the above models focus on online prediction problems, where a prediction needs to be made…”: I believe there is a lot of missing references, in particular to Socher’s work or older recursive networks.
“The norm of the gradient is clipped with a threshold of 1 (Pascanu et al., 2012)”: this is not the first work using gradient clipping. I believe it was introduced in Mikolov et al., 2010.

Missing references:
“Recurrent neural network based language model.”, Mikolov et al. 2010
“Learning long-term dependencies in NARX recurrent neural networks”, Lin et al. 1996
“Sequence labelling in structured domains with hierarchical recurrent neural networks“, Fernandez et al. 2007
“Learning sequential tasks by incrementally adding  higher  orders”, Ring, 1993

---

This paper proposes a novel variant of recurrent networks that is able to learn the hierarchy of information in sequential data (e.g., character->word). Their approach does not require boundary information to segment the sequence in meaningful groups (like in Chung et al., 2016).

Their model is organized as a set of layers that aim at capturing the information form different “level of abstraction”. The lowest level activate the upper one and decide when to update it based on a controller (or state cell, called c). A key feature of their model is that c is a discrete variable, allowing potentially fast inference time. However, this makes their model more challenging to learn, leading to the use of the straight-through estimator by Hinton, 2012. 

The experiment section is thorough and their model obtain competitive performance on several challenging tasks. The qualitative results show also that their model can capture natural boundaries.

Overall this paper presents a strong and novel model with promising experimental results.



On a minor note, I have few remarks/complaints about the writing and the related work:

- In the introduction:
“One of the key principles of learning in deep neural networks as well as in the human brain” : please provide evidence for the “human brain” part of this claim.
“For modelling temporal data, the recent resurgence of recurrent neural networks (RNN) has led to remarkable advances” I believe you re missing Mikolov et al. 2010 in the references.
“in spite of the fact that hierarchical multiscale structures naturally exist in many temporal data”: missing reference to Lin et al., 1996

- in the related work:
“A more recent model, the clockwork RNN (CW-RNN) (Koutník et al., 2014) extends the hierarchicalRNN (El Hihi & Bengio, 1995)” : It extends the NARX model of Lin et al. 1996, not the El Hihi & Bengio, 1995.
While the above models focus on online prediction problems, where a prediction needs to be made…”: I believe there is a lot of missing references, in particular to Socher’s work or older recursive networks.
“The norm of the gradient is clipped with a threshold of 1 (Pascanu et al., 2012)”: this is not the first work using gradient clipping. I believe it was introduced in Mikolov et al., 2010.

Missing references:
“Recurrent neural network based language model.”, Mikolov et al. 2010
“Learning long-term dependencies in NARX recurrent neural networks”, Lin et al. 1996
“Sequence labelling in structured domains with hierarchical recurrent neural networks“, Fernandez et al. 2007
“Learning sequential tasks by incrementally adding  higher  orders”, Ring, 1993

---

This extension to RNNs is clearly motivated, and the details of the proposed method are sensible. The paper would have benefitted from more experiments such as those in Figure 5 teasing out the representations learned by this model.

---

This paper proposes a new multiscale recurrent neural network, where each layer has different time scale, and the scale is not fixed but variable and determined by a neural network. The method is elegantly formulated within a recurrent neural network framework, and shows the state-of-the-art performance on several benchmarks. The paper is well written.

Question) Can you extend it to bidirectional RNN?

---

The paper proposes a modified RNN architecture with multiple layers, where higher layers are only passed lower layer states if a FLUSH operation is predicted, consisting of passing up the state and reseting the lower layer's state. In order to select one of three operations at each time step, the authors propose using the straight-through estimator with a slope-annealing trick during training. Empirical results and visualizations illustrate that the modified architecture performs well at boundary detection.

Pros:
- Paper is well-motivated, exceptionally well-composed
- Provides promising initial results on learning hierarchical representations through visualizations and thorough experiments on language modeling and handwriting generation
- The annealing trick with the straight-through estimator also seems potentially useful for other tasks containing discrete variables, and the trade-off in the flush operation is innovative.
Cons:
- In a couple cases the paper does not fully deliver. Empirical results on computational savings are not given, and hierarchy beyond a single level (where the data contains separators such as spaces and pen up/down) does not seem to be demonstrated.
- It's unclear whether better downstream performance is due to use of hierarchical information or due to the architecture changes acting as regularization, something which could hopefully be addressed.

---

This paper proposes a novel variant of recurrent networks that is able to learn the hierarchy of information in sequential data (e.g., character->word). Their approach does not require boundary information to segment the sequence in meaningful groups (like in Chung et al., 2016).

Their model is organized as a set of layers that aim at capturing the information form different “level of abstraction”. The lowest level activate the upper one and decide when to update it based on a controller (or state cell, called c). A key feature of their model is that c is a discrete variable, allowing potentially fast inference time. However, this makes their model more challenging to learn, leading to the use of the straight-through estimator by Hinton, 2012. 

The experiment section is thorough and their model obtain competitive performance on several challenging tasks. The qualitative results show also that their model can capture natural boundaries.

Overall this paper presents a strong and novel model with promising experimental results.



On a minor note, I have few remarks/complaints about the writing and the related work:

- In the introduction:
“One of the key principles of learning in deep neural networks as well as in the human brain” : please provide evidence for the “human brain” part of this claim.
“For modelling temporal data, the recent resurgence of recurrent neural networks (RNN) has led to remarkable advances” I believe you re missing Mikolov et al. 2010 in the references.
“in spite of the fact that hierarchical multiscale structures naturally exist in many temporal data”: missing reference to Lin et al., 1996

- in the related work:
“A more recent model, the clockwork RNN (CW-RNN) (Koutník et al., 2014) extends the hierarchicalRNN (El Hihi & Bengio, 1995)” : It extends the NARX model of Lin et al. 1996, not the El Hihi & Bengio, 1995.
While the above models focus on online prediction problems, where a prediction needs to be made…”: I believe there is a lot of missing references, in particular to Socher’s work or older recursive networks.
“The norm of the gradient is clipped with a threshold of 1 (Pascanu et al., 2012)”: this is not the first work using gradient clipping. I believe it was introduced in Mikolov et al., 2010.

Missing references:
“Recurrent neural network based language model.”, Mikolov et al. 2010
“Learning long-term dependencies in NARX recurrent neural networks”, Lin et al. 1996
“Sequence labelling in structured domains with hierarchical recurrent neural networks“, Fernandez et al. 2007
“Learning sequential tasks by incrementally adding  higher  orders”, Ring, 1993",,,,,,7.666666666666667,,,3.6666666666666665,,
517,"LEARNING TO DISCOVER SPARSE GRAPHICAL MODELS
Authors: Eugene Belilovsky, Kyle Kastner
Source file: 517.pdf

ABSTRACT
We consider structure discovery of undirected graphical models from observational data. Inferring likely structures from few examples is a complex task often requiring the formulation of priors and sophisticated inference procedures. In the setting of Gaussian Graphical Models (GGMs) a popular estimator is a maximum likelihood objective with a penalization on the precision matrix. Adapting this estimator to capture domain-specific knowledge as priors or a new data likelihood requires great effort. In addition, structure recovery is an indirect consequence of the data-fit term. By contrast, it may be easier to generate training samples of data that arise from graphs with the desired structure properties. We propose here to leverage this latter source of information as training data to learn a function mapping from empirical covariance matrices to estimated graph structures. Learning this function brings two benefits: it implicitly models the desired structure or sparsity properties to form suitable priors, and it can be tailored to the specific problem of edge structure discovery, rather than maximizing data likelihood. We apply this framework to several real-world problems in structure discovery and show that it can be competitive to standard approaches such as graphical lasso, at a fraction of the execution speed. We use convolutional neural networks to parametrize our estimators due to the compositional structure of the problem. Experimentally, our learnable graph-discovery method trained on synthetic data generalizes well: identifying relevant edges in real data, completely unknown at training time. We find that on genetics, brain imaging, and simulation data we obtain competitive (and generally superior) performance, compared with analytical methods.

1 INTRODUCTION
Probabilistic graphical models provide a powerful framework for describing the dependencies between a set of variables. Many applications infer the structure of a probabilistic graphical model from data to elucidate the relationships between variables. These relationships are often represented by an undirected graphical model also known as a Markov Random Field (MRF). We focus on a common MRF model, Gaussian graphical models (GGMs). GGMs are used in structure-discovery settings for rich data such as neuroimaging, genetics, or finance (Friedman et al., 2008; Ryali et al, 2012; Mohan et al., 2012; Belilovsky et al., 2016). Although multivariate Gaussian distributions are well-behaved, determining likely structures from few examples is a complex task when the data is high dimensional. It requires strong priors, typically a sparsity assumption, or other restrictions on the structure of the graph, which now make the distribution difficult to express analytically and use.
A standard approach to estimating structure with GGMs in high dimensions is based on the classic result that the zeros of a precision matrix correspond to zero partial correlation, a necessary and sufficient condition for conditional independence (Lauritzen, 1996). Assuming only a few conditional dependencies corresponds to a sparsity constraint on the entries of the precision matrix, leading to a combinatorial problem. Many popular approaches to learning GGMs can be seen as leveraging the
`1-norm to create convex surrogates to this problem. Meinshausen & Bühlmann (2006) use nodewise `1 penalized regressions. Other estimators penalize the precision matrix directly (Cai et al., 2011; Friedman et al., 2008; Ravikumar et al., 2011). The most popular being the graphical lasso
fglasso(Σ̂) = arg min Θ 0 − log |Θ|+ Tr (Σ̂Θ) + λ‖Θ‖1, (1)
which can be seen as a penalized maximum-likelihood estimator. Here Θ and Σ̂ are the precision and sample covariance matrices, respectively. A large variety of alternative regularization penalties extend the priors of the graphical lasso (Danaher et al., 2014; Ryali et al, 2012; Varoquaux et al., 2010). However, several problems arise in this approach. Constructing novel surrogates for structured-sparsity assumptions on MRF structures is challenging, as a prior needs to be formulated and incorporated into a penalized maximum likelihood objective which then needs an efficient optimization algorithm to be developed, often within a separate research effort. Furthermore, model selection in a penalized maximum likelihood setting is difficult as regularization parameters are often unintuitive.
We propose to learn the estimator. Rather than manually designing a specific graph-estimation procedure, we frame this estimator-engineering problem as a learning problem, selecting a function from a large flexible function class by risk minimization. This allows us to construct a loss function that explicitly aims to recover the edge structure. Indeed, sampling from a distribution of graphs and empirical covariances with desired properties is often possible, even when this distribution is not analytically tractable. As such we can perform empirical risk minimization to select an appropriate function for edge estimation. Such a framework gives more easy control on the assumed level of sparsity (as opposed to graph lasso) and can impose structure on the sampling to shape the expected distribution, while optimizing a desired performance metric.
For particular cases we show that the problem of interest can be solved with a polynomial function, which is learnable with a neural network (Andoni et al., 2014). Motivated by this fact, as well as theoretical and empricial results on learning smooth functions approximating solutions to combinatorial problems (Cohen et al., 2016; Vinyals et al., 2015), we propose to use a particular convolutional neural network as the function class. We train it by sampling small datasets, generated from graphs with the prescribed properties, with a primary focus on sparse graphical models. We estimate from this data small-sample covariance matrices (n < p), where n is the number of samples and p is the dimensionality of the data. Then we use them as training data for the neural network (Figure 2) where target labels are indicators of present and absent edges in the underlying GGM. The learned network can then be employed in various real-world structure discovery problems.
In Section 1.1 we review the related work. In Section 2 we formulate the risk minimization view of graph-structure inference and describe how it applies to sparse GGMs. Section 2.3 describes and motivates the deep-learning architecture we chose to use for the sparse GGM problem in this work. In Section 3 we describe the details of how we train an edge estimator for sparse GGMs. We then evaluate its properties extensively on simulation data. Finally, we show that this edge estimator trained only on synthetic data can obtain state of the art performance at inference time on real neuroimaging and genetics problems, while being much faster to execute than other methods.

1.1 RELATED WORK
Lopez-Paz et al. (2015) analyze learning functions to identify the structure of directed graphical models in causal inference using estimates of kernel-mean embeddings. As in our work, they demonstrate the use of simulations for training while testing on real data. Unlike our work, they primarily focus on finding the causal direction in two node graphs with many observations.
Our learning architecture is motivated by the recent literature on deep networks. Vinyals et al. (2015) have shown that neural networks can learn approximate solutions to NP-hard combinatorial problems, and the problem of optimal edge recovery in MRFs can be seen as a combinatorial optimization problem. Several recent works have been proposed which show neural architectures for graph input data (Henaff et al., 2015; Duvenaud et al, 2015; Li et al., 2016). These are based on multi layer convolutional networks, as in our work, or multi-step recurrent neural networks. The input in our approach can be viewed as a complete graph, while the ouput a sparse graph, thus none of these are directly applicable. A related use of deep networks to approximate a posterior distribution can be found in Balan et al. (2015). Finally, Gregor & LeCun (2010); Xin et al. (2016) use deep networks to approximate steps of a known sparse recovery algorithm.
Bayesian approaches to structure learning rely on priors on the graph combined with sampling techniques to estimate the posterior of the graph structure. Some approaches make assumptions on the decomposability of the graph (Moghaddam et al., 2009). The G-Wishart distribution is a popular distribution which forms part of a framework for structure inference, and advances have been recently made in efficient sampling (Mohammadi & Wit, 2015). These methods can still be rather slow compared to competing methods, and in the setting of p > n we find they are less powerful.

2 METHODS

2.1 LEARNING AN APPROXIMATE EDGE ESTIMATION PROCEDURE
We consider MRF edge estimation as a learnable function. Let X ∈ Rn×p be a matrix whose n rows are i.i.d. samples x ∼ P (x) of dimension p. Let G = (V,E) be an undirected and unweighted graph associated with the set of variables in x. Let L = {0, 1} and Ne = p(p−1)2 the maximum possible edges in E. Let Y ∈ LNe indicate the presence or absence of edges in the edge set E of G, namely
Y ij = { 0 xi ⊥ xj |xV \i,j 1 xi 6⊥ xj |xV \i,j
(2)
We define an approximate structure discovery method gw(X), which produces a prediction of the edge structure, Ŷ = gw(X), given a set of data X . We focus on X drawn from a Gaussian distribution. In this case, the empirical covariance matrix, Σ̂, is a sufficient statistic of the population covariance and therefore of the conditional dependency structure. We thus express our structure-recovery problem as a function of Σ̂: gw(X) := fw(Σ̂). fw is parametrized by w and belongs to the function class F . We note that the graphical lasso in Equation (1) is an fw for an appropriate choice of F . This view on the edge estimator now allows us to bring the selection of fw from the domain of human design to the domain of empirical risk minimization over F . Defining a distribution P on Rp×p × LNe such that (Σ̂, Y ) ∼ P, we would like our estimator, fw, to minimize the expected risk R(f) = E(Σ̂,Y )∼P[l(f(Σ̂), Y )] (3) Here l : LNe×LNe → R+ is the loss function. For graphical model selection the 0/1 loss function is the natural error metric to consider (Wang et al., 2010). The estimator with minimum risk is generally not possible to compute as a closed form expression for most interesting choices of P, such as those arising from sparse graphs. In this setting, Eq. (1) achieves the information theoretic optimal recovery rate up to a constant for certain P corresponding to uniformly sparse graphs with a maximum degree, but only when the optimal λ is used and the non-zero precision matrix values are bounded away from zero (Wang et al., 2010; Ravikumar et al., 2011).
The design of the estimator in Equation (1) is not explicitly minimizing this risk functional. Thus modifying the estimator to fit a different class of graphs (e.g. small-world networks) while minimizing R(f) is not obvious. Furthermore, in practical settings the optimal λ is unknown and precision matrix entries can be very small. We would prefer to directly minimize the risk functional. Desired structural assumptions on samples from P on the underlying graph, such as sparsity, may imply that the distribution is not tractable for analytic solutions. Meanwhile, we can often devise a sampling procedure for P allowing us to select an appropriate function via empirical risk minimization. Thus it is sufficient to define a rich enough F over which we can minimize the empirical risk over the samples generated, giving us a learning objective over N samples {Yk,Σk}Nk=1 drawn from P: min w 1 N ∑N k=1 l(fw(Σ̂k), Yk). To maintain tractability, we use the standard cross-entropy loss as a convex surrogate, l̂ : RNe × LNe , given by: l̂(fw(Σ̂), Y ) =
∑ i 6=j ( Y ij log(f ijw (Σ̂)) + (1− Y ij) log(1− f ijw (Σ̂)) ) . (4)
We now need to select a sufficiently rich function class for fw and a method to produce appropriate (Y, Σ̂) which model our desired data priors. This will allow us to learn a fw that explicitly attempts to minimize errors in edge discovery.

2.2 DISCOVERING SPARSE GAUSSIAN GRAPHICAL MODELS AND BEYOND
We discuss how the described approach can be applied to recover sparse Gaussian graphical models. A typical assumption in many modalities is that the number of edges is sparse. A convenient property of these GGMs is that the precision matrix has a zero value in the (i, j)th entry precisely when variables i and j are independent conditioned on all others. Additionally, the precision matrix and partial correlation matrix have the same sparsity pattern, while the partial correlation matrix has normalized entries.
Algorithm 1 Training a GGM edge estimator for i ∈ {1, .., N} do
Sample Gi ∼ P(G) Sample Σi ∼ P(Σ|G = Gi) Xi ← {xj ∼ N(0,Σi)}nj=1 Construct (Yi, Σ̂i) pair from (Gi,Xi)
end for Select Function Class F (e.g. CNN) Optimize: min
f∈F 1 N
∑N k=1 l̂(f(Σ̂k), Yk)) We propose to simulate our a priori assumptions of sparsity and Gaussianity to learn fw(Σ̂), which can then produce predictions of edges from the input data. We model P (x|G) as arising from a sparse prior on the graph G and correspondingly the entries of the precision matrix Θ. To obtain a single sample of X corresponds to n i.i.d. samples from N (0,Θ−1). We can now train fw(Σ̂) by generating sample pairs (Σ̂, Y ). At execution time we standardize the input data and compute the covariance matrix before evaluating fw(Σ̂). The process of learning fw for the sparse GGM is given in Algorithm 1. A weakly-informative sparsity prior is one where each edge is equally likely with small probability, versus structured sparsity where edges have specific configurations. For obtaining the training samples (Σ̂, Y ) in this case we would like to create a sparse precision matrix, Θ, with the desired number of zero entries distributed uniformly. One strategy to do this and assure the precision matrices lie in the positive definite cone is to first construct an upper triangular sparse matrix and then multiply it by its transpose. This process is described in detail in the experimental section. Alternatively, an MCMC based G-Wishart distribution sampler can be employed if specific structures of the graph are desired (Lenkoski, 2013).
The sparsity patterns in real data are often not uniformly distributed. Many real world networks have a small-world structure: graphs that are sparse and yet have a comparatively short average distance between nodes. These transport properties often hinge on a small number of high-degree nodes called hubs. Normally, such structural patterns require sophisticated adaptation when applying estimators like Eq. (1). Indeed, high-degree nodes break the small-sample, sparse-recovery properties of `1-penalized estimators (Ravikumar et al., 2011). In our framework such structural assumptions appear as a prior that can be learned offline during training of the prediction function. Similarly priors on other distributions such as general exponential families can be more easily integrated. As the structure discovery model can be trained offline, even a slow sampling procedure may suffice.

2.3 NEURAL NETWORK GRAPH ESTIMATOR
In this work we propose to use a neural network as our function fw. To motivate this let us consider the extreme case when n p. In this case Σ̂ ≈ Σ and thus entries of Σ̂−1 or the partial correlation that are almost equal to zero can give the edge structure. Definition 1 (P-consistency). A function class F is P-consistent if ∃f ∈ F such that

E(Σ̂,Y )∼P[l(f(Σ̂), Y )]→ 0 as n→∞ with high probability.
Proposition 1 (Existence of P-consistent neural network graph estimator). There exists a feed forward neural network function class F that is P-consistent. Proof. If the data is standardized, each entry of Σ corresponds to the correlation ρi,j . The partial correlation of edge (i, j) conditioned on nodes Z, is given recursively as
ρi,j|Z = (ρi,j|Z\zo − ρi,zo|Z\zoρj,zo|Z\zo) 1
D . (5)
We may ignore the denominator, D, as we are interested in I(ρi,j|Z = 0). Thus we are left with a recursive formula that yields a high degree polynomial. From Andoni et al. (2014, Theorem 3.1) using gradient descent, a neural network with only two layers can learn a polynomial function of degree d to arbitrary precision given sufficient hidden units.
Remark 1. Naïvely the polynomial from the recursive definition of partial correlation is of degree bounded by 2p−2. In the worst case, this would seem to imply that we would need an exponentially
growing number of hidden nodes to approximate it. However, this problem has a great deal of structure that can allow efficient approximation. Firstly, higher order monomials will go to zero quickly with a uniform prior on ρi,j , which takes values between 0 and 1, suggesting that in many cases a concentration bound exists that guarantees non-exponential growth. Furthermore, the existence result is shown already for a shallow network, and we expect a logarithmic decrease in the number of parameters to peform function estimation with a deep network (Cohen et al., 2016).
Moreover, there are a great deal of redundant computations in Eq. (5) and an efficient dynamic programming implementation can yield polynomial computation time and require only low order polynomial computations with appropriate storage of previous computation. Similarly we would like to design a network that would have capacity to re-use computations across edges and approximate low order polynomials. We also observe that the conditional independence of nodes i, j given Z can be computed equivalently in many ways by considering many paths through the nodes Z. Thus we can choose any valid ordering for traversing the nodes starting from a given edge.
We propose a series of shared operations at each edge. We consider a feedforward network where each edge i, j is associated with a fixed sized vector, oki,j , of dimensionality d at each layer, k > 0. o0i,j is initialized to the covariance entries at k = 0. For each edge we start with a neighborhood of the 6 adjacent nodes, i, j, i-1, i+1, j-1, j+1 for which we take all corresponding edge values from the covariance matrix illustrated in Figure 1. We proceed at each layer to increase the nodes considered for each edge, the output at each layer progressively increasing the receptive field making sure all values associated with the considered nodes are present. The receptive field here refers to the original covariance entries which are accessible by a given, oki,j (Luo et al., 2010). The equations defining the process are shown in Figure 1. Here a neural network fwk is applied at each edge at each layer and a dilation sequence dk is used. We call a network of this topology a D-Net of depth l. We use dilation here to allow the receptive field to grow fast, so the network does not need a great deal of layers. We make the following observations: Proposition 2. For general P it is a necessary condition for P-consistency that the receptive field of D-Net covers all entries of the covariance, Σ̂, at any edge it is applied. Proof. Consider nodes i and j and a chain graph such that i and j are adjacent to each other in the matrix but are at the terminal nodes of the chain graph. One would need to consider all other variables to be able to explain away the correlation. Alternatively we can see this directly from expanding Eq. (5).
Proposition 3. A p× p matrix Σ̂ will be covered by the receptive field for a D-Net of depth log2(p) and dk = 2k−1 Proof. The receptive field of a D-Net with dilation sequence dk = 2k−1 of depth l is O(2l). We can see this as oki,j will receive input from o k−1 a,b at the edge of it’s receptive field, effectively doubling it. It now follows that we need at least log2(p) layers to cover the receptive field.
Intuitively adjacent edges have a high overlap in their receptive fields and can easily share information about the non-overlapping components. This is analogous to a parametrized message passing. For example if edge (i, j) is explained by node k, as k enters the receptive field of edge (i, j − 1),
the path through (i, j) can already be discounted. In terms of Eq. 5 this can correspond to storing computations that can be used by neighbor edges from lower levels in the recursion.
Here fwk is shared amongst all nodes and thus we can implement this as a special kind of convolutional network. We make sure that to have considered all edges relevant to the current set of nodes in the receptive field which requires us to add values from filters applied at the diagonal to all edges. In Figure 1 we illustrate the nodes and receptive field considered with respect to the covariance matrix. This also motivates a straightforward implementation using 2D convolutions (adding separate convolutions at i, i and j, j to each i, j at each layer to achieve the specific input pattern described) shown in (Figure 2).
Ultimately our choice of architecture that has shared computations and multiple layers is highly scalable as compared with a naive fully connected approach and allows leveraging existing optimized 2-D convolutions. In preliminary work we have also considered fully connected layers but this proved to be much less efficient in terms of storage and scalibility than using deep convolutional networks.
Considering the general n p case is illustrative. However, the main advantages of making the computations differentiable and learned from data is that we can take advantage of the sparsity and structure assumptions on the target function to obtain more efficient results than naive computation of partial correlation or matrix inversion. As n decreases our estimate of ρ̂i,j becomes inexact and here a data driven model which can take advantage of the assumptions on the underlying distribution can more accurately recover the graph structure.
The convolution structure is dependent on the order of the variables used to build the covariance matrix, which is arbitrary. Permuting the input data we can obtain another estimate of the output. In the experiments, we leverage these various estimate in an ensembling approach, averaging the results of several permutations of input. We observe that this generally yields a modest increase in accuracy, but that even a single node ordering can show substantially improved performance over competing methods in the literature.

3 EXPERIMENTS
Our experimental evaluations focus on the challenging high dimensional settings in which p > n and consider both synthetic data and real data from genetics and neuroimaging. In our experiments we explore how well networks trained on parametric samples generalize, both to unseen synthetic data and to several real world problems. In order to highlight the generality of the learned networks, we apply the same network to multiple domains. We train networks taking in 39, 50, and 500 node graphs. The former sizes are chosen based on the real data we consider in subsequent sections. We refer to these networks as DeepGraph-39, 50, and 500. In all cases we have 50 feature maps of 3× 3 kernels. The 39 and 50 node network with 6 convolutional layers and dk = k + 1. For the 500 node network with 8 convolutional layers and dk = 2k+1. We use ReLU activations. The last layer has 1× 1 convolution and a sigmoid outputing a value of 0 to 1 for each edge. We sample P (X|G) with a sparse prior on P (G) as follows. We first construct a lower diagonal matrix, L, where each entry has α probability of being zero. Non-zero entries are set uniformly between −c and c. Multiplying LLT gives a sparse positive definite precision matrix, Θ. This gives us our P (Θ|G) with a sparse prior on P (G). We sample from the Gaussian N (0,Θ−1) to obtain
samples of X . Here α corresponds approximately to a specific sparsity level in the final precision matrix, which we set to produce matrices 92− 96% sparse and c chosen so that partial correlations range 0 to 1.
Each network is trained continously with new samples generated until the validation error saturates. For a given precision matrix we generate 5 possible X samples to be used as training data, with a total of approximately 100K training samples used for each network. The networks are optimized using ADAM (Kingma & Ba, 2015) coupled with cross-entropy loss as the objective function (cf. Sec. 2.1). We use batch normalization at each layer. Additionally, we found that using the absolute value of the true partial correlations as labels, instead of hard binary labels, improves results.
Synthetic Data Evaluation To understand the properties of our learned networks, we evaluated them on different synthetic data than the ones they were trained on. More specifically, we used a completely different third party sampler so as to avoid any contamination. We use DeepGraph-39 on a variety of settings. The same trained network is utilized in the subsequent neuroimaging evaluations as well. DeepGraph-500 is also used to evaluate larger graphs.
We used the BDGraph R-package to produce sparse precision matrices based on the G-Wishart distribution (Mohammadi & Wit, 2015) as well as the R-package rags2ridges (Peeters et al., 2015) to generate data from small-world networks corresponding to the Watts–Strogatz model (Watts & Strogatz, 1998). We compared our learned estimator against the scikit-learn (Pedregosa et al, 2011) implementation of Graphical Lasso with regularizer chosen by cross-validation as well as the Birth-Death Rate MCMC (BDMCMC) method from Mohammadi & Wit (2015).
For each scenario we repeat the experiment for 100 different graphs and small sample observations showing the average area under the ROC curve (AUC), precision@k corresponding to 5% of possible edges, and calibration error (CE) (Mohammadi & Wit, 2015).
For graphical lasso we use the partial correlations to indicate confidence in edges; BDGraph automatically returns posterior probabilities as does our method. Finally to understand the effect of the regularization parameter we additionally report the result of graphical lasso under optimal regularizer setting on the testing data.
Our method dominates all other approaches in all cases with p > n (which also corresponds to the training regime). For the case of random Gaussian graphs with n=35 (as in our training data), and graph sparsity of 95%, we have superior performance and can further improve on this by averaging permutations. Next we apply the method to a less straightforward synthetic data, with distributions typical of many applications. We found that, compared to baseline methods, our network performs particularly well with high-degree nodes and when the distribution becomes non-normal. In particular our method performs well on the relevant metrics with small-world networks, a very common family of graphs in real-world data, obtaining superior precision at the primary levels of interest. Figure 3 shows examples of random and Watts-Strogatz small-world graphs used in these experiments.
Training a new network for each number of samples can pose difficulties with our proposed method. Thus we evaluted how robust the network DeepGraph-39 is to input covariances obtained from fewer or more samples. We find that overall the performance is quite good even when lowering the number of samples to n = 15, we obtain superior performance to the other approaches (Table 1). We also applied DeepGraph-39 on data from a multivariate generalization of the Laplace distribution (Gómez et al., 1998). As in other experiments precision matrices were sampled from the G-Wishart at a sparsity of 95%. Gómez et al. (1998, Proposition 3.1) was applied to produce samples. We find that DeepGraph-39 performs competitively, despite the discrepancy between train and test distributions. Experiments with variable sparsity are considered in the supplementary material, which find that for very sparse graphs, the networks remain robust in performance, while for increased density performance degrades but remains competitive.
Using the small-world network data generator (Peeters et al., 2015), we demonstrate that we can update the generic sparse prior to a structured one. We re-train DeepGraph-39 using only 1000 examples of small-world graphs mixed with 1000 examples from the original uniform sparsity model. We perform just one epoch of training and observe markedly improved performance on this test case as seen in the last row of Table 1.
For our final scenario we consider the very challenging setting with 500 nodes and only n = 50 samples. We note that the MCMC based method fails to converge at this scale, while graphical lasso is very slow as seen in the timing performance and barely performs better than chance. Our method convincingly outperforms graphical lasso in this scenario. Here we additionally report precision at just the first 0.05% of edges since competitors perform nearly at chance at the 5% level.
Cancer Genome Data We perform experiments on a gene expression dataset described in Honorio et al. (2012). The data come from a cancer genome atlas from 2360 subjects for various types of cancer. We used the first 50 genes from Honorio et al. (2012, Appendix C.2) of commonly regulated genes in cancer. We evaluated on two groups of subjects, one with breast invasive carcinoma (BRCA) consisting of 590 subjects and the other colon adenocarcinoma (CODA) consisting of 174 subjects.
Evaluating edge selection in real-world data is challenging. We use the following methodology: for each method we select the top-k ranked edges, recomputing the maximum likelihood precision matrix with support given by the corresponding edge selection method. We then evaluate the likelihood on a held-out set of data. We repeat this procedure for a range of k. We rely on Algorithm 0 in Hara & Takemura (2010) to compute the maximum likelihood precision given a support. The experiment is repeated for each of CODA and BRCA subject groups 150 times. Results are shown in Figure 4. In all cases we use 40 samples for edge selection and precision estimation. We compare with graphical lasso as well as the Ledoit-Wolf shrinkage estimator (Ledoit & Wolf, 2004). We additionally consider the MCMC based approach described in previous section. For graphical lasso and Ledoit-Wolf, edge selection is based on thresholding partial correlation (Balmand & Dalalyan, 2016).
Additionally, we evaluate the stability of the solutions provided by the various methods. In several applications a low variance on the estimate of the edge set is important. On Table 3, we report
Spearman correlations between pairs of solutions, as it is a measure of a monotone link between two variables. DeepGraph has far better stability in the genome experiments and is competitive in the fMRI data.
Resting State Functional Connectivity We evaluate our graph discovery method to study brain functional connectivity in resting-state fMRI data. Correlations in brain activity measured via fMRI reveal functional interactions between remote brain regions. These are an important measure to study psychiatric diseases that have no known anatomical support. Typical connectome analysis describes each subject or group by a GGM measuring functional connectivity between a set of regions (Varoquaux & Craddock, 2013). We use the ABIDE dataset (Di Martino et al, 2014), a large scale resting state fMRI dataset. It gathers brain scans from 539 individuals suffering from autism spectrum disorder and 573 controls over 16 sites.1 For our experiments we use an atlas with 39 regions of interest derived in Varoquaux et al. (2011).
We use the network DeepGraph-39, the same network and parameters from synthetic experiments, using the same evaluation protocol as used in the genomic data. For both control and autism patients we use time series from 35 random subjects to estimate edges and corresponding precision matrices. We find that for both the Autism and Control group we can obtain edge selection comparable to graph lasso for very few selected edges. When the number of selected edges is in the range above 25 we begin to perform significantly better in edge selection as seen in Fig. 4. We evaluated stability of the results as shown in Tab. 3. DeepGraph outperformed the other methods across the board.
ABIDE has high variability across sites and subjects. As a result, to resolve differences between approaches, we needed to perform 1000 folds to obtain well-separated error bars. We found that the birth-death MCMC method took very long to converge on this data, moreover the need for many folds to obtain significant results amongst the methods made this approach prohibitively slow to evaluate.
1http://preprocessed-connectomes-project.github.io/abide/
We show the edges returned by Graph Lasso and DeepGraph for a sample from 35 subjects (Fig. 5) in the control group. We also show the result of a large-sample result based on 368 subjects from graphical lasso. In visual evaluation of the edges returned by DeepGraph we find that they closely align with results from a large-sample estimation procedure. Furthermore we can see several edges in the subsample which were particularly strongly activated in both methods.

4 DISCUSSION AND CONCLUSIONS
Our method was competitive with strong baselines. Even in cases that deviate from standard GGM sparsity assumptions (e.g. Laplacians, small-world) it performed substantially better. When finetuning on the target distribution performance further improves. Most importantly the learned estimator generalizes well to real data finding relevant stable edges. We also observed that the learned estimators generalize to variations not seen at training time (e.g. different n or sparsity), which points to this potentialy learning generic computations. This also shows potential to more easily scale the method to different graph sizes. One could consider transfer learning, where a network for one size of data is used as a starting point to learn a network working on larger dimension data.
Penalized maximum likelihood can provide performance guarantees under restrictive assumptions on the form of the distribution and not considering the regularization path. In the proposed method one could obtain empirical bounds under the prescribed data distribution. Additionally, at execution time the speed of the approach can allow for re-sampling based uncertainty estimates and efficient model selection (e.g. cross-validation) amongst several trained estimators.
We have introduced the concept of learning an estimator for determining the structure of an undirected graphical model. A network architecture and sampling procedure for learning such an estimator for the case of sparse GGMs was proposed. We obtained competitive results on synthetic data with various underlying distributions, as well as on challenging real-world data. Empirical results show that our method works particularly well compared to other approaches for small-world networks, an important class of graphs common in real-world domains. We have shown that neural networks can obtain improved results over various statistical methods on real datasets, despite being trained with samples from parametric distributions. Our approach enables straightforward specifications of new priors and opens new directions in efficient graphical structure discovery from few examples.

ACKNOWLEDGEMENTS
This work is partially funded by Internal Funds KU Leuven, FP7-MC-CIG 334380, DIGITEO 2013- 0788D - SOPRANO, and ANR-11-BINF-0004 NiConnect. We thank Jean Honorio for providing pre-processed Cancer Genome Data.

A SUPPLEMENTARY EXPERIMENTS

A.1 PREDICTING COVARIANCE MATRICES
Using our framework it is possible to attempt to directly predict an accurate covariance matrix given a noisy one constructed from few observations. This is a more challenging task than predicting the edges. In this section we show preliminay experiments which given an empirical covariance matrix from few observations attempts to predict a more accurate covariance matrix that takes into account underlying sparse data dependency structure.
One challenge is that outputs of our covariance predictor must be on the positive semidefinite cone, thus we choose to instead predict on the cholesky decompositions, which allows us to always produce positive definite covariances. We train a similar structure to DeepGraph-39 structure modifying the last layer to be fully connected linear layer that predicts on the cholesky decomposition of the true covariance matrices generated by our model with a squared loss.
We evaluate this network using the ABIDE dataset described in Section 3. The ABIDE data has a large number of samples allowing us to obtain a large sample estimate of the covariance and compare it to our estimator as well as graphical lasso and empirical covariance estimators. Using the large sample ABIDE empirical covariance matrix. We find that we can obtain competitive `2 and `∞ norm using few samples. We use 403 subjects from the ABIDE Control group each with a recording of 150 − 200 samples to construct covariance matrix, totaling 77 330 samples (some correlated). This acts as our very approximate estimate of the population Σ. We then evaluate covariance estimation on 35 samples using the empirical covariance estimator, graphical lasso, and DeepGraph trained to output covariance matrices. We repeat the experiment for 50 different subsamples of the data. We see in 5 that the prediction approach can obtain competitive results. In terms of `2 graphical lasso performs better, however our estimate is better than empirical covariance estimation and much faster then graphical lasso. In some applications such as robust estimation a fast estimate of the covariance matrix (automatically embedding sparsity assumptions) can be of great use. For `∞ error we see the empirical covariance estimation outperforms graphical lasso and DeepGraph for this dataset, while DeepGraph performs better in terms of this metric.
We note these results are preliminary, as the covariance predicting networks were not heavily optimized, moreover the ABIDE dataset is very noisy even when pre-processed and thus even the large sample covariance estimate may not be accurate. We believe this is an interesting alternate application of our paper.

A.2 ADDITIONAL SYNTHETIC RESULTS ON SPARSITY
We investigate the affect of sparsity on DeepGraph-39 which has been trained with input that has sparsity 96% − 92% sparse. We find that DeepGraph performs well at the 2% sparsity level despite not seeing this at training time. At the same time performance begins to degrade for 15% but is still competitive in several categories. The results are shown in Table 6. Future investigation can consider how alternate variation of sparsity at training time will affect these results.

A.3 APPLICATION OF LARGER NETWORK ON SMALLER INPUT
We perform preliminary investigation of application of a network trained for a larger number of nodes to a smaller set of nodes. Specifically, we consider the breast invasive carcinoma groups gene data. We now take all 175 valid genes from Appendix C.2 of Honorio et al. (2012). We take the network trained on 500 nodes in the synthetic experiments section. We use the same experimental setup as in the gene experiments. The 175× 175
covariance matrix from 40 samples and padded to the appropriate size. We observe that DeepGraph has similar performance to graph lasso while permuting the input and ensembling the result gives substantial improvement.

A.4 PERMUTATION AS ENSEMBLE METHOD
As discussed in Section 2.3, permuting the input and averaging several permutations can produce an improved result empirically. We interpret this as a typical ensembling method. This can be an advantage of the proposed architecture as we are able to easily use standard ensemble techniques. We perform an experiment to further verify that indeed the permutation of the input (and subsequent inverse permutation) allows us to produce separate classifiers that have uncorrelated errors.
We use the setup from the synthetic experiments with DeepGraph-39 in Section 3 with n = 35 and p = 39. We construct 20 permutation matrices as in the experimental section. Treating each as a separate classifier we compute the correlation coefficient of the errors on 50 synthetic input examples. We find that the average correlation coefficient of the errors of two classifiers is 0.028± 0.002, suggesting they are uncorrelated. Finally we note the individual errors are relatively small, as can already be inferred from our extensive experimental results in Section 3. We however compute the average absolute error of all the outputs across each permutation for this set of inputs as 0.03, notably the range of outputs is 0 to 1. Thus since prediction error differ at each permutation but are accurate we can average and yield a lower total prediction error.
Finally we note that our method is extremely efficient computationally thus averaging the results of several permutations is practical even as the graph becomes large.
","We consider structure discovery of undirected graphical models from observational data. Inferring likely structures from few examples is a complex task often requiring the formulation of priors and sophisticated inference procedures. In the setting of Gaussian Graphical Models (GGMs) a popular estimator is a maximum likelihood objective with a penalization on the precision matrix. Adapting this estimator to capture domain-specific knowledge as priors or a new data likelihood requires great effort. In addition, structure recovery is an indirect consequence of the data-fit term. By contrast, it may be easier to generate training samples of data that arise from graphs with the desired structure properties. We propose here to leverage this latter source of information as training data to learn a function mapping from empirical covariance matrices to estimated graph structures. Learning this function brings two benefits: it implicitly models the desired structure or sparsity properties to form suitable priors, and it can be tailored to the specific problem of edge structure discovery, rather than maximizing data likelihood. We apply this framework to several real-world problems in structure discovery and show that it can be competitive to standard approaches such as graphical lasso, at a fraction of the execution speed. We use convolutional neural networks to parametrize our estimators due to the compositional structure of the problem. Experimentally, our learnable graph-discovery method trained on synthetic data generalizes well: identifying relevant edges in real data, completely unknown at training time. We find that on genetics, brain imaging, and simulation data we obtain competitive (and generally superior) performance, compared with analytical methods.",ICLR 2017 conference submission,False,,"The paper proposes a novel algorithm to estimate graph structures by using a convolutional neural network to approximate the function that maps from empirical covariance matrix to the sparsity pattern of the graph. Compared with existing approaches, the new algorithm can adapt to different network structures, e.g. small-world networks, better under the same empirical risk minimization framework. Experiments on synthetic and real-world datasets show promising results compared with baselines.

In general, I think it is an interesting and novel paper. The idea of framing structure estimation as a learning problem is especially interesting and may inspire further research on related topics. The advantage of such an approach is that it allows easier adaptation to different network structure properties without designing specific regularization terms as in graph lasso.

The experiment results are also promising. In both synthetic and real-world datasets, the proposed algorithm outperforms other baselines in the small sample region. 

However, the paper can be made clearer in describing the network architectures. For example, in page 5, each o^k_{i,j} is said be a d-dimensional vector. But from the context, it seems o^k_{i,j} is a scalar (from o^0_{i,j} = p_{i,j}). It is not clear what o^k_{i,j} is exactly and what d is. Is it the number of channels for the convolutional filters?

Figure 1 is also quite confusing. Why in (b) the table is 16 x 16 whereas in (a) there are only six nodes? And from the figure, it seems there is only one channel in each layer? What do the black squares represent and why are there three blocks of them. There are some descriptions in the text, but it is still not clear what they mean exactly.

For real-world data, how are the training data (Y, Sigma) generated? Are they generated in the same way as in the synthetic experiments where the entries are uniformly sparse? This is also related to the more general question of how to sample from the distribution P, in the case of real-world data.

---

The authors provide a modern twist to the classical problem of graphical model selection. Traditionally, the sparsity priors to encourage selection of specific structures is hand-engineered. Instead, the authors propose using a neural network to train for these priors. Since graphical models are useful in the small-sample regime, using neural networks directly on the training data is not effective. Instead, the authors propose generating data based on the desired graph structures to train the neural network. 
 
 While this is a nice idea, the paper is not clear and convincing enough to be accepted to the conference, and instead, recommend it to the workshop track.

---

We thank the reviewers for their comments and for providing us with excellent feedback. We have updated the paper with clarifications in Section 2.3 as well as an Appendix (A.4) with some additional experiments which analyze permuted inputs. We have also made an early release of our code available at

---

I sincerely apologize for the late-arriving review. 

This paper proposes to frame the problem of structure estimation as a supervised classification problem. The input is an empirical covariance matrix of the observed data, the output the binary decision whether or not two variables share a link. The paper is sufficiently clear, the goals are clear and everything is well described. 

The main interesting point is the empirical results of the experimental section. The approach is simple and performs better than previous non-learning based methods. This observation is interesting and will be of interest in structure discovery problems. 

I rate the specific construction of the supervised learning method as a reasonable attempt attempt to approach this problem. There is not very much technical novelty in this part. E.g., an algorithmic contribution would have been a method that is invariant to data permutation could have been a possible target for a technical contribution. The paper makes no claims on this technical part, as said, the method is well constructed and well executed. 

It is good to precisely state the theoretical parts of a paper, the authors do this well. All results are rather straight-forward, I like that the claims are written down, but there is little surprise in the statements. 

In summary, the paper makes a very interesting observation. Graph estimation can be posed as a supervised learning problem and training data from a separate source is sufficient to learn structure in novel and unseen test data from a new source. Practically this may be relevant, on one hand the empirical results are stronger with this method, on the other hand a practitioner who is interested in structural discovery may have side constraints about interpretability of the deriving method. From the Discussion and Conclusion I understand that the authors consider this as future work. It is a good first step, it could be stronger but also stands on its own already.

---

The paper proposes a novel algorithm to estimate graph structures by using a convolutional neural network to approximate the function that maps from empirical covariance matrix to the sparsity pattern of the graph. Compared with existing approaches, the new algorithm can adapt to different network structures, e.g. small-world networks, better under the same empirical risk minimization framework. Experiments on synthetic and real-world datasets show promising results compared with baselines.

In general, I think it is an interesting and novel paper. The idea of framing structure estimation as a learning problem is especially interesting and may inspire further research on related topics. The advantage of such an approach is that it allows easier adaptation to different network structure properties without designing specific regularization terms as in graph lasso.

The experiment results are also promising. In both synthetic and real-world datasets, the proposed algorithm outperforms other baselines in the small sample region. 

However, the paper can be made clearer in describing the network architectures. For example, in page 5, each o^k_{i,j} is said be a d-dimensional vector. But from the context, it seems o^k_{i,j} is a scalar (from o^0_{i,j} = p_{i,j}). It is not clear what o^k_{i,j} is exactly and what d is. Is it the number of channels for the convolutional filters?

Figure 1 is also quite confusing. Why in (b) the table is 16 x 16 whereas in (a) there are only six nodes? And from the figure, it seems there is only one channel in each layer? What do the black squares represent and why are there three blocks of them. There are some descriptions in the text, but it is still not clear what they mean exactly.

For real-world data, how are the training data (Y, Sigma) generated? Are they generated in the same way as in the synthetic experiments where the entries are uniformly sparse? This is also related to the more general question of how to sample from the distribution P, in the case of real-world data.

---

This paper proposes a new method for learning graphical models. Combined with a neural network architecture, some sparse edge structure is estimated via sampling methods. In introduction, the authors say that a problem in graphical lasso is model selection. However, the proposed method still implicitly includes model selection. In the proposed method, $P(G)$ is a sparse prior, and should include some hyper-parameters. How do you tune the hyper-parameters? Is this tuning an equivalent problem to model section? Therefore, I do not understand real advantage of this method over previous methods. What is the advantage of the proposed method?

Another concern is that this paper is unorganized. In Algorithm 1, first, G_i and \Sigma_i are sampled, and then x_j is sampled from N(0, \Sigma). Here, what is \Sigma? Is it different from \Sigma_i? Furthermore, how do you construct (Y_i, \hat{\Sigma}_i) from (G_i, X_i )? Finally, I have a simple question: Where is input data X (not sampled data) is used in Algorithm 1?

What is the definition of the receptive field in Proposition 2 and Proposition 3?

---

The paper proposes a novel algorithm to estimate graph structures by using a convolutional neural network to approximate the function that maps from empirical covariance matrix to the sparsity pattern of the graph. Compared with existing approaches, the new algorithm can adapt to different network structures, e.g. small-world networks, better under the same empirical risk minimization framework. Experiments on synthetic and real-world datasets show promising results compared with baselines.

In general, I think it is an interesting and novel paper. The idea of framing structure estimation as a learning problem is especially interesting and may inspire further research on related topics. The advantage of such an approach is that it allows easier adaptation to different network structure properties without designing specific regularization terms as in graph lasso.

The experiment results are also promising. In both synthetic and real-world datasets, the proposed algorithm outperforms other baselines in the small sample region. 

However, the paper can be made clearer in describing the network architectures. For example, in page 5, each o^k_{i,j} is said be a d-dimensional vector. But from the context, it seems o^k_{i,j} is a scalar (from o^0_{i,j} = p_{i,j}). It is not clear what o^k_{i,j} is exactly and what d is. Is it the number of channels for the convolutional filters?

Figure 1 is also quite confusing. Why in (b) the table is 16 x 16 whereas in (a) there are only six nodes? And from the figure, it seems there is only one channel in each layer? What do the black squares represent and why are there three blocks of them. There are some descriptions in the text, but it is still not clear what they mean exactly.

For real-world data, how are the training data (Y, Sigma) generated? Are they generated in the same way as in the synthetic experiments where the entries are uniformly sparse? This is also related to the more general question of how to sample from the distribution P, in the case of real-world data.

---

The authors provide a modern twist to the classical problem of graphical model selection. Traditionally, the sparsity priors to encourage selection of specific structures is hand-engineered. Instead, the authors propose using a neural network to train for these priors. Since graphical models are useful in the small-sample regime, using neural networks directly on the training data is not effective. Instead, the authors propose generating data based on the desired graph structures to train the neural network. 
 
 While this is a nice idea, the paper is not clear and convincing enough to be accepted to the conference, and instead, recommend it to the workshop track.

---

We thank the reviewers for their comments and for providing us with excellent feedback. We have updated the paper with clarifications in Section 2.3 as well as an Appendix (A.4) with some additional experiments which analyze permuted inputs. We have also made an early release of our code available at

---

I sincerely apologize for the late-arriving review. 

This paper proposes to frame the problem of structure estimation as a supervised classification problem. The input is an empirical covariance matrix of the observed data, the output the binary decision whether or not two variables share a link. The paper is sufficiently clear, the goals are clear and everything is well described. 

The main interesting point is the empirical results of the experimental section. The approach is simple and performs better than previous non-learning based methods. This observation is interesting and will be of interest in structure discovery problems. 

I rate the specific construction of the supervised learning method as a reasonable attempt attempt to approach this problem. There is not very much technical novelty in this part. E.g., an algorithmic contribution would have been a method that is invariant to data permutation could have been a possible target for a technical contribution. The paper makes no claims on this technical part, as said, the method is well constructed and well executed. 

It is good to precisely state the theoretical parts of a paper, the authors do this well. All results are rather straight-forward, I like that the claims are written down, but there is little surprise in the statements. 

In summary, the paper makes a very interesting observation. Graph estimation can be posed as a supervised learning problem and training data from a separate source is sufficient to learn structure in novel and unseen test data from a new source. Practically this may be relevant, on one hand the empirical results are stronger with this method, on the other hand a practitioner who is interested in structural discovery may have side constraints about interpretability of the deriving method. From the Discussion and Conclusion I understand that the authors consider this as future work. It is a good first step, it could be stronger but also stands on its own already.

---

The paper proposes a novel algorithm to estimate graph structures by using a convolutional neural network to approximate the function that maps from empirical covariance matrix to the sparsity pattern of the graph. Compared with existing approaches, the new algorithm can adapt to different network structures, e.g. small-world networks, better under the same empirical risk minimization framework. Experiments on synthetic and real-world datasets show promising results compared with baselines.

In general, I think it is an interesting and novel paper. The idea of framing structure estimation as a learning problem is especially interesting and may inspire further research on related topics. The advantage of such an approach is that it allows easier adaptation to different network structure properties without designing specific regularization terms as in graph lasso.

The experiment results are also promising. In both synthetic and real-world datasets, the proposed algorithm outperforms other baselines in the small sample region. 

However, the paper can be made clearer in describing the network architectures. For example, in page 5, each o^k_{i,j} is said be a d-dimensional vector. But from the context, it seems o^k_{i,j} is a scalar (from o^0_{i,j} = p_{i,j}). It is not clear what o^k_{i,j} is exactly and what d is. Is it the number of channels for the convolutional filters?

Figure 1 is also quite confusing. Why in (b) the table is 16 x 16 whereas in (a) there are only six nodes? And from the figure, it seems there is only one channel in each layer? What do the black squares represent and why are there three blocks of them. There are some descriptions in the text, but it is still not clear what they mean exactly.

For real-world data, how are the training data (Y, Sigma) generated? Are they generated in the same way as in the synthetic experiments where the entries are uniformly sparse? This is also related to the more general question of how to sample from the distribution P, in the case of real-world data.

---

This paper proposes a new method for learning graphical models. Combined with a neural network architecture, some sparse edge structure is estimated via sampling methods. In introduction, the authors say that a problem in graphical lasso is model selection. However, the proposed method still implicitly includes model selection. In the proposed method, $P(G)$ is a sparse prior, and should include some hyper-parameters. How do you tune the hyper-parameters? Is this tuning an equivalent problem to model section? Therefore, I do not understand real advantage of this method over previous methods. What is the advantage of the proposed method?

Another concern is that this paper is unorganized. In Algorithm 1, first, G_i and \Sigma_i are sampled, and then x_j is sampled from N(0, \Sigma). Here, what is \Sigma? Is it different from \Sigma_i? Furthermore, how do you construct (Y_i, \hat{\Sigma}_i) from (G_i, X_i )? Finally, I have a simple question: Where is input data X (not sampled data) is used in Algorithm 1?

What is the definition of the receptive field in Proposition 2 and Proposition 3?",,,,,,6.0,,,2.6666666666666665,,
533,"SURPRISE-BASED INTRINSIC MOTIVATION FOR DEEP REINFORCEMENT LEARNING
Authors: Joshua Achiam, Shankar Sastry
Source file: 533.pdf

ABSTRACT
Exploration in complex domains is a key challenge in reinforcement learning, especially for tasks with very sparse rewards. Recent successes in deep reinforcement learning have been achieved mostly using simple heuristic exploration strategies such as -greedy action selection or Gaussian control noise, but there are many tasks where these methods are insufficient to make any learning progress. Here, we consider more complex heuristics: efficient and scalable exploration strategies that maximize a notion of an agent’s surprise about its experiences via intrinsic motivation. We propose to learn a model of the MDP transition probabilities concurrently with the policy, and to form intrinsic rewards that approximate the KL-divergence of the true transition probabilities from the learned model. One of our approximations results in using surprisal as intrinsic motivation, while the other gives the k-step learning progress. We show that our incentives enable agents to succeed in a wide range of environments with high-dimensional state spaces and very sparse rewards, including continuous control tasks and games in the Atari RAM domain, outperforming several other heuristic exploration techniques.

1 INTRODUCTION
A reinforcement learning agent uses experiences obtained from interacting with an unknown environment to learn behavior that maximizes a reward signal. The optimality of the learned behavior is strongly dependent on how the agent approaches the exploration/exploitation trade-off in that environment. If it explores poorly or too little, it may never find rewards from which to learn, and its behavior will always remain suboptimal; if it does find rewards but exploits them too intensely, it may wind up prematurely converging to suboptimal behaviors, and fail to discover more rewarding opportunities. Although substantial theoretical work has been done on optimal exploration strategies for environments with finite state and action spaces, we are here concerned with problems that have continuous state and/or action spaces, where algorithms with theoretical guarantees admit no obvious generalization or are prohibitively impractical to implement.
Simple heuristic methods of exploring such as -greedy action selection and Gaussian control noise have been successful on a wide range of tasks, but are inadequate when rewards are especially sparse. For example, the Deep Q-Network approach of Mnih et al. [13] used -greedy exploration in training deep neural networks to play Atari games directly from raw pixels. On many games, the algorithm resulted in superhuman play; however, on games like Montezuma’s Revenge, where rewards are extremely sparse, DQN (and its variants [25], [26], [15], [12]) with -greedy exploration failed to achieve scores even at the level of a novice human. Similarly, in benchmarking deep reinforcement learning for continuous control, Duan et al.[5] found that policy optimization algorithms that explored by acting according to the current stochastic policy, including REINFORCE and Trust Region Policy Optimization (TRPO), could succeed across a diverse slate of simulated robotics control tasks with well-defined, non-sparse reward signals (like rewards proportional to the forward velocity of the robot). Yet, when tested in environments with sparse rewards—where the agent would only be able to attain rewards after first figuring out complex motion primitives without reinforcement—every algorithm failed to attain scores better than random agents. The failure modes in all of these cases pertained to the nature of the exploration: the agents encountered reward signals so infrequently that they were never able to learn reward-seeking behavior.
One approach to encourage better exploration is via intrinsic motivation, where an agent has a task-independent, often information-theoretic intrinsic reward function which it seeks to maximize in addition to the reward from the environment. Examples of intrinsic motivation include empowerment, where the agent enjoys the level of control it has about its future; surprise, where the agent is excited to see outcomes that run contrary to its understanding of the world; and novelty, where the agent is excited to see new states (which is tightly connected to surprise, as shown in [2]). For in-depth reviews of the different types of intrinsic motivation, we direct the reader to [1] and [17].
Recently, several applications of intrinsic motivation to the deep reinforcement learning setting (such as [2], [7], [22]) have found promising success. In this work, we build on that success by exploring scalable measures of surprise for intrinsic motivation in deep reinforcement learning. We formulate surprise as the KL-divergence of the true transition probability distribution from a transition model which is learned concurrently with the policy, and consider two approximations to this divergence which are easy to compute in practice. One of these approximations results in using the surprisal of a transition as an intrinsic reward; the other results in using a measure of learning progress which is closer to a Bayesian concept of surprise. Our contributions are as follows:
1. we investigate surprisal and learning progress as intrinsic rewards across a wide range of environments in the deep reinforcement learning setting, and demonstrate empirically that the incentives (especially surprisal) result in efficient exploration,
2. we evaluate the difficulty of the slate of sparse reward continuous control tasks introduced by Houthooft et al. [7] to benchmark exploration incentives, and introduce a new task to complement the slate,
3. and we present an efficient method for learning the dynamics model (transition probabilities) concurrently with a policy.
We distinguish our work from prior work in a number of implementation details: unlike Bellemare et al. [2], we learn a transition model as opposed to a state-action occupancy density; unlike Stadie et al. [22], our formulation naturally encompasses environments with stochastic dynamics; unlike Houthooft et al. [7], we avoid the overhead of maintaining a distribution over possible dynamics models, and learn a single deep dynamics model.
In our empirical evaluations, we compare the performance of our proposed intrinsic rewards with other heuristic intrinsic reward schemes and to recent results from the literature. In particular, we compare to Variational Information Maximizing Exploration (VIME) [7], a method which approximately maximizes Bayesian surprise and currently achieves state-of-the-art performance on continuous control with sparse rewards. We show that our incentives can perform on the level of VIME at a lower computational cost.

2 PRELIMINARIES
We begin by introducing notation which we will use throughout the paper. A Markov decision process (MDP) is a tuple, (S,A,R, P, µ), where S is the set of states, A is the set of actions, R : S × A × S → R is the reward function, P : S × A × S → [0, 1] is the transition probability function (where P (s′|s, a) is the probability of transitioning to state s′ given that the previous state was s and the agent took action a in s), and µ : S → [0, 1] is the starting state distribution. A policy π : S × A → [0, 1] is a distribution over actions per state, with π(a|s) the probability of selecting a in state s. We aim to select a policy π which maximizes a performance measure, L(π), which usually takes the form of expected finite-horizon total return (sum of rewards in a fixed time period), or expected infinite-horizon discounted total return (discounted sum of all rewards forever). In this paper, we use the finite-horizon total return formulation.

3 SURPRISE INCENTIVES
To train an agent with surprise-based exploration, we alternate between making an update step to a dynamics model (an approximator of the MDP’s transition probability function), and making a policy update step that maximizes a trade-off between policy performance and a surprise measure.
The dynamics model step makes progress on the optimization problem
min φ − 1 |D| ∑ (s,a,s′)∈D logPφ(s ′|s, a) + αf(φ), (1)
where D is is a dataset of transition tuples from the environment, Pφ is the model we are learning, f is a regularization function, and α > 0 is a regularization trade-off coefficient. The policy update step makes progress on an approximation to the optimization problem
max π L(π) + η E s,a∼π
[DKL(P ||Pφ)[s, a]] , (2)
where η > 0 is an explore-exploit trade-off coefficient. The exploration incentive in (2), which we select to be the on-policy average KL-divergence of Pφ from P , is intended to capture the agent’s surprise about its experience. The dynamics model Pφ should only be close to P on regions of the transition state space that the agent has already visited (because those transitions will appear in D and thus the model will be fit to them), and as a result, the KL divergence of Pφ and P will be higher in unfamiliar places. Essentially, this exploits the generalization in the model to encourage the agent to go where it has not gone before. The surprise incentive in (2) gives the net effect of performing a reward shaping of the form
r′(s, a, s′) = r(s, a, s′) + η (logP (s′|s, a)− logPφ(s′|s, a)) , (3) where r(s, a, s′) is the original reward and r′(s, a, s′) is the transformed reward, so ideally we could solve (2) by applying any reinforcement learning algorithm with these reshaped rewards. In practice, we cannot directly implement this reward reshaping because P is unknown. Instead, we consider two ways of finding an approximate solution to (2).
In one method, we approximate the KL-divergence by the cross-entropy, which is reasonable when H(P ) is finite (and small) and Pφ is sufficiently far from P 1; that is, denoting the cross-entropy by H(P, Pφ)[s, a] . = Es′∼P (·|s,a)[− logPφ(s′|s, a)], we assume
DKL(P ||Pφ)[s, a] = H(P, Pφ)[s, a]−H(P )[s, a] ≈ H(P, Pφ)[s, a].
(4)
This approximation results in a reward shaping of the form
r′(s, a, s′) = r(s, a, s′)− η logPφ(s′|s, a); (5) here, the intrinsic reward is the surprisal of s′ given the model Pφ and the context (s, a).
In the other method, we maximize a lower bound on the objective in (2) by lower bounding the surprise term:
DKL(P ||Pφ)[s, a] = DKL(P ||Pφ′)[s, a] + E s′∼P
[ log Pφ′(s ′|s, a)
Pφ(s′|s, a) ] ≥ E s′∼P [ log Pφ′(s ′|s, a)
Pφ(s′|s, a)
] .
(6)
The bound (6) results in a reward shaping of the form
r′(s, a, s′) = r(s, a, s′) + η (logPφ′(s ′|s, a)− logPφ(s′|s, a)) , (7)
which requires a choice of φ′. From (6), we can see that the bound becomes tighter by minimizing DKL(P ||Pφ′). As a result, we choose φ′ to be the parameters of the dynamics model after k updates based on (1), and φ to be the parameters from before the updates. Thus, at iteration t, the reshaped rewards are
r′(s, a, s′) = r(s, a, s′) + η ( logPφt(s ′|s, a)− logPφt−k(s′|s, a) ) ; (8)
here, the intrinsic reward is the k-step learning progress at (s, a, s′). It also bears a resemblance to Bayesian surprise; we expand on this similarity in the next section.
In our experiments, we investigate both the surprisal bonus (5) and the k-step learning progress bonus (8) (with varying values of k).
1On the other hand, if H(P )[s, a] is non-finite everywhere—for instance if the MDP has continuous states and deterministic transitions—then as long as it has the same sign everywhere, Es,a∼π[H(P )[s, a]] is a constant with respect to π and we can drop it from the optimization problem anyway.

3.1 DISCUSSION
Ideally, we would like the intrinsic rewards to vanish in the limit as Pφ → P , because in this case, the agent should have sufficiently explored the state space, and should primarily learn from extrinsic rewards. For the proposed intrinsic reward in (5), this is not the case, and it may result in poor performance in that limit. The thinking goes that when Pφ = P , the agent will be incentivized to seek out states with the noisiest transitions. However, we argue that this may not be an issue, because the intrinsic motivation seems mostly useful long before the dynamics model is fully learned. As long as the agent is able to find the extrinsic rewards before the intrinsic reward is just the entropy in P , the pathological noise-seeking behavior should not happen. On the other hand, the intrinsic reward in (8) should not suffer from this pathology, because in the limit, as the dynamics model converges, we should have Pφt ≈ Pφt−k . Then the intrinsic reward will vanish as desired. Next, we relate (8) to Bayesian surprise. The Bayesian surprise associated with a transition is the reduction in uncertainty over possibly dynamics models from observing it ([1],[8]):
DKL (P (φ|ht, at, st+1)||P (φ|ht)) . Here, P (φ|ht) is meant to represent a distribution over possible dynamics models parametrized by φ given the preceding history of observed states and actions ht (so ht includes st), and P (φ|ht, at, st+1) is the posterior distribution over dynamics models after observing (at, st+1). By Bayes’ rule, the dynamics prior and posterior are related to the model-based transition probabilities by
P (φ|ht, at, st+1) = P (φ|ht)P (st+1|ht, at, φ)
Eφ∼P (·|ht) [P (st+1|ht, at, φ)] ,
so the Bayesian surprise can be expressed as E
φ∼Pt+1 [logP (st+1|ht, at, φ)]− log E φ∼Pt [P (st+1|ht, at, φ)] , (9)
where Pt+1 = P (·|ht, at, st+1) is the posterior and Pt = P (·|ht) is the prior. In this form, the resemblance between (9) and (8) is clarified. Although the update from φt−k to φt is not Bayesian— and is performed in batch, instead of per transition sample—we can imagine (8) might contain similar information to (9).

3.2 IMPLEMENTATION DETAILS
Our implementation usesL2 regularization in the dynamics model fitting, and we impose an additional constraint to keep model iterates close in the KL-divergence sense. Denoting the average divergence as
D̄KL(Pφ′ ||Pφ) = 1 |D| ∑
(s,a)∈D
DKL(Pφ′ ||Pφ)[s, a], (10)
our dynamics model update is
φi+1 = arg min φ − 1 |D| ∑ (s,a,s′)∈D logPφ(s ′|s, a) + α‖φ‖22 : D̄KL(Pφ||Pφi) ≤ κ. (11)
The constraint value κ is a hyper-parameter of the algorithm. We solve this optimization problem approximately using a single second-order step with a line search, as described by [20]; full details are given in supplementary material. D is a FIFO replay memory, and at each iteration, instead of using the entirety of D for the update step we sub-sample a batch d ⊂ D. Also, similarly to [7], we adjust the bonus coefficient η at each iteration, to keep the average bonus magnitude upper-bounded (and usually fixed). Let η0 denote the desired average bonus, and r+(s, a, s′) denote the intrinsic reward; then, at each iteration, we set
η = η0 max (
1, 1|B| ∣∣∣∑(s,a,s′)∈B r+(s, a, s′)∣∣∣) , where B is the batch of data used for the policy update step. This normalization improves the stability of the algorithm by keeping the scale of the bonuses fixed with respect to the scale of the extrinsic rewards. Also, in environments where the agent can die, we avoid the possibility of the intrinsic rewards becoming a living cost by translating all bonuses so that the mean is nonnegative. The basic outline of the algorithm is given as Algorithm 1. In all experiments, we use fully-factored Gaussian distributions for the dynamics models, where the means and variances are the outputs of neural networks.
Algorithm 1 Reinforcement Learning with Surprise Incentive Input: Initial policy π0, dynamics model Pφ0 repeat
collect rollouts on current policy πi add rollout (s, a, s′) tuples to replay memory D compute reshaped rewards using (5) or (8) with dynamics model Pφi normalize η by the average intrinsic reward of the current batch of data update policy to πi+1 using any RL algorithm with the reshaped rewards update the dynamics model to Pφi+1 according to (11)
until training is completed

4 EXPERIMENTS
We evaluate our proposed surprise incentives on a wide range of benchmarks that are challenging for naive exploration methods, including continuous control and discrete control tasks. Our continuous control tasks include the slate of sparse reward tasks introduced by Houthooft et al. [7]: sparse MountainCar, sparse CartPoleSwingup, and sparse HalfCheetah, as well as a new sparse reward task that we introduce here: sparse Swimmer. (We refer to these environments with the prefix ‘sparse’ to differentiate them from other versions which appear in the literature, where agents receive non-sparse reward signals.) Additionally, we evaluate performance on a highly-challenging hierarchical sparse reward task introduced by Duan et al [5], SwimmerGather. The discrete action tasks are several games from the Atari RAM domain of the OpenAI Gym [4]: Pong, BankHeist, Freeway, and Venture.
Environments with deterministic and stochastic dynamics are represented in our benchmarks: the continuous control domains have deterministic dynamics, while the Gym Atari RAM games have stochastic dynamics. (In the Atari games, actions are repeated for a random number of frames.)
We use Trust Region Policy Optimization (TRPO) [20], a state-of-the-art policy gradient method, as our base reinforcement learning algorithm throughout our experiments, and we use the rllab implementations of TRPO and the continuous control tasks [5]. Full details for the experimental set-up are included in the appendix.
On all tasks, we compare against TRPO without intrinsic rewards, which we refer to as using naive exploration (in contrast to intrinsically motivated exploration). For the continuous control tasks, we also compare against intrinsic motivation using the L2 model prediction error,
r+(s, a, s ′) = ‖s′ − µφ(s, a)‖2, (12)
where µφ is the mean of the learned Gaussian distribution Pφ. The model prediction error was investigated as intrinsic motivation for deep reinforcement learning by Stadie et al [22], although they used a different method for learning the model µφ. This comparison helps us verify whether or not our proposed form of surprise, as a KL-divergence from the true dynamics model, is useful. Additionally, we compare our performance against the performance reported by Houthooft et al. [7] for Variational Information Maximizing Exploration (VIME), a method where the intrinsic reward associated with a transition approximates its Bayesian surprise using variational methods. Currently, VIME has achieved state-of-the-art results on intrinsic motivation for continuous control.
As a final check for the continuous control tasks, we benchmark the tasks themselves, by measuring the performance of the surprisal bonus without any dynamics learning: r+(s, a, s′) = − logPφ0(s′|s, a), where φ0 are the original random parameters of Pφ. This allows us to verify whether our benchmark tasks actually require surprise to solve at all, or if random exploration strategies successfully solve them.

4.1 CONTINUOUS CONTROL RESULTS
Median performance curves are shown in Figure 1 with interquartile ranges shown in shaded areas. Note that TRPO without intrinsic motivation failed on all tasks: the median score and upper quartile range for naive exploration were zero everywhere. Also note that TRPO with random exploration bonuses failed on most tasks, as shown separately in Figure 2. We found that surprise was not needed to solve MountainCar, but was necessary to perform well on the other tasks.
The surprisal bonus was especially robust across tasks, achieving good results in all domains and substantially exceeding the other baselines on the more challenging ones. The learning progress bonus for k = 1 was successful on CartpoleSwingup and HalfCheetah but it faltered in the others. Its weak performance in MountainCar was due to premature convergence of the dynamics model, which resulted in the agent receiving intrinsic rewards that were identically zero. (Given the simplicity of the environment, it is not surprising that the dynamics model converged so quickly.) In Swimmer, however, it seems that the learning progress bonuses did not inspire sufficient exploration. Because the Swimmer environment is effectively a stepping stone to the harder SwimmerGather, where the agent has to learn a motion primitive and collect target pellets, on SwimmerGather, we only evaluated the intrinsic rewards that had been successful on Swimmer.
Both surprisal and learning progress (with k = 1) exceeded the reported performance of VIME on HalfCheetah by learning to solve the task more quickly. On CartpoleSwingup, however, both were more susceptible to getting stuck in locally optimal policies, resulting in lower median scores than VIME. Surprisal performed comparably to VIME on SwimmerGather, the hardest task in the slate—in the sense that after 1000 iterations, they both reached approximately the same median score—although with greater variance than VIME.
Our results suggest that surprisal is a viable alternative to VIME in terms of performance, and is highly favorable in terms of computational cost. In VIME, a backwards pass through the dynamics model must be computed for every transition tuple separately to compute the intrinsic rewards, whereas our surprisal bonus only requires forward passes through the dynamics model for intrinsic
reward computation. (Limitations of current deep learning tool kits make it difficult to efficiently compute separate backwards passes, whereas almost all of them support highly parallel forward computations.) Furthermore, our dynamics model is substantially simpler than the Bayesian neural network dynamics model of VIME. To illustrate this point, in Figure 3 we show the results of a speed comparison making use of the open-source VIME code [6], with the settings described in the VIME paper. In our speed test, our bonus had a per-iteration speedup of a factor of 3 over VIME.2 We give a full analysis of the potential speedup in Appendix C.

4.2 ATARI RAM DOMAIN RESULTS
Median performance curves are shown in Figure 4, with tasks arranged from (a) to (d) roughly in order of increasing difficulty.
In Pong, naive exploration naturally succeeds, so we are not surprised to see that intrinsic motivation does not improve performance. However, this serves as a sanity check to verify that our intrinsic rewards do not degrade performance. (As an aside, we note that the performance here falls short of the standard score of 20 for this domain because we truncate play at 5000 timesteps.)
In BankHeist, we find that intrinsic motivation accelerates the learning significantly. The agents with surprisal incentives reached high levels of performance (scores > 1000) 10% sooner than naive exploration, while agents with learning progress incentives reached high levels almost 20% sooner.
In Freeway, the median performance for TRPO without intrinsic motivation was adequate, but the lower quartile range was quite poor—only 6 out of 10 runs ever found rewards. With the learning progress incentives, 8 out of 10 runs found rewards; with the surprisal incentive, all 10 did. Freeway is a game with very sparse rewards, where the agent effectively has to cross a long hallway before it can score a point, so naive exploration tends to exhibit random walk behavior and only rarely reaches the reward state. The intrinsic motivation helps the agent explore more purposefully.
2We compute this by comparing the marginal time cost incurred just by the bonus in each case: that is, if Tvime, Tsurprisal, and Tnobonus denote the times to 15 iterations, we obtain the speedup as
Tvime − Tnobonus Tsurprisal − Tnobonus .
In Venture, we obtain our strongest results in the Atari domain. Venture is extremely difficult because the agent has to navigate a large map to find very sparse rewards, and the agent can be killed by enemies interspersed throughout. We found that our intrinsic rewards were able to substantially improve performance over naive exploration in this challenging environment. Here, the best performance was again obtained by the surprisal incentive, which usually inspired the agent to reach scores greater than 500.

4.3 COMPARING INCENTIVES
Among our proposed incentives, we found that surprisal worked the best overall, achieving the most consistent performance across tasks. The learning progress-based incentives worked well on some domains, but generally not as well as surprisal. Interestingly, learning progress with k = 10 performed much worse on the continuous control tasks than with k = 1, but we observed virtually no difference in their performance on the Atari games; it is unclear why this should be the case.
Surprisal strongly outperformed the L2 error based incentive on the harder continuous control tasks, learning to solve them more quickly and without forgetting. Because we used fully-factored Gaussians for all of our dyanmics models, the surprisal had the form
− logPφ(s′|s, a) = n∑ i=1
( (s′i − µφ,i(s, a))2
2σ2φ,i(s, a) + log σφ,i(s, a)
) + k
2 log 2π,
which essentially includes the L2-squared error norm as a sub-expression. The relative difference in performance suggests that the variance terms confer additional useful information about the novelty of a state-action pair.

5 RELATED WORK
Substantial theoretical work has been done on optimal exploration in finite MDPs, resulting in algorithms such as E3 [10], R-max [3], and UCRL [9], which scale polynomially with MDP size. However, these works do not permit obvious generalizations to MDPs with continuous state and action spaces. C-PACE [18] provides a theoretical foundation for PAC-optimal exploration in MDPs with continuous state spaces, but it requires a metric on state spaces. Lopes et al. [11] investigated exploration driven by learning progress and proved theoretical guarantees for their approach in the finite MDP case, but they did not address the question of scaling their approach to continuous or high-dimensional MDPs. Also, although they formulated learning progress in the same way as (8), they formed intrinsic rewards differently. Conceptually and mathematically, our work is closest to prior work on curiosity and surprise [8, 19, 23, 24], although these works focus mainly on small finite MDPs.
Recently, several intrinsic motivation strategies that deal specifically with deep reinforcement learning have been proposed. Stadie et al. [22] learn deterministic dynamics models by minimizing Euclidean loss—whereas in our work, we learn stochastic dynamics with cross entropy loss—and use L2 prediction errors for intrinsic motivation. Houthooft et al. [7] train Bayesian neural networks to approximate posterior distributions over dynamics models given observed data, by maximizing a variational lower bound; they then use second-order approximations of the Bayesian surprise as intrinsic motivation. Bellemare et al. [2] derived pseudo-counts from CTS density models over states and used those to form intrinsic rewards, notably resulting in dramatic performance improvement on Montezuma’s Revenge, one of the hardest games in the Atari domain. Mohamed and Rezende [14] developed a scalable method of approximating empowerment, the mutual information between an agent’s actions and the future state of the environment, using variational methods. Oh et al. [16] estimated state visit frequency using Gaussian kernels to compare against a replay memory, and used these estimates for directed exploration.

6 CONCLUSIONS
In this work, we formulated surprise for intrinsic motivation as the KL-divergence of the true transition probabilities from learned model probabilities, and derived two approximations—surprisal and k-step
learning progress—that are scalable, computationally inexpensive, and suitable for application to high-dimensional and continuous control tasks. We showed that empirically, motivation by surprisal and 1-step learning progress resulted in efficient exploration on several hard deep reinforcement learning benchmarks. In particular, we found that surprisal was a robust and effective intrinsic motivator, outperforming other heuristics on a wide range of tasks, and competitive with the current state-of-the-art for intrinsic motivation in continuous control.

ACKNOWLEDGEMENTS
We thank Rein Houthooft for interesting discussions and for sharing data from the original VIME experiments. We also thank Rocky Duan, Carlos Florensa, Vicenc Rubies-Royo, Dexter Scobee, and Eric Mazumdar for insightful discussions and reviews of the preliminary manuscript.
This work is supported by TRUST (Team for Research in Ubiquitous Secure Technology) which receives support from NSF (award number CCF-0424422).

A SINGLE STEP SECOND-ORDER OPTIMIZATION
In our experiments, we approximately solve several optimization problems by using a single secondorder step with a line search. This section will describe the exact methodology, which was originally given by Schulman et al. [20].
We consider the optimization problem
p∗ = max θ L(θ) : D(θ) ≤ δ, (13)
where θ ∈ Rn, and for some θold we have D(θold) = 0,∇θD(θold) = 0, and∇2θD(θold) 0; also, ∀θ,D(θ) ≥ 0. We suppose that δ is small, so the optimal point will be close to θold. We also suppose that the curvature of the constraint is much greater than the curvature of the objective. As a result, we feel justified in approximating the objective to linear order and the constraint to quadratic order:
L(θ) ≈ L(θold) + gT (θ − θold) g . = ∇θL(θold)
D(θ) ≈ 1 2 (θ − θold)TA(θ − θold) A . = ∇2θD(θold).
We now consider the approximate optimization problem,
p∗ ≈ max θ gT (θ − θold) :
1 2 (θ − θold)TA(θ − θold) ≤ δ.
This optimization problem is convex as long as A 0, which is an assumption that we make. (If this assumption seems to be empirically invalid, then we repair the issue by using the substitution A→ A+ I , where I is the identity matrix, and > 0 is a small constant chosen so that we usually have A+ I 0.) This problem can be solved analytically by applying methods of duality, and its optimal point is
θ∗ = θold +
√ 2δ
gTA−1g A−1g. (14)
It is possible that the parameter update step given by (14) may not exactly solve the original optimization problem (13)—in fact, it may not even satisfy the constraint—so we perform a line search between θold and θ∗. Our update with the line search included is given by
θ = θold + s k
√ 2δ
gTA−1g A−1g, (15)
where s ∈ (0, 1) is a backtracking coefficient, and k is the smallest integer for which L(θ) ≥ L(θold) and D(θ) ≤ δ. We select k by checking each of k = 1, 2, ...,K, where K is the maximum number of backtracks. If there is no value of k in that range which satisfies the conditions, no update is performed.
Because the optimization problems we solve with this method tend to involve thousands of parameters, inverting A is prohibitively computationally expensive. Thus in the implementation of this algorithm that we use, the search direction x = A−1g is found by using the conjugate gradient method to solve Ax = g; this avoids the need to invert A.
When A and g are sample averages meant to stand in for expectations, we employ an additional trick to reduce the total number of computations necessary to solve Ax = g. The computation of A is more expensive than g, and so we use a smaller fraction of the population to estimate it quickly. Concretely, suppose that the original optimization problem’s objective is Ez∼P [L(θ, z)], and the constraint is Ez∼P [D(θ, z)] ≤ δ, where z is some random variable and P is its distribution; furthermore, suppose that we have a dataset of samples D = {zi}i=1,...,N drawn on P , and we form an approximate optimization problem using these samples. Defining g(z) .= ∇θL(θold, z) and A(z)
. = ∇2θD(θold, z), we would need to solve(
1 |D| ∑ z∈D A(z)
) x = 1
|D| ∑ z∈D g(z)
to obtain the search direction x. However, because the computation of the average Hessian is expensive, we sub-sample a batch b ⊂ D to form it. As long as b is a large enough set, then the approximation
1 |b| ∑ z∈b A(z) ≈ 1 |D| ∑ z∈D A(z) ≈ E z∼P [A(z)]
is good, and the search direction we obtain by solving( 1
|b| ∑ z∈b A(z)
) x = 1
|D| ∑ z∈D g(z)
is reasonable. The sub-sample ratio |b|/|D| is a hyperparameter of the algorithm.

B EXPERIMENT DETAILS
B.1 ENVIRONMENTS
The environments have the following state and action spaces: for the sparse MountainCar environment, S ⊆ R2, A ⊆ R1; for the sparse CartpoleSwingup task, S ⊆ R4, A ⊆ R1; for the sparse HalfCheetah
task, S ⊂ R20, A ⊆ R6; for the sparse Swimmer task, S ⊆ R13, A ⊆ R2; for the SwimmerGather task, S ⊆ R33, A ⊆ R2; for the Atari RAM domain, S ⊆ R128, A ⊆ {1, ..., 18}. For the sparse MountainCar task, the agent receives a reward of 1 only when it escapes the valley. For the sparse CartpoleSwingup task, the agent receives a reward of 1 only when cos(β) > 0.8, with β the pole angle. For the sparse HalfCheetah task, the agent receives a reward of 1 when xbody ≥ 5. For the sparse Swimmer task, the agent receives a reward of 1 + |vbody| when |xbody| ≥ 2. Atari RAM states, by default, take on values from 0 to 256 in integer intervals. We use a simple preprocessing step to map them onto values in (−1/3, 1/3). Let x denote the raw RAM state, and s the preprocessed RAM state:
s = 1
3 ( x 128 − 1 ) .
B.2 POLICY AND VALUE FUNCTIONS
For all continuous control tasks we used fully-factored Gaussian policies, where the means of the action distributions were the outputs of neural networks, and the variances were separate trainable parameters. For the sparse MountainCar and sparse CartpoleSwingup tasks, the policy mean networks had a single hidden layer of 32 units. For sparse HalfCheetah, sparse Swimmer, and SwimmerGather, the policy mean networks were of size (64, 32). For the Atari RAM tasks, we used categorical distributions over actions, produced by neural networks of size (64, 32).
The value functions used for the sparse MountainCar and sparse CartpoleSwingup tasks were neural networks with a single hidden layer of 32 units. For sparse HalfCheetah, sparse Swimmer, and SwimmerGather, time-varying linear value functions were used, as described by Duan et al. [5]. For the Atari RAM tasks, the value functions were neural networks of size (64, 32). The neural network value functions were learned via single second-order step optimization; the linear baselines were obtained by least-squares fit at each iteration.
All neural networks were feed-forward, fully-connected networks with tanh activation units.
B.3 TRPO HYPERPARAMETERS
For all tasks, the MDP discount factor γ was fixed to 0.995, and generalized advantage estimators (GAE) [21] were used, with the GAE λ parameter fixed to 0.95.
In the table below, we show several other TRPO hyperparameters. Batch size refers to steps of experience collected at each iteration. The sub-sample factor is for the second-order optimization step, as detailed in Appendix A.
B.4 EXPLORATION HYPERPARAMETERS
For all tasks, fully-factored Gaussian distributions were used as dynamics models, where the means and variances of the distributions were the outputs of neural networks.
For the sparse MountainCar and sparse CartpoleSwingup tasks, the means and variances were parametrized by single hidden layer neural networks with 32 units. For all other tasks, the means and variances were parametrized by neural networks with two hidden layers of size 64 units each. All networks used tanh activation functions.
For all continuous control tasks except SwimmerGather, we used replay memories of size 5, 000, 000, and a KL-divergence step size of κ = 0.001. For SwimmerGather, the replay memory was the same size, but we set the KL-divergence size to κ = 0.005. For the Atari RAM domain tasks, we used replay memories of size 1, 000, 000, and a KL-divergence step size of κ = 0.01.
For all tasks except SwimmerGather and Venture, 5000 time steps of experience were sampled from the replay memory at each iteration of dynamics model learning to take a stochastic step on (11), and a sub-sample factor of 1 was used in the second-order step optimizer. For SwimmerGather and Venture, 10, 000 time steps of experience were sampled at each iteration, and a sub-sample factor of 0.5 was used in the optimizer.
For all continuous control tasks, the L2 penalty coefficient was set to α = 1. For the Atari RAM tasks except for Venture, it was set to α = 0.01. For Venture, it was set to α = 0.1.
For all continuous control tasks except SwimmerGather, η0 = 0.001. For SwimmerGather, η0 = 0.0001. For the Atari RAM tasks, η0 = 0.005.

C ANALYSIS OF SPEEDUP COMPARED TO VIME
In this section, we provide an analysis of the time cost incurred by using VIME or our bonuses, and derive the potential magnitude of speedup attained by our bonuses versus VIME.
At each iteration, bonuses based on learned dynamics models incur two primary costs:
• the time cost of fitting the dynamics model, • and the time cost of computing the rewards.
We denote the dynamics fitting costs for VIME and our methods as T fitvime and T fit ours. Although the Bayesian neural network dynamics model for VIME is more complex than our model, the fit times can work out to be similar depending on the choice of fitting algorithm. In our speed test, the fit times were nearly equivalent, but used different algorithms.
For the time cost of computing rewards, we first introduce the following quantities:
• n: the number of CPU threads available, • tf : time for a forward pass through the model, • tb: time for a backward pass through the model, • N : batch size (number of samples per iteration), • k: the number of forward passes that can be performed simultaneously.
For our method, the time cost of computing rewards is
T rewours = Ntf kn .
For VIME, things are more complex. Each reward requires the computation of a gradient through its model, which necessitates a forward and a backward pass. Because gradient calculations cannot be efficiently parallelized by any deep learning toolkits currently available3, each (s, a, s′) tuple requires its own forward/backward pass. As a result, the time cost of computing rewards for VIME is:
T rewvime = N(tf + tb)
n .
The speedup of our method over VIME is therefore
T fitvime + N(tf+tb) n
T fitours + Ntf kn
.
In the limit of large N , and with the approximation that tf ≈ tb, the speedup is a factor of ∼ 2k. 3If this is not correct, please contact the authors so that we can issue a correction! But to the best of our knowledge, this is currently true, at time of publication.
","Exploration in complex domains is a key challenge in reinforcement learning, especially for tasks with very sparse rewards. Recent successes in deep reinforcement learning have been achieved mostly using simple heuristic exploration strategies such as -greedy action selection or Gaussian control noise, but there are many tasks where these methods are insufficient to make any learning progress. Here, we consider more complex heuristics: efficient and scalable exploration strategies that maximize a notion of an agent’s surprise about its experiences via intrinsic motivation. We propose to learn a model of the MDP transition probabilities concurrently with the policy, and to form intrinsic rewards that approximate the KL-divergence of the true transition probabilities from the learned model. One of our approximations results in using surprisal as intrinsic motivation, while the other gives the k-step learning progress. We show that our incentives enable agents to succeed in a wide range of environments with high-dimensional state spaces and very sparse rewards, including continuous control tasks and games in the Atari RAM domain, outperforming several other heuristic exploration techniques.",ICLR 2017 conference submission,False,,"This paper explores the topic of intrinsic motivation in the context of deep RL. It proposes a couple of variants derived from an auxiliary model-learning process (prediction error, surprise and learning progress), and shows that those can help exploration on a number of continuous control tasks (and the Atari game “venture”, maybe).

Novelty: none of the proposed types of intrinsic motivation are novel, and it’s arguable whether the application to deep RL is novel (see e.g. Kompella et al 2012).

Potential: the idea of seeking out states where a transition model is uncertain is sensible, but also limited -- I would encourage the authors to also discuss the limitations. For example in a game like Go the transition model is trivially learned, so this approach would revert to random exploration. So other forms of learning progress or surprise derived from the agent’s competence instead might be more promising in the long run? See also Srivastava et al 2012 for further thoughts.

Computation time: I find the paper’s claimed superiority over VIME to be overblown: the gain seems to stem almost exclusively from a faster initialization, but have very similar per-step cost? So given that VIME is also performing very competitively, what arguments can you advance for your own method(s)?

---

The paper proposes an intuitive method for exploration, namely to build a model of the system dynamics and explore regions where this approximation differs from the observed data (i.e., how ""surprised"" the agent was by an observation). The idea is a nice one, and part of the benefit comes from the simplicity and wide applicability of the approach.
 
 The main drawback of this paper is simply that the resulting approach doesn't substantially outperform existing approaches, at least not to a degree where it seem like the paper should should be clearly accepted to ICLR. On the continuous control tasks, the advantage over VIME seems very unclear (at best the results are mixed, showing sometimes surprisal and sometime VIME do better), and on the Atari games no comparison is made against many of the methods tuned for this setting, such as Gorila (Nair, 2015) which achieves some of the best results we are aware of on the Venture game, which is definitely the strongest result in this current paper, but not as good as this previous work. We know the settings are different, but overall it seems like the approach is largely outperformed by existing approaches, and thus the advantage mainly comes from runtime. This is certainly an interesting take, but needs to be studied a lot more thoroughly before it would make a really compelling case. We would like to recomend this paper to the workshop track. The pros/cons are as follows:
 
 Pros:
 + Simple and intuitive method for exploration
 
 Cons:
 - Doesn't seem to substantially outperform existing methods
 - No comparison to many alternative approaches for some of the ""better"" results in the paper.

---

The authors present a novel approach to surprise-based intrinsic motivation in deep reinforcement learning. The authors clearly explain the difference from other recent approaches to intrinsic motivation and back up their method with results from a broad class of discrete and continuous action domains. They present two tractable approximations to their framework - one which ignores the stochasticity of the true environmental dynamics, and one which approximates the rate of information gain (somewhat similar to Schmidhuber's formal theory of creativity, fun and intrinsic motivation). The results of this exploration bonus when added to TRPO are generally better than standard TRPO. However, I would have appreciated a more thorough comparison against other recent work on intrinsic motivation. For instance, Bellemare et al 2016 recently achieved significant performance gains on challenging Atari games like Montezuma's Revenge by combining DQN with an exploration bonus, however Montezuma's Revenge is not presented as an experiment here. Such comparisons would significantly improve the strength of the paper.

---

This paper provides a surprise-based intrinsic reward method for reinforcement learning, along with two practical algorithms for estimating those rewards. The ideas are similar to previous work in intrinsic motivation (including VIME and other work in intrinsic motivation). 
As a positive, the methods are simple to implement, and provide benefits on a number of tasks.
However, they are almost always outmatched by VIME, and not one of their proposed method is consistently the best of those proposed (perhaps the most consistent is the surprisal, which is unfortunately not asymptotically equal to the true reward). The authors claim massive speed up, but the numerical measurements show that VIME is slower to initialize but not significantly slower per iteration otherwise (perhaps a big O analysis would clarify the claims).
Overall it's a decent, simple technique, perhaps slightly incremental on previous state of the art.

---

This paper explores the topic of intrinsic motivation in the context of deep RL. It proposes a couple of variants derived from an auxiliary model-learning process (prediction error, surprise and learning progress), and shows that those can help exploration on a number of continuous control tasks (and the Atari game “venture”, maybe).

Novelty: none of the proposed types of intrinsic motivation are novel, and it’s arguable whether the application to deep RL is novel (see e.g. Kompella et al 2012).

Potential: the idea of seeking out states where a transition model is uncertain is sensible, but also limited -- I would encourage the authors to also discuss the limitations. For example in a game like Go the transition model is trivially learned, so this approach would revert to random exploration. So other forms of learning progress or surprise derived from the agent’s competence instead might be more promising in the long run? See also Srivastava et al 2012 for further thoughts.

Computation time: I find the paper’s claimed superiority over VIME to be overblown: the gain seems to stem almost exclusively from a faster initialization, but have very similar per-step cost? So given that VIME is also performing very competitively, what arguments can you advance for your own method(s)?

---

This paper explores the topic of intrinsic motivation in the context of deep RL. It proposes a couple of variants derived from an auxiliary model-learning process (prediction error, surprise and learning progress), and shows that those can help exploration on a number of continuous control tasks (and the Atari game “venture”, maybe).

Novelty: none of the proposed types of intrinsic motivation are novel, and it’s arguable whether the application to deep RL is novel (see e.g. Kompella et al 2012).

Potential: the idea of seeking out states where a transition model is uncertain is sensible, but also limited -- I would encourage the authors to also discuss the limitations. For example in a game like Go the transition model is trivially learned, so this approach would revert to random exploration. So other forms of learning progress or surprise derived from the agent’s competence instead might be more promising in the long run? See also Srivastava et al 2012 for further thoughts.

Computation time: I find the paper’s claimed superiority over VIME to be overblown: the gain seems to stem almost exclusively from a faster initialization, but have very similar per-step cost? So given that VIME is also performing very competitively, what arguments can you advance for your own method(s)?

---

The paper proposes an intuitive method for exploration, namely to build a model of the system dynamics and explore regions where this approximation differs from the observed data (i.e., how ""surprised"" the agent was by an observation). The idea is a nice one, and part of the benefit comes from the simplicity and wide applicability of the approach.
 
 The main drawback of this paper is simply that the resulting approach doesn't substantially outperform existing approaches, at least not to a degree where it seem like the paper should should be clearly accepted to ICLR. On the continuous control tasks, the advantage over VIME seems very unclear (at best the results are mixed, showing sometimes surprisal and sometime VIME do better), and on the Atari games no comparison is made against many of the methods tuned for this setting, such as Gorila (Nair, 2015) which achieves some of the best results we are aware of on the Venture game, which is definitely the strongest result in this current paper, but not as good as this previous work. We know the settings are different, but overall it seems like the approach is largely outperformed by existing approaches, and thus the advantage mainly comes from runtime. This is certainly an interesting take, but needs to be studied a lot more thoroughly before it would make a really compelling case. We would like to recomend this paper to the workshop track. The pros/cons are as follows:
 
 Pros:
 + Simple and intuitive method for exploration
 
 Cons:
 - Doesn't seem to substantially outperform existing methods
 - No comparison to many alternative approaches for some of the ""better"" results in the paper.

---

The authors present a novel approach to surprise-based intrinsic motivation in deep reinforcement learning. The authors clearly explain the difference from other recent approaches to intrinsic motivation and back up their method with results from a broad class of discrete and continuous action domains. They present two tractable approximations to their framework - one which ignores the stochasticity of the true environmental dynamics, and one which approximates the rate of information gain (somewhat similar to Schmidhuber's formal theory of creativity, fun and intrinsic motivation). The results of this exploration bonus when added to TRPO are generally better than standard TRPO. However, I would have appreciated a more thorough comparison against other recent work on intrinsic motivation. For instance, Bellemare et al 2016 recently achieved significant performance gains on challenging Atari games like Montezuma's Revenge by combining DQN with an exploration bonus, however Montezuma's Revenge is not presented as an experiment here. Such comparisons would significantly improve the strength of the paper.

---

This paper provides a surprise-based intrinsic reward method for reinforcement learning, along with two practical algorithms for estimating those rewards. The ideas are similar to previous work in intrinsic motivation (including VIME and other work in intrinsic motivation). 
As a positive, the methods are simple to implement, and provide benefits on a number of tasks.
However, they are almost always outmatched by VIME, and not one of their proposed method is consistently the best of those proposed (perhaps the most consistent is the surprisal, which is unfortunately not asymptotically equal to the true reward). The authors claim massive speed up, but the numerical measurements show that VIME is slower to initialize but not significantly slower per iteration otherwise (perhaps a big O analysis would clarify the claims).
Overall it's a decent, simple technique, perhaps slightly incremental on previous state of the art.

---

This paper explores the topic of intrinsic motivation in the context of deep RL. It proposes a couple of variants derived from an auxiliary model-learning process (prediction error, surprise and learning progress), and shows that those can help exploration on a number of continuous control tasks (and the Atari game “venture”, maybe).

Novelty: none of the proposed types of intrinsic motivation are novel, and it’s arguable whether the application to deep RL is novel (see e.g. Kompella et al 2012).

Potential: the idea of seeking out states where a transition model is uncertain is sensible, but also limited -- I would encourage the authors to also discuss the limitations. For example in a game like Go the transition model is trivially learned, so this approach would revert to random exploration. So other forms of learning progress or surprise derived from the agent’s competence instead might be more promising in the long run? See also Srivastava et al 2012 for further thoughts.

Computation time: I find the paper’s claimed superiority over VIME to be overblown: the gain seems to stem almost exclusively from a faster initialization, but have very similar per-step cost? So given that VIME is also performing very competitively, what arguments can you advance for your own method(s)?",,,,,,6.0,,,3.3333333333333335,,
537,"RENDERGAN: GENERATING REALISTIC LABELED DATA
Authors: Leon Sixt, Benjamin Wild, Tim Landgraf
Source file: 537.pdf

ABSTRACT
Deep Convolutional Neuronal Networks (DCNNs) are showing remarkable performance on many computer vision tasks. Due to their large parameter space, they require many labeled samples when trained in a supervised setting. The costs of annotating data manually can render the use of DCNNs infeasible. We present a novel framework called RenderGAN that can generate large amounts of realistic, labeled images by combining a 3D model and the Generative Adversarial Network framework. In our approach, image augmentations (e.g. lighting, background, and detail) are learned from unlabeled data such that the generated images are strikingly realistic while preserving the labels known from the 3D model. We apply the RenderGAN framework to generate images of barcode-like markers that are attached to honeybees. Training a DCNN on data generated by the RenderGAN yields considerably better performance than training it on various baselines.

Deep Convolutional Neuronal Networks (DCNNs) are showing remarkable performance on many computer vision tasks. Due to their large parameter space, they require many labeled samples when trained in a supervised setting. The costs of annotating data manually can render the use of DCNNs infeasible. We present a novel framework called RenderGAN that can generate large amounts of realistic, labeled images by combining a 3D model and the Generative Adversarial Network framework. In our approach, image augmentations (e.g. lighting, background, and detail) are learned from unlabeled data such that the generated images are strikingly realistic while preserving the labels known from the 3D model. We apply the RenderGAN framework to generate images of barcode-like markers that are attached to honeybees. Training a DCNN on data generated by the RenderGAN yields considerably better performance than training it on various baselines.

1 INTRODUCTION
When an image is taken from a real world scene, many factors determine the final appearance: background, lighting, object shape, position and orientation of the object, the noise of the camera sensor, and more. In computer vision, high-level information such as class, shape, or pose is reconstructed from raw image data. Most real-world applications require the reconstruction to be invariant to noise, background, and lighting changes.
In recent years, deep convolutional neural networks (DCNNs) advanced to the state of the art in many computer vision tasks (Krizhevsky et al., 2012; He et al., 2015; Razavian et al., 2014). More training data usually increases the performance of DCNNs. While image data is mostly abundant, labels for supervised training must often be created manually – a time-consuming and tedious activity. For complex annotations such as human joint angles, camera viewpoint or image segmentation, the costs of labeling can be prohibitive.
In this paper, we propose a method to drastically reduce the costs of labeling such that we can train a model to predict even complex sets of labels. We present a generative model that can sample from the joint distribution of labels and data. The training procedure of our model does not require any manual labeling. We show that the generated data is of high quality and can be used to train a model in a supervised setting, i.e. a model that maps from real samples to labels, without using any manually labeled samples.
We propose two modifications to the recently introduced GAN framework (Goodfellow et al., 2014). First, a simple 3D model is embedded into the generator network to produce samples from corresponding input labels. Second, the generator learns to add missing image characteristics to the model output using a number of parameterized augmentation functions. In the adversarial training we leverage large amounts of unlabeled image data to learn the particular form of blur, lighting, background and image detail. By constraining the augmentation functions we ensure that the resulting image still represents the given set of labels. The resulting images are hard to distinguish from real samples and can be used to train a DCNN to predict the labels from real input data.
The RenderGAN framework was developed to solve the scarcity of labeled data in the BeesBook project (Wario et al., 2015) in which we analyze the social behavior of honeybees. A barcode-like marker is attached to the honeybees’ backs for identification (see Fig. 1). Annotating this data is tedious, and therefore only a limited amount of labeled data exists. A 3D model (see the upper row of Fig. 2) generates a simple image of the tag based on position, orientation, and bit configuration. The RenderGAN then learns from unlabeled data to add lighting, background, and image details.
Training a DCNN on data generated by the RenderGAN yields considerably better performance compared to various baselines. We furthermore include a previously used computer vision pipeline in the evaluation. The networks’ detections are used as feature to track the honeybees over time. When we use detections from the DCNN instead of the computer vision pipeline, the accuracy of assigning the true id increases from 55% to 96%.
Our contributions are as follows. We present an extension of the GAN framework that allows to sample from the joint distribution of data and labels. The generated samples are nearly indistinguishable from real data for a human observer and can be used to train a DCNN end-to-end to classify real samples. In a real-world use case, our approach significantly outperforms several baselines. Our approach requires no manual labeling. The simple 3D model is the only form of supervision.

2 RELATED WORK
There exists multiple approaches to reduce the costs associated with labeling.
A common approach to deal with limited amount of labels is data augmentation (Goodfellow et al., 2016, Chapter 7.4). Translation, noise, and other deformations can often be applied without changing the labels, thereby effectively increasing the number of training samples and reducing overfitting.
DCNNs learn a hierarchy of features – many of which are applicable to related domains (Yosinski et al., 2014). Therefore, a common technique is to pre-train a model on a larger dataset such as ImageNet (Deng et al., 2009) and then fine-tune its parameters to the task at hand (Girshick et al., 2014; Long et al., 2015; Razavian et al., 2014). This technique only works in cases where a large enough related dataset exists. Furthermore, labeling enough data for fine-tuning might still be costly.
If a basic model of the data exists (for example, a 3D model of the human body), it can be used to generate labeled data. Peng et al. (2015) generated training data for a DCNN with 3D-CAD models.
Su et al. (2015) used 3D-CAD models from large online repositories to generate large amounts of training images for the viewpoint estimation task on the PASCAL 3D+ dataset (Xiang et al., 2014). Massa et al. (2015) are matching natural images to 3D-CAD models with features extracted from a DCNN. Richter et al. (2016) and Ros et al. (2016) used 3D game engines to collect labeled data for image segmentation. However, the explicit modeling of the image acquisition physics (scene lighting, reflections, lense distortions, sensor noise, etc.) is cumbersome and might still not be able to fully reproduce the particularities of the imaging process such as unstructured background or object specific noise. Training a DCNN on generated data that misses certain features will result in overfitting and poor performance on the real data.
Generative Adversarial Networks (GAN) (see Fig. 3) can learn to generate high-quality samples (Goodfellow et al., 2014), i.e. sample from the data distribution p(x). Denton et al. (2015) synthesized images with a GAN on the CIFAR dataset (Krizhevsky, 2009), which were hard for humans to distinguish from real images. While a GAN implicitly learns a meaningful latent embedding of the data (Radford et al., 2015), there is no simple relationship between the latent dimensions and the labels of interest. Therefore, high-level information can’t be inferred from generated samples. cGANs are an extension of GANs to sample from a conditional distribution given some labels, i.e. p(x|l). However, training cGANs requires a labeled dataset. Springenberg (2015) showed that GANs can be used in a semi-supervised setting but restricted their analysis to categorical labels. Wang & Gupta (2016) trained two separate GANs, one to model the object normals and another one for the texture conditioned on the normals. As they rely on conditional GANs, they need large amounts of labeled data. Chen et al. (2016) used an information theoretic to disentangle the representation. They decomposed the representation into a structured and unstructured part. And successfully related on a qualitative level the structured part to high-level concepts such as camera viewpoint or hair style. However, explicitly controlling the relationship between the latent space and generated samples without using labeled data is an open problem, i.e. sampling from p(x, l) without requiring labels for training.

3 RENDERGAN
Most supervised learning tasks can be modeled as a regression problem, i.e. approximating a function f̂ : Rn 7→ L that maps from data space R to label space L. We consider f̂ to be the best available function on this particular task. Analogous to ground truth data, one could call f̂ the ground truth function.
In the RenderGAN framework, we aim to solve the inverse problem to this regression task: generate data given the labels. This is achieved by embedding a simple 3D model into the generator of a GAN. The samples generated by the simple model must correspond to the given labels but may lack many factors of the real data such as background or lightning. Through a cascade of augmentation functions, the generator can adapt the images from the 3D model to match the real data.
We formalize image augmentation as a function φ(x, d), which modifies the image x based on the augmentation parameter d (a tensor of any rank). The augmentation must preserve the labels of the image x. Therefore, it must hold for all images x and all augmentations parameters d:
f̂ (φ(x, d)) = f̂(x) (1)
The augmentation function must furthermore be differentiable w.r.t. x and d as the gradient will be back-propagated through φ to the generator. Image augmentations such as lighting, surrounding, and noise do preserve the labels and fit this definition. We will provide appropriate definitions of φ for the mentioned augmentations in the following section.
If appropriate augmentation functions are found that can model the missing factors and are differentiable, we can use the GAN framework to find parameters that result in realistic output images. Multiple augmentation functions can be combined to perform a more complex augmentation. Here, we will consider multiple augmentation functions applied sequentially, i.e. we have k augmentation functions φi and k corresponding outputs Gi from the generator. The output of the previous augmentation function is the input to the next one. Thus, we can write the generator given some labels l as:
g(z, l) = φk(φk−1(. . . φ0(M(l), G0(z)) . . . , Gk−1(z)), Gk(z)) (2)
where M is the 3D model. We can furthermore learn the label distribution with the generator. As the discriminator loss must be backpropagated through the 3D model M, it must be differentiable. This can be achieved by emulating the 3D model with a neural network (Dosovitskiy et al., 2015). The resulting generator g(z) can be written as (see Fig. 4 for a visual interpretation):
g(z) = φk(φk−1(. . . φ0(M(Gl(z)), G0(z)) . . . , Gk−1(z)), Gk(z)) (3)
As any differentiable function approximator can be employed in the GAN framework, the theoretical properties still hold. The training is carried out as in the conventional GAN framework. In a real application, the augmentation functions might restrict the generator from converging to the data distribution.
If the training converges, we can collect generated realistic data with g(z) and the high-level information captured in the 3D model with Gl(z). We can now train a supervised learning algorithm on the labeled generated data (Gl (z) , g (z)) and solve the regression task of approximating f̂ without depending on manual labels.

4 APPLICATION TO THE BEESBOOK PROJECT
In the BeesBook project, we aim to understand the complex social behavior of honey bees. For identification, a tag with a binary code is attached to the back of the bees.
The orientations in space, position, and bits of the tags are required to track the bees over time. Decoding this information is not trivial: the bees are oriented in any direction in space. The tag might be partially occluded. Moreover, spotlights on the tag can sometimes even confuse humans. A previously used computer vision pipeline did not perform reliably. Although we invested a substantial amount of time on labeling, a DCNN trained on this data did not perform significantly better (see Sec. 3). We therefore wanted to synthesize labeled images which are realistic enough to train an improved decoder network.
Following the idea outlined in section 3, we created a simple 3D model of a bee marker. The 3D model comprises a mesh which represents the structure of the marker and a simple camera model to project the mesh to the image plane. The model is parameterized by its position, its pitch, yaw and roll, and its ID. Given a parameter set, we obtain a marker image, a background segmentation mask
and a depth map. The generated images lack many important factors: blur, lighting, background, and image detail (see Fig. 2). A DCNN trained on this data does not generalize well (see Sec. 5).
Over the last years we collected a large amount of unlabeled image data. We successfully augmented the 3D model using this dataset, as described below.
We trained a neural network to emulate the 3D model. Its outputs are indistinguishable from the images of the 3D model. The discriminator error can now be backpropagated through the 3D model which allows the generator to also learn the distributions of positions and orientations of the bee marker. The IDs are sampled uniformly during training. The weights of the 3D model network are fixed during the RenderGAN training.
We apply different augmentation functions that account for blur, lighting, background, and image detail. The output of the 3D model and of each augmentation function is of shape (64, 64) and in the range [−1, 1]. In Fig. 5, the structure of the generator is shown. Blurriness: The 3D model produces hard edges, but the images of the real tags show a broad range of blur. The generator produces a scalar α ∈ [0, 1] per image that controls the blur.
φblur(x, α) = (1− α) (x− bσ (x)) + bσ(x) (4)
where bσ(x) = x ∗ kσ denotes convolving the image x with a Gaussian kernel kσ of scale σ. The implementation of the blur function is inspired by Laplacian pyramids (Burt & Adelson, 1983). As required for augmentation functions, the labels are preserved, because we limit the maximum amount of blur by picking σ = 2. φblur is also differentiable w.r.t the inputs α and x.
Lighting of the tag: The images from the 3D model are binary. In real images, tags exhibit different shades of gray. We model the lighting by a smooth scaling and shifting of the pixel intensities. The generator provides three outputs for the lighting: scaling of black parts sb, scaling of white parts sw and a shift t. All outputs have the same dimensions as the image x. An important invariant is that the black bits of the tag must stay darker than the white bits. Otherwise, a bit could flip, and the label would change. By restricting the scaling sw and sb to be between 0.10 and 1, we ensure that this invariant holds. The lighting is locally corrolated and should cause smooth changes in the image. Hence, Gaussian blur b(x) is applied to sb, sw, and t.
φlighting(x, sw, sb, t) = x · b(sw) ·W (x) + x · b(sb) · (1−W (x)) + b(t) (5)
The segmentation mask W (x) is one for white parts and zero for the black part of the image. As the intensity of the input is distributed around -1 and 1, we can use thresholding to differentiate between black and white parts.
Background: The background augmentation can change the background pixels arbitrarily. A segmentation mask Bx marks the background pixels of the image x which are replaced by the pixels from the generated image d.
φbg(x, d) = x · (1−Bx) + d ·Bx (6)
The 3D model provides the segmentation mask. As φbg can only change background pixels, the labels remain unchanged.
Details: In this stage, the generator can add small details to the whole image including the tag. The output of the generator d is passed through a high-pass filter to ensure that the added details are small enough not to flip a bit. Furthermore, d is restricted to be in [−2, 2] to make sure the generator cannot avoid the highpass filter by producing huge values. With the range [−2, 2], the generator has the possibility to change black pixels to white, which is needed to model spotlights.
φdetail(x, d) = x+ highpass(d) (7) The high-pass is implemented by taking the difference between the image and a blurred version of the image (σ = 3.5). As the spotlights on the tags are only a little smaller than the bits, we increase its slope after the cutoff frequency by repeating the high-pass filter three times.
The image augmentations are applied in the order as listed above: φdetail ◦φbackground ◦φlighting ◦ φblur. Please note that there exist parameters to the augmentation functions that could change the labels. As long as it is guaranteed that such augmentations will result in unrealistic looking images, the generator network will learn to avoid them. For example, even though the detail augmentation could be used to add high-frequency noise to obscure the tag, this artifact would be detected by the discriminator.
Architecture of the generator: The generator network has to produce outputs for each augmentation function. We will outline only the most important parts. See our code available online for all the details of the networks1. The generator starts with a small network consisting of dense layers, which predicts the parameters for the 3D model (position, orientations). The output of another dense layer is reshaped and used as starting block for a chain of convolution and upsampling layers. We found it advantageous to merge a depth map of the 3D model into the generator as especially the lighting depends on the orientation of the tag in space. The input to the blur augmentation is predicted by reducing an intermediate convolutional feature map to a single scalar. An additional network is branched off to predict the input to the lighting augmentation. For the background generation, the output of the lighting network is merged back into the main generator network together with the actual image from the 3D model.
For the discriminator architecture, we mostly rely on the architecture given by Radford et al. (2015), but doubled the number of convolutional layers and added a final dense layer. This change improved the quality of the generated images.
Clip layer: Some of the augmentation parameters have to be restricted to a range of values to ensure that the labels remain valid. The training did not converge when using functions like tanh or sigmoid due to vanishing gradients. We are using a combination of clipping and activity regularization to keep the output in a given interval [a, b]. If the input x is out of bounds, it is clipped and a regularization loss r depending on the distance between x and the appropriate bound is added.
r(x) =  γ||x− a||1 if x < a 0 if a ≤ x ≤ b γ||x− b||1 if x > b
(8)
f(x) = min(max(a, x), b) (9) With the scalar γ, the weight of the loss can be adapted. For us γ = 15 worked well. If γ is chosen too small, the regularization loss might not be big enough to move the output of the previous layer towards the interval [a, b]. During training, we observe that the loss decreases to a small value but never vanishes.
Training: We train generator and discriminator as in the normal GAN setting. We use 2.4M unlabeled images of tags to train the RenderGAN. We use Adam (Kingma & Ba, 2014) as an optimizer with a starting learning rate of 0.0002, which we reduce in epoch 200, 250, and 300 by a factor of 0.25. In Fig. 6b the training loss of the GAN is shown. The GAN does not converge to the point where the discriminator can no longer separate generated from real samples. The augmentation functions might restrict the generator too much such that it cannot model certain properties. Nevertheless, it is hard for a human to distinguish the generated from real images. In some cases, the
1https://github.com/berleon/deepdecoder
generator creates unrealistic high-frequencies artifacts. The discriminator unfailingly assigns a low score to theses images. We can therefore discard them for the training of the supervised algorithm. More generated images are shown in Appendix A. In Fig. 7, we show random points in the latent space, while fixing the tag parameters. The generator indeed learned to model the various lighting conditions, noise intensities, and backgrounds.

5 RESULTS
We constructed the RenderGAN to generate labeled data. But does a DCNN trained with the RenderGAN data perform better than one trained on the limited amounts of real data? And are learned augmentations indeed needed or do simple hand-designed augmentation achieve the same result? The following paragraphs describe the different datasets used in the evaluation. We focus on the performance of a DCNN on the generated data. Thus, we do not compare our method to conventional GANs as those do not provide labels and are generally hard to evaluate.
Data from the RenderGAN: We generate 5 million tags with the RenderGAN framework. Due to the abundance, one training sample is only used twice during training. It is not further augmented.
Real Data: The labels of the real data are extracted from ground truth data that was originally collected to evaluate bee trajectories. This ground truth data contains the path and id of each bee over multiple consecutive frames. Data from five different time spans was annotated – in total 66K tags. As the data is correlated in time (same ids, similar lighting conditions), we assign the data from one time span completely to either the train or test set. The data from three time spans forms the train set (40K). The test set (26K) contains data from the remaining two time spans. The ground truth data lacks the orientation of the tags, which is therefore omitted for this evaluation. Due to the smaller
size of the real training set, we augment it with random translation, rotation, shear transformation, histogram scaling, and noise (see Appendix C for exact parameters).
RenderGAN + Real: We also train a DCNN on generated and real data which is mixed at a 50:50 ratio.
Handmade augmentations: We tried to emulate the augmentations learned by the RenderGAN by hand. For example, we generate the background by an image pyramid where the pixel intensities are drawn randomly. We model all effects, i.e. blur, lighting, background, noise and spotlights (see Appendix B for details on their implementation). We apply the handmade augmentation to different learned representations of the RenderGAN, e.g. we use the learned lighting representation and add the remaining effects such as background and noise with handmade augmentations (HM LI). See Table 1 for the different combinations of learned representations and hand designed augmentations.
Computer vision pipeline CV : The previously used computer vision pipeline (Wario et al., 2015) is based on manual feature extraction. For example, a modified Hough transformation to find ellipses. The MHD obtained by this model is only a rough estimate given that the computer vision pipeline had to be evaluated and fine-tuned on the same data set due to label scarcity.
Training setup: An epoch consists of 1000 batches á 128 samples. We use early stopping to select the best parameters of the networks. For the training with generated data, we use the real training set as the validation set. When training on real data, the test set is also used for validation. We could alternatively reduce the real training set further and form an extra validation set, but this would harm the performance of the DCNN trained on the real data. We use the 34-layer ResNet architecture (He et al., 2015) but start with 16 feature maps instead of 64. The DCNNs are evaluated on the mean Hamming distance (MHD) i.e. the expected value of bits decoded wrong. Human experts can decode tags with a MHD of around 0.23.
Results: In Table 2, we present the results of the evaluation. The training losses of the networks are plotted in Fig. 8. The model trained with the data generated by the RenderGAN has an MHD of 0.424. The performance can furthermore be slightly improved by combining the generated with real data. The small gap in performance when adding real data is a further indicator of the quality of the generated samples.
If we use predictions from this DCNN instead of the computer vision pipeline, the accuracy of the tracking improves from 55% of the ids assigned correctly to 96%. At this quality, it is possible to analyze the social behavior of the honeybees reliably.
Compared to the handmade augmentations (HM 3D), data from the RenderGAN leads to considerably better performance. The large gap in performance between the HM 3D and HM LI data highlights the importance of the learned lighting augmentation.

6 DISCUSSION
We proposed a novel extension to the GAN framework that is capable of rendering samples from a basic 3D model more realistic. Compared to computer graphics pipelines, the RenderGAN can learn complex effects from unlabeled data that would be otherwise hard to model with explicit rules.
Contrary to conventional GANs, the generator provides explicit information about the synthesized images, which can be used as labels for a supervised algorithm. The training of the RenderGAN requires no labels.
We showed an application of the RenderGAN framework to the BeesBook project, in which the generator adds blur, lighting, background, and details to images from a basic 3D model. The generated data looks strikingly real and includes fine details such as spotlights, compression artifacts, and sensor noise.
In contrast to previous work that applied 3D models to produce training samples for DCNNs (Su et al., 2015; Richter et al., 2016; Ros et al., 2016), we were able to train a DCNN from scratch with only generated data that still generalizes to unseen real data.
While some work is required to adapt the RenderGAN to a specific domain, once set up, arbitrary amounts of labeled data can be acquired cheaply, even if the data distribution changes. For example, if the tag design changes to include more bits, small adaptions to the 3D model’s source code and eventually the hyperparameters of the augmentation functions would be sufficient. However, if we had labeled the data by hand, then we would have to annotate data again.
While the proposed augmentations represent common image characteristics, a disadvantage of the RenderGAN framework is that these augmentation functions must be carefully customized for the application at hand to ensure that high-level information is preserved. Furthermore, a suitable 3D model must be available.

7 FUTURE WORK
For future work, it would be interesting to see the RenderGAN framework used on other tasks where basic 3D models exist e.g. human faces, pose estimation, or viewpoint prediction. In this context, one could come up with different augmentation functions e.g. colorization, affine transformations, or diffeomorphism. The RenderGAN could be especially valuable to domains where pre-trained models are not available or when the annotations are very complex. Another direction of future work might be to extend the RenderGAN framework to other fields. For example, in speech synthesis, one could use an existing software as a basic model and improve the realism of the output with a similar approach as in the RenderGAN framework.

B HANDMADE AUGMENTATIONS
We constructed augmentations for blur, lighting, background, noise and spotlights manually. For synthesizing lighting, background and noise, we use image pyramids, i.e. a set of images L0, . . . , L6 of size (2i × 2i) for 0 ≤ i ≤ 6. Each level Li in the pyramid is weighted by a scalar ωi. Each pixel of the different level Li is drawn from N (0, 1). The generated image I6 is given by:
I0 = ω0L0 (10) Ii = ωiLi + upscale(Ii−1) (11)
, where upscale doubles the image dimensions. The pyramid enables us to generate random images while controlling their frequency domain by weighting the pyramid levels appropriately.
• Blur: Gaussian blur with randomly sampled scale. • Lighting: Similar as in the RenderGAN. Here, the scaling of the white and black parts
and shifting is constructed with image pyramids. • Background: image pyramids with the lower levels weight more. • Noise: image pyramids with only the last two layer. • Spotlights: overlay with possible multiple 2D Gauss function with a random position on
the tag and random covariance.
We selected all parameters manually by comparing the generated to real images. However, using slightly more unrealistic images resulted in better performance of the DCNN trained with the HM 3D data. The parameters of the handmade augmentations can be found online in our source code repository.

C AUGMENTATIONS OF THE REAL DATA
We scale and shift the pixel intensities randomly, i.e. sI+t, where I is the image and s, t are scalars. The noise is sampled for each pixel fromN (0, ), where ∼ max(0, N (µn, σn)) is drawn for each image separately. Different affine transformations (rotation, scale, translation, and shear) are used.

D TRAINING SAMPLES
","Deep Convolutional Neuronal Networks (DCNNs) are showing remarkable performance on many computer vision tasks. Due to their large parameter space, they require many labeled samples when trained in a supervised setting. The costs of annotating data manually can render the use of DCNNs infeasible. We present a novel framework called RenderGAN that can generate large amounts of realistic, labeled images by combining a 3D model and the Generative Adversarial Network framework. In our approach, image augmentations (e.g. lighting, background, and detail) are learned from unlabeled data such that the generated images are strikingly realistic while preserving the labels known from the 3D model. We apply the RenderGAN framework to generate images of barcode-like markers that are attached to honeybees. Training a DCNN on data generated by the RenderGAN yields considerably better performance than training it on various baselines.",ICLR 2017 conference submission,False,,"This paper addresses the problem of decoding barcode-like markers depicted in an image.  The main insight is to train a CNN from generated data produced from a GAN.  The GAN is trained using unlabeled images, and leverages a ""3D model"" that undergoes learnt image transformations (e.g., blur, lighting, background).  The parameters for the image transformations are trained such that it confuses a GAN discriminator.  A CNN is trained using images generated from the GAN and compared with hand-crafted features and from training with real images.  The proposed method out-performs both baselines on decoding the barcode markers.

The proposed GAN architecture could potentially be interesting.  However, I won’t champion the paper as the evaluation could be improved.

A critical missing baseline is a comparison against a generic GAN.  Without this it’s hard to judge the benefit of the more structured GAN.  Also, it would be worth seeing the result when one combines generated and real images for the final task. 

A couple of references that are relevant to this work (for object detection using rendered views of 3D shapes):

[A] Xingchao Peng, Baochen Sun, Karim Ali, Kate Saenko, Learning Deep Object Detectors from 3D Models; ICCV, 2015.

[B] Deep Exemplar 2D-3D Detection by Adapting from Real to Rendered Views. Francisco Massa, Bryan C. Russell, Mathieu Aubry. CVPR 2016.

The problem domain (decoding barcode markers on bees) is limited.  It would be great to see this applied to another problem domain, e.g., object detection from 3D models as shown in paper reference [A], where direct comparison against prior work could be performed.  

I found the writing to be somewhat vague throughout.  For instance, on first reading of the introduction it is not clear what exactly is the contribution of the paper.  

Minor comments:

Fig 3 - Are these really renders from a 3D model?  The images look like 2D images, perhaps spatially warped via a homography.  

Page 3: ""chapter"" => ""section"".

In Table 2, what is the loss used for the DCNN?

Fig 9 (a) - The last four images look like they have strange artifacts. Can you explain these?

---

This paper was fairly well received by the reviewers in terms of the underling idea but the fact that a very specialized problem was the focus of the paper held back reviewers from giving stronger ratings. The question of what sorts of baselines would be reasonable was discussed extensively as the reviewers felt that other credible baselines should be included. The authors argue certain baselines are not appropriate but they were not able to clearly sway the reviewers to a more positive rating based on their response to this issue. We recommend a workshop invitation for this paper.

---

We uploaded a new version of the paper based on the peer review feedback.

* Clarified that an unconstrained GAN is not a suitable baseline.
* Added additional references pointed out by reviewer 2.
* Extracted a related work section to improve the overall clarity and also stated our contribution explicitly.
* Various modifications to improve clarity.

---

Thank you very much for your reviews. Your feedback helped to improve the
manuscript significantly, and we are preparing a revised version of the
manuscript with changes outlined either below or in our responses to each
reviewer. Multiple valid points of criticism were raised during the review
process and have already been worked into the current version of the document.
For example, we included hand-designed augmentations for comparison with the
learned ones.

However, in two of the three reviews there seems to be a major misunderstanding
that we would like to clarify here. Since this relates to the central finding of
our paper, we would like to provide a detailed response to this point. We hope
that, in the light of this fact, the reviewer’s rating of our contribution’s
importance and novelty will be reconsidered.

> Reviewer 2: “A critical missing baseline is a comparison against a generic GAN.
> Without this it’s hard to judge the benefit of the more structured GAN.  Also,
> it would be worth seeing the result when one combines generated and real images
> for the final task.”

> Reviewer 3: “ [...] the proposed method is more model driven that previous GAN
> models. But does it pay off? how would a traditional GAN approach perform? [...]
> The answers of the authors only partially addresses the point. The key proposal
> of the submission seems parameterised modules that can be trained to match the
> real data distribution. but it remains unclear why not a more generic
> parameterisation can also do the job. E.g. a neural network - as done in regular
> GANs. The benefit of introducing a stronger model is unclear.”

The main point of critique here is that a comparison with a generic GAN
(Goodfellow et al. 2014) is missing. This comment implies that both methods (GAN
and RenderGAN) share the same task domain, which is incorrect. The task we
address is generating _labeled_ data. We emphasize that we do not refer to the
binary class label but rather to higher dimensional labels. In our example
scenario, this corresponds to images of bee markers and their respective bit
configuration (its ID) and rotations in 3D space. A generic GAN cannot generate
labels, it learns to generate realistic images _without_ labels. Ultimately, we
want to train a convnet (‘decoder network’) in a supervised setting to map an
image to its respective labels. Thus, we need labeled samples and hence,
a conventional GAN cannot be used as a baseline! Stated formally: the RenderGAN
samples from the joint distribution p(l, x) of labels l and data x whereas
a conventional GAN can only sample from the data distribution p(x).

There are two alternative approaches to our RenderGAN, one being a conventional
3-dimensional rendering pipeline that can be used to generate images of bee
markers with known ID and spatial orientation. Secondly, one could train the
decoder network with manually labeled data. Both approaches have been
implemented and tested against the RenderGAN and do not perform satisfyingly. To
improve both alternatives’ performance one would need to either tune the
rendering pipeline to match the details of the real world imaging process, or
label more data manually. Both measures are time-consuming and do not generalize
well when changing parts of the imaging process (lighting, cameras, compression,
etc.) or the marker design.

Our approach is to extend a generic GAN by adding several network modules, the
first being the network equivalent of a simple 3D model. Secondly, we learn
a number of parameterised augmentation functions. We would like to point out
that this approach was _not_ chosen to improve the generative capabilities of
the network but to constrain it in such a way that the image produced by the GAN
is correct with respect to the labels fed into the network. In our use case,
each image produced by the GAN has to preserve the given bit pattern and
rotation in space provided by the 3D model for the labels to remain valid. This
point was already addressed in our paper and the pre-review questions:

> Paper Introduction: “[...] We constrain the augmentation of the images such that
> the high-level information represented by the 3D model is preserved. The
> RenderGAN framework allows us to generate images of which the labels are known
> from the 3D model, and that also look strikingly real due to the GAN framework.
> The training procedure of the RenderGAN framework does not require any labels.
> We can generate high-quality, labeled data with a simple 3D model and a large
> amount of unlabeled data.”

> Our reply to Reviewer 3: “[...] The payoff is that we can generate labeled data
> with only a simple 3D model and unlabeled data. You are right. A DCGAN
> architecture can model all mentioned effects, even affine transformations. We
> trained a DCGAN on the data, and the quality of the synthesized images is
> similar. However, no labels can be collected in the conventional GAN framework.
> [...]”

All reviewers question the necessity of the constraints we introduced. One of
our early approaches was to add an offset to the 3D model, i.e. x = t + g(t)
where x is the synthesized image, t is an image from the 3D model, and g an
unconstrained generator. However, in our experiments, the generator learned to
synthesize realistic images but ignored the given template t completely. Thus,
no valid labels of the synthetic images could be collected. Since a decoder
network cannot be trained without labels, this approach cannot be used as
a baseline. We will revise our paper to clarify that an unconstrained GAN is not
a suitable baseline for our task.

---

This paper addresses the problem of decoding barcode-like markers depicted in an image.  The main insight is to train a CNN from generated data produced from a GAN.  The GAN is trained using unlabeled images, and leverages a ""3D model"" that undergoes learnt image transformations (e.g., blur, lighting, background).  The parameters for the image transformations are trained such that it confuses a GAN discriminator.  A CNN is trained using images generated from the GAN and compared with hand-crafted features and from training with real images.  The proposed method out-performs both baselines on decoding the barcode markers.

The proposed GAN architecture could potentially be interesting.  However, I won’t champion the paper as the evaluation could be improved.

A critical missing baseline is a comparison against a generic GAN.  Without this it’s hard to judge the benefit of the more structured GAN.  Also, it would be worth seeing the result when one combines generated and real images for the final task. 

A couple of references that are relevant to this work (for object detection using rendered views of 3D shapes):

[A] Xingchao Peng, Baochen Sun, Karim Ali, Kate Saenko, Learning Deep Object Detectors from 3D Models; ICCV, 2015.

[B] Deep Exemplar 2D-3D Detection by Adapting from Real to Rendered Views. Francisco Massa, Bryan C. Russell, Mathieu Aubry. CVPR 2016.

The problem domain (decoding barcode markers on bees) is limited.  It would be great to see this applied to another problem domain, e.g., object detection from 3D models as shown in paper reference [A], where direct comparison against prior work could be performed.  

I found the writing to be somewhat vague throughout.  For instance, on first reading of the introduction it is not clear what exactly is the contribution of the paper.  

Minor comments:

Fig 3 - Are these really renders from a 3D model?  The images look like 2D images, perhaps spatially warped via a homography.  

Page 3: ""chapter"" => ""section"".

In Table 2, what is the loss used for the DCNN?

Fig 9 (a) - The last four images look like they have strange artifacts. Can you explain these?

---

The paper proposes an approach to generating synthetic training data for deep networks, based on rendering 3D models and learning additional transformations with adversarial training. The approach is applied to generating barcode-like markers used for honeybee identification. The authors demonstrate that a classifier trained on synthetic data generated with the proposed approach outperforms both training on (limited) real data and training on data with hand-designed augmentations. 

The topic of the paper — using machine learning (in particular, adversarial training) for generating realistic synthetic training data — is very interesting and important. The proposed method looks reasonable, and the paper is written well. The downside is that experiments are limited to a fairly simple and not-widely-known domain of honeybee marker classification. While I am sure this is an important task by itself, in order to demonstrate general applicability of the method and to allow comparison with existing techniques, experiments on some standard and/or realistic datasets would be very helpful. Overall, I recommend acceptance, but encourage the authors to perform experiments on more datasets.

I appreciate that the authors added a baseline with manually designed transformations. This strengthens the paper.

As Reviewer3 points out, it would be interesting to analyze if restricting GAN to a fixed set of transformations is necessary here, and which transformations are most important. Perhaps this would provide some guidelines for designing sets of transformations for more complicated scenarios.

The authors should tone down their claims such as “Our method is an improvement over previous work  <...> Whereas previous work relied on real data for training using pre-trained models or mixing real and generated data, we were able to train a DCNN from scratch with generated data that performed well when tested on real data. “. This is not a fair comparison: the domain studied by authors in this work is much simpler than what was studied in these previous works, so this comparison is not appropriate.

---

The submission proposes an interesting way to match synthetic data to real data in a GAN type architecture.
The main novelty are parametric modules that emulate different transformations and artefact that allow to match the natural appearance.

several points were raised during the discussion:

1. the proposed method is more model driven that previous GAN models. But does it pay off? how would a traditional GAN approach perform? The mentioned effects like blur, lighting and background could also potentially be modelled by upsamling network that directly predicts the image. I would assume that blur and lighting can be modelled by convolutions. transformations to some extend by convolutions - or spatial transformer networks.
The answers of the authors only partially addresses the point. The key proposal of the submission seems parameterised modules that can be trained to match the real data distribution. but it remains unclear why not a more generic parameterisation can also do the job. E.g. a neural network - as done in regular GANs. The benefit of introducing a stronger model is unclear. Using a render engine to generate the initial sample appearance if of limited novelty.


2. how does it compare to traditional data augmentation techniques, e.g. noise, dropout, transformations. you are linking to keras code - where data augmentation is readily available and could be tested (ImageDataGenerator)
The authors reply that plenty of such augmentation was used and more details are going to be provided in the appendix. it would have been appreciated if such information was directly included in the revision - so that the procedure could be directly checked. right now - this remains a point of uncertainty.

3. How do the different stages (\phis) effect performance? which are the most important ones?
The authors do evaluate the effect of hand tuning the transformation stages vs. learning them. it would be great to also include results of including/excluding stages completely - and also reporting how much the initial jittering of the data helps.

While there is an interesting idea of (limited) novelty to the paper, there are some concerns about evalations and comparisons as outlined above. In addition, only success on a single dataset/task is shown. Yet the task is interesting and seems challenging. Overall, this remains makes only a weak recommendation for acceptance.

---

We updated our paper based on the feedback from the pre-review questions.  We
included handmade augmentation in the evaluation.  We also retrained the DCNN on
the real data. Thanks for the feedback.

---

This paper addresses the problem of decoding barcode-like markers depicted in an image.  The main insight is to train a CNN from generated data produced from a GAN.  The GAN is trained using unlabeled images, and leverages a ""3D model"" that undergoes learnt image transformations (e.g., blur, lighting, background).  The parameters for the image transformations are trained such that it confuses a GAN discriminator.  A CNN is trained using images generated from the GAN and compared with hand-crafted features and from training with real images.  The proposed method out-performs both baselines on decoding the barcode markers.

The proposed GAN architecture could potentially be interesting.  However, I won’t champion the paper as the evaluation could be improved.

A critical missing baseline is a comparison against a generic GAN.  Without this it’s hard to judge the benefit of the more structured GAN.  Also, it would be worth seeing the result when one combines generated and real images for the final task. 

A couple of references that are relevant to this work (for object detection using rendered views of 3D shapes):

[A] Xingchao Peng, Baochen Sun, Karim Ali, Kate Saenko, Learning Deep Object Detectors from 3D Models; ICCV, 2015.

[B] Deep Exemplar 2D-3D Detection by Adapting from Real to Rendered Views. Francisco Massa, Bryan C. Russell, Mathieu Aubry. CVPR 2016.

The problem domain (decoding barcode markers on bees) is limited.  It would be great to see this applied to another problem domain, e.g., object detection from 3D models as shown in paper reference [A], where direct comparison against prior work could be performed.  

I found the writing to be somewhat vague throughout.  For instance, on first reading of the introduction it is not clear what exactly is the contribution of the paper.  

Minor comments:

Fig 3 - Are these really renders from a 3D model?  The images look like 2D images, perhaps spatially warped via a homography.  

Page 3: ""chapter"" => ""section"".

In Table 2, what is the loss used for the DCNN?

Fig 9 (a) - The last four images look like they have strange artifacts. Can you explain these?

---

This paper was fairly well received by the reviewers in terms of the underling idea but the fact that a very specialized problem was the focus of the paper held back reviewers from giving stronger ratings. The question of what sorts of baselines would be reasonable was discussed extensively as the reviewers felt that other credible baselines should be included. The authors argue certain baselines are not appropriate but they were not able to clearly sway the reviewers to a more positive rating based on their response to this issue. We recommend a workshop invitation for this paper.

---

We uploaded a new version of the paper based on the peer review feedback.

* Clarified that an unconstrained GAN is not a suitable baseline.
* Added additional references pointed out by reviewer 2.
* Extracted a related work section to improve the overall clarity and also stated our contribution explicitly.
* Various modifications to improve clarity.

---

Thank you very much for your reviews. Your feedback helped to improve the
manuscript significantly, and we are preparing a revised version of the
manuscript with changes outlined either below or in our responses to each
reviewer. Multiple valid points of criticism were raised during the review
process and have already been worked into the current version of the document.
For example, we included hand-designed augmentations for comparison with the
learned ones.

However, in two of the three reviews there seems to be a major misunderstanding
that we would like to clarify here. Since this relates to the central finding of
our paper, we would like to provide a detailed response to this point. We hope
that, in the light of this fact, the reviewer’s rating of our contribution’s
importance and novelty will be reconsidered.

> Reviewer 2: “A critical missing baseline is a comparison against a generic GAN.
> Without this it’s hard to judge the benefit of the more structured GAN.  Also,
> it would be worth seeing the result when one combines generated and real images
> for the final task.”

> Reviewer 3: “ [...] the proposed method is more model driven that previous GAN
> models. But does it pay off? how would a traditional GAN approach perform? [...]
> The answers of the authors only partially addresses the point. The key proposal
> of the submission seems parameterised modules that can be trained to match the
> real data distribution. but it remains unclear why not a more generic
> parameterisation can also do the job. E.g. a neural network - as done in regular
> GANs. The benefit of introducing a stronger model is unclear.”

The main point of critique here is that a comparison with a generic GAN
(Goodfellow et al. 2014) is missing. This comment implies that both methods (GAN
and RenderGAN) share the same task domain, which is incorrect. The task we
address is generating _labeled_ data. We emphasize that we do not refer to the
binary class label but rather to higher dimensional labels. In our example
scenario, this corresponds to images of bee markers and their respective bit
configuration (its ID) and rotations in 3D space. A generic GAN cannot generate
labels, it learns to generate realistic images _without_ labels. Ultimately, we
want to train a convnet (‘decoder network’) in a supervised setting to map an
image to its respective labels. Thus, we need labeled samples and hence,
a conventional GAN cannot be used as a baseline! Stated formally: the RenderGAN
samples from the joint distribution p(l, x) of labels l and data x whereas
a conventional GAN can only sample from the data distribution p(x).

There are two alternative approaches to our RenderGAN, one being a conventional
3-dimensional rendering pipeline that can be used to generate images of bee
markers with known ID and spatial orientation. Secondly, one could train the
decoder network with manually labeled data. Both approaches have been
implemented and tested against the RenderGAN and do not perform satisfyingly. To
improve both alternatives’ performance one would need to either tune the
rendering pipeline to match the details of the real world imaging process, or
label more data manually. Both measures are time-consuming and do not generalize
well when changing parts of the imaging process (lighting, cameras, compression,
etc.) or the marker design.

Our approach is to extend a generic GAN by adding several network modules, the
first being the network equivalent of a simple 3D model. Secondly, we learn
a number of parameterised augmentation functions. We would like to point out
that this approach was _not_ chosen to improve the generative capabilities of
the network but to constrain it in such a way that the image produced by the GAN
is correct with respect to the labels fed into the network. In our use case,
each image produced by the GAN has to preserve the given bit pattern and
rotation in space provided by the 3D model for the labels to remain valid. This
point was already addressed in our paper and the pre-review questions:

> Paper Introduction: “[...] We constrain the augmentation of the images such that
> the high-level information represented by the 3D model is preserved. The
> RenderGAN framework allows us to generate images of which the labels are known
> from the 3D model, and that also look strikingly real due to the GAN framework.
> The training procedure of the RenderGAN framework does not require any labels.
> We can generate high-quality, labeled data with a simple 3D model and a large
> amount of unlabeled data.”

> Our reply to Reviewer 3: “[...] The payoff is that we can generate labeled data
> with only a simple 3D model and unlabeled data. You are right. A DCGAN
> architecture can model all mentioned effects, even affine transformations. We
> trained a DCGAN on the data, and the quality of the synthesized images is
> similar. However, no labels can be collected in the conventional GAN framework.
> [...]”

All reviewers question the necessity of the constraints we introduced. One of
our early approaches was to add an offset to the 3D model, i.e. x = t + g(t)
where x is the synthesized image, t is an image from the 3D model, and g an
unconstrained generator. However, in our experiments, the generator learned to
synthesize realistic images but ignored the given template t completely. Thus,
no valid labels of the synthetic images could be collected. Since a decoder
network cannot be trained without labels, this approach cannot be used as
a baseline. We will revise our paper to clarify that an unconstrained GAN is not
a suitable baseline for our task.

---

This paper addresses the problem of decoding barcode-like markers depicted in an image.  The main insight is to train a CNN from generated data produced from a GAN.  The GAN is trained using unlabeled images, and leverages a ""3D model"" that undergoes learnt image transformations (e.g., blur, lighting, background).  The parameters for the image transformations are trained such that it confuses a GAN discriminator.  A CNN is trained using images generated from the GAN and compared with hand-crafted features and from training with real images.  The proposed method out-performs both baselines on decoding the barcode markers.

The proposed GAN architecture could potentially be interesting.  However, I won’t champion the paper as the evaluation could be improved.

A critical missing baseline is a comparison against a generic GAN.  Without this it’s hard to judge the benefit of the more structured GAN.  Also, it would be worth seeing the result when one combines generated and real images for the final task. 

A couple of references that are relevant to this work (for object detection using rendered views of 3D shapes):

[A] Xingchao Peng, Baochen Sun, Karim Ali, Kate Saenko, Learning Deep Object Detectors from 3D Models; ICCV, 2015.

[B] Deep Exemplar 2D-3D Detection by Adapting from Real to Rendered Views. Francisco Massa, Bryan C. Russell, Mathieu Aubry. CVPR 2016.

The problem domain (decoding barcode markers on bees) is limited.  It would be great to see this applied to another problem domain, e.g., object detection from 3D models as shown in paper reference [A], where direct comparison against prior work could be performed.  

I found the writing to be somewhat vague throughout.  For instance, on first reading of the introduction it is not clear what exactly is the contribution of the paper.  

Minor comments:

Fig 3 - Are these really renders from a 3D model?  The images look like 2D images, perhaps spatially warped via a homography.  

Page 3: ""chapter"" => ""section"".

In Table 2, what is the loss used for the DCNN?

Fig 9 (a) - The last four images look like they have strange artifacts. Can you explain these?

---

The paper proposes an approach to generating synthetic training data for deep networks, based on rendering 3D models and learning additional transformations with adversarial training. The approach is applied to generating barcode-like markers used for honeybee identification. The authors demonstrate that a classifier trained on synthetic data generated with the proposed approach outperforms both training on (limited) real data and training on data with hand-designed augmentations. 

The topic of the paper — using machine learning (in particular, adversarial training) for generating realistic synthetic training data — is very interesting and important. The proposed method looks reasonable, and the paper is written well. The downside is that experiments are limited to a fairly simple and not-widely-known domain of honeybee marker classification. While I am sure this is an important task by itself, in order to demonstrate general applicability of the method and to allow comparison with existing techniques, experiments on some standard and/or realistic datasets would be very helpful. Overall, I recommend acceptance, but encourage the authors to perform experiments on more datasets.

I appreciate that the authors added a baseline with manually designed transformations. This strengthens the paper.

As Reviewer3 points out, it would be interesting to analyze if restricting GAN to a fixed set of transformations is necessary here, and which transformations are most important. Perhaps this would provide some guidelines for designing sets of transformations for more complicated scenarios.

The authors should tone down their claims such as “Our method is an improvement over previous work  <...> Whereas previous work relied on real data for training using pre-trained models or mixing real and generated data, we were able to train a DCNN from scratch with generated data that performed well when tested on real data. “. This is not a fair comparison: the domain studied by authors in this work is much simpler than what was studied in these previous works, so this comparison is not appropriate.

---

The submission proposes an interesting way to match synthetic data to real data in a GAN type architecture.
The main novelty are parametric modules that emulate different transformations and artefact that allow to match the natural appearance.

several points were raised during the discussion:

1. the proposed method is more model driven that previous GAN models. But does it pay off? how would a traditional GAN approach perform? The mentioned effects like blur, lighting and background could also potentially be modelled by upsamling network that directly predicts the image. I would assume that blur and lighting can be modelled by convolutions. transformations to some extend by convolutions - or spatial transformer networks.
The answers of the authors only partially addresses the point. The key proposal of the submission seems parameterised modules that can be trained to match the real data distribution. but it remains unclear why not a more generic parameterisation can also do the job. E.g. a neural network - as done in regular GANs. The benefit of introducing a stronger model is unclear. Using a render engine to generate the initial sample appearance if of limited novelty.


2. how does it compare to traditional data augmentation techniques, e.g. noise, dropout, transformations. you are linking to keras code - where data augmentation is readily available and could be tested (ImageDataGenerator)
The authors reply that plenty of such augmentation was used and more details are going to be provided in the appendix. it would have been appreciated if such information was directly included in the revision - so that the procedure could be directly checked. right now - this remains a point of uncertainty.

3. How do the different stages (\phis) effect performance? which are the most important ones?
The authors do evaluate the effect of hand tuning the transformation stages vs. learning them. it would be great to also include results of including/excluding stages completely - and also reporting how much the initial jittering of the data helps.

While there is an interesting idea of (limited) novelty to the paper, there are some concerns about evalations and comparisons as outlined above. In addition, only success on a single dataset/task is shown. Yet the task is interesting and seems challenging. Overall, this remains makes only a weak recommendation for acceptance.

---

We updated our paper based on the feedback from the pre-review questions.  We
included handmade augmentation in the evaluation.  We also retrained the DCNN on
the real data. Thanks for the feedback.",,,,,,5.666666666666667,,,3.6666666666666665,,
564,"HIGHER ORDER RECURRENT NEURAL NETWORKS
Authors: Rohollah Soltani
Source file: 564.pdf

ABSTRACT
In this paper, we study novel neural network structures to better model long term dependency in sequential data. We propose to use more memory units to keep track of more preceding states in recurrent neural networks (RNNs), which are all recurrently fed to the hidden layers as feedback through different weighted paths. By extending the popular recurrent structure in RNNs, we provide the models with better short-term memory mechanism to learn long term dependency in sequences. Analogous to digital filters in signal processing, we call these structures as higher order RNNs (HORNNs). Similar to RNNs, HORNNs can also be learned using the back-propagation through time method. HORNNs are generally applicable to a variety of sequence modelling tasks. In this work, we have examined HORNNs for the language modeling task using two popular data sets, namely the Penn Treebank (PTB) and English text8. Experimental results have shown that the proposed HORNNs yield the state-of-the-art performance on both data sets, significantly outperforming the regular RNNs as well as the popular LSTMs.

1 INTRODUCTION
In the recent resurgence of neural networks in deep learning, deep neural networks have achieved successes in various real-world applications, such as speech recognition, computer vision and natural language processing. Deep neural networks (DNNs) with a deep architecture of multiple nonlinear layers are an expressive model that can learn complex features and patterns in data. Each layer of DNNs learns a representation and transfers them to the next layer and the next layer may continue to extract more complicated features, and finally the last layer generates the desirable output. From early theoretical work, it is well known that neural networks may be used as the universal approximators to map from any fixed-size input to another fixed-size output. Recently, more and more empirical results have demonstrated that the deep structure in DNNs is not just powerful in theory but also can be reliably learned in practice from a large amount of training data.
Sequential modeling is a challenging problem in machine learning, which has been extensively studied in the past. Recently, many deep neural network based models have been successful in this area, as shown in various tasks such as language modeling Mikolov (2012), sequence generation Graves (2013); Sutskever et al. (2011), machine translation Sutskever et al. (2014) and speech recognition Graves et al. (2013). Among various neural network models, recurrent neural networks (RNNs) are appealing for modeling sequential data because they can capture long term dependency in sequential data using a simple mechanism of recurrent feedback. RNNs can learn to model sequential data over an extended period of time, then carry out rather complicated transformations on the sequential data. RNNs have been theoretically proved to be a turing complete machine Siegelmann & Sontag (1995). RNNs in principle can learn to map from one variable-length sequence to another. When unfolded in time, RNNs are equivalent to very deep neural networks that share model parameters and receive the input at each time step. The recursion in the hidden layer of RNNs can act as an excellent memory mechanism for the networks. In each time step, the learned recursion weights may decide what information to discard and what information to keep in order to relay onwards along time. While RNNs are theoretically powerful, the learning of RNNs needs to use the back-propagation through time (BPTT) method Werbos (1990) due to the internal recurrent cycles. Unfortunately, in practice, it turns out to be rather difficult to train RNNs to capture long-term dependency due to the fact that
the gradients in BPTT tend to either vanish or explode Bengio et al. (1994). Many heuristic methods have been proposed to solve these problems. For example, a simple method, called gradient clipping, is used to avoid gradient explosion Mikolov (2012). However, RNNs still suffer from the vanishing gradient problem since the gradients decay gradually as they are back-propagated through time. As a result, some new recurrent structures are proposed, such as long short-term memory (LSTM) Hochreiter & Schmidhuber (1997) and gated recurrent unit (GRU) Cho et al. (2014). These models use some learnable gates to implement rather complicated feedback structures, which ensure that some feedback paths can allow the gradients to flow back in time effectively. These models have given promising results in many practical applications, such as sequence modeling Graves (2013), language modeling Sundermeyer et al. (2012), hand-written character recognition Liwicki et al. (2012), machine translation Cho et al. (2014), speech recognition Graves et al. (2013).
In this paper, we explore an alternative method to learn recurrent neural networks (RNNs) to model long term dependency in sequential data. We propose to use more memory units to keep track of more preceding RNN states, which are all recurrently fed to the hidden layers as feedback through different weighted paths. Analogous to digital filters in signal processing, we call these new recurrent structures as higher order recurrent neural networks (HORNNs). At each time step, the proposed HORNNs directly combine multiple preceding hidden states from various history time steps, weighted by different matrices, to generate the feedback signal to each hidden layer. By aggregating more history information of the RNN states, HORNNs are provided with better short-term memory mechanism than the regular RNNs. Moreover, those direct connections to more previous RNN states allow the gradients to flow back smoothly in the BPTT learning stage. All of these ensure that HORNNs can be more effectively learned to capture long term dependency. Similar to RNNs and LSTMs, the proposed HORNNs are general enough for variety of sequential modeling tasks. In this work, we have evaluated HORNNs for the language modeling task on two popular data sets, namely the Penn Treebank (PTB) and English text8 sets. Experimental results have shown that HORNNs yield the state-of-the-art performance on both data sets, significantly outperforming the regular RNNs as well as the popular LSTMs.

2 RELATED WORK
Hierarchical recurrent neural network proposed in Hihi & Bengio (1996) is one of the earliest papers that attempt to improve RNNs to capture long term dependency in a better way. It proposes to add linear time delayed connections to RNNs to improve the gradient descent learning algorithm to find a better solution, eventually solving the gradient vanishing problem. However, in this early work, the idea of multi-resolution recurrent architectures has only been preliminarily examined for some simple small-scale tasks. This work is somehow relevant to our work in this paper but the higher order RNNs proposed here differs in several aspects. Firstly, we propose to use weighted connections in the structure, instead of simple multi-resolution short-cut paths. This makes our models fall into the category of higher order models. Secondly, we have proposed to use various pooling functions in generating the feedback signals, which is critical in normalizing the dynamic ranges of gradients flowing from various paths. Our experiments have shown that the success of our models is largely attributed to this technique.
The most successful approach to deal with vanishing gradients so far is to use long short term memory (LSTM) model Hochreiter & Schmidhuber (1997). LSTM relies on a fairly sophisticated structure made of gates to control flow of information to the hidden neurons. The drawback of the LSTM is that it is complicated and slow to learn. The complexity of this model makes the learning very time consuming, and hard to scale for larger tasks. Another approach to address this issue is to add a hidden layer to RNNs Mikolov et al. (2014). This layer is responsible for capturing longer term dependencies in input data by making its weight matrix close to identity. Recently, clockwork RNNs Koutnik et al. (2014) are proposed to address this problem as well, which splits each hidden layer into several modules running at different clocks. Each module receives signals from input and computes its output at a predefined clock rate. Gated feedback recurrent neural networks Chung et al. (2015) attempt to implement a generalized version using the gated feedback connection between layers of stacked RNNs, allowing the model to adaptively adjust the connection between consecutive hidden layers.
Besides, short-cut skipping connections were considered earlier in Wermter (1992), and more recently have been found useful in learning very deep feed-forward neural networks as well, such as Lee et al. (2014); He et al. (2015). These skipping connections between various layers of neural networks can improve the flow of information in both forward and backward passes. Among them, highway networks Srivastava et al. (2015) introduce rather sophisticated skipping connections between layers, controlled by some gated functions.

3 HIGHER ORDER RECURRENT NEURAL NETWORKS
A recurrent neural network (RNN) is a type of neural network suitable for modeling a sequence of arbitrary length. At each time step t, an RNN receives an input xt, the state of the RNN is updated recursively as follows (as shown in the left part of Figure 1):
ht = f(Winxt +Whht−1) (1)
where f(·) is an nonlinear activation function, such as sigmoid or rectified linear (ReLU), and Win is the weight matrix in the input layer and Wh is the state to state recurrent weight matrix. Due to the recursion, this hidden layer may act as a short-term memory of all previous input data.
Given the state of the RNN, i.e., the current activation signals in the hidden layer ht, the RNN generates the output according to the following equation:
yt = g(Woutht) (2)
where g(·) denotes the softmax function and Wout is the weight matrix in the output layer. In principle, this model can be trained using the back-propagation through time (BPTT) algorithm Werbos (1990). This model has been used widely in sequence modeling tasks like language modeling Mikolov (2012).

3.1 HIGHER ORDER RNNS (HORNNS)
RNNs are very deep in time and the hidden layer at each time step represents the entire input history, which acts as a short-term memory mechanism. However, due to the gradient vanishing problem in back-propagation, it turns out to be very difficult to learn RNNs to model long-term dependency in sequential data.
In this paper, we extend the standard RNN structure to better model long-term dependency in sequential data. As shown in the right part of Figure 1, instead of using only the previous RNN state as the feedback signal, we propose to employ multiple memory units to generate the feedback signal at each time step by directly combining multiple preceding RNN states in the past, where these timedelayed RNN states go through separate feedback paths with different weight matrices. Analogous to the filter structures used in signal processing, we call this new recurrent structure as higher order RNNs, HORNNs in short. The order of HORNNs depends on the number of memory units used for feedback. For example, the model used in the right of Figure 1 is a 3rd-order HORNN. On the other hand, regular RNNs may be viewed as 1st-order HORNNs.
In HORNNs, the feedback signal is generated by combining multiple preceding RNN states. Therefore, the state of an N -th order HORNN is recursively updated as follows:
ht = f ( Winxt +
N∑ n=1 Whnht−n
) (3)
where {Whn | n = 1, · · ·N} denotes the weight matrices used for various feedback paths. Similar to
RNNs, HORNNs can also be unfolded in time to get rid of the recurrent cycles. As shown in Figure 2, we unfold a 3rd-order HORNN in time, which clearly shows that each HORNN state is explicitly decided by the current input xt and all previous 3 states in the past. This structure looks similar to the skipping short-cut paths in deep neural networks but each path in HORNNs maintains a learnable weight matrix. The new structure in HORNNs can significantly improve the model capacity to capture long-term dependency in sequential data. At each time step, by explicitly aggregating multiple preceding hidden activities, HORNNs may derive a good representation of the history information in sequences, leading to a significantly enhanced short-term memory mechanism.
During the backprop learning procedure, these skipping paths directly connected to more previous hidden states of HORNNs may allow the gradients to flow more easily back in time, which eventually leads to a more effective learning of models to capture long term dependency in sequences. Therefore, this structure may help to largely alleviate the notorious problem of vanishing gradients in the RNN learning.
Obviously, HORNNs can be learned using the same BPTT algorithm as regular RNNs, except that the error signals at each time step need to be back-propagated to multiple feedback paths in the network. As shown in Figure 3, for a 3rd-order HORNN, at each time step t, the error signal from the hidden layer ht will have to be back-propagated into four different paths: i) the first one back to the input layer, xt; ii) three more feedback paths leading to three different histories in time scales, namely ht−1, ht−2 and ht−3.
Interestingly enough, if we use a fully-unfolded implementation for HORNNs as in Figure 2, the overall computation complexity is comparable with regular RNNs. Given a whole sequence, we may first simultaneously compute all hidden activities (from xt to ht for all t). Secondly, we recursively update ht for all t using eq.(3). Finally, we use GPUs to compute all outputs together from the updated hidden states (from ht to yt for all t) based on eq.(2). The backward pass in learning can also be implemented in the same three-step procedure. Except the recursive updates in the second step (this issue remains the same in regular RNNs), all remaining computation steps can be formulated as large matrix multiplications. As a result, the computation of HORNNs can be implemented fairly efficiently using GPUs.

3.2 POOLING FUNCTIONS FOR HORNNS
As discussed above, the shortcut paths in HORNNs may help the models to capture long-term dependency in sequential data. On the other hand, they may also complicate the learning in a different way. Due to different numbers of hidden layers along various paths, the signals flowing from different paths may vary dramatically in the dynamic range. For example, in the forward pass in Figure 2, three different feedback signals from different time scales, e.g. ht−1, ht−2 and ht−3, flow into
the hidden layer to compute the new hidden state ht. The dynamic range of these signals may vary dramatically from case to case. The situation may get even worse in the backward pass during the BPTT learning. For example, in a 3rd-order HORNN in Figure 2, the node ht−3 may directly receive an error signal from the node ht. In some cases, it may get so strong as to overshadow other error signals coming from closer neighbours of ht−1 and ht−2. This may impede the learning of HORNNs, yielding slow convergence or even poor performance.
Here, we have proposed to use some pooling functions to calibrate the signals from different feedback paths before they are used to recursively generate a new hidden state, as shown in Figure 4. In the following, we will investigate three different choices for the pooling function in Figure 4, including max-based pooling, FOFE-based pooling and gated pooling.

3.2.1 MAX-BASED POOLING
Max-based pooling is a simple strategy that chooses the most responsive unit (exhibiting the largest activation value) among various paths to transfer to the hidden layer to generate the new hidden state. Many biological experiments have shown that biological neuron networks tend to use a similar strategy in learning and firing.
In this case, instead of using eq.(3), we use the following formula to update the hidden state of HORNNs: ht = f ( Winxt +max N n=1 (Whnht−n) ) (4)
where maximization is performed element-wisely to choose the maximum value in each dimension to feed to the hidden layer to generate the new hidden state. The aim here is to capture the most relevant feature and map it to a fixed predefined size.
The max pooling function is simple and biologically inspired. However, the max pooling strategy also has some serious disadvantages. For example, it has no forgetting mechanism and the signals may get stronger and stronger. Furthermore, it loses the order information of the preceding histories since it only choose the maximum values but it does not know where the maximum comes from.

3.2.2 FOFE-BASED POOLING
The fixed-size ordinally-forgetting encoding (FOFE) method was proposed in Zhang et al. (2015) to encode any variable-length sequence of data into a fixed-size representation. In FOFE, a single forgetting factor α (0 < α < 1) is used to encode the position information in sequences based on the idea of exponential forgetting to derive invertible fixed-size representations. In this work, we borrow this simple idea of exponential forgetting to calibrate all preceding histories using a pre-selected forgetting factor as follows:
ht = f ( Winxt +
N∑ n=1 αn ·Whnht−n
) (5)
where the forgetting factor α is manually pre-selected between 0 < α < 1. The above constant coefficients related to α play an important role in calibrating signals from different paths in both
forward and backward passes of HORNNs since they slightly underweight the older history over the recent one in an explicit way.

3.2.3 GATED HORNNS
In this section, we follow the ideas of the learnable gates in LSTMs Hochreiter & Schmidhuber (1997) and GRUs Cho et al. (2014) as well as the recent soft-attention in Bahdanau et al. (2014). Instead of using constant coefficients derived from a forgetting factor, we may let the network automatically determine the combination weights based on the current state and input. In this case, we may use sigmoid gates to compute combination weights to regulate the information flowing from various feedback paths. The sigmoid gates take the current data and previous hidden state as input to decide how to weight all of the precede hidden states. The gate function weights how the current hidden state is generated based on all the previous time-steps of the hidden layer. This allows the network to potentially remember information for a longer period of time. In a gated HORNN, the hidden state is recursively computed as follows:
ht = f ( Winxt +
N∑ n=1 rn ( Whnht−n )) (6)
where denotes element-wise multiplication of two equally-sized vectors, and the gate signal rn is calculated as
rn = σ (W g 1nxt +W g 2nht−n) (7)
where σ(·) is the sigmoid function, and W g1n and W g 2n denote two weight matrices introduced for each gate.
Note that the computation complexity of gated HORNNs is comparable with LSTMs and GRUs, significantly exceeding the other HORNN structures because of the overhead from the gate functions in eq. (7).

4 EXPERIMENTS
In this section, we evaluate the proposed higher order RNNs (HORNNs) on several language modeling tasks. A statistical language model (LM) is a probability distribution over sequences of words in natural languages. Recently, neural networks have been successfully applied to language modeling Bengio et al. (2003); Mikolov et al. (2011), yielding the state-of-the-art performance. In language modeling tasks, it is quite important to take advantage of the long-term dependency of natural languages. Therefore, it is widely reported that RNN based LMs can outperform feedforward neural networks in language modeling tasks. We have chosen two popular LM data sets, namely the Penn Treebank (PTB) and English text8 sets, to compare our proposed HORNNs with traditional n-gram LMs, RNN-based LMs and the state-of-the-art performance obtained by LSTMs Graves (2013); Mikolov et al. (2014), FOFE based feedforward NNs Zhang et al. (2015) and memory networks Sukhbaatar et al. (2015).
In our experiments, we use the mini-batch stochastic gradient decent (SGD) algorithm to train all neural networks. The number of back-propagation through time (BPTT) steps is set to 30 for all recurrent models. Each model update is conducted using a mini-batch of 20 subsequences, each of which is of 30 in length. All model parameters (weight matrices in all layers) are randomly initialized based on a Gaussian distribution with zero mean and standard deviation of 0.1. A hard clipping is set to 5.0 to avoid gradient explosion during the BPTT learning. The initial learning rate is set to 0.5 and we halve the learning rate at the end of each epoch if the cross entropy function on the validation set does not decrease. We have used the weight decay, momentum and column normalization Pachitariu & Sahani (2013) in our experiments to improve model generalization. In the FOFE-based pooling function for HORNNs, we set the forgetting factor, α, to 0.6. We have used 400 nodes in each hidden layer for the PTB data set and 500 nodes per hidden layer for the English text8 set. In our experiments, we do not use the dropout regularization Zaremba et al. (2014) in all experiments since it significantly slows down the training speed, not applicable to any larger corpora. 1
1We will soon release the code for readers to reproduce all results reported in this paper.

4.1 LANGUAGE MODELING ON PTB
The standard Penn Treebank (PTB) corpus consists of about 1M words. The vocabulary size is limited to 10k. The preprocessing method and the way to split data into training/validation/test sets are the same as Mikolov et al. (2011). PTB is a relatively small text corpus. We first investigate various model configurations for the HORNNs based on PTB and then compare the best performance with other results reported on this task.

4.1.1 EFFECT OF ORDERS IN HORNNS
In the first experiment, we first investigate how the used orders in HORNNs may affect the performance of language models (as measured by perplexity). We have examined all different higher order model structures proposed in this paper, including HORNNs and various pooling functions in HORNNs. The orders of these examined models varies among 2, 3 and 4. We have listed the performance of different models on PTB in Table 1. As we may see, we are able to achieve a significant improvement in perplexity when using higher order RNNs for language models on PTB, roughly 10-20 reduction in PPL over regular RNNs. We can see that performance may improve slightly when the order is increased from 2 to 3 but no significant gain is observed when the order is further increased to 4. As a result, we choose the 3rd-order HORNN structure for the following experiments. Among all different HORNN structures, we can see that FOFE-based pooling and gated structures yield the best performance on PTB.
In language modeling, both input and output layers account for the major portion of model parameters. Therefore, we do not significantly increase model size when we go to higher order structures. For example, in Table 1, a regular RNN contains about 8.3 millions of weights while a 3rd-order HORNN (the same for max or FOFE pooling structures) has about 8.6 millions of weights. In comparison, an LSTM model has about 9.3 millions of weights and a 3rd-order gated HORNN has about 9.6 millions of weights.
As for the training speed, most HORNN models are only slightly slower than regular RNNs. For example, one epoch of training on PTB running in one NVIDIA’s TITAN X GPU takes about 80 seconds for an RNN, about 120 seconds for a 3rd-order HORNN (the same for max or FOFE pooling structures). Similarly, training of gated HORNNs is also slightly slower than LSTMs. For example, one epoch on PTB takes about 200 seconds for an LSTM, and about 225 seconds for a 3rd-order gates HORNN.

4.1.2 MODEL COMPARISON ON PENN TREEBANK
At last, we report the best performance of various HORNNs on the PTB test set in Table 2. We compare our 3rd-order HORNNs with all other models reported on this task, including RNN Mikolov et al. (2011), stack RNN Pascanu et al. (2014), deep RNN Pascanu et al. (2014), FOFE-FNN Zhang et al. (2015) and LSTM Graves (2013). 2 From the results in Table 2, we can see that our proposed higher order RNN architectures significantly outperform all other baseline models reported on this task. Both FOFE-based pooling and gated HORNNs have achieved the state-of-the-art performance,
2All models in Table 2 do not use the dropout regularization, which is somehow equivalent to data augmentation. In Zaremba et al. (2014); Kim et al. (2015), the proposed LSTM-LMs (word level or character level) achieve lower perplexity but they both use the dropout regularization and much bigger models and it takes days to train the models, which is not applicable to other larger tasks.
Table 2: Perplexities on the PTB test set for various examined models.
Models Test KN 5-gram Mikolov et al. (2011) 141 RNN Mikolov et al. (2011) 123 CSLM5Aransa et al. (2015) 118.08 LSTM Graves (2013) 117 genCNN Wang et al. (2015) 116.4 Gated word&charMiyamoto & Cho (2016) 113.52 E2E Mem Net Sukhbaatar et al. (2015) 111 Stack RNN Pascanu et al. (2014) 110 Deep RNN Pascanu et al. (2014) 107 FOFE-FNN Zhang et al. (2015) 108 HORNN (3rd order) 108 Max HORNN (3rd order) 109 FOFE HORNN (3rd order) 101 Gated HORNN (3rd order) 100
Table 3: Perplexities on the text8 test set for various models.
Models Test RNN Mikolov et al. (2014) 184 LSTM Mikolov et al. (2014) 156 SCRNN Mikolov et al. (2014) 161 E2E Mem Net Sukhbaatar et al. (2015) 147 HORNN (3rd order) 172 Max HORNN (3rd order) 163 FOFE HORNN (3rd order) 154 Gated HORNN (3rd order) 144
i.e., 100 in perplexity on this task. To the best of our knowledge, this is the best reported performance on PTB under the same training condition.

4.2 LANGUAGE MODELING ON ENGLISH TEXT8
In this experiment, we will evaluate our proposed HORNNs on a much larger text corpus, namely the English text8 data set. The text8 data set contains a preprocessed version of the first 100 million characters downloaded from the Wikipedia website. We have used the same preprocessing method as Mikolov et al. (2014) to process the data set to generate the training and test sets. We have limited the vocabulary size to about 44k by replacing all words occurring less than 10 times in the training set with an <UNK> token. The text8 set is about 20 times larger than PTB in corpus size. The model training on text8 takes longer to finish. We have not tuned hyperparameters in this data set. We simply follow the best setting used in PTB to train all HORNNs for the text8 data set. Meanwhile, we also follow the same learning schedule used in Mikolov et al. (2014): We first initialize the learning rate to 0.5 and run 5 epochs using this learning rate; After that, the learning rate is halved at the end of every epoch.
Because the training is time-consuming, we have only evaluated 3rd-order HORNNs on the text8 data set. The perplexities of various HORNNs are summarized in Table 3. We have compared our HORNNs with all other baseline models reported on this task, including RNN Mikolov et al. (2014), LSTM Mikolov et al. (2014), SCRNN Mikolov et al. (2014) and end-to-end memory networks Sukhbaatar et al. (2015). Results have shown that all HORNN models work pretty well in this data set except the normal HORNN significantly underperforms the other three models. Among them, the gated HORNN model has achieved the best performance, i.e., 144 in perplexity on this task, which is slightly better than the recent result obtained by end-to-end memory networks (using a rather complicated structure). To the best of our knowledge, this is the best performance reported on this task.

5 CONCLUSIONS
In this paper, we have proposed some new structures for recurrent neural networks, called as higher order RNNs (HORNNs). In these structures, we use more memory units to keep track of more preceding RNN states, which are all fed along various feedback paths to the hidden layer to generate the feedback signals. In this way, we may enhance the model to capture long term dependency in sequential data. Moreover, we have proposed to use several types of pooling functions to calibrate multiple feedback paths. Experiments have shown that the pooling technique plays a critical role in learning higher order RNNs effectively. In this work, we have examined HORNNs for the language modeling task using two popular data sets, namely the Penn Treebank (PTB) and text8 sets. Experimental results have shown that the proposed higher order RNNs yield the state-of-the-art per-
formance on both data sets, significantly outperforming the regular RNNs as well as the popular LSTMs. As the future work, we are going to continue to explore HORNNs for other sequential modeling tasks, such as speech recognition, sequence-to-sequence modelling and so on.
","In this paper, we study novel neural network structures to better model long term dependency in sequential data. We propose to use more memory units to keep track of more preceding states in recurrent neural networks (RNNs), which are all recurrently fed to the hidden layers as feedback through different weighted paths. By extending the popular recurrent structure in RNNs, we provide the models with better short-term memory mechanism to learn long term dependency in sequences. Analogous to digital filters in signal processing, we call these structures as higher order RNNs (HORNNs). Similar to RNNs, HORNNs can also be learned using the back-propagation through time method. HORNNs are generally applicable to a variety of sequence modelling tasks. In this work, we have examined HORNNs for the language modeling task using two popular data sets, namely the Penn Treebank (PTB) and English text8. Experimental results have shown that the proposed HORNNs yield the state-of-the-art performance on both data sets, significantly outperforming the regular RNNs as well as the popular LSTMs.",ICLR 2017 conference submission,False,,"This paper proposes an idea of looking n-steps backward when modelling sequences with RNNs. The proposed RNN does not only use the previous hidden state (t-1) but also looks further back ( (t - k) steps, where k=1,2,3,4 ). The paper also proposes a few different ways to aggregate multiple hidden states from the past.


The reviewer can see few issues with this paper.

Firstly, the writing of this paper requires improvement. The introduction and abstract are wasting too much space just to explain unrelated facts or to describe already well-known things in the literature. Some of the statements written in the paper are misleading. For instance, it explains, “Among various neural network models, recurrent neural networks (RNNs) are appealing for modeling sequential data because they can capture long term dependency in sequential data using a simple mechanism of recurrent feedback” and then it says RNNs cannot actually capture long-term dependencies that well. RNNs are appealing in the first place because they can handle variable length sequences and can model temporal relationships between each symbol in a sequence. The criticism against LSTMs is hard to accept when it says: LSTMs are slow and because of the slowness, they are hard to scale at larger tasks. But we all know that some companies are already using gigantic seq2seq models for their production (LSTMs are used as building blocks in their systems). This indicates that the LSTMs can be practically used in a very large-scale setting.


Secondly, the idea proposed in the paper is incremental and not new to the field. There are other previous works that propose to use direct connections to the previous hidden states [1]. However, the previous works do not use aggregation of multiple number of previous hidden states. Most importantly, the paper fails to deliver a proper analysis on whether its main contribution is actually helpful to improve the problem posed in the paper. The new architecture is said that it handles the long-term dependencies better, however, there is no rigorous proof or intuitive design in the architecture that help us to understand why it should work better. By the design of the architecture, and speaking in very high-level, it seems like the model maybe helpful to mitigate the vanishing gradients issue by a linear factor. It is always a good practice to have at least one page to analyze the empirical findings in the paper.


Thirdly, the baseline models used in this paper are very weak. Their are plenty of other models that are trained and tested on word-level language modelling task using Penn Treebank corpus, but the paper only contains a few of outdated models. I cannot fully agree on the statement “To the best of our knowledge, this is the best performance on PTB under the same training condition”, these days, RNN-based methods usually score below 80 in terms of the test perplexity, which are far lower than 100 achieved in this paper.


[1] Zhang et al., “Architectural Complexity Measures of Recurrent Neural Networks”, NIPS’16

---

Paper presents the idea of using higher order recurrence in LSTMs. The ideas are well presented and easy to follow.
 However, the results are far from convincing, easily being below well established numbers in the domain. Since the mode is but a very simple extension of the baseline recurrent models using LSTMs that are state of the art on language modelling, it should have been easy to make a fair comparison by replacing the LSTMs of the state of the art models with higher order versions, but the authors did not do that. Their claimed numbers for SOTA are far from previously reported numbers in that setup, as pointed out by reviewers.

---

The authors of the paper explore the idea of incorporating skip connections *over time* for RNNs. Even though the basic idea is not particularly innovative, a few proposals on how to merge that information into the current hidden state with different pooling functions are evaluated. The different models are compared on two popular text benchmarks.

Some points.

1) The experiments feature only NLP and only prediction tasks. It would have been nice to see the models in other domains, i.e. modelling a conditional distribution p(y|x), not only p(x). Further, sensory input data such as audio or video would have given further insight.

2) As pointed out by other reviewers, it does not feel as if the comparisons to other models are fair. SOTA on NLP changes quickly and it is hard to place the experiments in the complete picture.

3) It is claimed that this helps long-term prediction. I think the paper lacks a corresponding analysis, as pointed out in an earlier question of mine.

4)  It is claimed that LSTM trains slow and is hard to scale. For one does this not match my personal experience. Then, the prevalence of LSTM systems in production systems (e.g. Google, Baidu, Microsoft, …) clearly speaks against this.


I like the basic idea of the paper, but the points above make me think it is not ready for publication.

---

I think the backbone of the paper is interesting and could lead to something potentially quite useful. I like the idea of connecting signal processing with recurrent network and then using tools from one setting in the other. However, while the work has nuggets of very interesting observations, I feel they can be put together in a better way. 
I think the writeup and everything can be improved and I urge the authors to strive for this if the paper doesn't go through. I think some of the ideas of how to connect to the past are interesting, it would be nice to have more experiments or to try to understand better why this connections help and how.

---

This paper proposes an idea of looking n-steps backward when modelling sequences with RNNs. The proposed RNN does not only use the previous hidden state (t-1) but also looks further back ( (t - k) steps, where k=1,2,3,4 ). The paper also proposes a few different ways to aggregate multiple hidden states from the past.


The reviewer can see few issues with this paper.

Firstly, the writing of this paper requires improvement. The introduction and abstract are wasting too much space just to explain unrelated facts or to describe already well-known things in the literature. Some of the statements written in the paper are misleading. For instance, it explains, “Among various neural network models, recurrent neural networks (RNNs) are appealing for modeling sequential data because they can capture long term dependency in sequential data using a simple mechanism of recurrent feedback” and then it says RNNs cannot actually capture long-term dependencies that well. RNNs are appealing in the first place because they can handle variable length sequences and can model temporal relationships between each symbol in a sequence. The criticism against LSTMs is hard to accept when it says: LSTMs are slow and because of the slowness, they are hard to scale at larger tasks. But we all know that some companies are already using gigantic seq2seq models for their production (LSTMs are used as building blocks in their systems). This indicates that the LSTMs can be practically used in a very large-scale setting.


Secondly, the idea proposed in the paper is incremental and not new to the field. There are other previous works that propose to use direct connections to the previous hidden states [1]. However, the previous works do not use aggregation of multiple number of previous hidden states. Most importantly, the paper fails to deliver a proper analysis on whether its main contribution is actually helpful to improve the problem posed in the paper. The new architecture is said that it handles the long-term dependencies better, however, there is no rigorous proof or intuitive design in the architecture that help us to understand why it should work better. By the design of the architecture, and speaking in very high-level, it seems like the model maybe helpful to mitigate the vanishing gradients issue by a linear factor. It is always a good practice to have at least one page to analyze the empirical findings in the paper.


Thirdly, the baseline models used in this paper are very weak. Their are plenty of other models that are trained and tested on word-level language modelling task using Penn Treebank corpus, but the paper only contains a few of outdated models. I cannot fully agree on the statement “To the best of our knowledge, this is the best performance on PTB under the same training condition”, these days, RNN-based methods usually score below 80 in terms of the test perplexity, which are far lower than 100 achieved in this paper.


[1] Zhang et al., “Architectural Complexity Measures of Recurrent Neural Networks”, NIPS’16

---

This paper proposes an idea of looking n-steps backward when modelling sequences with RNNs. The proposed RNN does not only use the previous hidden state (t-1) but also looks further back ( (t - k) steps, where k=1,2,3,4 ). The paper also proposes a few different ways to aggregate multiple hidden states from the past.


The reviewer can see few issues with this paper.

Firstly, the writing of this paper requires improvement. The introduction and abstract are wasting too much space just to explain unrelated facts or to describe already well-known things in the literature. Some of the statements written in the paper are misleading. For instance, it explains, “Among various neural network models, recurrent neural networks (RNNs) are appealing for modeling sequential data because they can capture long term dependency in sequential data using a simple mechanism of recurrent feedback” and then it says RNNs cannot actually capture long-term dependencies that well. RNNs are appealing in the first place because they can handle variable length sequences and can model temporal relationships between each symbol in a sequence. The criticism against LSTMs is hard to accept when it says: LSTMs are slow and because of the slowness, they are hard to scale at larger tasks. But we all know that some companies are already using gigantic seq2seq models for their production (LSTMs are used as building blocks in their systems). This indicates that the LSTMs can be practically used in a very large-scale setting.


Secondly, the idea proposed in the paper is incremental and not new to the field. There are other previous works that propose to use direct connections to the previous hidden states [1]. However, the previous works do not use aggregation of multiple number of previous hidden states. Most importantly, the paper fails to deliver a proper analysis on whether its main contribution is actually helpful to improve the problem posed in the paper. The new architecture is said that it handles the long-term dependencies better, however, there is no rigorous proof or intuitive design in the architecture that help us to understand why it should work better. By the design of the architecture, and speaking in very high-level, it seems like the model maybe helpful to mitigate the vanishing gradients issue by a linear factor. It is always a good practice to have at least one page to analyze the empirical findings in the paper.


Thirdly, the baseline models used in this paper are very weak. Their are plenty of other models that are trained and tested on word-level language modelling task using Penn Treebank corpus, but the paper only contains a few of outdated models. I cannot fully agree on the statement “To the best of our knowledge, this is the best performance on PTB under the same training condition”, these days, RNN-based methods usually score below 80 in terms of the test perplexity, which are far lower than 100 achieved in this paper.


[1] Zhang et al., “Architectural Complexity Measures of Recurrent Neural Networks”, NIPS’16

---

Paper presents the idea of using higher order recurrence in LSTMs. The ideas are well presented and easy to follow.
 However, the results are far from convincing, easily being below well established numbers in the domain. Since the mode is but a very simple extension of the baseline recurrent models using LSTMs that are state of the art on language modelling, it should have been easy to make a fair comparison by replacing the LSTMs of the state of the art models with higher order versions, but the authors did not do that. Their claimed numbers for SOTA are far from previously reported numbers in that setup, as pointed out by reviewers.

---

The authors of the paper explore the idea of incorporating skip connections *over time* for RNNs. Even though the basic idea is not particularly innovative, a few proposals on how to merge that information into the current hidden state with different pooling functions are evaluated. The different models are compared on two popular text benchmarks.

Some points.

1) The experiments feature only NLP and only prediction tasks. It would have been nice to see the models in other domains, i.e. modelling a conditional distribution p(y|x), not only p(x). Further, sensory input data such as audio or video would have given further insight.

2) As pointed out by other reviewers, it does not feel as if the comparisons to other models are fair. SOTA on NLP changes quickly and it is hard to place the experiments in the complete picture.

3) It is claimed that this helps long-term prediction. I think the paper lacks a corresponding analysis, as pointed out in an earlier question of mine.

4)  It is claimed that LSTM trains slow and is hard to scale. For one does this not match my personal experience. Then, the prevalence of LSTM systems in production systems (e.g. Google, Baidu, Microsoft, …) clearly speaks against this.


I like the basic idea of the paper, but the points above make me think it is not ready for publication.

---

I think the backbone of the paper is interesting and could lead to something potentially quite useful. I like the idea of connecting signal processing with recurrent network and then using tools from one setting in the other. However, while the work has nuggets of very interesting observations, I feel they can be put together in a better way. 
I think the writeup and everything can be improved and I urge the authors to strive for this if the paper doesn't go through. I think some of the ideas of how to connect to the past are interesting, it would be nice to have more experiments or to try to understand better why this connections help and how.

---

This paper proposes an idea of looking n-steps backward when modelling sequences with RNNs. The proposed RNN does not only use the previous hidden state (t-1) but also looks further back ( (t - k) steps, where k=1,2,3,4 ). The paper also proposes a few different ways to aggregate multiple hidden states from the past.


The reviewer can see few issues with this paper.

Firstly, the writing of this paper requires improvement. The introduction and abstract are wasting too much space just to explain unrelated facts or to describe already well-known things in the literature. Some of the statements written in the paper are misleading. For instance, it explains, “Among various neural network models, recurrent neural networks (RNNs) are appealing for modeling sequential data because they can capture long term dependency in sequential data using a simple mechanism of recurrent feedback” and then it says RNNs cannot actually capture long-term dependencies that well. RNNs are appealing in the first place because they can handle variable length sequences and can model temporal relationships between each symbol in a sequence. The criticism against LSTMs is hard to accept when it says: LSTMs are slow and because of the slowness, they are hard to scale at larger tasks. But we all know that some companies are already using gigantic seq2seq models for their production (LSTMs are used as building blocks in their systems). This indicates that the LSTMs can be practically used in a very large-scale setting.


Secondly, the idea proposed in the paper is incremental and not new to the field. There are other previous works that propose to use direct connections to the previous hidden states [1]. However, the previous works do not use aggregation of multiple number of previous hidden states. Most importantly, the paper fails to deliver a proper analysis on whether its main contribution is actually helpful to improve the problem posed in the paper. The new architecture is said that it handles the long-term dependencies better, however, there is no rigorous proof or intuitive design in the architecture that help us to understand why it should work better. By the design of the architecture, and speaking in very high-level, it seems like the model maybe helpful to mitigate the vanishing gradients issue by a linear factor. It is always a good practice to have at least one page to analyze the empirical findings in the paper.


Thirdly, the baseline models used in this paper are very weak. Their are plenty of other models that are trained and tested on word-level language modelling task using Penn Treebank corpus, but the paper only contains a few of outdated models. I cannot fully agree on the statement “To the best of our knowledge, this is the best performance on PTB under the same training condition”, these days, RNN-based methods usually score below 80 in terms of the test perplexity, which are far lower than 100 achieved in this paper.


[1] Zhang et al., “Architectural Complexity Measures of Recurrent Neural Networks”, NIPS’16",,,,,,4.333333333333333,,,4.0,,
580,"INTELLIGIBLE LANGUAGE MODELING WITH INPUT SWITCHED AFFINE NETWORKS
Authors: Jakob N. Foerster, Justin Gilmer, Jan Chorowski, Jascha Sohl-Dickstein, David Sussillo
Source file: 580.pdf

ABSTRACT
The computational mechanisms by which nonlinear recurrent neural networks (RNNs) achieve their goals remains an open question. There exist many problem domains where intelligibility of the network model is crucial for deployment. Here we introduce a recurrent architecture composed of input-switched affine transformations, in other words an RNN without any nonlinearity and with one set of weights per input. We show that this architecture achieves near identical performance to traditional architectures on language modeling of Wikipedia text, for the same number of model parameters. It can obtain this performance with the potential for computational speedup compared to existing methods, by precomputing the composed affine transformations corresponding to longer input sequences. As our architecture is affine, we are able to understand the mechanisms by which it functions using linear methods. For example, we show how the network linearly combines contributions from the past to make predictions at the current time step. We show how representations for words can be combined in order to understand how context is transferred across word boundaries. Finally, we demonstrate how the system can be executed and analyzed in arbitrary bases to aid understanding.

1 INTRODUCTION
Neural networks and the general field of deep learning have made remarkable progress over the last few years in fields such as object recognition (Krizhevsky et al., 2012), language translation (Sutskever et al., 2014), and speech recognition (Graves et al., 2013). For all of the success of the deep learning approach however, there are certain application domains in which intelligibility of the system is an essential design requirement. One commonly used example is the necessity to understand the decisions that a self-driving vehicle makes when avoiding various obstacles in its path. Another example is the application of neural network methodologies to scientific discovery (Mante et al., 2013). Even where intelligibility is not an overt design requirement, it is fair to say that most users of neural networks would like to better understand the models they deploy.
There are at least two approaches to creating intelligible network models. One approach is to build networks as normal, and then apply analysis techniques after training. Often this approach yields systems that perform extremely well, and whose intelligibility is limited. A second approach is to build a neural network where intelligibility is an explicit design constraint. In this case, the typical result is a system that can be understood reasonably well, but may underperform. In this work we follow this second approach and build intelligibility into our network model, yet without sacrificing performance for the task we studied.
Designing intelligibility into neural networks for all application domains is a worthy, but daunting goal. Here we contribute to that larger goal by focusing on a commonly studied task, that of character based
∗This work was performed as an intern at Google Brain. †Work done as a member of the Google Brain Residency program (g.co/brainresidency) ‡Work performed when author was a visiting faculty at Google Brain.
language modeling. We develop and analyze a model trained on a one-step-ahead prediction task of the Text8 dataset, which is 10 million characters of Wikipedia text (Mahoney, 2011). The model we use is a switched affine system, where the input determines the switching behavior by selecting a transition matrix and bias as a function of that input, and there is no nonlinearity. Surprisingly, we find that this simple architecture performs as well as a vanilla RNN, Gated Recurrent Unit (GRU) (Cho et al., 2014), IRNN (Le et al., 2015), or Long Short Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997) in this task, despite being a simpler and potentially far more computationally efficient architecture.
In what follows, we discuss related work, define our Input Switched Affine Network (ISAN) model, demonstrate its performance on the one-step-ahead prediction task, and then analyze the model in a multitude of ways, most of which would be currently difficult or impossible to accomplish with modern nonlinear recurrent architectures.

2 RELATED WORK
Work by the authors of (Karpathy et al., 2015) attempted to use character-based language modeling to begin to understand how the LSTM (Hochreiter & Schmidhuber, 1997) functions. In it, they employ n-gram word models to highlight what the LSTM has – and has not – learned about the text corpus. They were able to break down LSTM language model errors into classes, such as e.g., ""rare word"" errors. The authors of (Greff et al., 2015) engaged in a large study to understand the relative importance of the various components of an LSTM. The authors of (Collins et al., 2016) performed an enormous hyperparameter study to disentangle the effects of capacity and trainability in a number of RNN architectures.
Attempts to understand networks in more general contexts include the use of linearization and nonlinear dynamical systems theory to understand RNNs in (Sussillo & Barak, 2013). In feedforward networks the use of linear probes has been suggested by (Alain & Bengio, 2016), and there exist a host of back-propagation techniques used to infer the most important input to drive various components of the feed-forward network, e.g. (Le et al., 2012).
The ISAN uses an input-switched affine model. The highly related linear time-varying systems are standard material in undergraduate electrical engineering text books. Probabilistic versions of switching linear models with discrete latent variables have a history in the context of probabilistic graphical models. A recent example is the switched linear dynamical system in (Linderman et al., 2016). Focusing on language modeling, (Belanger & Kakade, 2015) defined a probabilistic linear dynamical system as a generative language model for creating context-dependent token embeddings and then used steady-state Kalman filtering for inference over token sequences. They used singular value decomposition and discovered that the right and left singular vectors were semantically and syntactically related. One difference between the ISAN and the LDS is that the weight matrices of the ISAN are input token dependent (while the biases of both models are input dependent). Finally, multiplicative neural networks (MRNNs) were proposed precisely for character based language modeling in (Sutskever et al., 2011; Martens & Sutskever, 2011). The MRNN architecture is similar to our own, in that the dynamics matrix switches as a function of the input character. However, the MRNN relied on a tanh nonlinearity, while our model is explicitly linear. It is this property of our model which makes it both amenable to analysis, and computationally efficient.
The Observable Operator Model (OOM) (Jaeger, 2000) is similar to the ISAN in that the OOM updates a latent state using a separate transition matrix for each input symbol and performs probabilistic sequence modeling. Unlike the ISAN, the OOM requires that a linear projection of the hidden state corresponds to a normalized sequence probability. This imposes strong constraints on both the model parameters and the model dynamics, and restricts the choice of training algorithms. In contrast, the ISAN applies an affine readout to the hidden state to obtain logits, which are then pushed through a SoftMax to obtain probabilities. Therefore no constraints need to be imposed on the ISAN’s parameters and training is easy using backprop. Lastly, the ISAN is formulated as an affine, rather than linear model. While this doesn’t change the class of processes that can be modeled, it enhances the stability of training and greatly enhances interpretability. We elaborate upon these ideas in Section 6.1.

3 METHODS

3.1 MODEL DEFINITION
In what follows Wx and bx respectively denote a transition matrix and a bias vector for a specific input x, the symbol xt is the input at time t, and ht is the hidden state at time t. Our ISAN model is defined as
ht = Wxt ht−1 + bxt . (1)
The network also learns an initial hidden state h0. We emphasize the intentional absence of any nonlinear activation function.

3.2 CHARACTER LEVEL LANGUAGE MODELLING WITH RNNS
The RNNs are trained on the Text8 Wikipedia dataset, for one-step-ahead character prediction. The Text8 dataset consists only of the 27 characters ‘a’-‘z’ and ‘_’ (space). Given a character sequence of x1, ...,xt, the RNNs are trained to minimize the cross-entropy between the true next character, and the output prediction. We map from the hidden state, ht, into a logit space via an affine map. The probabilities are computed as
p (xt+1) = softmax (lt) (2) lt = Wro ht + bro, (3)
where Wro and bro are the readout weights and biases, and lt is the logit vector. In line with (Collins et al., 2016) we split the training data into 80%, 10%, and 10% for train, test, and evaluation set respectively. The network was trained with the same hyperparameter tuning infrastructure as in (Collins et al., 2016). Analysis in this paper is carried out on the best-performing ISAN model, which has 1, 271, 619 parameters, corresponding to 216 hidden units, and 27 dynamics matrices Wx and biases bx.

4 RESULTS AND ANALYSIS

4.1 ISAN PERFORMANCE ON THE TEXT8 TASK
The results on Text8 are shown in Figure 1a. For the largest parameter count, the ISAN matches almost exactly the performance of all other nonlinear models with the same number of maximum parameters: RNN, IRNN, GRU, LSTM. However, we note that for small numbers of parameters the ISAN performs considerably worse than other architectures. All analyses use ISAN trained with 1.28e6 maximum parameters (1.58 bpc cross entropy). Samples of generated text from this model are relatively coherent. We show two examples, after priming with ""annual reve"", at inverse temperature of 1.5, and 2.0, respectively:
• “annual revenue and producer of the telecommunications and former communist action and saving its new state house of replicas and many practical persons”
• “annual revenue seven five three million one nine nine eight the rest of the country in the united states and south africa new”.
As a preliminary, comparative analysis, we performed PCA on the state sequence over a large set of sequences for the vanilla RNN, GRU of varying sizes, and ISAN. This is shown in Figure 1b. The eigenvalue spectra, in log of variance explained, was significantly flatter for the ISAN than the other architectures.
We also compare the ISAN performance to a fully linear RNN without input switched dynamics. This achieves a cross-entropy of 3.1 bits / char, independent of network size. This perplexity is only slightly better than that of a Naive Bayes model on the task, at 3.3 bits / char. The output probability of the fully linear network is a product of contributions from each previous character, as in Naive Bayes. Those factorial contributions are learned however, giving ISAN a slight advantage. We also run a comparison to a fully linear network with a non-linear readout. This achieves 2.15 bits /
char, independent of network size. Both of these comparisons illustrate the importance of the input switched dynamics for achieving good results in the absence of non-linear hidden state dynamics.
Lastly we also test to what extent the ISAN can deal with large dictionaries by running it on a byte-pair encoding of the text8 task, where the input dictionary consists of the 272 different possible character combinations. We find that in this setup the LSTM consistently outperforms the ISAN for the same number of parameters. At 1.3m parameters the LSTM achieves a cross entropy of 3.4 bits / char-pair, while ISAN achieves 3.55. One explanation for this finding is that the matrices in ISAN are a factor of 27 smaller than the matrices of the LSTMs. For very large numbers of parameters the performance of any architecture saturates in the number of parameters, at which point the ISAN can ‘catch-up’ with more parameter efficient architectures like LSTMs.

4.2 DECOMPOSITION OF CURRENT PREDICTIONS BASED ON PREVIOUS TIME STEPS
Taking advantage of the linearity of the hidden state dynamics for any sequence of inputs, we can decompose the current latent state ht into contributions originating from different timepoints s in the history of the input:
ht = t∑ s=0
( t∏
s′=s+1
Wxs′ ) bxs , (4)
where the empty product when s + 1 > t is 1 by convention, and bx0 = h0 is the learned initial hidden state. This is useful because we can analyze which factors were important in the past, for determining the current character prediction.
Using this decomposition and the linearity of matrix multiplication we can also write the unnormalized logit-vector, lt, as a sum of terms linear in the biases,
lt = bro + t∑ s=0 κts (5)
κts = Wro
( t∏
s′=s+1
Wxs′ ) bxs , (6)
where κts is the contribution from timestep s to the logits at timestep t, and κ t t = bxt . For notational convenience we will sometimes replace the subscript s with the corresponding input character xs at step s when referring to κts – e.g. κ t ‘q’ to refer to the contribution from the character ‘q’ in a string.
Similarly, when discussing the summed contributions from a word or substring we will sometimes write κtword to mean the summed contributions of all the κ t s from that source word, ∑ s∈word κ t s – e.g. κt‘the’ to refer to the total logit contribution from the word ‘the’.
While in standard RNNs the nonlinearity causes interdependence of the bias terms across time steps, in the ISAN the bias terms can be interpreted as independent linear contributions to the state that are propagated and transformed through time. We emphasize that κts includes the multiplicative contributions from the Wxs′ for s < s
′ ≤ t. It is however independent of prior inputs, xs′ for s′ < s. This is the main difference between the analysis we can carry out with the ISAN compared to a non-linear RNN. In general the contribution of a specific character sequence will depend on the hidden state at the start of the sequence. Due to the linearity of the dynamics, this dependency does not exist in the ISAN.
In Figure 2 we show an example of how this decomposition allows us to understand why a particular prediction is made at a given point in time, and how previous characters influence the decoding. For example, the sequence ‘_annual_revenue_’ is processed by the ISAN: Starting with an all-zero hidden state, we use equation (6) to accumulate a sequence of κt‘_′ ,κ t ‘a′ ,κ t ‘n′ ,κ t ‘n′ , .... These values can then be used to understand the prediction of the network at some time t, by simple addition across the s index, which is shown in Figure 2.
In Figure 3 we provide a detailed view of how past characters contribute to the logits predicting the next character. There are two competing options for the next letter in the word stem ‘reve’: either ‘revenue’ or ‘reverse’. We show that without the contributions from ‘_annual’ the most likely decoding of the character after the second ‘e’ is ‘r’ (to form ‘reverse’), while the contributions from ‘_annual’ tip the balance in favor of ‘n’, decoding to ‘revenue’. In a standard RNN a similar analysis could be carried out by comparing the prediction given an artificially limited history.
Using the decomposition of current step predictions in to κts, we can also investigate how quickly the contributions of κts decay as a function of t− s. In Figure 4a we can see that this contribution decays on two different exponential timescales. We hypothesize that the first time scale corresponds to the decay within a word, while the next corresponds to the decay of information across words and sentences. This effect is also visible in Figure 5. We note that it would be difficult to carry out this analysis in a non-linear RNN.
We can also show the relevance of the κts contributions to the decoding of characters at different positions in the word. For examples, we observe that κt‘_’ makes important contributions to the prediction of the next character at time t. We show that using only the κt‘_’, the model can achieve
a cross entropy of < 1 / char when the position of the character is more than 3 letters from the beginning of the word.
Furthermore we can link back from the norm-decay to the importance of past characters for the decoding quality. By artificially limiting the number of past κ available for prediction, Figure 4c, we show that the prediction quality improves rapidly when extending the history from 0 to 10 characters and then saturates. This rapid improvement aligns with the range of faster decay in Figure 4a.

4.3 FROM CHARACTERS TO WORDS
The ISAN provides a natural means of moving from character level representation to word level. Using the linearity of the hidden state dynamics we can aggregate all of the κts belonging to a given
word and visualize them as a single contribution to the prediction of the letters in the next word. This allows us to understand how each preceding word impacts the decoding for the letters of later words. In Figure 5 we show that the words ‘higher’ and ‘than’ make large contributions to the prediction of the characters ‘h’ and ‘n’ in ‘tevenue’, as measured by the norm of the κt‘_the’ and κt‘_annual’.
In Figure 6 we show that these κtword are more than a mathematical convenience and even capture word-level semantic information. Shown is a t-SNE embedding of the κtword for the most common 4000 words in the data-set, with examples of the kind of clusters that arise.

4.4 CHANGE OF BASIS
We are free to perform a change of basis on the hidden state, and then to run the affine ISAN dynamics in that new basis. Note that this change of basis is not possible for other RNN architectures, since the action of the nonlinearity depends on the choice of basis.
In particular we can construct a ‘readout basis’ that explicitly divides the latent space into a subspace Pro‖ spanned by the rows of the readout matrix Wro, and its orthogonal complement P ro ⊥ . This representation explicitly divides the hidden state dynamics into a 27-dimensional ‘readout’ subspace that is accessed by the readout matrix to make predictions, and a ‘computational’ subspace comprising the remaining 216− 27 dimensions that are orthogonal to the readout matrix. We apply this change of basis to analyze an intriguing observation about the hidden offsets bx: As shown in Figure 7, the norm of the bx is strongly correlated to the log-probability of the unigram x in the training data. Re-expressing network parameters using the ‘readout basis’ shows that this correlation is not related to reading out the next-step prediction. This is because the norm of the projection of bx into Pro⊥ remains strongly correlated with character frequency, while the projections into Pro‖ have norms that show little correlation. This indicates that the information content or surprise of a letter is encoded through the norm of the component of bx in the computational space, rather than in the readout space.
Similarly, in Figure 8 we illustrate that the structure in the correlations between the bx is due to their components in Pro‖ , while the correlation in P ro ⊥ is relatively uniform. We can clearly see two blocks of high correlations between the vowels and consonants respectively, while b‘_’ is uncorrelated to either.
4.5 COMPARISON WITH n-GRAM MODEL WITH BACKOFF
We compared the computation performed by n-gram language models and those performed by the ISAN. An n-gram model with back-off weights expresses the conditional probability p (xt|x1...xt−1) as a sum of smoothed count ratios of n-grams of different lengths, with the contribution of shorter n-grams down-weighted by backoff weights. On the other hand, the computations performed by the ISAN start with the contribution of bro to the logits, which as shown in Figure 9a) corresponds to the unigram log-probabilities. The logits are then additively updated with contributions from longer n-grams, represented by κts. This additive contribution to the logits corresponds to a multiplicative modification of the emission probabilities from histories of different length. For long time lags, the additive correction to log-probabilities becomes small (Figure 2), which corresponds to multiplication by a uniform distribution. Despite these differences in how n-gram history is incorporated, we
nevertheless observe an agreement between empirical models estimated on the training set and model predictions for unigrams and bigrams. Figure 9 shows that the bias term bro gives the unigram probabilities of letters, while the addition of the offset terms bx accurately predict the bigram distribution of P (xt+1|xt). Shown are both an example, P (x|‘_′), and a summary plot for all 27 letters.
We further explore the n-gram comparison by artificially limiting the length of the character history that is available to the ISAN for making predictions, as shown in Figure 4c).

5 ANALYSES OF PARENTHESES COUNTING TASK
To show the interpretability of the ISAN we train a model on the parenthesis counting task. Bringing together ideas from sections 4.4 and 6.1 we re-express the transition dynamics in a new basis that fully reveals computations performed by the ISAN.
We analyze the simple task of parentheses counting, which was defined in (Collins et al., 2016). Briefly, the RNN is required keep track of the nesting level of 3 different types of parentheses
independently. The inputs are the one-hot encoding of the different opening and closing parentheses (e.g. ‘(’, ‘)’, ‘{’, ‘}’) as well as a noise character (‘a’). The output is the one-hot encoding of the nesting level between (0-5), one set of counts for each parentheses task. One change from (Collins et al., 2016) is that we slightly simplify the problem by exchanging the cross-entropy error with an L2 error and linear readout (this change leads to slightly cleaner figures, but does not qualitatively change the results).
We first re-express the transition dynamics in terms of linear, rather than affine operations. Consider the matrix W′ ∈ R(n+1)×(n+1):
W′ = [ W b 0 1 ] , (7)
where 0 is a row vector of zeros. The matrix W′ emulates the affine transition for any hidden state h ∈ Rn×1: W′ [ h 1 ] = [ Wh+ b 1 ] . (8)
The matrices W and W′ are closely connected. Each eigenvalue of W is also an eigenvalue of W′. Moreover, eigenvectors of W become the eigenvectors of W′ when expanded with a zero dimension. In fact, W′ only has one extra eigenvalue of exactly 1 that is necessary to preserve the last dimension of the expanded hidden state.
To analyze the the parentheses task we analyze W′. The key to understanding how the network solves the parentheses task is to find a change of bases that clarifies the dynamics necessary to count 3 sets of independent parentheses nesting levels. In this case we use a matrix composed of the readout matrix, modified by adding a set of vectors that spans the null space of the readout (including the additional bias dimension).
W′ro =
[ Wro bro
O
] (9)
where O is the orthogonal complement of the subspace spanned by the row vectors of [Wro bro] in an N + 1-dimensional space. We perform the following change of basis of the dynamics matrices,
Wx(ro) ′ = Wro ′Wx ′ (W′ro) −1 (10)
and visualize the results in Fig. 10. Figure 10 shows that this system created delay lines which count the nesting level, with fixed point dynamics at the 0 count (5 count), so that the system stays at both numbers when the input would otherwise increment (decrement) the count. The matrices also implement fixed point dynamics, as implemented via an identity submatrix to preserve the memory of the parenthesis nesting counts when an unrelated symbol enters the system (e.g. The ‘{}’ count is preserved by the ‘(’ matrix when a ‘(’ symbol enters the system).

6 DISCUSSION
In this paper we motivated an input-switched affine recurrent network for the purpose of intelligibility. We showed that a switched affine architecture achieves the same performance, for the same number of maximum parameters, on a language modeling task as do more common RNN architectures, including GRUs and LSTMs. We performed a series of analyses, demonstrating that the simplicity of the latent dynamics makes the trained RNN far easier to understand and interpret.

6.1 BENEFITS OF AFFINE TRANSITIONS OVER LINEAR
ISAN uses affine operators to model state transitions assigned to each input symbol. Following eq. (1) each transition consists of matrix multiplication and bias vector addition. An important question is whether the biases are needed and how the ISAN would be impacted if linear transition operators were used instead of affine ones. The answer is two-fold. First, affine dynamics can be exactly implemented using linear operators in a hidden space expanded by one additional dimension. Therefore, the expressivity of ISAN does not depend on choosing a linear or affine formulation. However, we found that the affine parametrization of transitions is much easier to train. We attempted to train models using only linear transitions, but achieved a loss of only 4.1 bits per character, which
corresponds to the performance of a unigram character model. Second, affine operators are easier to interpret because they permit easy visualization of contributions of each input token on the final network’s prediction, as demonstrated in Section 4.2.

6.2 COMPUTATIONAL BENEFITS
Switched affine networks hold the potential to be massively more computationally and memory efficient for text processing than standard RNNs, as explained in the next two subsections.

6.2.1 SPARSE PARAMETER ACCESS
As shown in Figure 1a, the performance for fixed parameter count is nearly identical between the ISAN and other recurrent networks. However, at each time step, only the parameters associated with a single input are used. For K possible inputs and N parameters, the computational cost per update
step is O ( N K ) , a factor of K speedup over non-switched architectures. Similarly, the number of
hidden units is O (√
N K
) , a factor of K 1 2 memory improvement for storage of the latent state.

6.2.2 COMPOSITION OF AFFINE UPDATES
The memory and computational benefits in Section 6.2.1 are shared by other switched networks. However, ISAN is unique in its ability to precompute affine transformations corresponding to input strings. This is possible because the composition of affine transformations is also an affine transformation. This property is used in Section 4.3 to evaluate the linear contributions of words, rather than characters. This means that the hidden state update corresponding to an entire input sequence can be computed with identical cost to the update for a single character (plus the dictionary lookup cost for the composed transformation). ISAN can therefore achieve very large speedups on input processing, at the cost of increased memory use, by accumulating large lookup tables of the Wx and bx corresponding to common input sequences. Of course, practical implementations will have to incorporate complexities of memory management, batching, etc.

6.3 FUTURE WORK
There are some obvious future directions to this work. Currently, we define switching behavior using an input set with finite and manageable cardinality. Studying word-level language models with enormous vocabularies may require some additional logic to scale. Adapting this model to continuous-valued inputs is another important direction. One approach is to use a tensor factorization similar to that employed by the MRNN (Sutskever et al., 2014). Another is to build a language model which switches on bigrams or trigrams, rather than characters or words, targeting an intermediate number of affine transformations.
Training very large switched linear models has the potential to be extremely fruitful, due both to their improved computational efficiency, and our ability to better understand and manipulate their behavior.

7 ACKNOWLEDGEMENTS
We would like to thank Jasmine Collins for her help and advice, and Quoc Le, David Ha and Mohammad Norouzi for helpful discussions.
","The computational mechanisms by which nonlinear recurrent neural networks (RNNs) achieve their goals remains an open question. There exist many problem domains where intelligibility of the network model is crucial for deployment. Here we introduce a recurrent architecture composed of input-switched affine transformations, in other words an RNN without any nonlinearity and with one set of weights per input. We show that this architecture achieves near identical performance to traditional architectures on language modeling of Wikipedia text, for the same number of model parameters. It can obtain this performance with the potential for computational speedup compared to existing methods, by precomputing the composed affine transformations corresponding to longer input sequences. As our architecture is affine, we are able to understand the mechanisms by which it functions using linear methods. For example, we show how the network linearly combines contributions from the past to make predictions at the current time step. We show how representations for words can be combined in order to understand how context is transferred across word boundaries. Finally, we demonstrate how the system can be executed and analyzed in arbitrary bases to aid understanding.",ICLR 2017 conference submission,False,,"All reviewers have carefully looked at the paper and weakly support acceptance of the paper. Program Chairs also looked at this paper and believe that its contribution is too marginal and incremental in its current form. We encourage the authors to resubmit.

---

We have released the demo of a fully-understandable ISAN network for counting parenthesis at

---

We would like to thank all reviewers for their careful and thorough reviews, and for recommending paper acceptance. Based on your specific actionable feedback, we have since improved the paper with additional analysis and experiments. We hope that based on these new results, you will raise your scores and more confidently recommend acceptance.

In particular we have included a new analysis in Section 5 which provides an end-to-end interpretation of the functioning of the ISAN on a parenthesis counting task. For this example we believe we have really `cracked the case’, and fully explain the neural network behavior. We have also included a new analysis (Figure 4c) that quantifies the importance of past characters for current predictions. Furthermore, we have prepared a standalone IPython demo featuring our implementation of the ISAN on the parenthesis counting task and are waiting for approval to release it.
Lastly we have a included a new plot (Figure 6) that uses the \kappa_word as an embedding space, clearly showing that semantic structure arises on a word level, even though the model is only trained on next character prediction. 

In terms of new experimental validation we have added the following comparisons (see Section 4.1):
1) fully linear dynamics (without switching) with linear readouts
2) fully linear dynamics (without switching) but with non-linear readouts 
3) naive bayes
These experiments highlight the crucial importance of input switching for the performance of the ISAN. 

To investigate the limits of the input switched architecture we also ran experiments to compare the ISAN to the LSTM on a word-fragment task with a large number of inputs and find that it performs less well than the LSTM, with a gap of around 0.15 bits / char-pair (details given below).

Finally, we have improved the text in a number of places to address reviewer concerns. These changes are detailed in the per-reviewer responses.

---

Summary:  The authors present a simple RNN with linear dynamics for language modeling. The linear dynamics greatly enhance the interpretability of the model, as well as provide the potential to improve performance by caching the dynamics for common sub-sequences. Overall, the quantitative comparison on a benchmark task is underwhelming. It’s unclear why the authors didn’t consider a more common dataset, and they only considered a single dataset. On the other hand, they present a number of well-executed techniques for analyzing the behavior of the model, many of which would be impossible to do for a non-linear RNN. 

Overall, I recommend that the paper is accepted, despite the results. It provides an interesting read and an important contribution to the research dialogue. 

Feedback

The paper could be improved by shortening the number of analysis experiments and increasing the discussion of related sequence models. Some of the experiments were very compelling, whereas some of them (eg. 4.6) sort of feels like you’re just showing the reader that the model fits the data well, not that the model has any particularly important property. We trust that the model fits the data well, since you get reasonable perplexity results. 

LSTMS/GRUs are great for for language modeling for data with rigid combinatorial structure, such as nested parenthesis. It would have been nice if you compared your model to non-linear methods on this sort of data. Don’t be scared of negative results! It would be interesting if the non-linear methods were substantially better on these tasks. 

You should definitely add a discussion of Belanger and Kakade 2015 to the related work. They have different motivations (fast, scalable learning algorithms) rather than you (interpretable latent state dynamics and simple credit assignment for future predictions given past). On the other hand, they also have linear dynamics, and look at the singular vectors of the transition matrix to analyze the model. 

More broadly, it would be useful for readers if you discussed LDS more directly. A lot of this comparison came up in the openreview discussion, and I recommend folding this into the paper. For example, it would be useful to emphasize that the bias vectors correspond to columns of the Kalman gain matrix. 

One last thing regarding LDS: your model corresponds to Kalman filtering but in an LDS you can also do Kalman smoothing, where state vectors are inferred using the future in addition to the past observations. Could you do something similar in your model?

What if you said that each matrix is a sparse/convex combination of a set of dictionary matrices? This parameter sharing could provide even more interpretability, since the characters are then represented by the low-dimensional weights used to combine the dictionary elements. This could also provide more scalability to word-level problems.

---

Summary: The authors propose an input switched affine network to do character-level language modeling, a kind of RNN without pointwise nonlinearity, but with switching the transition matrix & bias based on the input character. This is motivated by intelligibility, since it allows decomposition of output contribution into these kappa_s^t terms, and use of basic linear algebra to probe the network.

Regarding myself as a reviewer, I am quite sure I understood the main ideas and arguments of this paper, but am not an expert on RNN language models or intelligibility/interpretability in ML.
I did not read any papers with a similar premise - closest related work I'm familiar with would be deconvnet for insight into vision-CNNs.

PRO:
I think this is original and novel work. This work is high quality, well written, and clearly is the result of a lot of work.
I found section 4.5 about projecting into readout subspace vs ""computational"" subspace most interesting and meaningful.

CON:
+ The main hesitation I have is that the results on both parts (ISAN model, and analysis of it) are not entirely convincing:
   (1) ISAN is only trained on small task (text8), not clear whether it can be a strong char-LM on larger scale tasks,
   (2) nor do the analysis sections provide all that much real insight in the learned network.

(1b) Other caveat towards ISAN architecture: this model in its proposed form is really only fit for small-vocabulary (i.e. character-based) language modeling, not a general RNN with large-vocab discrete input nor continuous input.

(2a) For analysis: many cute plots and fun ideas of quantities to look at, but not much concrete insights.
(2b) Not very clear which analysis is specific to the ISAN model, and which ideas will generalize to general nonlinear RNNs.
(2c) Re sec 4.2 - 4.3: It seems that the quantity \kappa_s^t on which analysis rests, isn't all that meaningful. Elaborating a bit on what I wrote in the question:
For example: Fig 2, for input letter ""u"" in revenue, there's a red spot where '_' character massively positively impacts the logit of 'e'. This seems quite meaningless, what would be the meaning of influence of '_' character? So it looks ot me that the switching matrix W_u (and prior W_n W_e etc) are using previous state in an interesting way to produce that following e. So that metric \kappa_s^t just doesn't seem very meaningful.
This remark relates to the last paragraph of Sec4.2.

Even though the list of cons here is longer than pro's, I recommend accept; specifically because the originality of this work will in any case make it more vulnerable to critiques. This work is well-motivated, very well-executed, and can inspire many more interesting investigations along these lines.

---

The authors present a character language model that gains some interpretability without large losses in predictivity. 

CONTRIBUTION:

I'd characterize the paper as some experimental investigation of a cute insight.  Recall that multi-class logistic regression allows you to apportion credit for a prediction to the input features: some features raised the probability of the correct class, while others lowered it.  This paper points out that a sufficiently simple RNN model architecture is log-linear in the same way, so you can apportion credit for a prediction among elements of the past history.  

PROS:

The paper is quite well-written and was fun to read.  It's nice to see that a simple architecture still does respectably.
It's easy to imagine using this model for a classroom assignment.  
It should be easy to implement, and the students could replicate the authors' investigation of what influences the network's predictions.
The authors present some nice visualizations.

Section 5.2 also describes some computational benefits.

CAVEATS ON PREDICTIVE ACCURACY:

* Figure 1 says that the ISAN has ""near identical performance to other architectures.""  But this appears true only when comparing the largest models.  

Explanation: It appears that for smaller parameter sizes, a GRU still beats the authors' model by 22% to 39% in the usual metric of perplexity per word (ppw).  (That's how LM people usually report performance, with a 10% reduction in ppw traditionally being considered a good Ph.D. dissertation.  I assumed an average of 7 chars/word when converting cross-entropy/char to perplexity/word.)  

* In addition, it's not known whether this model family will remain competitive beyond the toy situations tested here.

Explanation: The authors tried it only on character-based language modeling, and only on a 10M-char dataset, so their ppw is extremely high: 2135 for the best models in this paper.  By contrast, a word-based RNN LM trained on 44M words gets ppw of 133, and trained on 800M words gets ppw of 51.  [Numbers copied from the paper I cited before:

---

Very cool work. Here's some possibly related work on treating text as one-hots coming from a latent linear dynamical system over unobserved embeddings, ""A Linear Dynamical System Model for Text"" by Belanger and Kakade:

---

All reviewers have carefully looked at the paper and weakly support acceptance of the paper. Program Chairs also looked at this paper and believe that its contribution is too marginal and incremental in its current form. We encourage the authors to resubmit.

---

We have released the demo of a fully-understandable ISAN network for counting parenthesis at

---

We would like to thank all reviewers for their careful and thorough reviews, and for recommending paper acceptance. Based on your specific actionable feedback, we have since improved the paper with additional analysis and experiments. We hope that based on these new results, you will raise your scores and more confidently recommend acceptance.

In particular we have included a new analysis in Section 5 which provides an end-to-end interpretation of the functioning of the ISAN on a parenthesis counting task. For this example we believe we have really `cracked the case’, and fully explain the neural network behavior. We have also included a new analysis (Figure 4c) that quantifies the importance of past characters for current predictions. Furthermore, we have prepared a standalone IPython demo featuring our implementation of the ISAN on the parenthesis counting task and are waiting for approval to release it.
Lastly we have a included a new plot (Figure 6) that uses the \kappa_word as an embedding space, clearly showing that semantic structure arises on a word level, even though the model is only trained on next character prediction. 

In terms of new experimental validation we have added the following comparisons (see Section 4.1):
1) fully linear dynamics (without switching) with linear readouts
2) fully linear dynamics (without switching) but with non-linear readouts 
3) naive bayes
These experiments highlight the crucial importance of input switching for the performance of the ISAN. 

To investigate the limits of the input switched architecture we also ran experiments to compare the ISAN to the LSTM on a word-fragment task with a large number of inputs and find that it performs less well than the LSTM, with a gap of around 0.15 bits / char-pair (details given below).

Finally, we have improved the text in a number of places to address reviewer concerns. These changes are detailed in the per-reviewer responses.

---

Summary:  The authors present a simple RNN with linear dynamics for language modeling. The linear dynamics greatly enhance the interpretability of the model, as well as provide the potential to improve performance by caching the dynamics for common sub-sequences. Overall, the quantitative comparison on a benchmark task is underwhelming. It’s unclear why the authors didn’t consider a more common dataset, and they only considered a single dataset. On the other hand, they present a number of well-executed techniques for analyzing the behavior of the model, many of which would be impossible to do for a non-linear RNN. 

Overall, I recommend that the paper is accepted, despite the results. It provides an interesting read and an important contribution to the research dialogue. 

Feedback

The paper could be improved by shortening the number of analysis experiments and increasing the discussion of related sequence models. Some of the experiments were very compelling, whereas some of them (eg. 4.6) sort of feels like you’re just showing the reader that the model fits the data well, not that the model has any particularly important property. We trust that the model fits the data well, since you get reasonable perplexity results. 

LSTMS/GRUs are great for for language modeling for data with rigid combinatorial structure, such as nested parenthesis. It would have been nice if you compared your model to non-linear methods on this sort of data. Don’t be scared of negative results! It would be interesting if the non-linear methods were substantially better on these tasks. 

You should definitely add a discussion of Belanger and Kakade 2015 to the related work. They have different motivations (fast, scalable learning algorithms) rather than you (interpretable latent state dynamics and simple credit assignment for future predictions given past). On the other hand, they also have linear dynamics, and look at the singular vectors of the transition matrix to analyze the model. 

More broadly, it would be useful for readers if you discussed LDS more directly. A lot of this comparison came up in the openreview discussion, and I recommend folding this into the paper. For example, it would be useful to emphasize that the bias vectors correspond to columns of the Kalman gain matrix. 

One last thing regarding LDS: your model corresponds to Kalman filtering but in an LDS you can also do Kalman smoothing, where state vectors are inferred using the future in addition to the past observations. Could you do something similar in your model?

What if you said that each matrix is a sparse/convex combination of a set of dictionary matrices? This parameter sharing could provide even more interpretability, since the characters are then represented by the low-dimensional weights used to combine the dictionary elements. This could also provide more scalability to word-level problems.

---

Summary: The authors propose an input switched affine network to do character-level language modeling, a kind of RNN without pointwise nonlinearity, but with switching the transition matrix & bias based on the input character. This is motivated by intelligibility, since it allows decomposition of output contribution into these kappa_s^t terms, and use of basic linear algebra to probe the network.

Regarding myself as a reviewer, I am quite sure I understood the main ideas and arguments of this paper, but am not an expert on RNN language models or intelligibility/interpretability in ML.
I did not read any papers with a similar premise - closest related work I'm familiar with would be deconvnet for insight into vision-CNNs.

PRO:
I think this is original and novel work. This work is high quality, well written, and clearly is the result of a lot of work.
I found section 4.5 about projecting into readout subspace vs ""computational"" subspace most interesting and meaningful.

CON:
+ The main hesitation I have is that the results on both parts (ISAN model, and analysis of it) are not entirely convincing:
   (1) ISAN is only trained on small task (text8), not clear whether it can be a strong char-LM on larger scale tasks,
   (2) nor do the analysis sections provide all that much real insight in the learned network.

(1b) Other caveat towards ISAN architecture: this model in its proposed form is really only fit for small-vocabulary (i.e. character-based) language modeling, not a general RNN with large-vocab discrete input nor continuous input.

(2a) For analysis: many cute plots and fun ideas of quantities to look at, but not much concrete insights.
(2b) Not very clear which analysis is specific to the ISAN model, and which ideas will generalize to general nonlinear RNNs.
(2c) Re sec 4.2 - 4.3: It seems that the quantity \kappa_s^t on which analysis rests, isn't all that meaningful. Elaborating a bit on what I wrote in the question:
For example: Fig 2, for input letter ""u"" in revenue, there's a red spot where '_' character massively positively impacts the logit of 'e'. This seems quite meaningless, what would be the meaning of influence of '_' character? So it looks ot me that the switching matrix W_u (and prior W_n W_e etc) are using previous state in an interesting way to produce that following e. So that metric \kappa_s^t just doesn't seem very meaningful.
This remark relates to the last paragraph of Sec4.2.

Even though the list of cons here is longer than pro's, I recommend accept; specifically because the originality of this work will in any case make it more vulnerable to critiques. This work is well-motivated, very well-executed, and can inspire many more interesting investigations along these lines.

---

The authors present a character language model that gains some interpretability without large losses in predictivity. 

CONTRIBUTION:

I'd characterize the paper as some experimental investigation of a cute insight.  Recall that multi-class logistic regression allows you to apportion credit for a prediction to the input features: some features raised the probability of the correct class, while others lowered it.  This paper points out that a sufficiently simple RNN model architecture is log-linear in the same way, so you can apportion credit for a prediction among elements of the past history.  

PROS:

The paper is quite well-written and was fun to read.  It's nice to see that a simple architecture still does respectably.
It's easy to imagine using this model for a classroom assignment.  
It should be easy to implement, and the students could replicate the authors' investigation of what influences the network's predictions.
The authors present some nice visualizations.

Section 5.2 also describes some computational benefits.

CAVEATS ON PREDICTIVE ACCURACY:

* Figure 1 says that the ISAN has ""near identical performance to other architectures.""  But this appears true only when comparing the largest models.  

Explanation: It appears that for smaller parameter sizes, a GRU still beats the authors' model by 22% to 39% in the usual metric of perplexity per word (ppw).  (That's how LM people usually report performance, with a 10% reduction in ppw traditionally being considered a good Ph.D. dissertation.  I assumed an average of 7 chars/word when converting cross-entropy/char to perplexity/word.)  

* In addition, it's not known whether this model family will remain competitive beyond the toy situations tested here.

Explanation: The authors tried it only on character-based language modeling, and only on a 10M-char dataset, so their ppw is extremely high: 2135 for the best models in this paper.  By contrast, a word-based RNN LM trained on 44M words gets ppw of 133, and trained on 800M words gets ppw of 51.  [Numbers copied from the paper I cited before:

---

Very cool work. Here's some possibly related work on treating text as one-hots coming from a latent linear dynamical system over unobserved embeddings, ""A Linear Dynamical System Model for Text"" by Belanger and Kakade:",,,,,,6.333333333333333,,,3.6666666666666665,,
590,"Authors: READING COMPREHENSION, Yang Yu, Wei Zhang, Bowen Zhou, Kazi Hasan, Mo Yu, Bing Xiang
Source file: 590.pdf

ABSTRACT
This paper proposes dynamic chunk reader (DCR), an end-to-end neural reading comprehension (RC) model that is able to extract and rank a set of answer candidates from a given document to answer questions. DCR is able to predict answers of variable lengths, whereas previous neural RC models primarily focused on predicting single tokens or entities. DCR encodes a document and an input question with recurrent neural networks, and then applies a word-by-word attention mechanism to acquire question-aware representations for the document, followed by the generation of chunk representations and a ranking module to propose the topranked chunk as the answer. Experimental results show that DCR could achieve a 66.3% Exact match and 74.7% F1 score on the Stanford Question Answering Dataset (Rajpurkar et al., 2016).

1 INTRODUCTION
Reading comprehension-based question answering (RCQA) is the task of answering a question with a chunk of text taken from related document(s). A variety of neural models have been proposed recently either for extracting a single entity or a single token as an answer from a given text (Hermann et al., 2015; Kadlec et al., 2016; Trischler et al., 2016b; Dhingra et al., 2016; Chen et al., 2016; Sordoni et al., 2016; Cui et al., 2016a); or for selecting the correct answer by ranking a small set of human-provided candidates (Yin et al., 2016; Trischler et al., 2016a). In both cases, an answer boundary is either easy to determine or already given.
Different from the above two assumptions for RCQA, in the real-world QA scenario, people may ask questions about both entities (factoid) and non-entities such as explanations and reasons (nonfactoid) (see Table 1 for examples).
In this regard, RCQA has the potential to complement other QA approaches that leverage structured data (e.g., knowledge bases) for both the above question types. This is because RCQA can exploit the textual evidences to ensure increased answer coverage, which is particularly helpful for nonfactoid answers. However, it is also challenging for RCQA to identify answer in arbitrary position in the passage with arbitrary length, especially for non-factoid answers which might be clauses or sentences.
As a result, apart from a few exceptions (Rajpurkar et al., 2016; Wang & Jiang, 2016), this research direction has not been fully explored yet.
Compared to the relatively easier RC task of predicting single tokens/entities1, predicting answers of arbitrary lengths and positions significantly increase the search space complexity:
the number of possible candidates to consider is in the order of O(n2), where n is the number of passage words. In contrast, for previous works in which answers are single tokens/entities or from candidate lists, the complexity is in O(n) or the size of candidate lists l (usually l ≤5), respectively. To address the above complexity, Rajpurkar et al. (Rajpurkar et al., 2016) used a two-step chunkand-rank approach that employs a rule-based algorithm to extract answer candidates from a passage,
∗Both authors contribute equally 1State-of-the-art RC models have a decent accuracy of ∼70% on the widely used CNN/DailyMail dataset
(Hermann et al., 2015).
followed by a ranking approach with hand-crafted features to select the best answer. The rule-based chunking approach suffered from low coverage (≈ 70% recall of answer chunks) that cannot be improved during training; and candidate ranking performance depends greatly on the quality of the hand-crafted features. More recently, Wang and Jiang (Wang & Jiang, 2016) proposed two end-toend neural network models, one of which chunks a candidate answer by predicting the answer’s two boundary indices and the other classifies each passage word into answer/not-answer. Both models improved significantly over the method proposed by Rajpurkar et al. (Rajpurkar et al., 2016).
Our proposed model, called dynamic chunk reader (DCR), not only significantly differs from both the above systems in the way that answer candidates are generated and ranked, but also shares merits with both works. First, our model uses deep networks to learn better representations for candidate answer chunks, instead of using fixed feature representations as in (Rajpurkar et al., 2016). Second, it represents answer candidates as chunks, as in (Rajpurkar et al., 2016), instead of wordlevel representations (Wang & Jiang, 2016), to make the model aware of the subtle differences among candidates (importantly, overlapping candidates).
The contributions of this paper are three-fold. (1) We propose a novel neural network model for joint candidate answer chunking and ranking, where the candidate answer chunks are dynamically constructed and ranked in an end-to-end manner. (2) we propose a new question-attention mechanism to enhance passage word representation, which is subsequently used to construct chunk representations. (3) We also propose several simple but effective features to strengthen the attention mechanism, which fundamentally improves candidate ranking, with the by-product of higher exact boundary match accuracy.
The experiments on the Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016), which contains a variety of human-generated factoid and non-factoid questions, have shown the effectiveness of above three contributions.
Our paper is organized as follows. We formally define the RCQA problem first. Next, we describe our baseline with a neural network component. We present the end-to-end dynamic chunk reader model next. Finally, we analyze our experimental results and discuss the related work. In appendix, we show formal equations and details of the model.

2 PROBLEM DEFINITION
Table 1 shows an example of our RC setting where the goal is to answer a question Qi, factoid (Q1) or non-factoid (Q2 and Q3), based on a supporting passage Pi, by selecting a continuous sequence of text Ai ⊆ Pi as answer. Qi, Pi, and Ai are all word sequences, where each word is drawn from a vocabulary, V . The i-th instance in the training set is a triple in the form of (Pi, Qi, Ai), where Pi = (pi1, . . . , pi|Pi|), Qi = (qi1, . . . , qi|Qi|), and Ai = (ai1, . . . , ai|Ai|) (pi·, qi·, ai· ∈ V ). Owing to the disagreement among annotators, there could be more than one correct answer for the same question; and the k-th answer to Qi is denoted by Aki = {aki1, . . . , aki|Aki |}. An answer candidate for the i-th training example is defined as cm,ni , a sub-sequence in Pi, that spans from position m to n (1 ≤ m ≤ n ≤ |Pi|). The ground truth answer Ai could be included in the set of all candidates
Ci = {cm,ni |∀m,n ∈ N+, subj(m,n, Pi) and 1 ≤ m ≤ n ≤ |Pi|}, where subj(m,n, Pi) is the constraint put on the candidate chunk for Pi, such as, “c m,n i can have at most 10 tokens”, or “cm,ni must have a pre-defined POS pattern”. To evaluate a system’s performance, its top answer to a question is matched against the corresponding gold standard answer(s).
Remark: Categories of RC Tasks Other simpler variants of the aforementioned RC task were explored in the past. For example, quiz-style datasets (e.g., MCTest (Richardson et al., 2013), MovieQA (Tapaswi et al., 2015)) have multiple-choice questions with answer options. Cloze-style datesets(Hermann et al., 2015; Hill et al., 2015; Onishi et al., 2016), usually automatically generated, have factoid “question”s created by replacing the answer in a sentence from the text with blank. For the answer selection task this paper focuses on, several datasets exist, e.g. TREC-QA for factoid answer extraction from multiple given passages, bAbI (Weston et al., 2014) designed for inference purpose, and the SQuAD dataset (Rajpurkar et al., 2016) used in this paper. To the best of our knowledge, the SQuAD dataset is the only one for both factoid and non-factoid answer extraction with a question distribution more close to real-world applications.

3 BASELINE: CHUNK-AND-RANK PIPELINE WITH NEURAL RC
In this section we modified a state-of-the-art RC system for cloze-style tasks for our answer extraction purpose, to see how much gap we have for the two type of tasks, and to inspire our end-to-end system in the next section. In order to make the cloze-style RC system to make chunk-level decision, we use the RC model to generate features for chunks, which are further used in a feature-based ranker like in (Rajpurkar et al., 2016). As a result, this baseline can be viewed as a deep learning based counterpart of the system in (Rajpurkar et al., 2016). It has two main components: 1) a standalone answer chunker, which is trained to produce overlapping candidate chunks, and 2) a neural RC model, which is used to score each word in a given passage to be used thereafter for generating chunk scores.
Answer Chunking To reduce the errors generated by the rule-based chunker in (Rajpurkar et al., 2016), first, we capture the part-of-speech (POS) pattern of all answer sub-sequences in the training dataset to form a POS pattern trie tree, and then apply the answer POS patterns to passage Pi to acquire a collection of all subsequences (chunk candidates) Ci whose POS patterns can be matched to the POS pattern trie. This is equivalent to putting an constraint subj(m,n, Pi) to candidate answer chunk generation process that only choose the chunk with a POS pattern seen for answers in the training data. Then the sub-sequences Ci are used as answer candidates for Pi. Note that overlapping chunks could be generated for a passage, and we rely on the ranker to choose the best candidate based on features from the cloze-style RC system. Experiments showed that for > 90% of the questions on the development set, the ground truth answer is included in the candidate set constructed in such manner.
Feature Extraction and Ranking For chunk ranking, we (1) use neural RCQA model to annotate each pij in passage Pi to get score sij , then (2) for every chunk c m,n i in passage i, collect scores (sim, . . . , sin) for all the (pim, ..., pin) contained within c m,n i , and (3) extract features on the sequence of scores (sim, . . . , sin) to characterize its scale and distribution information, which serves as the feature representation of cm,ni . In step (1) to acquire sij we train and apply a word-level single-layer Gated Attention Reader 2 (Dhingra et al., 2016), which has state-of-the-art performance on CNN/DailyMail cloze-style RC task. In step (3) for chunk cm,ni , we designed 5 features, including 4 statistics on (sim, . . . , sin): maximum, minimum, average and sum; as well as the count of matched POS pattern within the chunk, which serves as an answer prior. We use these 5 features in a state-of-the-art ranker (Ganjisaffar et al., 2011).

4 DYNAMIC CHUNK READER
The dynamic chunk reader (DCR) model is presented in Figure 1. Inspired by the baseline we built, DCR is deemed to be superior to the baseline for 3 reasons. First, each chunk has a representation constructed dynamically, instead of having a set of pre-defined feature values. Second, each passage
2We tried using more than one layers in Gated Attention Reader, but no improvement was observed.
word’s representation is enhanced by word-by-word attention that evaluates the relevance of the passage word to the question. Third, these components are all within a single, end-to-end model that can be trained in a joint manner.
DCR works in four steps. First, the encoder layer encodes passage and question separately, by using bidirectional recurrent neural networks (RNN).
Second, the attention layer calculates the relevance of each passage word to the question.
Third, the convolution layer generates unigram, bigram and trigram representation for each word. bigram and trigram of a word ends with the same word, and proper padding is applied on the first word to make sure the output is the same length as input to CNN layer.
Fourth, the chunk representation layer dynamically extracts the candidate chunks from the given passage, and create chunk representation that encodes the contextual information of each chunk.
Fifth, the ranker layer scores the relevance between the representations of a chunk and the given question, and ranks all candidate chunks using a softmax layer.
We describe each step below.
Encoder Layer We use bi-directional RNN encoder to encode Pi and Qi of example i, and get hidden state for each word position pij and qik.3 As RNN input, a word is represented by a row vector x ∈ Rn. x can be the concatenation of word embedding and word features (see Fig. 1). The word vector for the t-th word is xt. A word sequence is processed using an RNN encoder with gated recurrent units (GRU) (Cho et al., 2014), which was proved to be effective in RC and neural machine translation tasks (Bahdanau et al., 2015; Kadlec et al., 2016; Dhingra et al., 2016). For each position t, GRU computes ht with input xt and previous state ht−1, as:
3We can have separated parameters for question and passage encoders but a single shared encoder for both works better in the experiments.
rt = σ(Wrxt + Urht−1) (1) ut = σ(Wuxt + Uuht−1) (2) h̄t = tanh(Wxt + U(rt ht−1)) (3) ht = (1− ut) · ht−1 + ut · h̄t (4)
where ht, rt, and ut ∈ Rd are d-dimensional hidden state, reset gate, and update gate, respectively; W{r,u}, W ∈ Rn×d and U{r,u}, U ∈ Rd×d are the parameters of the GRU; σ is the sigmoid function, and denotes element-wise production. For a word at t, we use the hidden state −→h t from the forward RNN as a representation of the preceding context, and the←−h t from a backward RNN that encodes text reversely, to incorporate the context after t. Next, ht = [ −→ ht ; ←− ht ], the bi-directional contextual encoding of xt, is formed. [·; ·] is the concatenation operator. To distinguish hidden states from different sources, we denote the hj of j-th word in P and the hk of k-th word in Q as h p j and hqk respectively.
Attention Layer Attention mechanism in previous RC tasks (Kadlec et al., 2016; Hermann et al., 2015; Sordoni et al., 2016; Dhingra et al., 2016; Cui et al., 2016a;b) enables question-aware passage representations. We propose a novel attention mechanism inspired by word-by-word style attention methods (Rocktäschel et al., 2015; Wang & Jiang, 2015; Santos et al., 2016). For each pj , a questionattended representation vj is computed as follows (example index i is omitted for simplicity):
αjk = h p j · h q k, (5)
βj = |Q|∑ k=1 αjkh q k (6) vj = [h p j ;βj ] (7)
where hpj and h q k are hidden states from the bi-directional RNN encoders (see Figure 1). An inner product, αjk, is calculated between h p j and every question word h q k. It indicates how well the passage word pj matches with every question word qk. βj is a weighted pooling of |Q| question hidden states, which serves as a pj-aware question representation. The concatenation of h p j and βj leads to a passage-question joint representation, vj ∈ R4d.4 Next, we apply a second bi-GRU layer taking the vjs as inputs, and obtain forward and backward representations −→γj and←−γj ∈ Rd, and in turn their concatenation, γj = [−→γj ;←−γj ]. Convolution Layer Every word is encoded with complete passage context through attention layer RNN. We would like to model more complex representation of the words, by introducing unigram, bigram and trigram representations. There are two benefits for this enhanced representation: 1) each word could be enhanced with local context information to help identify the boundary of the answer chunk. Using previous words has been a common feature used in POS tagging and Named entity recognition; and 2) The information brought in by the ngram into the word representation could enhance the semantic match between the answer chunk internal and the question. Imagine scenario of a three word candidate, where the last word representation includes the two previous words through the convolution layer. Matching to the last word could also lead to the match to the semantics of the internal of the chunk. Specifically, we create for every word position j three representations, by using ngrams ending with the hidden state j:
γ̃j1 = γj ·Wc1 (8) γ̃j2 = [γj−1; γj ] ·Wc2 (9) γ̃j3 = [γj−2; γj−1; γj ] ·Wc3 (10)
4We tried another word-by-word attention methods as in (Santos et al., 2016), which has similar passage representation input to question side. However, this does not lead to improvement due to the confusion caused by long passages in RC. Consequently, we used the proposed simplified version of word-by-word attention on passage side only.
The details shown in equations above. We used three different convolution kernels for different n-grams.
Chunk Representation Layer A candidate answer chunk representation is dynamically created given convolution layer output. We first decide the text boundary for the candidate chunk, and then form a chunk representation using all or part of those γj outputs inside the chunk. To decide a candidate chunk (boundary): we tried two ways: (1) adopt the POS trie-based approach used in our baseline, and (2) enumerate all possible chunks up to a maximum number of tokens. For (2), we create up to N (max chunk length) chunks starting from any position j in Pj . Approach (1) can generate candidates with arbitrary lengths, but fails to recall candidates whose POS pattern is unseen in training set; whereas approach (2) considers all possible candidates within a window and is more flexible, but over-generates invalid candidates.
For a candidate answer chunk cm,n spanning from position m to n inclusively, we construct chunk representation γlm,n ∈ R2d using every γ̃jl within range [m,n], with a function g(·), and l ∈ {1, 2, 3}. Formally,
γlm,n = g(γ̃ml, . . . , γ̃nl)
Each γ̃jl is a convolution output over concatenated forward and backward RNN hidden states from attention layer. So the first half in γ̃jl encodes information in forward RNN hidden states and the second half encodes information in backward RNN hidden states. We experimented with several pooling functions (e.g., max, average) for g(·), and found out that, instead of pooling, the best g(·) function is to concatenate the first half of convolution output of the chunk’s first word and the second half of convolution output of the chunk’s last word. Formally,
γlm,n = g(γ̃ml, . . . , γ̃nl) = [ −→ γ̃ml; ←− γ̃nl] (11)
where −→γ̃ml is half of the hidden state for l-gram word representation corresponding to forward attention RNN output. We hypothesize that the hidden states at that two ends can better represent the chunk’s contexts, which is critical for this task, than the states within the chunk. This observation also agrees with (Kobayashi et al., 2016).
Ranker Layer A score slm,n for each l-gram chunk representation γlm,n denoting the probability of that chunk to be the true answer is calculated by dot product with question representation. The question representation is the concatenation of the last hidden state in forward RNN and the first hidden state in backward RNN. Formally for the chunk cm,ni we have
sl(cm,ni |Pi, Qi) = γ l m,n · [ −−→ hQi|Qi|; ←−− hQi1 ] (12)
where sl denotes the score generated from l-gram representation. −−→ hQik or ←−− hQik is the k-th hidden state output from question Qi’s forward and backward RNN encoder, respectively.
After that, the final score for cm,ni is evaluated as the linear combination of three scores, followed by a softmax:
s(cm,ni |Pi, Qi) = softmax(W · [s 1; s2; s3]) (13)
where sl is the shorthand notation for sl(cm,ni |Pi, Qi); W ∈ R3. In runtime, the chunk with the highest probability is taken as the answer. In training, the following negative log likelihood is minimized:
L = − N∑ i=1 logP(Ai|Pi, Qi) (14)
Note that the i-th training instance is only used when Ai is included in the corresponding candidate chunk set Ci, i.e. ∃m,nAi = cm,ni . The softmax in the final layer serves as the list-wise ranking module similar in spirit to (Cao et al., 2007).

5 EXPERIMENTS
Dataset We used the Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016) for the experiment. SQuAD came into our sight because it is a mix of factoid and non-factoid
questions, a real-world data (crowd-sourced), and of large scale (over 100K question-answer pairs collected from 536 Wikipedia articles). Answers range from single words to long, variable-length phrase/clauses. It is a relaxation of assumptions by the cloze-style and quiz-style RC datasets in the Problem Definition section.
Features The input vector representation of each word w to encoder RNNs has six parts including a pre-trained 300-dimensional GloVe embedding (Pennington et al., 2014) and five features (see Figure 1): (1) a one-hot encoding (46 dimensions) for the part-of-speech (POS) tag of w; (2) a one-hot encoding (14 dimensions) for named entity (NE) tag of w; (3) a binary value indicating whether w’s surface form is the same to any word in the quesiton; (4) if the lemma form of w is the same to any word in the question; and (5) if w is caplitalized. Feature (3) and (4) are designed to help the model align the passage text with question. Note that some types of questions (e.g., “who”, “when” questions) have answers that have a specific POS/NE tag pattern. For instance, “who” questions mostly have proper nouns/persons as answers and “when” questions may frequently have numbers/dates (e.g., a year) as answers. Thus, we believe that the model could exploit the co-relation between question types and answer POS/NE patterns easier with POS and NE tag features. Implementation Details We pre-processed the SQuAD dataset using Stanford CoreNLP tool5 (Manning et al., 2014) with its default setting to tokenize the text and obtain the POS and NE annotations. To train our model, we used stochastic gradient descent with the ADAM optimizer (Kingma & Ba, 2014), with an initial learning rate of 0.001. All GRU weights were initialized from a uniform distribution between (-0.01, 0.01). The hidden state size, d, was set to 300 for all GRUs. The question bi-GRU shared parameters with the passage bi-GRU, while the attention-based passage bi-GRU had its own parameters. We shuffled all training examples at the beginning of each epoch and adopted a curriculum learning approach (Bengio et al., 2009), by sorting training instances by length in every 10 batches, to enable the model start learning from relatively easier instances and to harder ones. We also applied dropout of rate 0.2 to the embedding layer of input bi-GRU encoder, and gradient clipping when the norm of gradients exceeded 10. We trained in mini-batch style (mini-batch size is 180) and applied zero-padding to the passage and question inputs in each batch. We also set the maximum passage length to be 300 tokens, and pruned all the tokens after the 300-th token in the training set to save memory and speed up the training process. This step reduced the training set size by about 1.6%. During test, we test on the full length of passage, so that we don’t prune out the potential candidates. We trained the model for at most 30 epochs, and in case the accuracy did not improve for 10 epochs, we stopped training.
For the feature ranking-based system, we used jforest ranker (Ganjisaffar et al., 2011) with LambdaMART-RegressionTree algorithm and the ranking metric was NDCG@10. For the Gated Attention Reader in baseline system, we replicated the method and use the same configurations as in (Dhingra et al., 2016).
Results
Table 2 shows our main results on the SQuAD dataset. Compared to the scores reported in (Wang & Jiang, 2016), our exact match (EM) and F1 on the development set and EM score on the test set are better, and F1 on the test set is comparable. We also studied how each component in our model contributes to the overall performance. Table 3 shows the details as well as the results of the baseline ranker. As the first row of Table 3 shows, our baseline system improves 10% (EM) over Rajpurkar et al. (Rajpurkar et al., 2016) (Table 2, row 1), the feature-based ranking system. However when compared to our DCR model (Table 3, row 2), the baseline (row 1) is more than 12% (EM) behind
5 stanfordnlp.github.io/CoreNLP/
We also did ablation tests on our DCR model. First, replacing the word-by-word attention with Attentive Reader style attention (Hermann et al., 2015) decreases the EM score by about 4.5%, showing the strength of our proposed attention mechanism.
Second, we remove the features in input to see the contribution of each feature. The result shows that POS feature (1) and question-word feature (3) are the two most important features.
Finally, combining the DCR model with the proposed POS-trie constraints yields a score similar to the one obtained using the DCR model with all possible n-gram chunks. The result shows that (1) our chunk representations are powerful enough to differentiate even a huge amount of chunks when no constraints are applied; and (2) the proposed POS-trie reduces the search space at the cost of a small drop in performance.
Analysis To better understand our system, we calculated the accuracy of the attention mechanism of the gated attention reader used in our deep learning-based baseline. We found that it is 72% accurate i.e., 72% of the times a word with the highest attention score is inside the correct answer span. This means that, if we could accurately detect the boundary around the word with the highest attention score to form the answer span, we could achieve an accuracy close to 72%. In addition, we checked the answer recall of our candidate chunking approach. When we use a window size of 10, 92% of the time, the ground truth answer will be included in the extracted Candidate chunk set. Thus the upper bound of the exact match score of our baseline system is around 66% (92% (the answer recall) × 72%). From the results, we see our DCR system’s exact match score is at 62%. This shows that DCR is proficient at differentiating answer spans dynamically.
To further analyze the system’s performance while predicting answers of different lengths, we show the exact match (EM) and F1 scores for answers with lengths up to 10 tokens in Figure 2(a). From the graph, we can see that, with the increase of answer length, both EM and F1 drops, but in different speed. The gap between F1 and exact match also widens as answer length increases. However, the model still yields a decent accuracy when the answer is longer than a single word. Additionally, Figure 2(b) shows that the system is better at “when” and “who” questions, but performs poorly
on “why” questions. The large gap between exact match and F1 on “why” questions means that perfectly identifying the span is harder than locating the core of the answer span.
Since “what”, “which”, and “how” questions contain a broad range of question types, we split them further based on the bigram a question starts with, and Figure 3 shows the breakdown for “what” questions. We can see that “what” questions asking for explanations such as “what happens” and “what happened” have lower EM and F1 scores. In contrast, “what” questions asking for year and numbers have much higher scores and, for these questions, exact match scores are close to F1 scores, which means chunking for these questions are easier for DCR.

6 RELATED WORK
Attentive Reader was the first neural model for factoid RCQA (Hermann et al., 2015). It uses Bidirectional RNN (Cho et al., 2014; Chung et al.,2014) to encode document and query respectively, and use query representation to match with every token from the document. Attention Sum Reader (Kadlec et al., 2016) simplifies the model to just predicting positions of correct answer in the document and the training speed and test accuracy are both greatly improved on the CNN/Daily Mail dataset. (Chen et al., 2016) also simplified Attentive Reader and reported higher accuracy. Windowbased Memory Networks (MemN2N) is introduced along with the CBT dataset (Hill et al., 2015), which does not use RNN encoders, but embeds contexts as memory and matches questions with embedded contexts. Those models’ mechanism is to learn the match between answer context with question/query representation. In contrast, memory enhanced neural networks like Neural Turing Machines (Graves et al., 2014) and its variants (Zhang et al., 2015; Gulcehre et al., 2016; Zaremba & Sutskever, 2015; Chandar et al., 2016; Grefenstette et al., 2015) were also potential candidates for the task, and Gulcehre et al. (Gulcehre et al., 2016) reported results on the bAbI task, which is worse than memory networks. Similarly, sequence-to-sequence models were also used (Yu et al., 2015; Hermann et al., 2015), but they did not yield better results either.
Recently, several models have been proposed to enable more complex inference for RC task. For instance, gated attention model (Dhingra et al., 2016) employs a multi-layer architecture, where each layer encodes the same document, but the attention is updated from layer to layer. EpiReader (Trischler et al., 2016b) adopted a joint training model for answer extractor and reasoner, where the extractor proposes top candidates, and the reasoner weighs each candidate by examining entailment relationship between question-answer representation and the document. An iterative alternating attention mechanism and gating strategies were proposed in (Sordoni et al., 2016) to optimize the attention through several hops. In contrast, Cui et al. (Cui et al., 2016a;b) introduced fine-grained document attention from each question word and then aggregated those attentions from each question token by summation with or without weights. This system achieved the state-of-the-art score on the CNN dataset. Those different variations all result in roughly 3-5% improvement over attention sum reader, but none of those could achieve higher than that. Other methods include using dynamic entity representation with max-pooling (Kobayashi et al., 2016) that aims to change entity representation with context, and Weissenborn’s (Weissenborn, 2016) system, which tries to separate entity from the context and then matches the question to context, scoring an accuracy around 70% on the CNN dataset.
However, all of those models assume that the answers are single tokens. This limits the type of questions the models can answer. Wang and Jiang (Wang & Jiang, 2016) proposed a match-lstm and achieved good results on SQuAD. However, this approach predicts a chunk boundary or whether a word is part of a chunk or not. In contrast, our approach explicitly constructs the chunk representations and similar chunks are compared directly to determine correct answer boundaries.

7 CONCLUSION
In this paper we proposed a novel neural reading comprehension model for question answering. Different from the previously proposed models for factoid RCQA, the proposed model, dynamic chunk reader, is not restricted to predicting a single named entity as an answer or selecting an answer from a small, pre-defined candidate list. Instead, it is capable of answering both factoid and nonfactoid questions as it learns to select answer chunks that are suitable for an input question. DCR achieves this goal with a joint deep learning model enhanced with a novel attention mechanism and five simple yet effective features. Error analysis shows that the DCR model achieves good performance, but still needs to improve on predicting longer answers, which are usually non-factoid in nature.
","This paper proposes dynamic chunk reader (DCR), an end-to-end neural reading comprehension (RC) model that is able to extract and rank a set of answer candidates from a given document to answer questions. DCR is able to predict answers of variable lengths, whereas previous neural RC models primarily focused on predicting single tokens or entities. DCR encodes a document and an input question with recurrent neural networks, and then applies a word-by-word attention mechanism to acquire question-aware representations for the document, followed by the generation of chunk representations and a ranking module to propose the topranked chunk as the answer. Experimental results show that DCR could achieve a 66.3% Exact match and 74.7% F1 score on the Stanford Question Answering Dataset (Rajpurkar et al., 2016).",ICLR 2017 conference submission,False,,"SYNOPSIS: The paper proposes a new neural network-based model for reading comprehension (reading a passage of text and answering questions based on the passage). It is similar in spirit to several other recent models, with the main exception that it is able to predict answers of different lengths, as opposed to single words/tokens/entities. The authors compare their model on the Stanford Question Answering Dataset (SQuAD), and show improvements over the baselines, while apparently lagging quite far behind the current state of the art reported on the SQuAD leaderboard.

THOUGHTS: The main novelty of the method is to be able to identify phrases of different lengths as possible answers to the question. However, both approaches considered -- using a POS pattern trie tree to filter out word sequences with POS tags matching those of answers in the training set, and brute-force enumeration of all phrases up to length N -- seem somewhat orthogonal to the idea of ""learning end-to-end "" an answer chunk extraction model. Furthermore, as other reviews have pointed out, it seems that the linguistic features actually contribute a lot to the final accuracy (Table 3). One could argue that these are easy to obtain using standard taggers, but it takes away even more from the idea of an ""end-to-end trained"" system.

The paper is generally well written, but there are several crucial sections in parts describing the model where it was really hard for me to follow the descriptions. In particular, the attention mechanism seems fairly standard to me in a seq2seq sense (i.e. there is nothing architecturally novel about it, as is for instance the case with the Gated Attentive Reader). I may be missing something, but even after the clarification round I still don't understand how it is novel compared to standard attention used in for instance seq2seq models.

Finally, although the method is shown to outperform the baseline method reported in the original paper introducing the SQuAD dataset, it currently seems to be 12th (out of 15 systems) on the leaderboard (

---

The program committee appreciates the authors' response to the clarification questions and one review. Unfortunately, reviews are not leaning sufficiently towards acceptance. Reviewers have concerns about the novelty of the approach, its effectiveness in terms of empirical performance, and lack of analysis that would help determine the main contributions of the proposed approach. Authors are strongly encouraged to incorporate reviewer feedback in future iterations of the work.

---

SYNOPSIS: The paper proposes a new neural network-based model for reading comprehension (reading a passage of text and answering questions based on the passage). It is similar in spirit to several other recent models, with the main exception that it is able to predict answers of different lengths, as opposed to single words/tokens/entities. The authors compare their model on the Stanford Question Answering Dataset (SQuAD), and show improvements over the baselines, while apparently lagging quite far behind the current state of the art reported on the SQuAD leaderboard.

THOUGHTS: The main novelty of the method is to be able to identify phrases of different lengths as possible answers to the question. However, both approaches considered -- using a POS pattern trie tree to filter out word sequences with POS tags matching those of answers in the training set, and brute-force enumeration of all phrases up to length N -- seem somewhat orthogonal to the idea of ""learning end-to-end "" an answer chunk extraction model. Furthermore, as other reviews have pointed out, it seems that the linguistic features actually contribute a lot to the final accuracy (Table 3). One could argue that these are easy to obtain using standard taggers, but it takes away even more from the idea of an ""end-to-end trained"" system.

The paper is generally well written, but there are several crucial sections in parts describing the model where it was really hard for me to follow the descriptions. In particular, the attention mechanism seems fairly standard to me in a seq2seq sense (i.e. there is nothing architecturally novel about it, as is for instance the case with the Gated Attentive Reader). I may be missing something, but even after the clarification round I still don't understand how it is novel compared to standard attention used in for instance seq2seq models.

Finally, although the method is shown to outperform the baseline method reported in the original paper introducing the SQuAD dataset, it currently seems to be 12th (out of 15 systems) on the leaderboard (

---

The paper proposed an end-to-end machine learning model called dynamic reader for the machine reading comprehension task. Compared to earlier systems, the proposed model is able to extract and rank a set of answer candidates from a given document.

There are many recent models focusing on building good question answering systems by extracting phrases from a given article. It seems that there are two different aspects that are unique in this work:

1.	The use of convolution model, and
2.	Dynamic chunking

Convolution network is often only used for modeling character-based word embeddings so I am curious about its effectiveness on representing phrases. Therefore, I wish there could be more analysis on how effective it is, as the authors do not compare the convolution framework to other alternative approaches such as LSTM. The comparisons are important, as the authors uses uni-gram, bi-gram and tri-gram information in the convolution network, and it is not clear to me that if tri-gram information is still needed for LSTM models.

The dynamic chunking is a good idea, and a very similar idea is proposed in some of the recent papers such as [Kenton et al, 16], which also targets at the same dataset. However, I would like to see more analysis on the dynamic chunking. Why this approach is a good approach for representing answer chunks? Given the representation of the chunk is constructed by the first and the end word representations generated by a convolution network, I am not sure about the ability of this representation to capture the long answer phrases.

The authors do not use character base embedding but use some of the previous trained NLP models. It would be interesting if the authors could show what are the advantages and disadvantages of using linguistic features compared to character embeddings.

In short, there are several good ideas proposed in the paper, but the lack of proper analysis make it difficult to judge how important the proposed techniques are.

---

SUMMARY.
The paper propose a reading-comprehension question answering system for the recent QA task where answers of a question can be either single tokens or spans in the given text passage.
The model first encodes the passage and the query using a recurrent neural network.
With an attention mechanism the model calculates the importance of each word on the passage with respect to each word in the question.
The encoded words in the passage are concatenated with the attention; the resulting vector is re-encoded with a further RNN.
Three convolutional neural networks with different filter size (1,2,3-gram) are used to further capture local features.
Candidate answers are selected either matching POS patterns of answers in the training set or choosing all possible text span until a certain length.
Each candidate answer has three representations, one for each n-gram representation. The compatibility of these representation with the question representation is then calculated.
The scores are combined linearly and used for calculating the probability of the candidate answer being the right answer for the question.

The method is tested on the SQUAD dataset and outperforms the proposed baselines.

----------

OVERALL JUDGMENT
The method presented in this paper is interesting but not very motivated in some points.
For example, it is not explained why in the attention mechanism it is beneficial to concatenate the original passage encoding with the attention-weighted ones.
The contributions of the paper are moderately novel proposing mainly the attention mechanism and the convolutional re-encoding.
In fact, combining questions and passages and score their compatibility has became a fairly standard procedure in all QA models.

----------

DETAILED COMMENTS

Equation (13) i should be s, not s^l.

I still do not understand the sentence "" the best function is to concatenate the hidden stat of the fist word in a chunk in forward RNN and that of the last word in backward RNN"". The RNN is over what all the words in the chunk? in the passage? 
The answer the authors gave in the response does not clarify this point.

---

SYNOPSIS: The paper proposes a new neural network-based model for reading comprehension (reading a passage of text and answering questions based on the passage). It is similar in spirit to several other recent models, with the main exception that it is able to predict answers of different lengths, as opposed to single words/tokens/entities. The authors compare their model on the Stanford Question Answering Dataset (SQuAD), and show improvements over the baselines, while apparently lagging quite far behind the current state of the art reported on the SQuAD leaderboard.

THOUGHTS: The main novelty of the method is to be able to identify phrases of different lengths as possible answers to the question. However, both approaches considered -- using a POS pattern trie tree to filter out word sequences with POS tags matching those of answers in the training set, and brute-force enumeration of all phrases up to length N -- seem somewhat orthogonal to the idea of ""learning end-to-end "" an answer chunk extraction model. Furthermore, as other reviews have pointed out, it seems that the linguistic features actually contribute a lot to the final accuracy (Table 3). One could argue that these are easy to obtain using standard taggers, but it takes away even more from the idea of an ""end-to-end trained"" system.

The paper is generally well written, but there are several crucial sections in parts describing the model where it was really hard for me to follow the descriptions. In particular, the attention mechanism seems fairly standard to me in a seq2seq sense (i.e. there is nothing architecturally novel about it, as is for instance the case with the Gated Attentive Reader). I may be missing something, but even after the clarification round I still don't understand how it is novel compared to standard attention used in for instance seq2seq models.

Finally, although the method is shown to outperform the baseline method reported in the original paper introducing the SQuAD dataset, it currently seems to be 12th (out of 15 systems) on the leaderboard (

---

The program committee appreciates the authors' response to the clarification questions and one review. Unfortunately, reviews are not leaning sufficiently towards acceptance. Reviewers have concerns about the novelty of the approach, its effectiveness in terms of empirical performance, and lack of analysis that would help determine the main contributions of the proposed approach. Authors are strongly encouraged to incorporate reviewer feedback in future iterations of the work.

---

SYNOPSIS: The paper proposes a new neural network-based model for reading comprehension (reading a passage of text and answering questions based on the passage). It is similar in spirit to several other recent models, with the main exception that it is able to predict answers of different lengths, as opposed to single words/tokens/entities. The authors compare their model on the Stanford Question Answering Dataset (SQuAD), and show improvements over the baselines, while apparently lagging quite far behind the current state of the art reported on the SQuAD leaderboard.

THOUGHTS: The main novelty of the method is to be able to identify phrases of different lengths as possible answers to the question. However, both approaches considered -- using a POS pattern trie tree to filter out word sequences with POS tags matching those of answers in the training set, and brute-force enumeration of all phrases up to length N -- seem somewhat orthogonal to the idea of ""learning end-to-end "" an answer chunk extraction model. Furthermore, as other reviews have pointed out, it seems that the linguistic features actually contribute a lot to the final accuracy (Table 3). One could argue that these are easy to obtain using standard taggers, but it takes away even more from the idea of an ""end-to-end trained"" system.

The paper is generally well written, but there are several crucial sections in parts describing the model where it was really hard for me to follow the descriptions. In particular, the attention mechanism seems fairly standard to me in a seq2seq sense (i.e. there is nothing architecturally novel about it, as is for instance the case with the Gated Attentive Reader). I may be missing something, but even after the clarification round I still don't understand how it is novel compared to standard attention used in for instance seq2seq models.

Finally, although the method is shown to outperform the baseline method reported in the original paper introducing the SQuAD dataset, it currently seems to be 12th (out of 15 systems) on the leaderboard (

---

The paper proposed an end-to-end machine learning model called dynamic reader for the machine reading comprehension task. Compared to earlier systems, the proposed model is able to extract and rank a set of answer candidates from a given document.

There are many recent models focusing on building good question answering systems by extracting phrases from a given article. It seems that there are two different aspects that are unique in this work:

1.	The use of convolution model, and
2.	Dynamic chunking

Convolution network is often only used for modeling character-based word embeddings so I am curious about its effectiveness on representing phrases. Therefore, I wish there could be more analysis on how effective it is, as the authors do not compare the convolution framework to other alternative approaches such as LSTM. The comparisons are important, as the authors uses uni-gram, bi-gram and tri-gram information in the convolution network, and it is not clear to me that if tri-gram information is still needed for LSTM models.

The dynamic chunking is a good idea, and a very similar idea is proposed in some of the recent papers such as [Kenton et al, 16], which also targets at the same dataset. However, I would like to see more analysis on the dynamic chunking. Why this approach is a good approach for representing answer chunks? Given the representation of the chunk is constructed by the first and the end word representations generated by a convolution network, I am not sure about the ability of this representation to capture the long answer phrases.

The authors do not use character base embedding but use some of the previous trained NLP models. It would be interesting if the authors could show what are the advantages and disadvantages of using linguistic features compared to character embeddings.

In short, there are several good ideas proposed in the paper, but the lack of proper analysis make it difficult to judge how important the proposed techniques are.

---

SUMMARY.
The paper propose a reading-comprehension question answering system for the recent QA task where answers of a question can be either single tokens or spans in the given text passage.
The model first encodes the passage and the query using a recurrent neural network.
With an attention mechanism the model calculates the importance of each word on the passage with respect to each word in the question.
The encoded words in the passage are concatenated with the attention; the resulting vector is re-encoded with a further RNN.
Three convolutional neural networks with different filter size (1,2,3-gram) are used to further capture local features.
Candidate answers are selected either matching POS patterns of answers in the training set or choosing all possible text span until a certain length.
Each candidate answer has three representations, one for each n-gram representation. The compatibility of these representation with the question representation is then calculated.
The scores are combined linearly and used for calculating the probability of the candidate answer being the right answer for the question.

The method is tested on the SQUAD dataset and outperforms the proposed baselines.

----------

OVERALL JUDGMENT
The method presented in this paper is interesting but not very motivated in some points.
For example, it is not explained why in the attention mechanism it is beneficial to concatenate the original passage encoding with the attention-weighted ones.
The contributions of the paper are moderately novel proposing mainly the attention mechanism and the convolutional re-encoding.
In fact, combining questions and passages and score their compatibility has became a fairly standard procedure in all QA models.

----------

DETAILED COMMENTS

Equation (13) i should be s, not s^l.

I still do not understand the sentence "" the best function is to concatenate the hidden stat of the fist word in a chunk in forward RNN and that of the last word in backward RNN"". The RNN is over what all the words in the chunk? in the passage? 
The answer the authors gave in the response does not clarify this point.",,,,,,5.0,,,3.0,,
598,"SPEECH RECOGNITION SYSTEM
Authors: Ronan Collobert, Christian Puhrsch
Source file: 598.pdf

ABSTRACT
This paper presents a simple end-to-end model for speech recognition, combining a convolutional network based acoustic model and a graph decoding. It is trained to output letters, with transcribed speech, without the need for force alignment of phonemes. We introduce an automatic segmentation criterion for training from sequence annotation without alignment that is on par with CTC (Graves et al., 2006) while being simpler. We show competitive results in word error rate on the Librispeech corpus (Panayotov et al., 2015) with MFCC features, and promising results from raw waveform.

1 INTRODUCTION
We present an end-to-end system to speech recognition, going from the speech signal (e.g. MelFrequency Cepstral Coefficients (MFCC), power spectrum, or raw waveform) to the transcription. The acoustic model is trained using letters (graphemes) directly, which take out the need for an intermediate (human or automatic) phonetic transcription. Indeed, the classical pipeline to build state of the art systems for speech recognition consists in first training an HMM/GMM model to force align the units on which the final acoustic model operates (most often context-dependent phone states). This approach takes its roots in HMM/GMM training (Woodland & Young, 1993). The improvements brought by deep neural networks (DNNs) (Mohamed et al., 2012; Hinton et al., 2012) and convolutional neural networks (CNNs) (Sercu et al., 2015; Soltau et al., 2014) for acoustic modeling only extend this training pipeline.
The current state of the art on Librispeech (the dataset that we used for our evaluations) uses this approach too (Panayotov et al., 2015; Peddinti et al., 2015b), with an additional step of speaker adaptation (Saon et al., 2013; Peddinti et al., 2015a). Recently, Senior et al. (2014) proposed GMMfree training, but the approach still requires to generate a force alignment. An approach that cut ties with the HMM/GMM pipeline (and with force alignment) was to train with a recurrent neural network (RNN) (Graves et al., 2013) for phoneme transcription. There are now competitive end-to-end approaches of acoustic models toppled with RNNs layers as in (Hannun et al., 2014; Miao et al., 2015; Saon et al., 2015; Amodei et al., 2015), trained with a sequence criterion (Graves et al., 2006). However these models are computationally expensive, and thus take a long time to train.
Compared to classical approaches that need phonetic annotation (often derived from a phonetic dictionary, rules, and generative training), we propose to train the model end-to-end, using graphemes directly. Compared to sequence criterion based approaches that train directly from speech signal to graphemes (Miao et al., 2015), we propose a simple(r) architecture (23 millions of parameters for our best model, vs. 100 millions of parameters in (Amodei et al., 2015)) based on convolutional networks
for the acoustic model, toppled with a graph transformer network (Bottou et al., 1997), trained with a simpler sequence criterion. Our word-error-rate on clean speech is slightly better than (Hannun et al., 2014), and slightly worse than (Amodei et al., 2015), in particular factoring that they train on 12,000 hours while we only train on the 960h available in LibriSpeech’s train set. Finally, some of our models are also trained on the raw waveform, as in (Palaz et al., 2013; 2015; Sainath et al., 2015). The rest of the paper is structured as follows: the next section presents the convolutional networks used for acoustic modeling, along with the automatic segmentation criterion. The following section shows experimental results comparing different features, the criterion, and our current best word error rates on LibriSpeech.

2 ARCHITECTURE
Our speech recognition system is a standard convolutional neural network (LeCun & Bengio, 1995) fed with various different features, trained through an alternative to the Connectionist Temporal Classification (CTC) (Graves et al., 2006), and coupled with a simple beam search decoder. In the following sub-sections, we detail each of these components.
2.1 FEATURES
We consider three types of input features for our model: MFCCs, power-spectrum, and raw wave. MFCCs are carefully designed speech-specific features, often found in classical HMM/GMM speech systems (Woodland & Young, 1993) because of their dimensionality compression (13 coefficients are often enough to span speech frequencies). Power-spectrum features are found in most recent deep learning acoustic modeling features (Amodei et al., 2015). Raw wave has been somewhat explored in few recent work (Palaz et al., 2013; 2015). ConvNets have the advantage to be flexible enough to be used with either of these input feature types. Our acoustic models output letter scores (one score per letter, given a dictionary L).

2.2 CONVNET ACOUSTIC MODEL
The acoustic models we considered in this paper are all based on standard 1D convolutional neural networks (ConvNets). ConvNets interleave convolution operations with pointwise non-linearity operations. Often ConvNets also embark pooling layers: these type of layers allow the network to “see” a larger context, without increasing the number of parameters, by locally aggregating the previous convolution operation output. Instead, our networks leverage striding convolutions. Given (xt)t=1...Tx an input sequence with Tx frames of dx dimensional vectors, a convolution with kernel width kw, stride dw and dy frame size output computes the following:
yit = bi + dx∑ j=1 kw∑ k=1 wi,j,k x j dw×(t−1)+k ∀1 ≤ i ≤ dy, (1)
where b ∈ Rdy and w ∈ Rdy×dx×kw are the parameters of the convolution (to be learned).
Pointwise non-linear layers are added after convolutional layers. In our experience, we surprisingly found that using hyperbolic tangents, their piecewise linear counterpart HardTanh (as in (Palaz et al., 2015)) or ReLU units lead to similar results.
There are some slight variations between the architectures, depending on the input features. MFCC-based networks need less striding, as standard MFCC filters are applied with large strides on the input
raw sequence. With power spectrum-based and raw wave-based networks, we observed that the overall stride of the network was more important than where the convolution with strides were placed. We found thus preferrable to set the strided convolutions near the first input layers of the network, as it leads to the fastest architectures: with power spectrum features or raw wave, the input sequences are very long and the first convolutions are thus the most expensive ones.
The last layer of our convolutional network outputs one score per letter in the letter dictionary (dy = |L|). Our architecture for raw wave is shown in Figure 1 and is inspired by (Palaz et al., 2015). The architectures for both power spectrum and MFCC features do not include the first layer. The full network can be seen as a non-linear convolution, with a kernel width of size 31280 and stride equal to 320; given the sample rate of our data is 16KHz, label scores are produced using a window of 1955 ms, with steps of 20ms.

2.3 INFERRING SEGMENTATION WITH AUTOSEGCRITERION
Most large labeled speech databases provide only a text transcription for each audio file. In a classification framework (and given our acoustic model produces letter predictions), one would need the segmentation of each letter in the transcription to train properly the model. Unfortunately, manually labeling the segmentation of each letter would be tedious. Several solutions have been explored in the speech community to alleviate this issue: HMM/GMM models use an iterative EM procedure: (i) during the Estimation step, the best segmentation is inferred, according to the current model, by maximizing the joint probability of the letter (or any sub-word unit) transcription and input sequence. (ii) During the Maximization step the model is optimized by minimizing a frame-level criterion, based on the (now fixed) inferred segmentation. This approach is also often used to boostrap the training of neural network-based acoustic models.
Other alternatives have been explored in the context of hybrid HMM/NN systems, such as the MMI criterion (Bahl et al., 1986) which maximizes the mutual information between the acoustic sequence and word sequences or the Minimum Bayse Risk (MBR) criterion (Gibson & Hain, 2006).
More recently, standalone neural network architectures have been trained using criterions which jointly infer the segmentation of the transcription while increase the overall score of the right transcription (Graves et al., 2006; Palaz et al., 2014). The most popular one is certainly the Connectionist Temporal Classification (CTC) criterion, which is at the core of Baidu’s Deep Speech architecture (Amodei et al., 2015). CTC assumes that the network output probability scores, normalized at the frame level. It considers all possible sequence of letters (or any sub-word units), which can lead to a to a given transcription. CTC also allow a special “blank” state to be optionally inserted between each letters. The rational behind the blank state is two-folds: (i) modeling “garbage” frames which might occur between each letter and (ii) identifying the separation between two identical consecutive letters in a transcription. Figure 2a shows an example of the sequences accepted by CTC for a given transcription. In practice, this graph is unfolded as shown in Figure 2b, over the available frames output by the acoustic model. We denote Gctc(θ, T ) an unfolded graph over T frames for a given transcription θ, and π = π1, . . . , πT ∈ Gctc(θ, T ) a path in this graph representing a (valid) sequence of letters for this transcription. At each time step t, each node of the graph is assigned with the corresponding log-probability letter (that we denote ft(·)) output by the acoustic model. CTC aims at maximizing the “overall” score of paths in Gctc(θ, T ); for that purpose, it minimizes the Forward score:
CTC(θ, T ) = − logadd π∈Gctc(θ,T ) T∑ t=1 fπt(x) , (2)
where the “logadd” operation, also often called “log-sum-exp” is defined as logadd(a, b) = exp(log(a) + log(b)). This overall score can be efficiently computed with the Forward algorithm. To put things in perspective, if one would replace the logadd(·) by a max(·) in (2) (which can be then efficiently computed by the Viterbi algorithm, the counterpart of the Forward algorithm), one would then maximize the score of the best path, according to the model belief. The logadd(·) can be seen as a smooth version of the max(·): paths with similar scores will be attributed the same weight in the overall score (and hence receive the same gradient), and paths with much larger score will have much more overall weight than paths with low scores. In practice, using the logadd(·) works much better than the max(·). It is also worth noting that maximizing (2) does not diverge, as the acoustic model is assumed to output normalized scores (log-probabilities) fi(·).
In this paper, we explore an alternative to CTC, with three differences: (i) there are no blank labels, (ii) un-normalized scores on the nodes (and possibly un-normalized transition scores on the edges) (iii) global normalization instead of per-frame normalization:
• The advantage of (i) is that it produces a much simpler graph (see Figure 3a and Figure 3b). We found that in practice there was no advantage of having a blank class to model the possible “garbage” frames between letters. Modeling letter repetitions (which is also an important quality of the blank label in CTC) can be easily replaced by repetition character labels (we used two extra labels for two and three repetitions). For example “caterpillar” could be written as “caterpil2ar”, where “2” is a label to represent the repetition of the previous letter. Not having blank labels also simplifies the decoder.
• With (ii) one can easily plug an external language model, which would insert transition scores on the edges of the graph. This could be particularly useful in future work, if one wanted to model representations more high-level than letters. In that respect, avoiding normalized transitions is important to alleviate the problem of “label bias” Bottou (1991); Lafferty et al. (2001). In this work, we limited ourselves to transition scalars, which are learned together with the acoustic model.
• The normalization evoked in (iii) is necessary when using un-normalized scores on nodes or edges; it insures incorrect transcriptions will have a low confidence.
In the following, we name our criterion “Auto Segmentation Criterion” (ASG). Considering the same notations than for CTC in (2), and an unfolded graph Gasg(θ, T ) over T frames for a given transcription θ (as in Figure 3b), as well as a fully connected graph Gfull(θ, T ) over T frames (representing all possible sequence of letters, as in Figure 3c), ASG aims at minimizing:
ASG(θ, T ) = − logadd π∈Gasg(θ,T ) T∑ t=1 (fπt(x) + gπt−1,πt(x)) + logadd π∈Gfull(θ,T ) T∑ t=1 (fπt(x) + gπt−1,πt(x)) , (3) where gi,j(·) is a transition score model to jump from label i to label j. The left-hand part of 3 promotes sequences of letters leading to the right transcription, and the right-hand part demotes all sequences of letters. As for CTC, these two parts can be efficiently computed with the Forward algorithm. Derivatives with respect to fi(·) and gi,j(·) can be obtained (maths are a bit tedious) by applying the chain rule through the Forward recursion.

2.4 BEAM-SEARCH DECODER
We wrote our own one-pass decoder, which performs a simple beam-search with beam threholding, histogram pruning and language model smearing Steinbiss et al. (1994). We kept the decoder as
simple as possible (under 1000 lines of C code). We did not implement any sort of model adaptation before decoding, nor any word graph rescoring. Our decoder relies on KenLM Heafield et al. (2013) for the language modeling part. It also accepts un-normalized acoustic scores (transitions and emissions from the acoustic model) as input. The decoder attempts to maximize the following:
L(θ) = logadd π∈Gasg(θ,T ) T∑ t=1 (fπt(x) + gπt−1,πt(x)) + α logPlm(θ) + β|θ| , (4)
where Plm(θ) is the probability of the language model given a transcription θ, α and β are two hyper-parameters which control the weight of the language model and the word insertion penalty respectively.

3 EXPERIMENTS

3.1 SETUP
We implemented everything using Torch71. The ASG criterion as well as the decoder were implemented in C (and then interfaced into Torch).
We consider as benchmark LibriSpeech, a large speech database freely available for download (Panayotov et al., 2015). LibriSpeech comes with its own train, validation and test sets. Except when specified, we used all the available data (about 1000h of audio files) for training and validating our models. We use the original 16 KHz sampling rate. The vocabulary L contains 30 graphemes: the standard English alphabet plus the apostrophe, silence, and two special “repetition” graphemes which encode the duplication (once or twice) of the previous letter (see Section 2.3).
The architecture hyper-parameters, as well the decoder ones were tuned using the validation set. In the following, we either report letter-error-rates (LERs) or word-error-rates (WERs). WERs have been obtained by using our own decoder (see Section 2.4), with the standard 4-gram language model provided with LibriSpeech2.
1http://www.torch.ch. 2http://www.openslr.org/11.
MFCC features are computed with 13 coefficients, a 25 ms sliding window and 10 ms stride. We included first and second order derivatives. Power spectrum features are computed with a 25 ms window, 10 ms stride, and have 257 components. All features are normalized (mean 0, std 1) per input sequence.

3.2 RESULTS
Table 1 reports a comparison between CTC and ASG, in terms of LER and speed. Our ASG criterion is implemented in C (CPU only), leveraging SSE instructions when possible. Our batching is done with an OpenMP parallel for. We picked the CTC criterion implementation provided by Baidu3. Both criteria lead to the same LER. For comparing the speed, we report performance for sequence sizes as reported initially by Baidu, but also for longer sequence sizes, which corresponds to our average use case. ASG appears faster on long sequences, even though it is running on CPU only. Baidu’s GPU CTC implementation seems more aimed at larger vocabularies (e.g. 5000 Chinese characters).
We also investigated the impact of the training size on the dataset, as well as the effect of a simple data augmentation procedure, where shifts were introduced in the input frames, as well as stretching. For that purpose, we tuned the size of our architectures (given a particular size of the dataset), to avoid over-fitting. Figure 4a shows the augmentation helps for small training set size. However, with enough training data, the effect of data augmentation vanishes, and both type of features appear to perform similarly. Figure 4b reports the WER with respect to the available training data size. We observe that we compare very well against Deep Speech 1 & 2 which were trained with much more data Hannun et al. (2014); Amodei et al. (2015).
Finally, we report in Table 2 the best results of our system so far, trained on 1000h of speech, for each type of features. The overall stride of architectures is 320 (see Figure 1), which produces a label every 20 ms. We found that one could squeeze out about 1% in performance by refining the precision of the output. This is efficiently achieved by shifting the input sequence, and feeding it to the network several times. Results in Table 2 were obtained by a single extra shift of 10 ms. Both power spectrum and raw features are performing slightly worse than MFCCs. One could expect, however, that with enough data (see Figure 4) the gap would vanish.
3https://github.com/baidu-research/warp-ctc.

4 CONCLUSION
We have introduced a simple end-to-end automatic speech recognition system, which combines a standard 1D convolutional neural network, a sequence criterion which can infer the segmentation, and a simple beam-search decoder. The decoding results are competitive on the LibriSpeech corpus with MFCC features (7.2% WER), and promising with power spectrum and raw speech (9.4% WER and 10.1% WER respectively). We showed that our AutoSegCriterion can be faster than CTC (Graves et al., 2006), and as accurate (table 1). Our approach breaks free from HMM/GMM pre-training and force-alignment, as well as not being as computationally intensive as RNN-based approaches (Amodei et al., 2015) (on average, one LibriSpeech sentence is processed in less than 60ms by our ConvNet, and the decoder runs at 8.6x on a single thread).
","This paper presents a simple end-to-end model for speech recognition, combining a convolutional network based acoustic model and a graph decoding. It is trained to output letters, with transcribed speech, without the need for force alignment of phonemes. We introduce an automatic segmentation criterion for training from sequence annotation without alignment that is on par with CTC (Graves et al., 2006) while being simpler. We show competitive results in word error rate on the Librispeech corpus (Panayotov et al., 2015) with MFCC features, and promising results from raw waveform.",ICLR 2017 conference submission,False,,"This submission proposes a letter-level decoder with a variation of the CTC approach they call ASG, where the blank symbol is dropped and replaced by letter repetition symbols, and where explicit normalization is dropped. Both the description of a letter-level model (though not novel), as well as the CTC-variant are interesting. 

The approach is evaluated on the LibriSpeech task. The authors claim that their approach is competitive. They compare their modelling variant ASG to CTC, but a comparison of the letter-level approach to available word-level results are missing. Compared to the results obtained in Panayotov et al. 2015, the performance obtained here seems only comparable to word-level GMM/HMM models, but worse than word-level hybrid DNN/HMM models, though Panayotov et al. also appled speaker adaptation, which was not done, as far as I can see. I suggest to add a comparison to Panyotov's results (in addition to mentioning Baidu's results on Librispeech, which are not comparable due to much larger amounts of training data), to allow readers to get a quantitative idea. As pointed out by the authors in the text, Baidu's GPU implementation for CTC is more aimed at larger vocabularies, therefore the comparison to GPU in Tables 1a-c do not seem to be helpful for this work, without further discussing the implementations.

You are using quite a huge analysis window (nearly 2s). Even though other authors also use windows up to 0.5s to 1s (e.g. MRASTA features), some comments on how you arrive at such a large window, and what advantages you observe for it, would be interesting.

The submission is well written, though more details on the experiences with using non-normalized (transition) scores and beam pruning would be desirable. Table 1 would be better readable if the units of the numbers shown in a/b/c would be shown within the tables, and not only in the caption.

Prior (partial) publications of this work (your NIPS end-to-end workshop paper) should clearly be mentioned/referenced.

What do you mean by transition ""scalars""?

I do not repeat further comments here, which were already given in the pre-review period.

Minor comments:
 - Sec. 2.3, end of 2nd sentence: train properly the model -> train the model properly
   End of same paragraph: boostrap -> bootstrap (such errors should be avoided by performing an automatic spell check)
 - Sec. 2.3: Bayse -> Bayes
 - definition of logadd is wrong (see comment) - (applies also for your NIPS end-to-end workshop paper).
 - line before Eq. (3): all possible sequence of letters -> all possible sequences of letters (plural)
 - Sec. 2.4, first line: threholding -> thresholding (spell check..)
 - Figure 4: mention the corpus used here - dev?

---

Without revisions to this paper or a rebuttal from the authors, it is hard to accept this paper. The main contribution of the paper is removing the blank from CTC to create a somewhat different criterion, but this is not particularly novel (see, for example,

---

This paper describes an end-to-end system for speech recognition that uses a linear conditional random field framework.  A convnet estimates node potentials, while transition scores are provided by trained scalar values.  The convnet acoustic model computes scores for letters, not phones, which reduces the need for expert knowledge in training the system.  At test time, scores from a word-level language model, the convnet node potentials, learned letter-to-letter transition scores, and a word insertion penalty are combined to find the best-scoring word hypothesis.  The model may be trained from the raw audio waveform, power spectra, or MFCC features using conditional maximum likelihood estimation.  Experiments on the Librispeech corpus show that the model achieves a 7.2% WER on the test-clean set from Librispeech using MFCC features, a 9.4% WER using power spectral features, and a 10.1% WER using the raw waveform.

Pros
+ It is interesting to see that a convnet trained from scratch using conditional maximum likelihood can perform reasonably well in a speech recognition system for English that uses graphemic (letter-based) acoustic models instead of phonetic models.  This is a promising research direction.

Cons
- The paper is missing a lot of context / prior work that deserves to be cited.  In addition to the papers I already mentioned in various comments, the authors should also be aware of another 2016 Interspeech paper: Zhang et al., ""Towards End-to-End Speech Recognition with Deep Convolutional Neural Networks"",

---

​There have been numerous works ​on learning from raw waveforms and training letter-based CTC networks for speech recognition, however, there are very few works on combining both of them with purely ConvNet as it is done in this paper. It is interesting to see results on a large scale corpus such as Librispeech that is used in this paper, though some baseline results from hybrid NN/HMM systems should be provided. To readers, it is unclear how this system is close to state-of-the-art only from Table 2.

The key contribution of this paper may be the end-to-end sequence training criterion for their CTC variant (where the blank symbol is dropped), which may be viewed as sequence training of CTC as H. Sak, et al. ""Learning acoustic frame labeling for speech recognition with recurrent neural networks"", 2015. However, instead of generating the denominator lattices using a frame-level trained CTC model first, this paper directly compute the sequence-level loss by considering all the competing hypothesis in the normalizer. Therefore, the model is trained end-to-end. From this perspective, it is closely related to D. Povey's LF-MMI for sequence-training of HMMs. As another reviewer has pointed out, references and discussions on that should be provided. 

This approach should be more expensive than frame-level training of CTCs, however, from Table 1, the authors' implementation is much faster. Did the systems there use the same sampling rate? You said at the end of 2.2 that the step size for your model is 20ms. Is it also the same for Baidu's CTC system. Also, have you tried increasing the step size, e.g. to 30ms or 40ms, as people have found that it may work (equally) better, while significantly cut down the computational cost.

---

Dear authors,

Here are some missing relevant citations.

You should definitely cite the original paper that used CTC with characters.
Graves et al., ""Towards End-to-End Speech Recognition with Recurrent Neural Networks"", in ICML 2014.

You should probably also cite and have a related work section with attention-based models such as:
Chan et al., ""Listen, Attend and Spell: A Neural Network for Large Vocabulary Conversational Speech Recognition"", in ICASSP 2016.
Bahdanau et al., ""End-to-End Attention-based Large Vocabulary Speech Recognition"", in ICASSP 2016.

both of which are highly relevant to end-to-end ASR.

Question:
Why did you use Librispeech as opposed to WSJ and/or SWBD. Most end-to-end ASR papers publish on WSJ, especially since there is an established benchmark for comparison (i.e., Graves et al., 2014, Bahdanau et al., 2016, Chan et al., 2016). SWBD also has much stronger benchmarks from the general speech community, and even for end-to-end ASR (i.e., see MSR's CTC paper by Zweig et., ""Advances in All-Neural Speech Recognition"", 2016). You should also definitely comment and compare to Zweig's paper, since they used a similar encoding mechanism.

Question:
Is ""Letter Error Rate"" (LER) the common terminology? From Alex Graves papers and others I see ""Character Error Rate"" (CER). What is the difference?

Question:
Very cool that you can combine wav+cnns+ctc->ASR, but still a little bit disappointed that handcrafted features perform better. Do you expect this to change with more data?

---

This submission proposes a letter-level decoder with a variation of the CTC approach they call ASG, where the blank symbol is dropped and replaced by letter repetition symbols, and where explicit normalization is dropped. Both the description of a letter-level model (though not novel), as well as the CTC-variant are interesting. 

The approach is evaluated on the LibriSpeech task. The authors claim that their approach is competitive. They compare their modelling variant ASG to CTC, but a comparison of the letter-level approach to available word-level results are missing. Compared to the results obtained in Panayotov et al. 2015, the performance obtained here seems only comparable to word-level GMM/HMM models, but worse than word-level hybrid DNN/HMM models, though Panayotov et al. also appled speaker adaptation, which was not done, as far as I can see. I suggest to add a comparison to Panyotov's results (in addition to mentioning Baidu's results on Librispeech, which are not comparable due to much larger amounts of training data), to allow readers to get a quantitative idea. As pointed out by the authors in the text, Baidu's GPU implementation for CTC is more aimed at larger vocabularies, therefore the comparison to GPU in Tables 1a-c do not seem to be helpful for this work, without further discussing the implementations.

You are using quite a huge analysis window (nearly 2s). Even though other authors also use windows up to 0.5s to 1s (e.g. MRASTA features), some comments on how you arrive at such a large window, and what advantages you observe for it, would be interesting.

The submission is well written, though more details on the experiences with using non-normalized (transition) scores and beam pruning would be desirable. Table 1 would be better readable if the units of the numbers shown in a/b/c would be shown within the tables, and not only in the caption.

Prior (partial) publications of this work (your NIPS end-to-end workshop paper) should clearly be mentioned/referenced.

What do you mean by transition ""scalars""?

I do not repeat further comments here, which were already given in the pre-review period.

Minor comments:
 - Sec. 2.3, end of 2nd sentence: train properly the model -> train the model properly
   End of same paragraph: boostrap -> bootstrap (such errors should be avoided by performing an automatic spell check)
 - Sec. 2.3: Bayse -> Bayes
 - definition of logadd is wrong (see comment) - (applies also for your NIPS end-to-end workshop paper).
 - line before Eq. (3): all possible sequence of letters -> all possible sequences of letters (plural)
 - Sec. 2.4, first line: threholding -> thresholding (spell check..)
 - Figure 4: mention the corpus used here - dev?

---

A slightly more compressed version of this submission will be presented at the NIPS end-to-end workshop on Dec. 10, 2016. The NIPS submission seems to be a clear subset of this submission and should at least be mentioned in this paper.

---

When dropping the normalization of acoustic model scores, the range of scores obtained might vary and would have an effect on beam pruning and on its relation to the normalized LM scores. Did you analyse this?

---

Sec. 2.3: you use digits to label character repetitions. How do you handle numbers?

---

It seems that you use inconsistent notation - the variable 't' is used for different time scales: in Eq. (1) t represents strided time frames, whereas in x_t above it enumerates frames directly.

---

This submission proposes a letter-level decoder with a variation of the CTC approach they call ASG, where the blank symbol is dropped and replaced by letter repetition symbols, and where explicit normalization is dropped. Both the description of a letter-level model (though not novel), as well as the CTC-variant are interesting. 

The approach is evaluated on the LibriSpeech task. The authors claim that their approach is competitive. They compare their modelling variant ASG to CTC, but a comparison of the letter-level approach to available word-level results are missing. Compared to the results obtained in Panayotov et al. 2015, the performance obtained here seems only comparable to word-level GMM/HMM models, but worse than word-level hybrid DNN/HMM models, though Panayotov et al. also appled speaker adaptation, which was not done, as far as I can see. I suggest to add a comparison to Panyotov's results (in addition to mentioning Baidu's results on Librispeech, which are not comparable due to much larger amounts of training data), to allow readers to get a quantitative idea. As pointed out by the authors in the text, Baidu's GPU implementation for CTC is more aimed at larger vocabularies, therefore the comparison to GPU in Tables 1a-c do not seem to be helpful for this work, without further discussing the implementations.

You are using quite a huge analysis window (nearly 2s). Even though other authors also use windows up to 0.5s to 1s (e.g. MRASTA features), some comments on how you arrive at such a large window, and what advantages you observe for it, would be interesting.

The submission is well written, though more details on the experiences with using non-normalized (transition) scores and beam pruning would be desirable. Table 1 would be better readable if the units of the numbers shown in a/b/c would be shown within the tables, and not only in the caption.

Prior (partial) publications of this work (your NIPS end-to-end workshop paper) should clearly be mentioned/referenced.

What do you mean by transition ""scalars""?

I do not repeat further comments here, which were already given in the pre-review period.

Minor comments:
 - Sec. 2.3, end of 2nd sentence: train properly the model -> train the model properly
   End of same paragraph: boostrap -> bootstrap (such errors should be avoided by performing an automatic spell check)
 - Sec. 2.3: Bayse -> Bayes
 - definition of logadd is wrong (see comment) - (applies also for your NIPS end-to-end workshop paper).
 - line before Eq. (3): all possible sequence of letters -> all possible sequences of letters (plural)
 - Sec. 2.4, first line: threholding -> thresholding (spell check..)
 - Figure 4: mention the corpus used here - dev?

---

Without revisions to this paper or a rebuttal from the authors, it is hard to accept this paper. The main contribution of the paper is removing the blank from CTC to create a somewhat different criterion, but this is not particularly novel (see, for example,

---

This paper describes an end-to-end system for speech recognition that uses a linear conditional random field framework.  A convnet estimates node potentials, while transition scores are provided by trained scalar values.  The convnet acoustic model computes scores for letters, not phones, which reduces the need for expert knowledge in training the system.  At test time, scores from a word-level language model, the convnet node potentials, learned letter-to-letter transition scores, and a word insertion penalty are combined to find the best-scoring word hypothesis.  The model may be trained from the raw audio waveform, power spectra, or MFCC features using conditional maximum likelihood estimation.  Experiments on the Librispeech corpus show that the model achieves a 7.2% WER on the test-clean set from Librispeech using MFCC features, a 9.4% WER using power spectral features, and a 10.1% WER using the raw waveform.

Pros
+ It is interesting to see that a convnet trained from scratch using conditional maximum likelihood can perform reasonably well in a speech recognition system for English that uses graphemic (letter-based) acoustic models instead of phonetic models.  This is a promising research direction.

Cons
- The paper is missing a lot of context / prior work that deserves to be cited.  In addition to the papers I already mentioned in various comments, the authors should also be aware of another 2016 Interspeech paper: Zhang et al., ""Towards End-to-End Speech Recognition with Deep Convolutional Neural Networks"",

---

​There have been numerous works ​on learning from raw waveforms and training letter-based CTC networks for speech recognition, however, there are very few works on combining both of them with purely ConvNet as it is done in this paper. It is interesting to see results on a large scale corpus such as Librispeech that is used in this paper, though some baseline results from hybrid NN/HMM systems should be provided. To readers, it is unclear how this system is close to state-of-the-art only from Table 2.

The key contribution of this paper may be the end-to-end sequence training criterion for their CTC variant (where the blank symbol is dropped), which may be viewed as sequence training of CTC as H. Sak, et al. ""Learning acoustic frame labeling for speech recognition with recurrent neural networks"", 2015. However, instead of generating the denominator lattices using a frame-level trained CTC model first, this paper directly compute the sequence-level loss by considering all the competing hypothesis in the normalizer. Therefore, the model is trained end-to-end. From this perspective, it is closely related to D. Povey's LF-MMI for sequence-training of HMMs. As another reviewer has pointed out, references and discussions on that should be provided. 

This approach should be more expensive than frame-level training of CTCs, however, from Table 1, the authors' implementation is much faster. Did the systems there use the same sampling rate? You said at the end of 2.2 that the step size for your model is 20ms. Is it also the same for Baidu's CTC system. Also, have you tried increasing the step size, e.g. to 30ms or 40ms, as people have found that it may work (equally) better, while significantly cut down the computational cost.

---

Dear authors,

Here are some missing relevant citations.

You should definitely cite the original paper that used CTC with characters.
Graves et al., ""Towards End-to-End Speech Recognition with Recurrent Neural Networks"", in ICML 2014.

You should probably also cite and have a related work section with attention-based models such as:
Chan et al., ""Listen, Attend and Spell: A Neural Network for Large Vocabulary Conversational Speech Recognition"", in ICASSP 2016.
Bahdanau et al., ""End-to-End Attention-based Large Vocabulary Speech Recognition"", in ICASSP 2016.

both of which are highly relevant to end-to-end ASR.

Question:
Why did you use Librispeech as opposed to WSJ and/or SWBD. Most end-to-end ASR papers publish on WSJ, especially since there is an established benchmark for comparison (i.e., Graves et al., 2014, Bahdanau et al., 2016, Chan et al., 2016). SWBD also has much stronger benchmarks from the general speech community, and even for end-to-end ASR (i.e., see MSR's CTC paper by Zweig et., ""Advances in All-Neural Speech Recognition"", 2016). You should also definitely comment and compare to Zweig's paper, since they used a similar encoding mechanism.

Question:
Is ""Letter Error Rate"" (LER) the common terminology? From Alex Graves papers and others I see ""Character Error Rate"" (CER). What is the difference?

Question:
Very cool that you can combine wav+cnns+ctc->ASR, but still a little bit disappointed that handcrafted features perform better. Do you expect this to change with more data?

---

This submission proposes a letter-level decoder with a variation of the CTC approach they call ASG, where the blank symbol is dropped and replaced by letter repetition symbols, and where explicit normalization is dropped. Both the description of a letter-level model (though not novel), as well as the CTC-variant are interesting. 

The approach is evaluated on the LibriSpeech task. The authors claim that their approach is competitive. They compare their modelling variant ASG to CTC, but a comparison of the letter-level approach to available word-level results are missing. Compared to the results obtained in Panayotov et al. 2015, the performance obtained here seems only comparable to word-level GMM/HMM models, but worse than word-level hybrid DNN/HMM models, though Panayotov et al. also appled speaker adaptation, which was not done, as far as I can see. I suggest to add a comparison to Panyotov's results (in addition to mentioning Baidu's results on Librispeech, which are not comparable due to much larger amounts of training data), to allow readers to get a quantitative idea. As pointed out by the authors in the text, Baidu's GPU implementation for CTC is more aimed at larger vocabularies, therefore the comparison to GPU in Tables 1a-c do not seem to be helpful for this work, without further discussing the implementations.

You are using quite a huge analysis window (nearly 2s). Even though other authors also use windows up to 0.5s to 1s (e.g. MRASTA features), some comments on how you arrive at such a large window, and what advantages you observe for it, would be interesting.

The submission is well written, though more details on the experiences with using non-normalized (transition) scores and beam pruning would be desirable. Table 1 would be better readable if the units of the numbers shown in a/b/c would be shown within the tables, and not only in the caption.

Prior (partial) publications of this work (your NIPS end-to-end workshop paper) should clearly be mentioned/referenced.

What do you mean by transition ""scalars""?

I do not repeat further comments here, which were already given in the pre-review period.

Minor comments:
 - Sec. 2.3, end of 2nd sentence: train properly the model -> train the model properly
   End of same paragraph: boostrap -> bootstrap (such errors should be avoided by performing an automatic spell check)
 - Sec. 2.3: Bayse -> Bayes
 - definition of logadd is wrong (see comment) - (applies also for your NIPS end-to-end workshop paper).
 - line before Eq. (3): all possible sequence of letters -> all possible sequences of letters (plural)
 - Sec. 2.4, first line: threholding -> thresholding (spell check..)
 - Figure 4: mention the corpus used here - dev?

---

A slightly more compressed version of this submission will be presented at the NIPS end-to-end workshop on Dec. 10, 2016. The NIPS submission seems to be a clear subset of this submission and should at least be mentioned in this paper.

---

When dropping the normalization of acoustic model scores, the range of scores obtained might vary and would have an effect on beam pruning and on its relation to the normalized LM scores. Did you analyse this?

---

Sec. 2.3: you use digits to label character repetitions. How do you handle numbers?

---

It seems that you use inconsistent notation - the variable 't' is used for different time scales: in Eq. (1) t represents strided time frames, whereas in x_t above it enumerates frames directly.",,,,,,5.666666666666667,,,4.666666666666667,,
614,"RECURRENT COEVOLUTIONARY FEATURE EMBEDDING PROCESSES FOR RECOMMENDATION
Authors: Hanjun Dai, Yichen Wang, Rakshit Trivedi
Source file: 614.pdf

ABSTRACT
Recommender systems often use latent features to explain the behaviors of users and capture the properties of items. As users interact with different items over time, user and item features can influence each other, evolve and co-evolve over time. To accurately capture the fine grained nonlinear coevolution of these features, we propose a recurrent coevolutionary feature embedding process model, which combines recurrent neural network (RNN) with a multi-dimensional point process model. The RNN learns a nonlinear representation of user and item embeddings which take into account mutual influence between user and item features, and the feature evolution over time. We also develop an efficient stochastic gradient algorithm for learning parameters. Experiments on diverse real-world datasets demonstrate significant improvements in user behavior prediction compared to state-of-the-arts.

1 INTRODUCTION
E-commerce platforms and social service websites, such as Reddit, Amazon, and Netflix, attracts thousands of users every second. Effectively recommending the appropriate service items to users is a fundamentally important task for these online services. It can significantly boost the user activities on these sites and leads to increased product purchases and advertisement clicks.
The interactions between users and items play a critical role in driving the evolution of user interests and item features. For example, for music streaming services, a long-time fan of Rock music listens to an interesting Blues one day, and starts to listen to more Blues instead of Rock music. Similarly, a single music may also serve different audiences at different times,e.g., a music initially targeted for an older generation may become popular among the young, and the features of this music need to be updated. Furthermore, as users interact with different items, users’ interests and items’ features can also co-evolve over time, i.e., their features are intertwined and can influence each other:
• User → item. In online discussion forums such as Reddit, although a group (item) is initially created for statistics topics, users with very different interest profiles can join this group. Hence, the participants can shape the features of the group through their postings. It is likely that this group can finally become one about deep learning because most users concern about deep learning. • Item→ user. As the group is evolving towards topics on deep learning, some users may become more interested in deep learning topics, and they may participate in other specialized groups on deep learning. On the opposite side, some users may gradually gain interests in pure math groups, lose interests in statistics and become inactive in this group.
Such co-evolutionary nature of user-item interactions raises very important questions on how to learn them from the increasingly available data. However, existing methods either treat the temporal user-item interactions data as a static graph or use epoch based methods such as tensor factorization to learn the latent features (Chi & Kolda, 2012; Koren, 2009; Yang et al., 2011). These methods are not able to capture the fine grained temporal dynamics of user-item interactions. Recent point process based models treat time as a random variable and improves over the traditional methods significantly (Du et al., 2015; Wang et al., 2016b). However, these works make strong assumptions
∗Authors have equal contributions.
about the function form of the generative processes, which may not reflect the reality or accurate enough to capture the complex and nonlinear user-item influence in real world.
In this paper, we propose a recurrent coevolutionary feature embedding process framework. It combines recurrent neural network (RNN) with point process models, and efficiently captures the co-evolution of user-item features. Our model can automatically find an efficient representation of the underlying user and item latent feature without assuming a fixed parametric forms in advance. Figure 1 summarizes our framework. In particular, our work makes the following contributions:
• Novel model. We propose a novel model that captures the nonlinear co-evolution nature of users’ and items’ embeddings. It assigns an evolving feature embedding process for each user and item, and the co-evolution of these latent feature processes is modeled with two parallel components: (i) item→ user component, a user’s latent feature is determined by the nonlinear embedding of latent features of the items he interacted with; and (ii) user→ item component, an item’s latent features are also determined by the latent features of the users who interact with the item. • Technical Challenges. We use RNN to parametrize the interdependent and intertwined user and item embeddings. The increased flexibility and generality further introduces technical challenges on how to train RNN on the co-evolving graphs. The co-evolution nature of the model makes the samples inter-dependent and not identically distributed, which is contrary to the assumptions in the traditional setting and significantly more challenging. We are the first to propose an efficient stochastic training algorithm that makes the BTPP tractable in the co-evolving graph. • Strong performance. We evaluate our method over multiple datasets, verifying that our method can lead to significant improvements in user behavior prediction compared to previous state-of-thearts. Precise time prediction is especially novel and not possible by most prior work.

2 RELATED WORK
Recent work predominantly fix the latent features assigned to each user and item (Salakhutdinov & Mnih, 2008; Chen et al., 2009; Agarwal & Chen, 2009; Ekstrand et al., 2011; Koren & Sill, 2011; Yang et al., 2011; Yi et al., 2014; Wang & Pal, 2015). In more sophisticated methods, the time is divided into epochs, and static latent feature models are applied to each epoch to capture some temporal aspects of the data (Koren, 2009; Karatzoglou et al., 2010; Xiong et al., 2010; Karatzoglou et al., 2010; Xiong et al., 2010; Chi & Kolda, 2012; Gultekin & Paisley, 2014; Charlin et al., 2015; Preeti Bhargava, 2015; Gopalan et al., 2015; Hidasi & Tikk, 2015; Wang et al., 2016a). For such methods, it is not clear how to choose the epoch length parameter. First, different users may have very different timescale when they interact with those service items, making it difficult to choose a unified epoch length. Second, it is not easy for these methods to answer time-sensitive queries such as when a user will return to the service item. The predictions are only in the resolution of the chosen epoch length. Recently, (Du et al., 2015) proposed a low-rank point process based model for time-sensitive recommendations from recurrent user activities. However, it fails to capture the heterogeneous coevolutionary properties of user-item interactions. Wang et al. (2016b) models the co-evolutionary property, but uses a simple linear representation of the users’ and items’ latent features, which might not be expressive enough to capture the real world patterns. As demonstrated in Du et al. (2016),
the nonlinear RNN is quite flexible to approximate many point process models. Also we will show that, our model only has O(#user + #item) regardless of RNN related parameters, and can also be potentially applied to online setting.
In the deep learning community, (Wang et al., 2015a) proposed a hierarchical Bayesian model that jointly performs learning for the content features and collaborative filtering for the ratings matrix. (Hidasi et al., 2016) applied RNN and adopt item-to-item recommendation approach with session based data. (Tan et al., 2016) improved this model with techniques like data augmentation, temporal change adaptation. (Ko et al., 2016) proposed collaborative RNN that extends collaborative filtering method to capture history of user behavior. Specifically, they used static global latent factors for items and assign separate latent factors for users that are dependent on their past history. (Song et al., 2016) extended the deep semantic structured model to capture multi-granularity temporal preference of users. They use separate RNN for each temporal granularity and combine them with feed forward network which models users’ and items’ long term static features. However, none of these works model the coevolution of users’ and items’ latent features and are still extensions of epoch based methods. Our work is unique since we explicitly treat time as a random variable and captures the coevolution of users’ and items’ latent features using temporal point processes. Finally, our work is inspired from the recurrent marked temporal point process model (Du et al., 2016). However, this work only focuses on learning a one-dimension point process. Our work is significantly different since we focus on the recommendation system setting with the novel idea of feature coevolution and we use multi-dimensional point processes to capture user-item interactions.

3 BACKGROUND ON TEMPORAL POINT PROCESSES
A temporal point process (Cox & Isham, 1980; Cox & Lewis, 2006; Aalen et al., 2008) is a random process whose realization consists of a list of discrete events localized in time, {ti} with ti ∈ R+. Equivalently, a given temporal point process can be represented as a counting process, N(t), which records the number of events before time t. An important way to characterize temporal point processes is via the conditional intensity function λ(t), a stochastic model for the time of the next event given all the previous events. Formally, λ(t)dt is the conditional probability of observing an event in a small window [t, t+dt) given the historyH(t) up to t and that the event has not happen before t, i.e.,
λ(t)dt := P {event in [t, t+ dt)|H(t)} = E[dN(t)|H(t)], where one typically assumes that only one event can happen in a small window of size dt, i.e., dN(t) ∈ {0, 1}. Then, given a time t > 0, we can also characterize the conditional probability that no event happens during [0, t) as: S(t) = exp ( − ∫ t 0 λ(τ) dτ ) and the conditional density that an event occurs at time t is defined as f(t) = λ(t)S(t) (1)
The function form of the intensity λ(t) is often designed to capture the phenomena of interests. Some commonly used form includes:
• Hawkes processes (Hawkes, 1971; Wang et al., 2016c), whose intensity models the mutual excitation between events, i.e., λ(t) = µ + α ∑ ti∈H(t) κω(t − ti), where κω(t) := exp(−ωt)
is an exponential triggering kernel, µ > 0 is a baseline intensity. Here, the occurrence of each historical event increases the intensity by a certain amount determined by the kernel κω and the weight α > 0, making the intensity history dependent and a stochastic process by itself. • Rayleigh process, whose intensity function is λ(t) = αt, where α > 0 is the weight parameter.

4 RECURRENT COEVOLUTIONARY FEATURE EMBEDDING PROCESSES
In this section, we present the generative framework for modeling the temporal dynamics of user-item interactions. We first use RNN to explicitly capture the co-evolving nature of users’ and items’ latent feature. Then, based on the compatibility between the users’ and items’ latent feature, we model the user-item interactions by a multi-dimensional temporal point process. We further parametrize the intensity function by the compatibility between users’ and items’ latent features.

4.1 EVENT REPRESENTATION
Given m users and n items, we denote the ordered list of N observed events as O = {ej = (uj , ij , tj , qj)}Nj=1 on time window [0, T ], where uj ∈ {1, . . . ,m}, ij ∈ {1, . . . , n}, tj ∈ R+, 0 6 t1 6 t2 . . . 6 T . This represents the interaction between user uj , item ij at time tj , with the interaction context qj ∈ Rd. Here qj can be a high dimension vector such as the text review, or
simply the embedding of static user/item features such as user’s profile and item’s categorical features. For notation simplicity, we defineOu = {euj = (iuj , tuj , quj )} as the ordered listed of all events related to user u, and Oi = {eij = (uij , tij , qij)} as the ordered list of all events related to item i. We also set ti0 = t u 0 = 0 for all the users and items. tk− denotes the time point just before time tk.

4.2 RECURRENT FEATURE EMBEDDING PROCESSES
We associate feature embeddings uu(t) ∈ Rk with each user u and ii(t) ∈ Rk with each item i. These features represent the subtle properties which cannot be directly observed, such as the interests of a user and the semantic topics of an item. Specifically, we model the drift, evolution, and co-evolution of uu(t) and ii(t) as a piecewise constant function of time that has jumps only at event times. Specifically, we define:
User latent feature embedding process. For each user u, the corresponding embedding after user u’s k-th event euk = (i u k , t u k , q u k ) can be formulated as:
uu(t u k) = σ ( W1(t
u k − tuk−1)︸ ︷︷ ︸
temporal drift
+W2uu(t u k−1)︸ ︷︷ ︸
self evolution
+ W3iik(t u k−)︸ ︷︷ ︸
co-evolution: item feature
+ W4q u,ik k︸ ︷︷ ︸
interaction feature
) (2)
Item latent feature embedding process. For each item i, we specify ii(t) at time tik as:
ii(t i k) = σ ( V1(t
i k − tik−1)︸ ︷︷ ︸
temporal drift
+V2ii(t i k−1)︸ ︷︷ ︸
self evolution
+ V3uuk(t i k−)︸ ︷︷ ︸
co-evolution: item feature
+ V4q i,uk k︸ ︷︷ ︸
interaction feature
) (3)
where t− means the time point just before time t, W4,V4 ∈ Rk×d are the embedding matrices mapping from the explicit high-dimensional feature space into the low-rank latent feature space and W1,V1 ∈ Rk, W2,V2,W3,V3 ∈ Rk×k are weights parameters. σ(·) is the nonlinear activation function, such as commonly used Tanh or Sigmoid for RNN. For simplicity, we use basic recurrent neural network to formulate the recurrence, but it is also straightforward to extend it using GRU or LSTM to gain more expressive power. Figure 1 summarizes the basic setting of our model.
Here both the user and item’s feature embedding processes are piecewise constant functions of time and only updated if an interaction event happens. A user’s attribute changes only when he has a new interaction with some item. For example, a user’s taste for music changes only when he listens to some new or old musics. Also, an item’s attribute changes only when some user interacts with it. Different from Chen et al. (2013) who also models the time change with piecewise constant function, but their work has no coevolve modeling, and is not capable of predicting the future time point.
Next we discuss the rationale of each term in detail:
• Temporal drift. The first term is defined based on the time difference between consecutive events of specific user or item. It allows the basic features of users (e.g., a user’s self-crafted interests) and items (e.g., textual categories and descriptions) to smoothly drift through time. Such changes of basic features normally are caused by external influences. • Self evolution. The current user feature should also be influenced by its feature at the earlier time. This captures the intrinsic evolution of user/item features. For example, a user’s current taste should be more or less similar to his/her tastes two days ago. • User-item coevolution. Users’ and items’ latent features can mutually influence each other. This term captures the two parallel processes. First, a user’s embedding is determined by the latent features of the items he interacted with. At each time tk, the latent item feature is iik(t u k−).
We capture both the temporal influence and feature of each history item as a latent embedding. Conversely, an item’s embedding is determined by the feature embedding of the user who just interacts with the item. • Evolution with interaction features. Users’ and items’ features can evolve and be influenced by the characteristics of their interactions. For instance, the genre changes of movies indicate the changing tastes of users. The theme of a chatting-group can be easily shifted to certain topics of the involved discussions. In consequence, this term captures the influence of the current interaction features to the changes of the latent user (item) features. • Interaction feature. This is the additional information happened in the user-item interactions. For example, in online discussion forums such as Reddit, the interaction features are the posts and comments. In online review sites such as Yelp, it is the reviews of the businesses.
To summarize, each feature embedding process evolves according to the respective base temporal user (item) features and also are mutually dependent on each other due to the endogenous influences from the interaction features and the entangled latent features.

4.3 USER-ITEM INTERACTIONS AS TEMPORAL POINT PROCESSES
For each user, we model the recurrent occurrences of all users interaction with all items as a multidimensional temporal point process, with each user-item pair as one dimension. In particular, the intensity function in the (u, i)-th dimension (user u and item i) is modeled as a Rayleigh process:
λu,i(t|t′) = exp ( uu(t ′)>ii(t ′) )︸ ︷︷ ︸
user-item compatibility
· (t− t′)︸ ︷︷ ︸ time lapse
(4)
where t > t′, and t′ is the last time point where either user u’s embedding or item i’s embedding changes before time t. The rationale behind this formulation is three-fold:
• Time as a random variable. Instead of discretizing the time into epochs as traditional methods (Charlin et al., 2015; Preeti Bhargava, 2015; Gopalan et al., 2015; Hidasi & Tikk, 2015; Wang et al., 2016a), we explicitly model the timing of each interaction event as a random variable, which naturally captures the heterogeneity of the temporal interactions between users and items. • Short term preference. The probability for user u to interact with item i depends on the compatibility of their instantaneous embeddings, which is evaluated through the inner product at the last event time t′. Because uu(t) and ii(t) co-evolve through time, their inner-product measures a general representation of the cumulative influence from the past interactions to the occurrence of the current event. The exp(·) function ensures the intensity is positive and well defined. • Rayleigh time distribution. The user and item embeddings are piecewise constant, and we use the time lapse term to make the intensity piecewise linear. This form leads to a Rayleigh distribution for the time intervals between consecutive events in each dimension. It is well-adapted to modeling fads, where the event-happening likelihood f(·) in (1) rises to a peak and then drops extremely rapidly. Furthermore, it is computationally easy to obtain an analytic form of f(·). One can then use f(·) to make item recommendation by finding the dimension that f(·) reaches the peak.
With the parameterized intensity function, we can further estimate the parameters using maximum likelihood estimation of all events. The joint negative log-likelihood is (Daley & Vere-Jones, 2007):
` = − N∑ j=1 log ( λuj ,ij (tj |t′j) ) ︸ ︷︷ ︸
intensity of interaction event
+ m∑ u=1 n∑ i=1 ∫ T 0
λu,i(τ |τ ′) dτ︸ ︷︷ ︸ survival probability of event not happened
(5)
The rationale of the objective two-fold: (i) the negative intensity summation term ensures the probability of all interaction events is maximized; (ii) the second survival probability term penalizes the non-presence of an interaction between all possible user-item pairs on the observation window. Hence, our framework not only explains why an event happens, but also why an event did not happen.

5 PARAMETER LEARNING
In this section, we propose an efficient algorithm to learn the parameters {Vi}4i=1 and {Wi} 4 i=1. The batch objective function is presented in (5). The Back Propagation Through Time (BPTT) is the standard way to train a RNN. To make the back propagation tractable, one typically needs to do truncation during training. However, due to the novel co-evolutionary nature of our model, all the events are related to each other by the user-item bipartite graph (Figure 2), which makes it hard to decompose.
Hence, in sharp contrast to works (Hidasi et al., 2016; Du et al., 2016) in sequential data where one can easily break the sequences into multiple segments to make the BPTT trackable, it is a challenging task to design BPTT in our case. To efficiently solve this problem, we first order all the events globally and then do mini-batch training in a sliding window fashion. Each time when conducting feed forward and back propagation, we take the consecutive events within current sliding window to build the computational graph. Thus in our case the truncation is on the global timeline, instead over individual independent sequences as in prior works.
Next, we explain our procedure in detail. Given a mini-batch of M ordered events Õ = {ej}Mj=1, we set the time span to be [T0 = t1, T = tM ]. Below we show how to compute the intensity and survival probability term in the objective function (5) respectively.
embedding is shown. (b) Survival probability for a user-item pair (u, i). The integral
∫ T
0 λu,i(τ |τ ′)dτ is decomposed into 4 inter-event intervals separated by {t0, · · · , t3}, with close form on each interval.
Computing the intensity function. Each time when a new event ej happens between uj and ij , their corresponding feature embeddings will evolve according to a computational graph, as illustrated in Figure 2a. Due to the change of feature embedding, all the dimensions related to uj or ij will be influenced and the intensity function for that dimension will change consequently. Such crossdimension influence dependency is shown in Figure 2b. In our implementation, we first compute the corresponding intensity λuj ,ij (tj |t′j) according to (4), and then update the embedding of uj and ij . This operation takes O(M) complexity, and is independent to the number of users or items.
Computing the survival function. To compute the survival probability − ∫ T T0 λu,i(τ |τ ′)dτ for each pair (u, i), we first collect all the time stamps {tk} that have events related to either u or i. For notation simplicity, let |{tk}| = nu,i and t1 = T0, tnu,i = T . Since the embeddings are piecewise constant, the corresponding intensity function is piecewise linear, according to (4). Thus, the integration is decomposed into each time interval where the intensity is constant, i.e.,∫ T
T0 λu,i(τ |τ ′)dτ = nu,i−1∑ k=1 ∫ tk+1 tk λu,i(τ |τ ′)dτ = nu,i−1∑ k=1 (t2k+1 − t2k) exp ( uu(tk) >ii(tk) ) (6)
Figure 3 visualizes the computation. Although the survival probability term exists in close form, we still need to solve two challenges. First, it is still expensive to compute it for each user item pair. Moreover, since the user-item interaction bipartite graph is very sparse, it is not necessary to monitor each dimension in the stochastic training setting. To speed up the computation, we propose a novel random-sampling scheme as follows.
Note that the intensity term in the objective function (5) tries to maximize the inner product between user and item that has interaction event, while the survival term penalize over all other pairs of inner
products. We observe that this is similar to Softmax computing for classification problem. Hence, inspired by the noise-contrastive estimation method (Gutmann & Hyvärinen, 2012) that is widely used in language models (Mnih & Kavukcuoglu, 2013), we keep the dimensions that have events on them, while randomly sample dimensions without events in current mini-batch.
The second challenge lies in the fact that the user-item interactions vary a lot across mini-batches, hence the corresponding computational graph also changes greatly. To make the learning efficient, we use the graph embedding framework (Dai et al., 2016) which allows training deep learning models where each term in the objective has a different computational graphs but with shared parameters. The Adam Optimizer (Kingma & Ba, 2014) together with gradient clip is used in our experiment.

6 EXPERIMENTS
We evaluate our model on real-world datasets. For each sequence of user activities, we use all the events up to time T · p as the training data, and the rest events as the testing data, where T is the observation window. We tune the latent rank of other baselines using 5-fold cross validation with grid search. We vary the proportion p ∈ {0.7, 0.72, 0.74, 0.76, 0.78} and report the averaged results over five runs on two tasks (we will release code and data once published):
• Item prediction. At each test time t, we predict the item that the user u will interact with. We rank all the items in the descending order of the conditional density fu,i(t) = λu,i(t)Su,i(t). We report the Mean Average Rank (MAR) of each test item at the test time. Ideally, the item associated with the test time t should rank one, hence smaller value indicates better predictive performance. • Time prediction. We predict the expected time when a testing event will occur between a given user-item pair. Using Rayleigh distribution, it is given by Et∼fu,i(t)(t) = √ π
2 exp(uu(t−)>ii(t−)) .
We report the Mean Absolute Error (MAE) between the predicted and true time.

6.1 COMPETITORS
We compared our DEEPCOEVOLVE with the following methods. Table 1 summarizes the differences.
• LowRankHawkes (Du et al., 2015): This is a low rank Hawkes process model which assumes user-item interactions to be independent of each other and does not capture the co-evolution of user and item features. • Coevolving (Wang et al., 2016b): This is a multi-dimensional point process model which uses a simple linear embedding to model the co-evolution of user and item features. • PoissonTensor (Chi & Kolda, 2012): Poisson Tensor Factorization has been shown to perform better than factorization methods based on squared loss (Karatzoglou et al., 2010; Xiong et al., 2010; Wang et al., 2015b) on recommendation tasks. The performance for this baseline is reported using the average of the parameters fitted over all time intervals. • TimeSVD++ (Koren, 2009) and FIP (Yang et al., 2011): These two methods are only designed for explicit ratings, the implicit user feedbacks (in the form of a series of interaction events) are converted into the explicit ratings by the respective frequency of interactions with users. • STIC (Kapoor et al., 2015): it fits a semi-hidden markov model (HMM) to each observed user-item pair and is only designed for time prediction.

6.2 DATASETS
We use three real world datasets as follows.
• IPTV. It contains 7,100 users’ watching history of 385 TV programs in 11 months (Jan 1 - Nov 30 2012), with around 2M events, and 1,420 movie features (including 1,073 actors, 312 directors, 22 genres, 8 countries and 5 years). • Yelp. This data was available in Yelp Dataset challenge Round 7. It contains reviews for various businesses from October, 2004 to December, 2015. The dataset we used here contains 1,005 users and 47,924 businesses, with totally 291,716 reviews.
• Reddit. We collected discussion related data on different subreddits (groups) for the month of January 2014. We filtered all bot users’ and their posts from this dataset. Furthermore, we randomly selected 1,000 users, 1,403 groups, and 10,000 discussion events.

6.3 PREDICTION RESULTS
Figure 4 shows that DEEPCOEVOLVE significantly outperforms both epoch-based baselines and state-of-arts point process based methods. LOWRANKHAWKES has good performance on item prediction but not on time prediction, while COEVOLVING has good performance on time prediction but not on item prediction. We discuss the performance regarding the two metrics below.
Item prediction. Note that the best possible MAR one can achieve is 1, and our method gets quite accurate results: with the value of 1.7 on IPTV and 1.9 on Reddit. Note LOWRANKHAWKES achieves comparable item prediction performance, but not as good on the time prediction task. We think the reason is as follows. Since one only need the rank of conditional density f(·) in (1) to conduct item prediction, LOWRANKHAWKES may still be good at differentiating the conditional density function, but could not learn its actual value accurately, as shown in the time prediction task where the value of the conditional density function is needed for precise prediction.
Time prediction. The second row of Figure 4 shows that DEEPCOEVOLVE outperforms other methods. Compared with LOWRANKHAWKES that achieves comparable time predication performance, 6× improvement on Reddit, it has 10× improvement on Yelp, and 30× improvement on IPTV. The time unit is hour. Hence it has 2 weeks accuracy improvement on IPTV and 2 days on Reddit. This is important for online merchants to make time sensitive recommendations. An intuitive explanation is that our method accurately captures the nonlinear pattern between user and item interactions. The competitor LOWRANKHAWKES assumes specific parametric forms of the user-item interaction process, hence may not be accurate or expressive enough to capture real world temporal patterns. Furthermore, it models each user-item interaction dimension independently, which may lose the important affection from user’s interaction with other items while predicting the current item’s reoccurrence time. Our work also outperforms COEVOLVING, e.g., with around 3×MAE improve on IPTV. Moreover, the item prediction performance is also much better than COEVOLVING. It shows the importance of using RNN to capture the nonlinear embedding of user and item latent features, instead of the simple parametrized linear embedding in COEVOLVING.

6.4 INSIGHT OF RESULTS
We will look deeper and provide rationale behind the prediction results in the following two subsections. First, to understand the difficulty of conducting prediction tasks in each dataset, we study their different sparsity properties. For the multidimensional point process models, the fewer events we observe in each dimension, the more sparse the dataset is. Our approach alleviates the sparsity problem via the modeling of dependencies among dimensions, thus is consistently doing better than other baseline algorithms.
Next, we fix one dataset and evaluate how different levels of sparsity in training data influences each algorithm’s performance.

6.4.1 UNDERSTANDING THE DATASETS
We visualize the three datasets in Figure 5 according to (i) the number of events per user, and (ii) the user-item interaction graph.
Sparsity in terms of the number of events per user. Typically, the more user history data we have, the better results we will obtain in the prediction tasks. We can see in IPTV dataset, users typically have longer length of history than the users in Reddit and Yelp datasets. Thus our algorithm and all other baseline methods have their best performance on this dataset. However, the Reddit dataset and Yelp dataset are hard to tell the performance based only on the distribution of history length, thus we do a more detailed visualization.
Sparsity in terms of diversity of items to recommend. From the bipartite graph, it is easy to see that Yelp dataset has higher density than the other two datasets. The density of the interaction graph reflects the variety of history per each user. For example, the users in IPTV only has 385 programs to watch, but they can have 47,924 businesses to choose in Yelp dataset. Also, the Yelp dataset has 9 times more items than IPTV and Reddit dataset in the bipartite graph. This means the users in Yelp dataset has more diverse tastes than users in other two datasets. This is because if users has similar tastes, the distinct number of items in the union of their history should be small.
Based on the above two facts, we can see Yelp dataset is the most sparse, since it has shorter length of history per user, and much more diversity of the items, it is not surprising that this dataset is much harder than the other IPTV and Reddit dataset.

6.4.2 ROBUSTNESS OF THE ALGORITHM
With the case study on the most challenging Yelp dataset, we further evaluate how each algorithm performs with lower level of sparsity as compared to the one used in Figure 4 (c).We use this to demonstrate that our work is most robust and performs well across different levels of sparsity.
We first create Yelp100, a more dense dataset, by filtering the original Yelp dataset to keep the top 100 users. Each user would have at least 200 events. Figure 6 (a) shows the statistics of this dataset. On average the users have more history events than the original Yelp dataset in Figure 5(c).
On this dense dataset, Figure 6 (b) and (c) show that all the algorithms’ performances improve with more history events, comparing to the performance in original Yelp dataset. For example, LOWRANKHAWKES has similar rank prediction results as our DEEPCOEVOLVE on this dense dataset. However, as the dataset becomes sparse, the performance of LOWRANKHAWKES drops significantly, as shown in Figure 4(c). For example, the rank prediction error goes from 90 to 2128, and the
time error goes from 724 to 11043.5. We think it is because this model relies more on the history information per each user-item pair.
On the contrary, our DEEPCOEVOLVE still has superior performance with such high level of sparsity. The rank error only changes from 87 to 107, and the time error changes from 72 to 884 as the data becomes sparse. It shows that our work is the most robust to the sparsity in the data. We think it is because our work accurately captures the nonlinear multidimensional dependencies between users and items latent features.

7 CONCLUSION
We have proposed an efficient framework to model the nonlinear co-evolution nature of users’ and items’ latent features. Moreover, the user and item’s evolving and co-evolving processes are captured by the RNN. It is based on temporal point processes and models time as a random variable. Hence it is in sharp contrast to prior epoch based works. We demonstrate the superior performance of our method on both the time and item prediction task, which is not possible by most prior work. Future work includes extending to other social applications, such as group dynamics in message services.

A DETAILS ON GRADIENT COMPUTATION
Computing gradient. For illustration purpose, we here use Sigmoid as the nonlinear activation function σ. In order to get gradient with respect to parameter W ’s, we first compute gradients with respect to each varying points of embeddings. For user u’s embedding after his k-th event, the corresponding partial derivatives are computed by:
∂`
∂uu(tuk) = −iiuk︸︷︷︸
from intensity
+ n∑ i=1
∂ ∫ tuk+1 tuk
λu,i(τ |τ ′)dτ ∂uu(tuk)︸ ︷︷ ︸
from survival
+ ∂`
∂uu(tuk+1) (1− uu(tuk+1)) uu(tuk+1)W2︸ ︷︷ ︸ from user u’s next embedding
+ ∂`
∂iiuk+1(t u k+1)
(1− iiuk+1(t u k+1)) iiuk+1(t u k+1)︸ ︷︷ ︸
from user u’s next item embedding
where denotes element-wise multiplication. The gradient coming from the second term (i.e., the survival term) is also easy to compute, since the Rayleigh distribution has closed form of survival function. For a certain item i, if its feature doesn’t changed between time interval [tuk , t u k+1], then we have
∂ ∫ tuk+1 tuk
λu,i(τ |τ ′)dτ ∂uu(tuk) = (tuk+1 − tuk)2 2 exp
( uu(t u k) >ii(t u k)ii(t u k) )
(7)
On the other hand, if the embedding of item i changes during this time interval, then we should break this interval into segments and compute the summation of gradients in each segment in a way similar to (7). Thus, we are able to compute the gradients with respect to Wi, i ∈ {1, 2, 3, 4} as follows.
∂`
∂W1 = m∑ u=1 ∑ k
∂`
∂uu(tuk) (i− uu(tuk)) uu(tuk)(tuk − tuk−1)
∂`
∂W2 = m∑ u=1 ∑ k ( ∂` ∂uu(tuk) (i− uu(tuk)) uu(tuk) ) uu(t u k−1) >
∂`
∂W3 = m∑ u=1 ∑ k ( ∂` ∂uu(tuk) (i− uu(tuk)) uu(tuk) ) iik(t u k−)>
∂`
∂W4 = m∑ u=1 ∑ k ( ∂` ∂uu(tuk) (i− uu(tuk)) uu(tuk) ) qu,ikk
Since the items are treated symmetrically as users, the corresponding derivatives can be obtained in a similar way.
","Recommender systems often use latent features to explain the behaviors of users and capture the properties of items. As users interact with different items over time, user and item features can influence each other, evolve and co-evolve over time. To accurately capture the fine grained nonlinear coevolution of these features, we propose a recurrent coevolutionary feature embedding process model, which combines recurrent neural network (RNN) with a multi-dimensional point process model. The RNN learns a nonlinear representation of user and item embeddings which take into account mutual influence between user and item features, and the feature evolution over time. We also develop an efficient stochastic gradient algorithm for learning parameters. Experiments on diverse real-world datasets demonstrate significant improvements in user behavior prediction compared to state-of-the-arts.",ICLR 2017 conference submission,False,,"The paper introduces a time dependent recommender system based on point processes parametrized by time dependent user and item latent representations. The later are modeled as coupled – autoregressive processes – i.e. the representation of a user/item changes when he interacts with an item/user, and is a function of both the user and the item representations before time t. This is called coevolution here and the autoregressive process is called recurrent NN. The model may also incorporate heterogeneous inputs. Experiments are performed on several datasets, and the model is compared with different baselines.
There are several contributions in the paper: 1) modeling recommendation via parametrized point processes where the parameter dynamics are modeled by latent user/item representations, 2) an optimization algorithm for maximizing the likelihood of this process, with different technical tricks that seem to break its intrinsic complexity, 3) evaluation experiments for time dependent recommendation.
The paper by the same authors (NIPS 2016) describes a similar model of continuous time coevolution, and a similar evaluation. The difference lies in the details of the model: the point process model is not the same and of the latent factor dynamic model is slightly different, but the modeling approach and the arguments are exactly the same. By the end, one does not know what makes this model perform better than the one proposed in NIPS, is it the choice for the process, the new parametrization? Both are quite similar. There is no justification on the choice of the specific form of the point process in the two papers. Did the authors tried other forms as well? The same remark applies for the form of the dynamical process: the non-linearity used for the modeling of the latent user/item vectors here is limited to a sigmoid function, which probably does not change much w.r.t. a linear model, but there is no evidence of the role of this non linearity in the paper. Note that there are some inconsistencies between the results in the two papers.
Concerning the evaluation, the authors introduce two criteria. I did not get exactly how they evaluate the item recommendation: it is mentioned that at each time t, the model predicts the item the user will interact with. Do you mean, the next item the user will interact with after time t? For the time prediction, why is it a relevant metric for recommendation?
A comparison of the complexity, or execution time of the different methods would be helpful. The complexity of your method is apparently proportional to #items*#users, what are the complexity limits of your methods.
Overall, the paper is quite nice and looks technically sound, albeit many details are missing. On the other hand, I have a mixed feeling because of the similarity with NIPS paper. The authors should have make a better work at convincing us that this is not a marginal extension of previous work by the authors.  I was not convinced either by the evaluation criteria and there is no evidence that the model can be used for large datasets.

---

A nice paper, with sufficient experimental validation, and the idea of incorporating a form of change point detection is good. However, the technical contribution relative to the NIPS paper by the same authors is not significant, in that it primarily involves using an RNN instead of a Hawkes process to model the temporal dynamics. The results are significantly better than this earlier paper -- the authors should explore if this is due only to the RNN, or to the optimization method.

---

Dear reviewers, we have revised our paper according to your insightful suggestions and comments.

1) We highlight the importance and difficulty of modeling the nonlinearity in the point process models in the introduction part. 

2) We added the discussion with Chen et.al ICML 2013 and a detailed comparison with Wang et.al NIPS 2016.
 
3) We added the experiment on the large Yelp dataset, which contains 1,005 users and 47,924 items. We run our algorithm and all the baselines on this one. The result of our algorithm is consistently better than alternatives. 

4) We added the section 6.4.1 explaining the quantitative results we get in the experiment. Specifically, we studied the performance of different history length of users, and how the diversity of tastes of users affect the results by visualizing the user-item interaction graph. 

5) We also added the section 6.4.2 that quantitatively compare the effect of different history length on Yelp dataset.

---

The paper introduces a time dependent recommender system based on point processes parametrized by time dependent user and item latent representations. The later are modeled as coupled – autoregressive processes – i.e. the representation of a user/item changes when he interacts with an item/user, and is a function of both the user and the item representations before time t. This is called coevolution here and the autoregressive process is called recurrent NN. The model may also incorporate heterogeneous inputs. Experiments are performed on several datasets, and the model is compared with different baselines.
There are several contributions in the paper: 1) modeling recommendation via parametrized point processes where the parameter dynamics are modeled by latent user/item representations, 2) an optimization algorithm for maximizing the likelihood of this process, with different technical tricks that seem to break its intrinsic complexity, 3) evaluation experiments for time dependent recommendation.
The paper by the same authors (NIPS 2016) describes a similar model of continuous time coevolution, and a similar evaluation. The difference lies in the details of the model: the point process model is not the same and of the latent factor dynamic model is slightly different, but the modeling approach and the arguments are exactly the same. By the end, one does not know what makes this model perform better than the one proposed in NIPS, is it the choice for the process, the new parametrization? Both are quite similar. There is no justification on the choice of the specific form of the point process in the two papers. Did the authors tried other forms as well? The same remark applies for the form of the dynamical process: the non-linearity used for the modeling of the latent user/item vectors here is limited to a sigmoid function, which probably does not change much w.r.t. a linear model, but there is no evidence of the role of this non linearity in the paper. Note that there are some inconsistencies between the results in the two papers.
Concerning the evaluation, the authors introduce two criteria. I did not get exactly how they evaluate the item recommendation: it is mentioned that at each time t, the model predicts the item the user will interact with. Do you mean, the next item the user will interact with after time t? For the time prediction, why is it a relevant metric for recommendation?
A comparison of the complexity, or execution time of the different methods would be helpful. The complexity of your method is apparently proportional to #items*#users, what are the complexity limits of your methods.
Overall, the paper is quite nice and looks technically sound, albeit many details are missing. On the other hand, I have a mixed feeling because of the similarity with NIPS paper. The authors should have make a better work at convincing us that this is not a marginal extension of previous work by the authors.  I was not convinced either by the evaluation criteria and there is no evidence that the model can be used for large datasets.

---

This paper proposes a method to model time changing dynamics in collaborative filtering.
Comments:
1) The main idea of the paper is build upon similar to a previous work by the same group of author (Wang et.al KDD), the major difference appears to be change some of the latent factors to be RNN
2) The author describes a BPTT technique to train the model  
3) The author introduced time prediction as a new metric to evaluate the effectiveness of time dependent model. However, this need to be condition on a given user-item pair.
4) It would be interesting to consider other metrics, for example
- The switching time where a user changes his/her to another item 
- Jointly predict the next item and switching time. 
In summary, this is a paper that improves over an existing work on time dynamics model in recommender system. The time prediction metric is interesting and opens up interesting discussion on how we should evaluate recommender systems when time is involved (see also comments).

---

The paper seeks to predict user events (interactions with items at a particular point in time). Roughly speaking the contributions are as follows:
(a) the paper models the co-evolutionary process of users' preferences toward items
(b) the paper is able to incorporate external sources of information, such as user and item features
(c) the process proposed is generative, so is able to estimate specific time-points at which events occur
(d) the model is able to account for non-linearities in the above

Following the pre-review questions, I understand that it is the combination of (a) and (c) that is the most novel aspect of the paper. A fully generative process which can be sampled is certainly nice (though of course, non-generative processes like regular old regression can estimate specific time points and such too, so not sure in practice how relevant this distinction is).

Other than that the above parts have all appeared in some combination in previous work, though the combination of parts here certainly passes the novelty bar.

I hadn't quite followed the issue mentioned in the pre-review discussion that the model requires multiple interactions per userXitem pair in order to fit the model (e.g. a user interacts with the same business multiple times). This is a slightly unusual setting compared to most temporal recommender systems work. I question to some extent whether this problem setting isn't a bit restrictive. That being said I take the point about why the authors had to subsample the Yelp data, but keeping only users with ""hundreds"" of events means that you're left with a very biased sample of the user base.

Other than the above issues, the paper is technically nice, and the experiments include strong baselines and reports good performance.

---

Nice work, but is seems to me that it was already published at DLRS 2016 in September (

---

The paper introduces a time dependent recommender system based on point processes parametrized by time dependent user and item latent representations. The later are modeled as coupled – autoregressive processes – i.e. the representation of a user/item changes when he interacts with an item/user, and is a function of both the user and the item representations before time t. This is called coevolution here and the autoregressive process is called recurrent NN. The model may also incorporate heterogeneous inputs. Experiments are performed on several datasets, and the model is compared with different baselines.
There are several contributions in the paper: 1) modeling recommendation via parametrized point processes where the parameter dynamics are modeled by latent user/item representations, 2) an optimization algorithm for maximizing the likelihood of this process, with different technical tricks that seem to break its intrinsic complexity, 3) evaluation experiments for time dependent recommendation.
The paper by the same authors (NIPS 2016) describes a similar model of continuous time coevolution, and a similar evaluation. The difference lies in the details of the model: the point process model is not the same and of the latent factor dynamic model is slightly different, but the modeling approach and the arguments are exactly the same. By the end, one does not know what makes this model perform better than the one proposed in NIPS, is it the choice for the process, the new parametrization? Both are quite similar. There is no justification on the choice of the specific form of the point process in the two papers. Did the authors tried other forms as well? The same remark applies for the form of the dynamical process: the non-linearity used for the modeling of the latent user/item vectors here is limited to a sigmoid function, which probably does not change much w.r.t. a linear model, but there is no evidence of the role of this non linearity in the paper. Note that there are some inconsistencies between the results in the two papers.
Concerning the evaluation, the authors introduce two criteria. I did not get exactly how they evaluate the item recommendation: it is mentioned that at each time t, the model predicts the item the user will interact with. Do you mean, the next item the user will interact with after time t? For the time prediction, why is it a relevant metric for recommendation?
A comparison of the complexity, or execution time of the different methods would be helpful. The complexity of your method is apparently proportional to #items*#users, what are the complexity limits of your methods.
Overall, the paper is quite nice and looks technically sound, albeit many details are missing. On the other hand, I have a mixed feeling because of the similarity with NIPS paper. The authors should have make a better work at convincing us that this is not a marginal extension of previous work by the authors.  I was not convinced either by the evaluation criteria and there is no evidence that the model can be used for large datasets.

---

A nice paper, with sufficient experimental validation, and the idea of incorporating a form of change point detection is good. However, the technical contribution relative to the NIPS paper by the same authors is not significant, in that it primarily involves using an RNN instead of a Hawkes process to model the temporal dynamics. The results are significantly better than this earlier paper -- the authors should explore if this is due only to the RNN, or to the optimization method.

---

Dear reviewers, we have revised our paper according to your insightful suggestions and comments.

1) We highlight the importance and difficulty of modeling the nonlinearity in the point process models in the introduction part. 

2) We added the discussion with Chen et.al ICML 2013 and a detailed comparison with Wang et.al NIPS 2016.
 
3) We added the experiment on the large Yelp dataset, which contains 1,005 users and 47,924 items. We run our algorithm and all the baselines on this one. The result of our algorithm is consistently better than alternatives. 

4) We added the section 6.4.1 explaining the quantitative results we get in the experiment. Specifically, we studied the performance of different history length of users, and how the diversity of tastes of users affect the results by visualizing the user-item interaction graph. 

5) We also added the section 6.4.2 that quantitatively compare the effect of different history length on Yelp dataset.

---

The paper introduces a time dependent recommender system based on point processes parametrized by time dependent user and item latent representations. The later are modeled as coupled – autoregressive processes – i.e. the representation of a user/item changes when he interacts with an item/user, and is a function of both the user and the item representations before time t. This is called coevolution here and the autoregressive process is called recurrent NN. The model may also incorporate heterogeneous inputs. Experiments are performed on several datasets, and the model is compared with different baselines.
There are several contributions in the paper: 1) modeling recommendation via parametrized point processes where the parameter dynamics are modeled by latent user/item representations, 2) an optimization algorithm for maximizing the likelihood of this process, with different technical tricks that seem to break its intrinsic complexity, 3) evaluation experiments for time dependent recommendation.
The paper by the same authors (NIPS 2016) describes a similar model of continuous time coevolution, and a similar evaluation. The difference lies in the details of the model: the point process model is not the same and of the latent factor dynamic model is slightly different, but the modeling approach and the arguments are exactly the same. By the end, one does not know what makes this model perform better than the one proposed in NIPS, is it the choice for the process, the new parametrization? Both are quite similar. There is no justification on the choice of the specific form of the point process in the two papers. Did the authors tried other forms as well? The same remark applies for the form of the dynamical process: the non-linearity used for the modeling of the latent user/item vectors here is limited to a sigmoid function, which probably does not change much w.r.t. a linear model, but there is no evidence of the role of this non linearity in the paper. Note that there are some inconsistencies between the results in the two papers.
Concerning the evaluation, the authors introduce two criteria. I did not get exactly how they evaluate the item recommendation: it is mentioned that at each time t, the model predicts the item the user will interact with. Do you mean, the next item the user will interact with after time t? For the time prediction, why is it a relevant metric for recommendation?
A comparison of the complexity, or execution time of the different methods would be helpful. The complexity of your method is apparently proportional to #items*#users, what are the complexity limits of your methods.
Overall, the paper is quite nice and looks technically sound, albeit many details are missing. On the other hand, I have a mixed feeling because of the similarity with NIPS paper. The authors should have make a better work at convincing us that this is not a marginal extension of previous work by the authors.  I was not convinced either by the evaluation criteria and there is no evidence that the model can be used for large datasets.

---

This paper proposes a method to model time changing dynamics in collaborative filtering.
Comments:
1) The main idea of the paper is build upon similar to a previous work by the same group of author (Wang et.al KDD), the major difference appears to be change some of the latent factors to be RNN
2) The author describes a BPTT technique to train the model  
3) The author introduced time prediction as a new metric to evaluate the effectiveness of time dependent model. However, this need to be condition on a given user-item pair.
4) It would be interesting to consider other metrics, for example
- The switching time where a user changes his/her to another item 
- Jointly predict the next item and switching time. 
In summary, this is a paper that improves over an existing work on time dynamics model in recommender system. The time prediction metric is interesting and opens up interesting discussion on how we should evaluate recommender systems when time is involved (see also comments).

---

The paper seeks to predict user events (interactions with items at a particular point in time). Roughly speaking the contributions are as follows:
(a) the paper models the co-evolutionary process of users' preferences toward items
(b) the paper is able to incorporate external sources of information, such as user and item features
(c) the process proposed is generative, so is able to estimate specific time-points at which events occur
(d) the model is able to account for non-linearities in the above

Following the pre-review questions, I understand that it is the combination of (a) and (c) that is the most novel aspect of the paper. A fully generative process which can be sampled is certainly nice (though of course, non-generative processes like regular old regression can estimate specific time points and such too, so not sure in practice how relevant this distinction is).

Other than that the above parts have all appeared in some combination in previous work, though the combination of parts here certainly passes the novelty bar.

I hadn't quite followed the issue mentioned in the pre-review discussion that the model requires multiple interactions per userXitem pair in order to fit the model (e.g. a user interacts with the same business multiple times). This is a slightly unusual setting compared to most temporal recommender systems work. I question to some extent whether this problem setting isn't a bit restrictive. That being said I take the point about why the authors had to subsample the Yelp data, but keeping only users with ""hundreds"" of events means that you're left with a very biased sample of the user base.

Other than the above issues, the paper is technically nice, and the experiments include strong baselines and reports good performance.

---

Nice work, but is seems to me that it was already published at DLRS 2016 in September (",,,,,,6.0,,,3.6666666666666665,,
621,"Authors: Cheng-Zhi Anna Huang, Tim Cooijmans, Aaron Courville, Douglas Eck
Source file: 621.pdf

ABSTRACT
Machine learning models of music typically break down the task of composition into a chronological process, composing a piece of music in a single pass from beginning to end. On the contrary, human composers write music in a nonlinear fashion, scribbling motifs here and there, often revisiting choices previously made. We explore the use of blocked Gibbs sampling as an analogue to the human approach, and introduce COCONET, a convolutional neural network in the NADE family of generative models (Uria et al., 2016). Despite ostensibly sampling from the same distribution as the NADE ancestral sampling procedure, we find that a blocked Gibbs approach significantly improves sample quality. We provide evidence that this is due to some conditional distributions being poorly modeled. Moreover, we show that even the cheap approximate blocked Gibbs procedure from Yao et al. (2014) yields better samples than ancestral sampling. We demonstrate the versatility of our method on unconditioned polyphonic music generation. Step 0 Step 1 Step 4 Step 16 Step 64 Ground Truth Figure 1: Ancestral inpainting of a corrupted Bach chorale by COCONET. Colors are used to distinguish the four voices. Grayscale heatmaps show predictions p(xi | xC). The pitch sampled in the current step is indicated by a rectangular outline. The original Bach chorale is shown in the bottom right. Step 0 shows the corrupted Bach chorale. Step 64 shows the result. ∗Work done while at Google Brain. †Work done while at Google Brain.
","Machine learning models of music typically break down the task of composition into a chronological process, composing a piece of music in a single pass from beginning to end. On the contrary, human composers write music in a nonlinear fashion, scribbling motifs here and there, often revisiting choices previously made. We explore the use of blocked Gibbs sampling as an analogue to the human approach, and introduce COCONET, a convolutional neural network in the NADE family of generative models (Uria et al., 2016). Despite ostensibly sampling from the same distribution as the NADE ancestral sampling procedure, we find that a blocked Gibbs approach significantly improves sample quality. We provide evidence that this is due to some conditional distributions being poorly modeled. Moreover, we show that even the cheap approximate blocked Gibbs procedure from Yao et al. (2014) yields better samples than ancestral sampling. We demonstrate the versatility of our method on unconditioned polyphonic music generation. Step 0 Step 1 Step 4 Step 16 Step 64 Ground Truth Figure 1: Ancestral inpainting of a corrupted Bach chorale by COCONET. Colors are used to distinguish the four voices. Grayscale heatmaps show predictions p(xi | xC). The pitch sampled in the current step is indicated by a rectangular outline. The original Bach chorale is shown in the bottom right. Step 0 shows the corrupted Bach chorale. Step 64 shows the result. ∗Work done while at Google Brain. †Work done while at Google Brain.",ICLR 2017 conference submission,False,,"This paper proposed COCONET, which is a neural autoregressive model with convolution, to do music composition task. This paper also proposed to use blocked Gibbs sampling instead of the ancestral sampling of the original NADE model to generate better pieces of music. The experimental results showed that the NLL of COCONET is better than the other baselines and the human evaluation task by Amazon’s Mechanical Turk illustrated that the model can generate compelling music.

In general, I think the paper is good. Using NADE based model with convolution operations on music generation tasks and using blocked Gibbs sampling contains some kind of novelty. However, the novelty of the paper is incremental, since the blocked Gibbs sampling for NADE model is already proposed by Yao et al., (2014) and the using NADE based model for music modeling has also been proposed by Boulanger-Lewandowski  et al., (2012).

---

This paper applies an existing idea (Yao's block Gibbs sampling of NADE) to a music model. There is also prior art for applying NADE to music. The main novel and interesting result is that block Gibbs sampling (an approximation) actually improves performance, highlighting problems with NADE.
 
 This work is borderline for inclusion. The paper is mainly an application of existing ideas. The implications of the interesting results could perhaps have been explored further for a paper at a meeting on learning representations.

---

Thank you all for your reviews!  

We share your concern regarding quantitative comparison to other works, and regarding the generality of our method.  We have looked into the datasets used in prior works, but found that their preprocessing severely degraded the musical structure. For example, the temporal granularity used in Boulanger-Lewandowski et al. is too coarse: in several pieces, discarding all notes that do not have their onsets on the eighth-note grid results in sparse notes with long stretches of silence between them. The pieces become unrecognizable, and (worse) non-musical.  Downsampling or blurring does not work in symbolic music like it does in images: music is more similar to language, and using a coarse grid is analogous to removing words from a sentence and asking a model to learn this new distribution of broken sentences. This turns it into a different task, and makes qualitative judgement of samples not very meaningful.

We also considered applying our method to image data as suggested. Unlike (symbolic) music which is discrete and intricately structured, the domain of images is smooth and forgiving of small errors. It is plausible that the NADE sampling approach generates fine images (indeed, that's what Yao et al. found), for reasons that don't carry over to our domain of interest, which is music.

The development of a larger-scale, more diverse and higher-quality MIDI music dataset is a major component of our ongoing project.  We nonetheless believe that the advances we show are of sufficient interest to justify publication at this time.

---

The paper tackles the task of music generation. They use an orderless NADE model for the task of ""fill in the notes"". Given a roll of T timesteps of pitches, they randomly mask out some pitches, and the model is trained to predict the missing notes. This follows how the orderless NADE model can be trained. During sampling, one normally follows an ancestral sampling procedure. For this, an ordering is defined over outputs, and one runs the model on the current input, samples one of the outputs according to the order, adds this output to the next input, and continues this procedure until all outputs have been sampled. The key point of the paper is that this is a bad sampling strategy. Instead, they suggest the strategy of Yao et al. 2014, which uses a blocked Gibbs sampling approach. The blocked Gibbs strategy instead masks N inputs randomly and independently, samples them, and repeats this procedure. The point of this strategy is the make sure the sampling chain mixes well, which will happen for large N. However, since the samples are independent, having a large N gives incoherent samples. Thus, the authors follow an annealed schedule for N, making it smaller over time, which will eventually reduce to ancestral sampling (giving global structure to the sample). They conduct a variety of experiments involving both normal metrics and human evaluations, and find that this blocked Gibbs sampling outperforms other sampling procedures.

This is a well written paper - great job.

My main problem with the paper is that having read Uria and Yao, I don't know how much I have learned from this work in the context of this being an ICLR submission. If this was submitted to some computational music / art conference, this paper would be a clear accept. However, for ICLR, I don't see enough novelty compared with previous works this builds upon. Orderless NADE is an established model. The blocked Gibbs sampling and annealing scheme are basically the exact same one used in Yao. Thus, the main novelty of this paper is its application to the music domain, and finding that Yao's method works better for sampling music. This is a good contribution, but more tailored to those working in the music domain. If the authors found that these results also hold for other domains like images (e.g. on CIFAR / tiny Imagenet) and text (e.g. document generation), then I would change my mind and accept this paper for ICLR. Even just trying musical domains other than Bach chorales would be useful. However, as it stands, the experiments are not convincing enough.

---

The paper presents a way to model the distribution of four-part Bach chorales using Convolutional Neural Networks. Furthermore it addresses the task of artificial music generation by sampling from the model using blocked Gibbs sampling and shows

The CNN model for the distribution seems very appropriate for the data at hand. Also the analysis of the proposed sampling schemes with the analogy between Gibbs sampling and human music composition are very interesting.
I am not too sure about the evaluation though. Since the reported likelihoods are not directly comparable to previous work, I have difficulties judging the quality of the quantitative results.  For the human evaluation I would like to see the data for the direct comparisons between the models. E.g. How did NADE vs. Bach perform. Also I find the question: ‘what piece of music do you prefer’ a stronger test than the question ‘what piece is more musical to you’ because I don’t really know what ‘musical’ means to the AMT workers.

Finally, while I think the Bach Chorales are interesting musical pieces that deserve to be subject of the analysis but I find it hard to judge how well this modelling approach will transfer to other types of music which might have a very different data distribution.

Nevertheless, in conclusion, I believe this is an exciting model for an interesting task that produces non-trivial musical data.

---

This paper proposed COCONET, which is a neural autoregressive model with convolution, to do music composition task. This paper also proposed to use blocked Gibbs sampling instead of the ancestral sampling of the original NADE model to generate better pieces of music. The experimental results showed that the NLL of COCONET is better than the other baselines and the human evaluation task by Amazon’s Mechanical Turk illustrated that the model can generate compelling music.

In general, I think the paper is good. Using NADE based model with convolution operations on music generation tasks and using blocked Gibbs sampling contains some kind of novelty. However, the novelty of the paper is incremental, since the blocked Gibbs sampling for NADE model is already proposed by Yao et al., (2014) and the using NADE based model for music modeling has also been proposed by Boulanger-Lewandowski  et al., (2012).

---

This paper proposed COCONET, which is a neural autoregressive model with convolution, to do music composition task. This paper also proposed to use blocked Gibbs sampling instead of the ancestral sampling of the original NADE model to generate better pieces of music. The experimental results showed that the NLL of COCONET is better than the other baselines and the human evaluation task by Amazon’s Mechanical Turk illustrated that the model can generate compelling music.

In general, I think the paper is good. Using NADE based model with convolution operations on music generation tasks and using blocked Gibbs sampling contains some kind of novelty. However, the novelty of the paper is incremental, since the blocked Gibbs sampling for NADE model is already proposed by Yao et al., (2014) and the using NADE based model for music modeling has also been proposed by Boulanger-Lewandowski  et al., (2012).

---

This paper applies an existing idea (Yao's block Gibbs sampling of NADE) to a music model. There is also prior art for applying NADE to music. The main novel and interesting result is that block Gibbs sampling (an approximation) actually improves performance, highlighting problems with NADE.
 
 This work is borderline for inclusion. The paper is mainly an application of existing ideas. The implications of the interesting results could perhaps have been explored further for a paper at a meeting on learning representations.

---

Thank you all for your reviews!  

We share your concern regarding quantitative comparison to other works, and regarding the generality of our method.  We have looked into the datasets used in prior works, but found that their preprocessing severely degraded the musical structure. For example, the temporal granularity used in Boulanger-Lewandowski et al. is too coarse: in several pieces, discarding all notes that do not have their onsets on the eighth-note grid results in sparse notes with long stretches of silence between them. The pieces become unrecognizable, and (worse) non-musical.  Downsampling or blurring does not work in symbolic music like it does in images: music is more similar to language, and using a coarse grid is analogous to removing words from a sentence and asking a model to learn this new distribution of broken sentences. This turns it into a different task, and makes qualitative judgement of samples not very meaningful.

We also considered applying our method to image data as suggested. Unlike (symbolic) music which is discrete and intricately structured, the domain of images is smooth and forgiving of small errors. It is plausible that the NADE sampling approach generates fine images (indeed, that's what Yao et al. found), for reasons that don't carry over to our domain of interest, which is music.

The development of a larger-scale, more diverse and higher-quality MIDI music dataset is a major component of our ongoing project.  We nonetheless believe that the advances we show are of sufficient interest to justify publication at this time.

---

The paper tackles the task of music generation. They use an orderless NADE model for the task of ""fill in the notes"". Given a roll of T timesteps of pitches, they randomly mask out some pitches, and the model is trained to predict the missing notes. This follows how the orderless NADE model can be trained. During sampling, one normally follows an ancestral sampling procedure. For this, an ordering is defined over outputs, and one runs the model on the current input, samples one of the outputs according to the order, adds this output to the next input, and continues this procedure until all outputs have been sampled. The key point of the paper is that this is a bad sampling strategy. Instead, they suggest the strategy of Yao et al. 2014, which uses a blocked Gibbs sampling approach. The blocked Gibbs strategy instead masks N inputs randomly and independently, samples them, and repeats this procedure. The point of this strategy is the make sure the sampling chain mixes well, which will happen for large N. However, since the samples are independent, having a large N gives incoherent samples. Thus, the authors follow an annealed schedule for N, making it smaller over time, which will eventually reduce to ancestral sampling (giving global structure to the sample). They conduct a variety of experiments involving both normal metrics and human evaluations, and find that this blocked Gibbs sampling outperforms other sampling procedures.

This is a well written paper - great job.

My main problem with the paper is that having read Uria and Yao, I don't know how much I have learned from this work in the context of this being an ICLR submission. If this was submitted to some computational music / art conference, this paper would be a clear accept. However, for ICLR, I don't see enough novelty compared with previous works this builds upon. Orderless NADE is an established model. The blocked Gibbs sampling and annealing scheme are basically the exact same one used in Yao. Thus, the main novelty of this paper is its application to the music domain, and finding that Yao's method works better for sampling music. This is a good contribution, but more tailored to those working in the music domain. If the authors found that these results also hold for other domains like images (e.g. on CIFAR / tiny Imagenet) and text (e.g. document generation), then I would change my mind and accept this paper for ICLR. Even just trying musical domains other than Bach chorales would be useful. However, as it stands, the experiments are not convincing enough.

---

The paper presents a way to model the distribution of four-part Bach chorales using Convolutional Neural Networks. Furthermore it addresses the task of artificial music generation by sampling from the model using blocked Gibbs sampling and shows

The CNN model for the distribution seems very appropriate for the data at hand. Also the analysis of the proposed sampling schemes with the analogy between Gibbs sampling and human music composition are very interesting.
I am not too sure about the evaluation though. Since the reported likelihoods are not directly comparable to previous work, I have difficulties judging the quality of the quantitative results.  For the human evaluation I would like to see the data for the direct comparisons between the models. E.g. How did NADE vs. Bach perform. Also I find the question: ‘what piece of music do you prefer’ a stronger test than the question ‘what piece is more musical to you’ because I don’t really know what ‘musical’ means to the AMT workers.

Finally, while I think the Bach Chorales are interesting musical pieces that deserve to be subject of the analysis but I find it hard to judge how well this modelling approach will transfer to other types of music which might have a very different data distribution.

Nevertheless, in conclusion, I believe this is an exciting model for an interesting task that produces non-trivial musical data.

---

This paper proposed COCONET, which is a neural autoregressive model with convolution, to do music composition task. This paper also proposed to use blocked Gibbs sampling instead of the ancestral sampling of the original NADE model to generate better pieces of music. The experimental results showed that the NLL of COCONET is better than the other baselines and the human evaluation task by Amazon’s Mechanical Turk illustrated that the model can generate compelling music.

In general, I think the paper is good. Using NADE based model with convolution operations on music generation tasks and using blocked Gibbs sampling contains some kind of novelty. However, the novelty of the paper is incremental, since the blocked Gibbs sampling for NADE model is already proposed by Yao et al., (2014) and the using NADE based model for music modeling has also been proposed by Boulanger-Lewandowski  et al., (2012).",,,,,,5.666666666666667,,,4.0,,
657,"Authors: Armand Joulin, Edouard Grave, Piotr Bojanowski, Matthijs Douze, Hervé Jégou
Source file: 657.pdf

ABSTRACT
We consider the problem of producing compact architectures for text classification, such that the full model fits in a limited amount of memory. After considering different solutions inspired by the hashing literature, we propose a method built upon product quantization to store word embeddings. While the original technique leads to a loss in accuracy, we adapt this method to circumvent quantization artefacts. Combined with simple approaches specifically adapted to text classification, our approach derived from fastText requires, at test time, only a fraction of the memory compared to the original FastText, without noticeably sacrificing quality in terms of classification accuracy. Our experiments carried out on several benchmarks show that our approach typically requires two orders of magnitude less memory than fastText while being only slightly inferior with respect to accuracy. As a result, it outperforms the state of the art by a good margin in terms of the compromise between memory usage and accuracy.

1 INTRODUCTION
Text classification is an important problem in Natural Language Processing (NLP). Real world usecases include spam filtering or e-mail categorization. It is a core component in more complex systems such as search and ranking. Recently, deep learning techniques based on neural networks have achieved state of the art results in various NLP applications. One of the main successes of deep learning is due to the effectiveness of recurrent networks for language modeling and their application to speech recognition and machine translation (Mikolov, 2012). However, in other cases including several text classification problems, it has been shown that deep networks do not convincingly beat the prior state of the art techniques (Wang & Manning, 2012; Joulin et al., 2016).
In spite of being (typically) orders of magnitude slower to train than traditional techniques based on n-grams, neural networks are often regarded as a promising alternative due to compact model sizes, in particular for character based models. This is important for applications that need to run on systems with limited memory such as smartphones.
This paper specifically addresses the compromise between classification accuracy and the model size. We extend our previous work implemented in the fastText library1. It is based on n-gram features, dimensionality reduction, and a fast approximation of the softmax classifier (Joulin et al., 2016). We show that a few key ingredients, namely feature pruning, quantization, hashing, and retraining, allow us to produce text classification models with tiny size, often less than 100kB when trained on several popular datasets, without noticeably sacrificing accuracy or speed.
We plan to publish the code and scripts required to reproduce our results as an extension of the fastText library, thereby providing strong reproducible baselines for text classifiers that optimize the compromise between the model size and accuracy. We hope that this will help the engineering community to improve existing applications by using more efficient models.
This paper is organized as follows. Section 2 introduces related work, Section 3 describes our text classification model and explains how we drastically reduce the model size. Section 4 shows the effectiveness of our approach in experiments on multiple text classification benchmarks.
1https://github.com/facebookresearch/fastText

2 RELATED WORK
Models for text classification. Text classification is a problem that has its roots in many applications such as web search, information retrieval and document classification (Deerwester et al., 1990; Pang & Lee, 2008). Linear classifiers often obtain state-of-the-art performance while being scalable (Agarwal et al., 2014; Joachims, 1998; Joulin et al., 2016; McCallum & Nigam, 1998). They are particularly interesting when associated with the right features (Wang & Manning, 2012). They usually require storing embeddings for words and n-grams, which makes them memory inefficient.
Compression of language models. Our work is related to compression of statistical language models. Classical approaches include feature pruning based on entropy (Stolcke, 2000) and quantization. Pruning aims to keep only the most important n-grams in the model, leaving out those with probability lower than a specified threshold. Further, the individual n-grams can be compressed by quantizing the probability value, and by storing the n-gram itself more efficiently than as a sequence of characters. Various strategies have been developed, for example using tree structures or hash functions, and are discussed in (Talbot & Brants, 2008).
Compression for similarity estimation and search. There is a large body of literature on how to compress a set of vectors into compact codes, such that the comparison of two codes approximates a target similarity in the original space. The typical use-case of these methods considers an indexed dataset of compressed vectors, and a query for which we want to find the nearest neighbors in the indexed set. One of the most popular is Locality-sensitive hashing (LSH) by Charikar (2002), which is a binarization technique based on random projections that approximates the cosine similarity between two vectors through a monotonous function of the Hamming distance between the two corresponding binary codes. In our paper, LSH refers to this binarization strategy2. Many subsequent works have improved this initial binarization technique, such as spectal hashing (Weiss et al., 2009), or Iterative Quantization (ITQ) (Gong & Lazebnik, 2011), which learns a rotation matrix minimizing the quantization loss of the binarization. We refer the reader to two recent surveys by Wang et al. (2014) and Wang et al. (2015) for an overview of the binary hashing literature.
Beyond these binarization strategies, more general quantization techniques derived from Jegou et al. (2011) offer better trade-offs between memory and the approximation of a distance estimator. The Product Quantization (PQ) method approximates the distances by calculating, in the compressed domain, the distance between their quantized approximations. This method is statistically guaranteed to preserve the Euclidean distance between the vectors within an error bound directly related to the quantization error. The original PQ has been concurrently improved by Ge et al. (2013) and Norouzi & Fleet (2013), who learn an orthogonal transform minimizing the overall quantization loss. In our paper, we will consider the Optimized Product Quantization (OPQ) variant (Ge et al., 2013).
Softmax approximation The aforementioned works approximate either the Euclidean distance or the cosine similarity (both being equivalent in the case of unit-norm vectors). However, in the context of fastText, we are specifically interested in approximating the maximum inner product involved in a softmax layer. Several approaches derived from LSH have been recently proposed to achieve this goal, such as Asymmetric LSH by Shrivastava & Li (2014), subsequently discussed by Neyshabur & Srebro (2015). In our work, since we are not constrained to purely binary codes, we resort a more traditional encoding by employing a magnitude/direction parametrization of our vectors. Therefore we only need to encode/compress an unitary d-dimensional vector, which fits the aforementioned LSH and PQ methods well.
Neural network compression models. Recently, several research efforts have been conducted to compress the parameters of architectures involved in computer vision, namely for state-of-theart Convolutional Neural Networks (CNNs) (Han et al., 2016; Lin et al., 2015). Some use vector quantization (Gong et al., 2014) while others binarize the network (Courbariaux et al., 2016). Denil et al. (2013) show that such classification models are easily compressed because they are overparametrized, which concurs with early observations by LeCun et al. (1990).
2In the literature, LSH refers to multiple distinct strategies related to the Johnson-Lindenstrauss lemma. For instance, LSH sometimes refers to a partitioning technique with random projections allowing for sublinear search via cell probes, see for instance the E2LSH variant of Datar et al. (2004).
Some of these works both aim at reducing the model size and the speed. In our case, since the fastText classifier on which our proposal is built upon is already very efficient, we are primilarly interested in reducing the size of the model while keeping a comparable classification efficiency.

3 PROPOSED APPROACH

3.1 TEXT CLASSIFICATION
In the context of text classification, linear classifiers (Joulin et al., 2016) remain competitive with more sophisticated, deeper models, and are much faster to train. On top of standard tricks commonly used in linear text classification (Agarwal et al., 2014; Wang & Manning, 2012; Weinberger et al., 2009), Joulin et al. (2016) use a low rank constraint to reduce the computation burden while sharing information between different classes. This is especially useful in the case of a large output space, where rare classes may have only a few training examples. In this paper, we focus on a similar model, that is, which minimizes the softmax loss ` over N documents:
N∑ n=1 `(yn, BAxn), (1)
where xn is a bag of one-hot vectors and yn the label of the n-th document. In the case of a large vocabulary and a large output space, the matrices A and B are big and can require gigabytes of memory. Below, we describe how we reduce this memory usage.

3.2 BOTTOM-UP PRODUCT QUANTIZATION
Product quantization is a popular method for compressed-domain approximate nearest neighbor search (Jegou et al., 2011). As a compression technique, it approximates a real-valued vector by finding the closest vector in a pre-defined structured set of centroids, referred to as a codebook. This codebook is not enumerated, since it is extremely large. Instead it is implicitly defined by its structure: a d-dimensional vector x ∈ Rd is approximated as
x̂ = k∑ i=1 qi(x), (2)
where the different subquantizers qi : x 7→ qi(x) are complementary in the sense that their respective centroids lie in distinct orthogonal subspaces, i.e., ∀i 6= j, ∀x, y, 〈qi(x)|qj(y)〉 = 0. In the original PQ, the subspaces are aligned with the natural axis, while OPQ learns a rotation, which amounts to alleviating this constraint and to not depend on the original coordinate system. Another way to see this is to consider that PQ splits a given vector x into k subvectors xi, i = 1 . . . k, each of dimension d/k: x = [x1 . . . xi . . . xk], and quantizes each sub-vector using a distinct k-means quantizer. Each subvector xi is thus mapped to the closest centroid amongst 2b centroids, where b is the number of bits required to store the quantization index of the subquantizer, typically b = 8. The reconstructed vector can take 2kb distinct reproduction values, and is stored in kb bits.
PQ estimates the inner product in the compressed domain as
x>y ≈ x̂>y = k∑
i=1
qi(x i)>yi. (3)
This is a straightforward extension of the square L2 distance estimation of Jegou et al. (2011). In practice, the vector estimate x̂ is trivially reconstructed from the codes, i.e., from the quantization indexes, by concatenating these centroids.
The two parameters involved in PQ, namely the number of subquantizers k and the number of bits b per quantization index, are typically set to k ∈ [2, d/2], and b = 8 to ensure byte-alignment.
Discussion. PQ offers several interesting properties in our context of text classification. Firstly, the training is very fast because the subquantizers have a small number of centroids, i.e., 256 centroids for b = 8. Secondly, at test time it allows the reconstruction of the vectors with almost no
computational and memory overhead. Thirdly, it has been successfully applied in computer vision, offering much better performance than binary codes, which makes it a natural candidate to compress relatively shallow models. As observed by Sánchez & Perronnin (2011), using PQ just before the last layer incurs a very limited loss in accuracy when combined with a support vector machine.
In the context of text classification, the norms of the vectors are widely spread, typically with a ratio of 1000 between the max and the min. Therefore kmeans performs poorly because it optimizes an absolute error objective, so it maps all low-norm vectors to 0. A simple solution is to separate the norm and the angle of the vectors and to quantize them separately. This allows a quantization with no loss of performance, yet requires an extra b bits per vector.
Bottom-up strategy: re-training. The first works aiming at compressing CNN models like the one proposed by (Gong et al., 2014) used the reconstruction from off-the-shelf PQ, i.e., without any re-training. However, as observed in Sablayrolles et al. (2016), when using quantization methods like PQ, it is better to re-train the layers occurring after the quantization, so that the network can re-adjust itself to the quantization. There is a strong argument arguing for this re-training strategy: the square magnitude of vectors is reduced, on average, by the average quantization error for any quantizer satisfying the Lloyd conditions; see Jegou et al. (2011) for details.
This suggests a bottom-up learning strategy where we first quantize the input matrix, then retrain and quantize the output matrix (the input matrix being frozen). Experiments in section 4 show that it is worth adopting this strategy.
Memory savings with PQ. In practice, the bottom-up PQ strategy offers a compression factor of 10 without any noticeable loss of performance. Without re-training, we notice a drop in accuracy between 0.1% and 0.5%, depending on the dataset and setting; see Section 4 and the appendix.

3.3 FURTHER TEXT SPECIFIC TRICKS
The memory usage strongly depends on the size of the vocabulary, which can be large in many text classification tasks. While it is clear that a large part of the vocabulary is useless or redundant, directly reducing the vocabulary to the most frequent words is not satisfactory: most of the frequent words, like “the” or “is” are not discriminative, in contrast to some rare words, e.g., in the context of tag prediction. In this section, we discuss a few heuristics to reduce the space taken by the dictionary. They lead to major memory reduction, in extreme cases by a factor 100. We experimentally show that this drastic reduction is complementary with the PQ compression method, meaning that the combination of both strategies reduces the model size by a factor up to ×1000 for some datasets.
Pruning the vocabulary. Discovering which word or n-gram must be kept to preserve the overall performance is a feature selection problem. While many approaches have been proposed to select groups of variables during training (Bach et al., 2012; Meier et al., 2008), we are interested in selecting a fixed subset of K words and ngrams from a pre-trained model. This can be achieved by selecting the K embeddings that preserve as much of the model as possible, which can be reduced to selecting the K words and ngrams associated with the highest norms.
While this approach offers major memory savings, it has one drawback occurring in some particular cases: some documents may not contained any of the K best features, leading to a significant drop in performance. It is thus important to keep the K best features under the condition that they cover the whole training set. More formally, the problem is to find a subset S in the feature set V that maximizes the sum of their norms ws under the constraint that all the documents in the training set D are covered:
max S⊆V ∑ s∈S ws s.t. |S| ≤ K, P1S ≥ 1D,
where P is a matrix such that Pds = 1 if the s-th feature is in the d-th document, and 0 otherwise. This problem is directly related to set covering problems that are NP-hard (Feige, 1998). Standard greedy approaches require the storing of an inverted index or to do multiple passes over the dataset, which is prohibitive on very large dataset (Chierichetti et al., 2010). This problem can be cast as an instance of online submodular maximization with a rank constraint (Badanidiyuru et al., 2014;
Bateni et al., 2010). In our case, we use a simple online parallelizable greedy approach: For each document, we verify if it is already covered by a retained feature and, if not, we add the feature with the highest norm to our set of retained features. If the number of features is below k, we add the features with the highest norm that have not yet been picked.
Hashing trick & Bloom filter. On small models, the dictionary can take a significant portion of the memory. Instead of saving it, we extend the hashing trick used in Joulin et al. (2016) to both words and n-grams. This strategy is also used in Vowpal Wabbit (Agarwal et al., 2014) in the context of online training. This allows us to save around 1-2Mb with almost no overhead at test time (just the cost of computing the hashing function).
Pruning the vocabulary while using the hashing trick requires keeping a list of the indices of the K remaining buckets. At test time, a binary search over the list of indices is required. It has a complexity of O(log(K)) and a memory overhead of a few hundreds of kilobytes. Using Bloom filters instead reduces the complexityO(1) at test time and saves a few hundred kilobytes. However, in practice, it degrades performance.

4 EXPERIMENTS
This section evaluates the quality of our model compression pipeline and compare it to other compression methods on different text classification problems, and to other compact text classifiers.
Evaluation protocol and datasets. Our experimental pipeline is as follows: we train a model using fastText with the default setting unless specified otherwise. That is 2M buckets, a learning rate of 0.1 and 10 training epochs. The dimensionality d of the embeddings is set to powers of 2 to avoid border effects that could make the interpretation of the results more difficult. As baselines, we use Locality-Sensitive Hashing (LSH) (Charikar, 2002), PQ (Jegou et al., 2011) and OPQ (Ge et al., 2013) (the non-parametric variant). Note that we use an improved version of LSH where random orthogonal matrices are used instead of random matrix projection Jégou et al. (2008). In a first series of experiments, we use the 8 datasets and evaluation protocol of Zhang et al. (2015). These datasets contain few million documents and have at most 10 classes. We also explore the limit of quantization on a dataset with an extremely large output space, that is a tag dataset extracted from the YFCC100M collection (Thomee et al., 2016)3, referred to as FlickrTag in the rest of this paper.

4.1 SMALL DATASETS
Compression techniques. We compare three popular methods used for similarity estimation with compact codes: LSH, PQ and OPQ on the datasets released by Zhang et al. (2015). Figure 1 shows the accuracy as a function of the number of bytes used per embedding, which corresponds to the number k of subvectors in the case of PQ and OPQ. See more results in the appendix. As discussed in Section 2, LSH reproduces the cosine similarity and is therefore not adapted to un-normalized data. Therefore we only report results with normalization. Once normalized, PQ and OPQ are almost lossless even when using only k = 4 subquantizers per embedding (equivalently, bytes). We observe in practice that using k = d/2, i.e., half of the components of the embeddings, works well in practice. In the rest of the paper and if not stated otherwise, we focus on this setting. The difference between the normalized versions of PQ and OPQ is limited and depends on the dataset. Therefore we adopt the normalized PQ (NPQ) for the rest of this study, since it is faster to train.
3Data available at https://research.facebook.com/research/fasttext/
Pruning. Figure 2 shows the performance of our model with different sizes. We fix k = d/2 and use different pruning thresholds. NPQ offers a compression rate of×10 compared to the full model. As the pruning becomes more agressive, the overall compression can increase up up to ×1, 000 with little drop of performance and no additional overhead at test time. In fact, using a smaller dictionary makes the model faster at test time. We also compare with character-level Convolutional Neural Networks (CNN) (Zhang et al., 2015; Xiao & Cho, 2016). They are attractive models for text classification because they achieve similar performance with less memory usage than linear models (Xiao & Cho, 2016). Even though fastText with the default setting uses more memory, NPQ is already on par with CNNs’ memory usage. Note that CNNs are not quantized, and it would be worth seeing how much they can be quantized with no drop of performance. Such a study is beyond the scope of this paper. Our pruning is based on the norm of the embeddings according to the guidelines of Section 3.3. Table 1 compares the ranking obtained with norms to the ranking obtained using entropy, which is commonly used in unsupervised settings Stolcke (2000).
Extreme compression. Finally, in Table 2, we explore the limit of quantized model by looking at the performance obtained for models under 64KiB. Surprisingly, even at 64KiB and 32KiB, the drop of performance is only around 0.8% and 1.7% despite a compression rate of ×1, 000− 4, 000.

4.2 LARGE DATASET: FLICKRTAG
In this section, we explore the limit of compression algorithms on very large datasets. Similar to Joulin et al. (2016), we consider a hashtag prediction dataset containing 312, 116 labels. We set the minimum count for words at 10, leading to a dictionary of 1, 427, 667 words. We take 10M buckets for n-grams and a hierarchical softmax. We refer to this dataset as FlickrTag.
Output encoding. We are interested in understanding how the performance degrades if the classifier is also quantized (i.e., the matrix B in Eq. 1) and when the pruning is at the limit of the minimum number of features required to cover the full dataset.
Table 3 shows that quantizing both the “input” matrix (i.e., A in Eq. 1) and the “output” matrix (i.e., B) does not degrade the performance compared to the full model. We use embeddings with d = 256 dimensions and use k = d/2 subquantizers. We do not use any text specific tricks, which leads to a compression factor of 8. Note that even if the output matrix is not retrained over the embeddings, the performance is only 0.2% away from the full model. As shown in the Appendix, using less subquantizers significantly decreases the performance for a small memory gain.
Pruning. Table 4 shows how the performance evolves with pruning. We measure this effect on top of a fully quantized model. The full model misses 11.6% of the test set because of missing words (some documents are either only composed of hashtags or have only rare words). There are 312, 116 labels and thus it seems reasonable to keep embeddings in the order of the million. A naive pruning with 1M features misses about 30−40% of the test set, leading to a significant drop of performance. On the other hand, even though the max-coverage pruning approach was set on the train set, it does not suffer from any coverage loss on the test set. This leads to a smaller drop of performance. If the pruning is too aggressive, however, the coverage decreases significantly.

5 FUTURE WORK
It may be possible to obtain further reduction of the model size in the future. One idea is to condition the size of the vectors (both for the input features and the labels) based on their frequency (Chen et al., 2015; Grave et al., 2016). For example, it is probably not worth representing the rare labels by full 256-dimensional vectors in the case of the FlickrTag dataset. Thus, conditioning the vector size on the frequency and norm seems like an interesting direction to explore in the future.
We may also consider combining the entropy and norm pruning criteria: instead of keeping the features in the model based just on the frequency or the norm, we can use both to keep a good set of features. This could help to keep features that are both frequent and discriminative, and thereby to reduce the coverage problem that we have observed.
Additionally, instead of pruning out the less useful features, we can decompose them into smaller units (Mikolov et al., 2012). For example, this can be achieved by splitting every non-discriminative word into a sequence of character trigrams. This could help in cases where training and test examples are very short (for example just a single word).

6 CONCLUSION
In this paper, we have presented several simple techniques to reduce, by several orders of magnitude, the memory complexity of certain text classifiers without sacrificing accuracy nor speed. This is achieved by applying discriminative pruning which aims to keep only important features in the trained model, and by performing quantization of the weight matrices and hashing of the dictionary.
We will publish the code as an extension of the fastText library. We hope that our work will serve as a baseline to the research community, where there is an increasing interest for comparing the performance of various deep learning text classifiers for a given number of parameters. Overall, compared to recent work based on convolutional neural networks, fastText.zip is often more accurate, while requiring several orders of magnitude less time to train on common CPUs, and incurring a fraction of the memory complexity.
","We consider the problem of producing compact architectures for text classification, such that the full model fits in a limited amount of memory. After considering different solutions inspired by the hashing literature, we propose a method built upon product quantization to store word embeddings. While the original technique leads to a loss in accuracy, we adapt this method to circumvent quantization artefacts. Combined with simple approaches specifically adapted to text classification, our approach derived from fastText requires, at test time, only a fraction of the memory compared to the original FastText, without noticeably sacrificing quality in terms of classification accuracy. Our experiments carried out on several benchmarks show that our approach typically requires two orders of magnitude less memory than fastText while being only slightly inferior with respect to accuracy. As a result, it outperforms the state of the art by a good margin in terms of the compromise between memory usage and accuracy.",ICLR 2017 conference submission,False,,"This paper describes how to approximate the FastText approach such that its memory footprint is reduced by several orders of magnitude, while preserving its classification accuracy. The original FastText approach was based on a linear classifier on top of bag-of-words embeddings. This type of method is extremely fast to train and test, but the model size can be quite large.

This paper focuses on approximating the original approach with lossy compression techniques. Namely, the embeddings and classifier matrices A and B are compressed with Product Quantization, and an aggressive dictionary pruning is carried out. Experiments on various datasets (either with small or large number of classes) are conducted to tune the parameters and demonstrate the effectiveness of the approach. With a negligible loss in classification accuracy, an important reduction in term of model size (memory footprint) can be achieved, in the order of 100~1000 folds compared to the original size.

The paper is well written overall. The goal is clearly defined and well carried out, as well as the experiments. Different options for compressing the model data are evaluated and compared (e.g. PQ vs LSH), which is also interesting. Nevertheless the paper does not propose by itself any novel idea for text classification. It just focuses on adapting existing lossy compression techniques, which is not necessarily a problem. Specifically, it introduces:
  - a straightforward variant of PQ for unnormalized vectors,
  - dictionary pruning is cast as a set covering problem (which is NP-hard), but a greedy approach is shown to yield excellent results nonetheless,
  - hashing tricks and bloom filter are simply borrowed from previous papers.

These techniques are quite generic and could as well be used in other works. 


Here are some minor problems with the paper:

  - it is not made clear how the full model size is computed. What is exactly in the model? Which proportion of the full size do the A and B matrices, the dictionary, and the rest, account for? It is hard to follow where is the size bottleneck, which also seems to depend on the target application (i.e. small or large number of test classes). It would have been nice to provide a formula to calculate the total model size as a function of all parameters (k,b for PQ and K for dictionary, number of classes).
  
  - some parts lack clarity. For instance, the greedy approach to prune the dictionary is exposed in less than 4 lines (top of page 5), though it is far from being straightforward. Likewise, it is not clear why the binary search used for the hashing trick would introduce an overhead of a few hundreds of KB.
  

Overall this looks like a solid work, but with potentially limited impact research-wise.

---

The submission describes a method for compressing shallow and wide text classification models. The paper is well written, but the proposed method is not particularly novel, as it's comprised of existing model compression techniques. Overall, the contributions are incremental and the potential impact seems rather limited.

---

The paper presents a few tricks to compress a wide and shallow text classification model based on n-gram features. These tricks include (1) using (optimized) product quantization to compress embedding weights (2) pruning some of the vocabulary elements (3) hashing to reduce the storage of the vocabulary (this is a minor component of the paper). The paper focuses on models with very large vocabularies and shows a reduction in the size of the models at a relatively minor reduction of the accuracy.

The problem of compressing neural models is important and interesting. The methods section of the paper is well written with good high level comments and references. However, the machine learning contributions of the paper are marginal to me. The experiments are not too convincing mainly focusing on benchmarks that are not commonly used. The implications of the paper on the state-of-the-art RNN text classification models is unclear.

The use of (optimized) product quantization for approximating inner product is not particularly novel. Previous work also considered doing this. Most of the reduction in the model sizes comes from pruning vocabulary elements. The method proposed for pruning vocabulary elements is simply based on the assumption that embeddings with larger L2 norm are more important. A coverage heuristic is taken into account too. From a machine learning point of view, the proper baseline to solve this problem is to have a set of (relaxed) binary coefficients for each embedding vector and learn the coefficients jointly with the weights. An L1 regularizer on the coefficients can be used to encourage sparsity. From a practical point of view, I believe an important baseline is missing: what if one simply uses fewer vocabulary elements (e.g based on subword units - see

---

The paper proposes a series of tricks for compressing fast (linear) text classification models. The paper is clearly written, and the results are quite strong. The main compression is achieved via product quantization, a technique which has been explored in other applications within the neural network model compression literature. In addition to the Gong et al. work which was cited, it would be worth mentioning Quantized Convolutional Neural Networks for Mobile Devices (CVPR 2016,

---

This paper describes how to approximate the FastText approach such that its memory footprint is reduced by several orders of magnitude, while preserving its classification accuracy. The original FastText approach was based on a linear classifier on top of bag-of-words embeddings. This type of method is extremely fast to train and test, but the model size can be quite large.

This paper focuses on approximating the original approach with lossy compression techniques. Namely, the embeddings and classifier matrices A and B are compressed with Product Quantization, and an aggressive dictionary pruning is carried out. Experiments on various datasets (either with small or large number of classes) are conducted to tune the parameters and demonstrate the effectiveness of the approach. With a negligible loss in classification accuracy, an important reduction in term of model size (memory footprint) can be achieved, in the order of 100~1000 folds compared to the original size.

The paper is well written overall. The goal is clearly defined and well carried out, as well as the experiments. Different options for compressing the model data are evaluated and compared (e.g. PQ vs LSH), which is also interesting. Nevertheless the paper does not propose by itself any novel idea for text classification. It just focuses on adapting existing lossy compression techniques, which is not necessarily a problem. Specifically, it introduces:
  - a straightforward variant of PQ for unnormalized vectors,
  - dictionary pruning is cast as a set covering problem (which is NP-hard), but a greedy approach is shown to yield excellent results nonetheless,
  - hashing tricks and bloom filter are simply borrowed from previous papers.

These techniques are quite generic and could as well be used in other works. 


Here are some minor problems with the paper:

  - it is not made clear how the full model size is computed. What is exactly in the model? Which proportion of the full size do the A and B matrices, the dictionary, and the rest, account for? It is hard to follow where is the size bottleneck, which also seems to depend on the target application (i.e. small or large number of test classes). It would have been nice to provide a formula to calculate the total model size as a function of all parameters (k,b for PQ and K for dictionary, number of classes).
  
  - some parts lack clarity. For instance, the greedy approach to prune the dictionary is exposed in less than 4 lines (top of page 5), though it is far from being straightforward. Likewise, it is not clear why the binary search used for the hashing trick would introduce an overhead of a few hundreds of KB.
  

Overall this looks like a solid work, but with potentially limited impact research-wise.

---

This paper describes how to approximate the FastText approach such that its memory footprint is reduced by several orders of magnitude, while preserving its classification accuracy. The original FastText approach was based on a linear classifier on top of bag-of-words embeddings. This type of method is extremely fast to train and test, but the model size can be quite large.

This paper focuses on approximating the original approach with lossy compression techniques. Namely, the embeddings and classifier matrices A and B are compressed with Product Quantization, and an aggressive dictionary pruning is carried out. Experiments on various datasets (either with small or large number of classes) are conducted to tune the parameters and demonstrate the effectiveness of the approach. With a negligible loss in classification accuracy, an important reduction in term of model size (memory footprint) can be achieved, in the order of 100~1000 folds compared to the original size.

The paper is well written overall. The goal is clearly defined and well carried out, as well as the experiments. Different options for compressing the model data are evaluated and compared (e.g. PQ vs LSH), which is also interesting. Nevertheless the paper does not propose by itself any novel idea for text classification. It just focuses on adapting existing lossy compression techniques, which is not necessarily a problem. Specifically, it introduces:
  - a straightforward variant of PQ for unnormalized vectors,
  - dictionary pruning is cast as a set covering problem (which is NP-hard), but a greedy approach is shown to yield excellent results nonetheless,
  - hashing tricks and bloom filter are simply borrowed from previous papers.

These techniques are quite generic and could as well be used in other works. 


Here are some minor problems with the paper:

  - it is not made clear how the full model size is computed. What is exactly in the model? Which proportion of the full size do the A and B matrices, the dictionary, and the rest, account for? It is hard to follow where is the size bottleneck, which also seems to depend on the target application (i.e. small or large number of test classes). It would have been nice to provide a formula to calculate the total model size as a function of all parameters (k,b for PQ and K for dictionary, number of classes).
  
  - some parts lack clarity. For instance, the greedy approach to prune the dictionary is exposed in less than 4 lines (top of page 5), though it is far from being straightforward. Likewise, it is not clear why the binary search used for the hashing trick would introduce an overhead of a few hundreds of KB.
  

Overall this looks like a solid work, but with potentially limited impact research-wise.

---

The submission describes a method for compressing shallow and wide text classification models. The paper is well written, but the proposed method is not particularly novel, as it's comprised of existing model compression techniques. Overall, the contributions are incremental and the potential impact seems rather limited.

---

The paper presents a few tricks to compress a wide and shallow text classification model based on n-gram features. These tricks include (1) using (optimized) product quantization to compress embedding weights (2) pruning some of the vocabulary elements (3) hashing to reduce the storage of the vocabulary (this is a minor component of the paper). The paper focuses on models with very large vocabularies and shows a reduction in the size of the models at a relatively minor reduction of the accuracy.

The problem of compressing neural models is important and interesting. The methods section of the paper is well written with good high level comments and references. However, the machine learning contributions of the paper are marginal to me. The experiments are not too convincing mainly focusing on benchmarks that are not commonly used. The implications of the paper on the state-of-the-art RNN text classification models is unclear.

The use of (optimized) product quantization for approximating inner product is not particularly novel. Previous work also considered doing this. Most of the reduction in the model sizes comes from pruning vocabulary elements. The method proposed for pruning vocabulary elements is simply based on the assumption that embeddings with larger L2 norm are more important. A coverage heuristic is taken into account too. From a machine learning point of view, the proper baseline to solve this problem is to have a set of (relaxed) binary coefficients for each embedding vector and learn the coefficients jointly with the weights. An L1 regularizer on the coefficients can be used to encourage sparsity. From a practical point of view, I believe an important baseline is missing: what if one simply uses fewer vocabulary elements (e.g based on subword units - see

---

The paper proposes a series of tricks for compressing fast (linear) text classification models. The paper is clearly written, and the results are quite strong. The main compression is achieved via product quantization, a technique which has been explored in other applications within the neural network model compression literature. In addition to the Gong et al. work which was cited, it would be worth mentioning Quantized Convolutional Neural Networks for Mobile Devices (CVPR 2016,

---

This paper describes how to approximate the FastText approach such that its memory footprint is reduced by several orders of magnitude, while preserving its classification accuracy. The original FastText approach was based on a linear classifier on top of bag-of-words embeddings. This type of method is extremely fast to train and test, but the model size can be quite large.

This paper focuses on approximating the original approach with lossy compression techniques. Namely, the embeddings and classifier matrices A and B are compressed with Product Quantization, and an aggressive dictionary pruning is carried out. Experiments on various datasets (either with small or large number of classes) are conducted to tune the parameters and demonstrate the effectiveness of the approach. With a negligible loss in classification accuracy, an important reduction in term of model size (memory footprint) can be achieved, in the order of 100~1000 folds compared to the original size.

The paper is well written overall. The goal is clearly defined and well carried out, as well as the experiments. Different options for compressing the model data are evaluated and compared (e.g. PQ vs LSH), which is also interesting. Nevertheless the paper does not propose by itself any novel idea for text classification. It just focuses on adapting existing lossy compression techniques, which is not necessarily a problem. Specifically, it introduces:
  - a straightforward variant of PQ for unnormalized vectors,
  - dictionary pruning is cast as a set covering problem (which is NP-hard), but a greedy approach is shown to yield excellent results nonetheless,
  - hashing tricks and bloom filter are simply borrowed from previous papers.

These techniques are quite generic and could as well be used in other works. 


Here are some minor problems with the paper:

  - it is not made clear how the full model size is computed. What is exactly in the model? Which proportion of the full size do the A and B matrices, the dictionary, and the rest, account for? It is hard to follow where is the size bottleneck, which also seems to depend on the target application (i.e. small or large number of test classes). It would have been nice to provide a formula to calculate the total model size as a function of all parameters (k,b for PQ and K for dictionary, number of classes).
  
  - some parts lack clarity. For instance, the greedy approach to prune the dictionary is exposed in less than 4 lines (top of page 5), though it is far from being straightforward. Likewise, it is not clear why the binary search used for the hashing trick would introduce an overhead of a few hundreds of KB.
  

Overall this looks like a solid work, but with potentially limited impact research-wise.",,,,,,5.666666666666667,,,3.6666666666666665,,
663,"Authors: IN E-COMMERCE, Tom Zahavy, Shie Mannor
Source file: 663.pdf

ABSTRACT
Classifying products into categories precisely and efficiently is a major challenge in modern e-commerce. The high traffic of new products uploaded daily and the dynamic nature of the categories raise the need for machine learning models that can reduce the cost and time of human editors. In this paper, we propose a decision level fusion approach for multi-modal product classification using text and image inputs. We train input specific state-of-the-art deep neural networks for each input source, show the potential of forging them together into a multi-modal architecture and train a novel policy network that learns to choose between them. Finally, we demonstrate that our multi-modal network improves the top-1 accuracy % over both networks on a real-world large-scale product classification dataset that we collected from Walmart.com. While we focus on image-text fusion that characterizes e-commerce domains, our algorithms can be easily applied to other modalities such as audio, video, physical sensors, etc.

1 INTRODUCTION
Product classification is a key issue in e-commerce domains. A product is typically represented by metadata such as its title, image, color, weight and so on, and most of them are assigned manually by the seller. Once a product is uploaded to an e-commerce website, it is typically placed in multiple categories. Categorizing products helps e-commerce websites to provide costumers a better shopping experience, for example by efficiently searching the products catalog or by developing recommendation systems. A few examples of categories are internal taxonomies (for business needs), public taxonomies (such as groceries and office equipment) and the product’s shelf (a group of products that are presented together on an e-commerce web page). These categories vary with time in order to optimize search efficiency and to account for special events such as holidays and sports events. In order to address these needs, e-commerce websites typically hire editors and use crowdsourcing platforms to classify products. However, due to the high amount of new products uploaded daily and the dynamic nature of the categories, machine learning solutions for product classification are very appealing as means to reduce the time and economic costs. Thus, precisely categorizing items emerges as a significant issue in e-commerce domains.
A shelf is a group of products presented together on an e-commerce website page, and usually contain products with a given theme/category (e.g., Women boots, folding tables). Product to shelf classification is a challenging problem due to data size, category skewness, and noisy metadata and labels. In particular, it presents three fundamental challenges for machine learning algorithms. First, it is typically a multi-class problem with thousands of classes. Second, a product may belong to multiple shelves making it a multi-label problem. And last, a product has both an image and a text input making it a multi-modal problem.
Products classification is typically addressed as a text classification problem because most metadata of items are represented as textual features (Pyo et al., 2010). Text classification is a classic topic for natural language processing, in which one needs to assign predefined categories to text inputs.
Standard methods follow a classical two-stage scheme of extraction of (handcrafted) features, followed by a classification stage. Typical features include bag-of-words or n-grams, and their TF-IDF. On the other hand, Deep Neural Networks use generic priors instead of specific domain knowledge (Bengio et al., 2013) and have been shown to give competitive results on text classification tasks (Zhang et al., 2015). In particular, Convolutional neural networks (CNNs) (Kim, 2014; Zhang et al., 2015; Conneau et al., 2016) and Recurrent NNs (Lai et al., 2015; Pyo et al., 2010; Xiao & Cho, 2016) can efficiently capture the sequentiality of the text. These methods are typically applied directly to distributed embedding of words (Kim, 2014; Lai et al., 2015; Pyo et al., 2010) or characters (Zhang et al., 2015; Conneau et al., 2016; Xiao & Cho, 2016), without any knowledge on the syntactic or semantic structures of a language. However, all of these architectures were only applied on problems with a small amount of labels (∼ 20) while e-commerce shelf classification problems typically have thousands of labels with multiple labels per product.
In Image classification, CNNs are widely considered the best models, and achieve state-of-theart results on the ImageNet Large-Scale Visual Recognition Challenge (Russakovsky et al., 2015; Krizhevsky et al., 2012; Simonyan & Zisserman, 2014; He et al., 2015). However, as good as they are, the classification accuracy of machine learning systems is often limited in problems with many classes of object categories. One remedy is to leverage data from other sources, such as text data. However, the studies on multi-modal deep learning for large-scale item categorization are still rare to the best of our belief. In particular in a setting where there is a significant difference in discriminative power between the two types of signals.
In this work, we propose a multi-modal deep neural network model for product classification. Our design principle is to leverage the specific prior for each data type by using the current state-of-
the-art classifiers from the image and text domains. The final architecture has 3 main components (Figure 2, Right): a text CNN (Kim, 2014), an image CNN (Simonyan & Zisserman, 2014) and a policy network that learns to choose between them. We collected a large-scale data set of 1.2 million products from the Walmart.com website. Each product has a title and an image and needs to be classified to a shelf (label) with 2890 possible shelves. Examples from this dataset can be seen in Figure 1 and are also available on-line at the Walmart.com website. For most of the products, both the image and the title of each product contain relevant information for customers. However, it is interesting to observe that for some of the products, both input types may not be informative for shelf prediction (Figure 1). This observation motivates our work and raises interesting questions: which input type is more useful for product classification? is it possible to forge the inputs into a better architecture?
In our experiments, we show that the text CNN outperforms the image one. However, for a relatively large number of products (∼ 8%), the image CNN is correct while the text CNN is wrong, indicating a potential gain from using a multi-modal architecture. We also show that the policy is able to choose between the two models and give a performance improvement over both state-of-the-art networks.
To the best of our knowledge, this is the first work that demonstrates a performance improvement on top-1 classification accuracy by using images and text on a large-scale classification problem. In particular, our main contributions are:
• We demonstrate that the text classification CNN (Kim, 2014) outperforms the VGG network (Simonyan & Zisserman, 2014) on a real-world large-scale product to shelf classification problem.
• We analyze the errors made by the different networks and show the potential gain of multimodality.
• We propose a novel decision-level fusion policy that learns to choose between the text and image networks and improve over both.

2 MULTI-MODALITY
Over the years, a large body of research has been devoted to improving classification using ensembles of classifiers (Kittler et al., 1998; Hansen & Salamon, 1990). Inspired by their success, these methods have also been used in multi-modal settings (e.g.,Guillaumin et al. (2010); Poria et al. (2016)), where the source of the signals, or alternatively their modalities, are different. Some examples include audio-visual speech classification (Ngiam et al., 2011), image and text retrieval (Kiros et al.), sentiment analysis and semi-supervised learning (Guillaumin et al., 2010).
Combining classifiers from different input sources presents multiple challenges. First, classifiers vary in their discriminative power, thus, an optimal unification method should be able to adapt itself for specific combinations of classifiers. Second, different data sources have different state-ofthe-art architectures, typically deep neural networks, which vary in depth, width, and optimization algorithm; making it non-trivial to merge them. Moreover, a multi-modal architecture potentially has more local minima that may give unsatisfying results. Finally, most of the publicly available real-world big data classification datasets, an essential building block of deep learning systems, typically contain only one data type.
Nevertheless, the potential performance boost of multi-modal architectures has motivated researchers over the years. Frome et al. (2013) combined an image network (Krizhevsky et al., 2012) with a Skip-gram Language Model in order to improve classification results on ImageNet. However, they were not able to improve the top-1 accuracy prediction, possibly because the text input they used (image labels) didn’t contain a lot of information. Other works, used multi-modality to learn good embedding but did not present results on classification benchmarks (Lynch et al., 2015; Kiros et al.; Gong et al., 2014). Kannan et al. (2011) suggested to improve text-based product classification by adding an image signal, training an image classifier and learning a decision rule between the two. However, they only experimented with a small dataset and a low number of labels, and it is not clear how to scale their method for extreme multi-class multi-label applications that characterize real-world problems in e-commerce.
Adding modalities can improve the classification of products that have a non-informative input source (e.g., image or text). In e-commerce, for example, classifiers that rely exclusively on text suffer from short and non-informative titles, differences in style between vendors and overlapping text across categories (i.e., a word that helps to classify a certain class may appear in other classes). Figure 1 presents a few examples of products that have only one informative input type. These examples suggest that a multi-modal architecture can potentially outperform a classifier with a single input type.
Most unification techniques for multi-modal learning are partitioned between feature-level fusion techniques and decision-level fusion techniques (Figure 2, Left).

2.1 FEATURE LEVEL FUSION
Feature-level fusion is characterized by three phases: (a) learning a representation, (b) supervised training, and (c) testing. The different unification techniques are distinguished by the availability of the data in each phase (Guillaumin et al., 2010). For example, in cross-modality training, the representation is learned from all the modalities, but only one modality is available for supervised training and testing. In other cases, all of the modalities are available at all stages but we may want (or not) to limit their usage given a certain budget. Another source for the distinction is the order in which phases (a) and (b) are made. For example, one may first learn the representation and then learn a classifier from it, or learn both the representation and the classifier in parallel. In the deep learning context, there are two common approaches. In the first approach, we learn an end-to-end deep NN; the NN has multiple input-specific pipes that include a data source followed by input specific layers. After a certain depth, the pipes are concatenated followed by additional layers such that the NN is trained end-to-end. In the second approach, input specific deep NNs are learned first, and a multi-modal representation vector is created by concatenating the input specific feature vectors (e.g., the neural network’s last hidden layer). Then, an additional classifier learns to classify from the multi-modal representation vector. While multi-modal methods have shown potential to boost performance on small datasets (Poria et al., 2016), or on top-k accuracy measures (Frome et al., 2013), we are not familiar with works that succeeded with applying it on a large-scale classification problem and received performance improvement in top-1 accuracy.

2.2 DECISION-LEVEL FUSION
In this approach, an input specific classifier is learned for each modality, and the goal is to find a decision rule between them. The decision rule is typically a pre-defined rule (Guillaumin et al., 2010) and is not learned from the data. For example, Poria et al. (2016) chose the classifier with the maximal confidence, while Krizhevsky et al. (2012) average classifier predictions. However, in this work we show that learning the decision rule yields significantly better results on our data.

3 METHODS AND ARCHITECTURES
In this section, we give the details of our multi-modal product classification architecture. The architecture is composed of a text CNN and an image CNN which are forged together by a policy network, as can be seen in Figure 2, Right.

3.1 MULTI LABEL COST FUNCTION
Our cost function is the weighted sigmoid cross entropy with logits, a common cost function for multi-label problems. Let x be the logits, z be the targets, q be a positive weight coefficient, used as a multiplier for the positive targets, and σ(x) = 11+exp(−x) . The loss is given by:
Cost(x,z;q) = − qz · log(σ(x))− (1− z) · log(1− σ(x)) = (1− z) · x+ (1 + (q − 1) · z) · log(1 + exp(−x)).
The positive coefficient q, allows one to trade off recall and precision by up- or down-weighting the cost of a positive error relative to a negative error. We found it to have a significant effect in practice.

3.2 TEXT CLASSIFICATION
For the text signal, we use the text CNN architecture of Kim (2014). The first layer embeds words into low-dimensional vectors using random embedding (different than the original paper). The next layer performs convolutions over time on the embedded word vectors using multiple filter sizes (3, 4 and 5), where we use 128 filters from each size. Next, we max-pool-over-time the result of each convolution filter and concatenated all the results together. We add a dropout regularization layer (0.5 dropping rate), followed by a fully connected layer, and classify the result using a softmax layer. An illustration of the Text CNN can be seen in Figure 2.

3.3 IMAGE CLASSIFICATION
For the image signal, we use the VGG Network (Simonyan & Zisserman, 2014). The input to the network is a fixed-size 224 x 224 RGB image. The image is passed through a stack of convolutional layers with a very small receptive field: 3 x 3. The convolution stride is fixed to 1 pixel; the spatial padding of the convolutional layer is 1 pixel. Spatial pooling is carried out by five max-pooling layers, which follow some of the convolutional layers. Max-pooling is performed over a 2 x 2 pixel window, with stride 2. A stack of convolutional layers is followed by three Fully-Connected (FC) layers: the first two have 4096 channels each, the third performs 2890-way product classification and thus contains 2890 channels (one for each class). All hidden layers are followed by a ReLu non-linearity. The exact details can be seen in Figure 2.

3.4 MULTI-MODAL ARCHITECTURE
We experimented with four types of multi-modal architectures. (1) Learning decision-level fusion policies from different inputs. (1a) Policies that use the text and image CNNs class probabilities as input (Figure 2). We experimented with architectures that have one or two fully connected layers (the two-layered policy is using 10 hidden units and a ReLu non-linearity between them). (1b) Policies that use the text and/or image as input. For these policies, the architecture of policy network was either the text CNN or the VGG network. In order to train policies, labels are collected from the image and text networks predictions, i.e., the label is 1 if the image network made a correct prediction while the text network made a mistake, and 0 otherwise. On evaluation, we use the policy predictions to select between the models, i.e., if the policy prediction is 1 we use the image network, and use the text network otherwise. (2) Pre-defined policies that average the predictions of the different CNNs or choose the CNN with the highest confidence. (3) End-to-end feature-level fusion, each input type is processed by its specific CNN. We concatenate the last hidden layers of the CNNs and add one or two fully connected layers. All the layers are trained together end-to-end (we also tried to initialize the input specific weights from pre-trained single-modal networks). (4) Multistep feature-level fusion. As in (3), we create shared representation vector by concatenating the last hidden layers. However, we now keep the shared representation fixed and learn a new classifier from it.

4 EXPERIMENTS

4.1 SETUP
Our dataset contains 1.2 million products (title image and shelf) that we collected from Walmart.com (offered online and can be viewed at the website) and were deemed the hardest to classify by the current production system. We divide the data into training (1.1 million) validation (50k) and test (50k). We train both the image network and the text network on the training dataset and evaluate them on the test dataset. The policy is trained on the validation dataset and is also evaluated on the test dataset. The objective is to classify the product’s shelf, from 2890 possible choices. Each product is typically assigned to more than one shelf (3 on average), and the network is considered accurate if its most probable shelf is one of them.

4.2 TRAINING THE TEXT ARCHITECTURE
Preprocess: we build a dictionary of all the words in the training data and embed each word using a random embedding into a one hundred dimensional vector. We trim titles with more than 40 words and pad shorter titles with nulls.
We experimented with different batch sizes, dropout rates, and filters stride, but found that the vanilla architecture (Kim, 2014) works well on our data. This is consistent with Zhang & Wallace (2015), who showed that text CNNs are not very sensitive to hyperparameters. We tuned the cost function positive coefficient parameter q, and found out that the value 30 performed best in practice (we will also use this value for the image network). The best CNN that we trained classified 70.1% of the products from the test set correctly (Table 1).

4.3 TRAINING THE IMAGE ARCHITECTURE
Preprocess: we re-size all the images into 224 x 224 pixels and reduce the image mean.
The VGG network that we trained classified 57% of the products from the test set correctly. This is a bit disappointing if we compare it to the performance of the VGG network on ImageNet (∼ 75%). There are a few differences between these two datasets that may explain this gap. First, our data has 3 times more classes and contains multiple labels per image making the classification harder, and second, Figure 1 implies that some of our images are not informative for shelf classification. Some works claim that the features learned by VGG on ImageNet are global feature extractors (Lynch et al., 2015). We therefore decided to use the weights learned by VGG on ImageNet and learn only the last layer. This configuration yielded only 36.7% accuracy. We believe that the reason is that some of the ImageNet classes are irrelevant for e-commerce (e.g., vehicles and animals) while some relevant categories are misrepresented (e.g., electronics and office equipment). It could also be that our images follow some specific pattern of white background, well-lit studio etc., that characterizes e-commerce.

4.4 ERROR ANALYSIS
Is a picture worth a thousand words? Inspecting Figure 3, we can see that the text network outperformed the image network on this dataset, classifying more products correctly. Similar results were reported before (Pyo et al., 2010; Kannan et al., 2011) but to the best of our knowledge, this is the first work that compares state-of-the-art text and image CNNs on a real-world large-scale ecommerce dataset. What is the potential of multi-modality? We identified that for 7.8% of the products the image network made a correct prediction while the text network was wrong. This observation is encouraging since it implies that there is a relative big potential to harness via multi-modality. We find this large gap surprising since different neural networks applied to the same problem tend to make the same mistakes (Szegedy et al., 2013). Unification techniques for multi-modal problems typically use the last hidden layer of each network as features (Frome et al., 2013; Lynch et al., 2015; Pyo et al., 2010). We therefore decided to visualize the activations of this layer using a tSNE map (Maaten & Hinton, 2008). Figure 3, depicts such a map for the activations of the text model (the image model yielded similar results). In particular,
we were looking for regions in the tSNE map where the image predictions are correct and the text is wrong (Figure 3, green). Finding such a region will imply that a policy network can learn good decision boundaries. However, we can see that there are no well-defined regions in the tSNE maps where the image network is correct and the title is wrong (green), thus implying that it might be hard to identify these products using the activations of the last layers.

4.5 MULTI-MODAL UNIFICATION TECHNIQUES
Our error analysis experiment highlights the potential of merging image and text. Still, we found it hard to achieve the upper bound provided by the error analysis in practice. We now describe the policies that managed to achieve performance boost in top-1 accuracy % over the text and image networks, and then provide discussion on other approaches that we tried but didn’t work.
Decision-level fusion: We trained policies from different data sources (e.g., title, image, and each CNN class probabilities), using different architectures and different hyperparameters. Looking at Table 1, we can see that the best policies were trained using the class probabilities (the softmax probabilities) of the image and text CNNs as inputs. The amount of class probabilities that were used (top-1, top-3 or all) did not have a significant effect on the results, indicating that the top-1 probability contains enough information to learn good policies. This result makes sense since the top-1 probability measures the confidence of the network in making a prediction. Still, the top-3 probabilities performed slightly better, indicating that the difference between the top probabilities may also matter. We can also see that the 2-layer architecture outperformed the 1-layer, indicating that a linear policy is too simple, and deeper models can yield better results. Last, the cost function positive coefficient q had a big impact on the results. We can see that for q = 1, the policy network is more accurate in its prediction however it achieves worse results on shelf classification. For q = 5 we get the best results, while higher values of q (e.g., 7 or 10) resulted in inaccurate policies that did not perform well in practice.
While it may not seem surprising that combining text and image will improve accuracy, in practice we found it extremely hard to leverage this potential. To the best of our knowledge, this is the first work that demonstrates a direct performance improvement on top-1 classification accuracy from using images and text on a large-scale classification problem. We experimented with pre-defined policies that do not learn from the data. Specifically, we tried to average the logits, following (Krizhevsky et al., 2012; Simonyan & Zisserman, 2014), and to choose the network with the maximal confidence following (Poria et al., 2016). Both of these experiments yielded significantly worse results, probably, since the text network is much more accurate than the image one (Table 1). We also tried to learn policies from the text and/or the image input, using a policy network which is either a text CNN, a VGG network or a combination. However, all of these experiments resulted in policies that overfit the data and performed worse than the title model on the test data (Table 1). We also experimented with early stopping criteria, various regularization methods (dropout, l1, l2) and reduced model size but none could make the policy network generalize.
Feature-level fusion: Training a CNN end-to-end can be tricky. For example, each input source has its own specific architecture, with specific learning rate and optimization algorithm. We experimented with training the network end-to-end, but also with first training each part separately and then learning the concatenated parts. We tried different unification approaches such as gating functions (Srivastava et al., 2015), cross products and a different number of fully connected layers after the concatenation. These experiments resulted in models that were inferior to the text model. While this may seem surprising, the only successful feature level fusion that we are aware of (Frome et al., 2013), was not able to gain accuracy improvement on top-1 accuracy.

5 CONCLUSIONS
In this work, we investigated a multi-modal multi-class multi-label product classification problem and presented results on a challenging real-world dataset that we collected from Walmart.com. We discovered that the text network outperforms the image network on our dataset, and observed a big potential of fusing text and image inputs. Finally, we suggested a multi-modal decision-level fusion approach that leverages state-of-the-art results from image and text classification and forges them into a multi-modal architecture that outperforms both.
State-of-the-art image CNNs are much larger than text CNNs, and take more time to train and to run. Thus, extracting image features during run time, or getting the image network predictions may be prohibitively expensive. In this context, an interesting observation is that feature level fusion methods require using the image signal for each product, while decision level fusion methods require using the image network selectively making them more appealing. Moreover, our experiments suggest that decision-level fusion performs better than feature-level fusion in practice.
Finally, we were only able to realize a fraction of the potential of multi-modality. In the future, we plan to investigate deeper policy networks and more sophisticated measures of confidence. We also plan to investigate ensembles of image networks (Krizhevsky et al., 2012) and text networks (Pyo et al., 2010). We believe that the insights from training policy networks will eventually lead us to train end to end differential multi-modal networks.
","Classifying products into categories precisely and efficiently is a major challenge in modern e-commerce. The high traffic of new products uploaded daily and the dynamic nature of the categories raise the need for machine learning models that can reduce the cost and time of human editors. In this paper, we propose a decision level fusion approach for multi-modal product classification using text and image inputs. We train input specific state-of-the-art deep neural networks for each input source, show the potential of forging them together into a multi-modal architecture and train a novel policy network that learns to choose between them. Finally, we demonstrate that our multi-modal network improves the top-1 accuracy % over both networks on a real-world large-scale product classification dataset that we collected from Walmart.com. While we focus on image-text fusion that characterizes e-commerce domains, our algorithms can be easily applied to other modalities such as audio, video, physical sensors, etc.",ICLR 2017 conference submission,False,,"This paper presents a system approach to combine multiple modalities to perform classification in a practical scenario (e-commerce).

In general, I find the proposed approach in the paper sound and solid, but do not see novelty in the paper: feature fusion and decision time fusion are both standard practices in multi-modal analysis, and the rest of the paper offers no surprise in implementing such approaches. This seems to be a better fit for venues that focus more on production systems, and seems to be a bad fit for ICLR where the focus is more on research of novel algorithms and theories.

---

Three knowledgable reviewers recommend rejection. While the application is interesting and of commercial value, the technical contribution falls below the ICLR's bar. I encourage the authors to improve the paper and submit it to a future conference.

---

This paper introduces a large-scale multi-model product classification system. The model consists of three modules, Image CNN (VGG 16 architecture), text CNN (Kim 2014) and decision-level fusion policies. The authors have tried several fusion methods: including policies taking inputs from text and image CNN probabilities; choose either CNN; average the predictions; end-to-end training. Experimental results show that text CNN alone works better than image CNN and multi-model fusion can improve the accuracy by a small margin. It is a little bit surprising that end-to-end feature level fusion works worse than text CNN alone. The writing is clear and there are a lot of useful practical experiences of learning large-scale model. However, I lean toward rejecting the paper because the following:

1) No other dataset reported. The authors haven't mentioned releasing the walmart dataset and it is going to be really hard to reproduce the results without the dataset. 
2) Technical novelty is limited. All the decision-level fusion policies have been investigated by some previous methods before. 
3) Performance gain is also limited.

---

This paper tackles the problem of multi-modal classification of text and images.

Pros:
- Interesting dataset and application.

Cons:
- The results are rather lacklustre, showing a very mild improvement compared to the oracle improvement. But perhaps some insights as to whether the incorrect decisions are humanly possible would help with significance of the results.
- Could have explored some intermediate architectures such as feature fusion + class probabilities with/without finetuning. There are no feature fusion results reported.
- No evaluation on standard datasets or comparison to previous works.

What is the policy learnt for CP-1? Given 2 input class probabilities, how does the network perform better than max or mean?

---

This paper presents a system approach to combine multiple modalities to perform classification in a practical scenario (e-commerce).

In general, I find the proposed approach in the paper sound and solid, but do not see novelty in the paper: feature fusion and decision time fusion are both standard practices in multi-modal analysis, and the rest of the paper offers no surprise in implementing such approaches. This seems to be a better fit for venues that focus more on production systems, and seems to be a bad fit for ICLR where the focus is more on research of novel algorithms and theories.

---

Hi, I am Jung-Woo Ha and the authour of a paper you cited in your work. 

In the references of your manuscript, I think that ""Hyuna Pyo, Jung-Woo Ha, and Jeonghee Kim. Large-scale item categorization in e-commerce using
multiple recurrent neural networks. 2010."" should be changed into ""Jung-Woo Ha, Hyuna Pyo, and Jeonghee Kim. Large-scale item categorization in e-commerce using
multiple recurrent neural networks. In Proceedings of KDD 2016.""
The url is

---

This paper presents a system approach to combine multiple modalities to perform classification in a practical scenario (e-commerce).

In general, I find the proposed approach in the paper sound and solid, but do not see novelty in the paper: feature fusion and decision time fusion are both standard practices in multi-modal analysis, and the rest of the paper offers no surprise in implementing such approaches. This seems to be a better fit for venues that focus more on production systems, and seems to be a bad fit for ICLR where the focus is more on research of novel algorithms and theories.

---

Three knowledgable reviewers recommend rejection. While the application is interesting and of commercial value, the technical contribution falls below the ICLR's bar. I encourage the authors to improve the paper and submit it to a future conference.

---

This paper introduces a large-scale multi-model product classification system. The model consists of three modules, Image CNN (VGG 16 architecture), text CNN (Kim 2014) and decision-level fusion policies. The authors have tried several fusion methods: including policies taking inputs from text and image CNN probabilities; choose either CNN; average the predictions; end-to-end training. Experimental results show that text CNN alone works better than image CNN and multi-model fusion can improve the accuracy by a small margin. It is a little bit surprising that end-to-end feature level fusion works worse than text CNN alone. The writing is clear and there are a lot of useful practical experiences of learning large-scale model. However, I lean toward rejecting the paper because the following:

1) No other dataset reported. The authors haven't mentioned releasing the walmart dataset and it is going to be really hard to reproduce the results without the dataset. 
2) Technical novelty is limited. All the decision-level fusion policies have been investigated by some previous methods before. 
3) Performance gain is also limited.

---

This paper tackles the problem of multi-modal classification of text and images.

Pros:
- Interesting dataset and application.

Cons:
- The results are rather lacklustre, showing a very mild improvement compared to the oracle improvement. But perhaps some insights as to whether the incorrect decisions are humanly possible would help with significance of the results.
- Could have explored some intermediate architectures such as feature fusion + class probabilities with/without finetuning. There are no feature fusion results reported.
- No evaluation on standard datasets or comparison to previous works.

What is the policy learnt for CP-1? Given 2 input class probabilities, how does the network perform better than max or mean?

---

This paper presents a system approach to combine multiple modalities to perform classification in a practical scenario (e-commerce).

In general, I find the proposed approach in the paper sound and solid, but do not see novelty in the paper: feature fusion and decision time fusion are both standard practices in multi-modal analysis, and the rest of the paper offers no surprise in implementing such approaches. This seems to be a better fit for venues that focus more on production systems, and seems to be a bad fit for ICLR where the focus is more on research of novel algorithms and theories.

---

Hi, I am Jung-Woo Ha and the authour of a paper you cited in your work. 

In the references of your manuscript, I think that ""Hyuna Pyo, Jung-Woo Ha, and Jeonghee Kim. Large-scale item categorization in e-commerce using
multiple recurrent neural networks. 2010."" should be changed into ""Jung-Woo Ha, Hyuna Pyo, and Jeonghee Kim. Large-scale item categorization in e-commerce using
multiple recurrent neural networks. In Proceedings of KDD 2016.""
The url is",,,,,,4.666666666666667,,,4.0,,
673,"HIERARCHICAL MEMORY NETWORKS
Authors: Sarath Chandar, Sungjin Ahn, Hugo Larochelle, Pascal Vincent, Gerald Tesauro, Yoshua Bengio
Source file: 673.pdf

ABSTRACT
Memory networks are neural networks with an explicit memory component that can be both read and written to by the network. The memory is often addressed in a soft way using a softmax function, making end-to-end training with backpropagation possible. However, this is not computationally scalable for applications which require the network to read from extremely large memories. On the other hand, it is well known that hard attention mechanisms based on reinforcement learning are challenging to train successfully. In this paper, we explore a form of hierarchical memory network, which can be considered as a hybrid between hard and soft attention memory networks. The memory is organized in a hierarchical structure such that reading from it is done with less computation than soft attention over a flat memory, while also being easier to train than hard attention over a flat memory. Specifically, we propose to incorporate Maximum Inner Product Search (MIPS) in the training and inference procedures for our hierarchical memory network. We explore the use of various state-of-the art approximate MIPS techniques and report results on SimpleQuestions, a challenging large scale factoid question answering task.

1 INTRODUCTION
Until recently, traditional machine learning approaches for challenging tasks such as image captioning, object detection, or machine translation have consisted in complex pipelines of algorithms, each being separately tuned for better performance. With the recent success of neural networks and deep learning research, it has now become possible to train a single model end-to-end, using backpropagation. Such end-to-end systems often outperform traditional approaches, since the entire model is directly optimized with respect to the final task at hand. However, simple encode-decode style neural networks often underperform on knowledge-based reasoning tasks like question-answering or dialog systems. Indeed, in such cases it is nearly impossible for regular neural networks to store all the necessary knowledge in their parameters.
Neural networks with memory (Graves et al., 2014; Weston et al., 2015b) can deal with knowledge bases by having an external memory component which can be used to explicitly store knowledge. The memory is accessed by reader and writer functions, which are both made differentiable so that the entire architecture (neural network, reader, writer and memory components) can be trained end-to-end using backpropagation. Memory-based architectures can also be considered as generalizations of RNNs and LSTMs, where the memory is analogous to recurrent hidden states. However they are much richer in structure and can handle very long-term dependencies because once a vector (i.e., a memory) is stored, it is copied from time step to time step and can thus stay there for a very long time (and gradients correspondingly flow back time unhampered).
There exists several variants of neural networks with a memory component: Memory Networks (Weston et al., 2015b), Neural Turing Machines (NTM) (Graves et al., 2014), Dynamic Memory Net-
∗Corresponding author: apsarathchandar@gmail.com
works (DMN) (Kumar et al., 2015). They all share five major components: memory, input module, reader, writer, and output module.
Memory: The memory is an array of cells, each capable of storing a vector. The memory is often initialized with external data (e.g. a database of facts), by filling in its cells with a pre-trained vector representations of that data.
Input module: The input module is to compute a representation of the input that can be used by other modules.
Writer: The writer takes the input representation and updates the memory based on it. The writer can be as simple as filling the slots in the memory with input vectors in a sequential way (as often done in memory networks). If the memory is bounded, instead of sequential writing, the writer has to decide where to write and when to rewrite cells (as often done in NTMs).
Reader: Given an input and the current state of the memory, the reader retrieves content from the memory, which will then be used by an output module. This often requires comparing the input’s representation or a function of the recurrent state with memory cells using some scoring function such as a dot product.
Output module: Given the content retrieved by the reader, the output module generates a prediction, which often takes the form of a conditional distribution over multiple labels for the output.
For the rest of the paper, we will use the name memory network to describe any model which has any form of these five components. We would like to highlight that all the components except the memory are learnable. Depending on the application, any of these components can also be fixed. In this paper, we will focus on the situation where a network does not write and only reads from the memory.
In this paper, we focus on the application of memory networks to large-scale tasks. Specifically, we focus on large scale factoid question answering. For this problem, given a large set of facts and a natural language question, the goal of the system is to answer the question by retrieving the supporting fact for that question, from which the answer can be derived. Application of memory networks to this task has been studied by Bordes et al. (2015). However, Bordes et al. (2015) depended on keyword based heuristics to filter the facts to a smaller set which is manageable for training. However heuristics are invariably dataset dependent and we are interested in a more general solution which can be used when the facts are of any structure. One can design soft attention retrieval mechanisms, where a convex combination of all the cells is retrieved or design hard attention retrieval mechanisms where one or few cells from the memory are retrieved. Soft attention is achieved by using softmax over the memory which makes the reader differentiable and hence learning can be done using gradient descent. Hard attention is achieved by using methods like REINFORCE (Williams, 1992), which provides a noisy gradient estimate when discrete stochastic decisions are made by a model.
Both soft attention and hard attention have limitations. As the size of the memory grows, soft attention using softmax weighting is not scalable. It is computationally very expensive, since its complexity is linear in the size of the memory. Also, at initialization, gradients are dispersed so much that it can reduce the effectiveness of gradient descent. These problems can be alleviated by a hard attention mechanism, for which the training method of choice is REINFORCE. However, REINFORCE can be brittle due to its high variance and existing variance reduction techniques are complex. Thus, it is rarely used in memory networks (even in cases of a small memory).
In this paper, we propose a new memory selection mechanism based on Maximum Inner Product Search (MIPS) which is both scalable and easy to train. This can be considered as a hybrid of soft and hard attention mechanisms. The key idea is to structure the memory in a hierarchical way such that it is easy to perform MIPS, hence the name Hierarchical Memory Network (HMN). HMNs are scalable at both training and inference time. The main contributions of the paper are as follows:
• We explore hierarchical memory networks, where the memory is organized in a hierarchical fashion, which allows the reader to efficiently access only a subset of the memory.
• While there are several ways to decide which subset to access, we propose to pose memory access as a maximum inner product search (MIPS) problem.
• We empirically show that exact MIPS-based algorithms not only enjoy similar convergence as soft attention models, but can even improve the performance of the memory network. • Since exact MIPS is as computationally expensive as a full soft attention model, we propose
to train the memory networks using approximate MIPS techniques for scalable memory access. • We empirically show that unlike exact MIPS, approximate MIPS algorithms provide a
speedup and scalability of training, though at the cost of some performance.

2 HIERARCHICAL MEMORY NETWORKS
In this section, we describe the proposed Hierarchical Memory Network (HMN). In this paper, HMNs only differ from regular memory networks in two of its components: the memory and the reader.
Memory: Instead of a flat array of cells for the memory structure, HMNs leverages a hierarchical memory structure. Memory cells are organized into groups and the groups can further be organized into higher level groups. The choice for the memory structure is tightly coupled with the choice of reader, which is essential for fast memory access. We consider three classes of approaches for the memory’s structure: hashing-based approaches, tree-based approaches, and clustering-based approaches. This is explained in detail in the next section.
Reader: The reader in the HMN is different from the readers in flat memory networks. Flat memorybased readers use either soft attention over the entire memory or hard attention that retrieves a single cell. While these mechanisms might work with small memories, with HMNs we are more interested in achieving scalability towards very large memories. So instead, HMN readers use soft attention only over a selected subset of the memory. Selecting memory subsets is guided by a maximum inner product search algorithm, which can exploit the hierarchical structure of the organized memory to retrieve the most relevant facts in sub-linear time. The MIPS-based reader is explained in more detail in the next section.
In HMNs, the reader is thus trained to create MIPS queries such that it can retrieve a sufficient set of facts. While most of the standard applications of MIPS (Ram & Gray, 2012; Bachrach et al., 2014; Shrivastava & Li, 2014) so far have focused on settings where both query vector and database (memory) vectors are precomputed and fixed, memory readers in HMNs are learning to do MIPS by updating the input representation such that the result of MIPS retrieval contains the correct fact(s).
3 MEMORY READER WITH K-MIPS ATTENTION
In this section, we describe how the HMN memory reader uses Maximum Inner Product Search (MIPS) during learning and inference.
We begin with a formal definition of K-MIPS. Given a set of points X = {x1, . . . , xn} and a query vector q, our goal is to find
argmax (K) i∈X q >xi (1)
where the argmax(K) returns the indices of the top-K maximum values. In the case of HMNs, X corresponds to the memory and q corresponds to the vector computed by the input module.
A simple but inefficient solution for K-MIPS involves a linear search over the cells in memory by performing the dot product of q with all the memory cells. While this will return the exact result for K-MIPS, it is too costly to perform when we deal with a large-scale memory. However, in many practical applications, it is often sufficient to have an approximate result for K-MIPS, trading speed-up at the cost of the accuracy. There exist several approximate K-MIPS solutions in the literature (Shrivastava & Li, 2014; 2015; Bachrach et al., 2014; Neyshabur & Srebro, 2015).
All the approximate K-MIPS solutions add a form of hierarchical structure to the memory and visit only a subset of the memory cells to find the maximum inner product for a given query. Hashingbased approaches (Shrivastava & Li, 2014; 2015; Neyshabur & Srebro, 2015) hash cells into multiple bins, and given a query they search for K-MIPS cell vectors only in bins that are close to the bin
associated with the query. Tree-based approaches (Ram & Gray, 2012; Bachrach et al., 2014) create search trees with cells in the leaves of the tree. Given a query, a path in the tree is followed and MIPS is performed only for the leaf for the chosen path. Clustering-based approaches (Auvolat et al., 2015) cluster cells into multiple clusters (or a hierarchy of clusters) and given a query, they perform MIPS on the centroids of the top few clusters. We refer the readers to (Auvolat et al., 2015) for an extensive comparison of various state-of-the-art approaches for approximate K-MIPS.
Our proposal is to exploit this rich approximate K-MIPS literature to achieve scalable training and inference in HMNs. Instead of filtering the memory with heuristics, we propose to organize the memory based on approximate K-MIPS algorithms and then train the reader to learn to perform MIPS. Specifically, consider the following softmax over the memory which the reader has to perform for every reading step to retrieve a set of relevant candidates:
Rout = softmax(h(q)M T ) (2)
where h(q) ∈ Rd is the representation of the query, M ∈ RN×d is the memory with N being the total number of cells in the memory. We propose to replace this softmax with softmax(K) which is defined as follows:
C = argmax(K) h(q)MT (3)
Rout = softmax (K)(h(q)MT ) = softmax(h(q)M [C]T ) (4)
where C is the indices of top-K MIP candidate cells and M [C] is a sub-matrix of M where the rows are indexed by C.
One advantage of using the softmax(K) is that it naturally focuses on cells that would normally receive the strongest gradients during learning. That is, in a full softmax, the gradients are otherwise more dispersed across cells, given the large number of cells and despite many contributing a small gradient. As our experiments will show, this results in slower training.
One problematic situation when learning with the softmax(K) is when we are at the initial stages of training and the K-MIPS reader is not including the correct fact candidate. To avoid this issue, we always include the correct candidate to the top-K candidates retrieved by the K-MIPS algorithm, effectively performing a fully supervised form of learning.
During training, the reader is updated by backpropagation from the output module, through the subset of memory cells. Additionally, the log-likelihood of the correct fact computed using Ksoftmax is also maximized. This second supervision helps the reader learn to modify the query such that the maximum inner product of the query with respect to the memory will yield the correct supporting fact in the top K candidate set.
Until now, we described the exact K-MIPS-based learning framework, which still requires a linear look-up over all memory cells and would be prohibitive for large-scale memories. In such scenarios, we can replace the exact K-MIPS in the training procedure with the approximate K-MIPS. This is achieved by deploying a suitable memory hierarchical structure. The same approximate K-MIPSbased reader can be used during inference stage as well. Of course, approximate K-MIPS algorithms might not return the exact MIPS candidates and will likely to hurt performance, but at the benefit of achieving scalability.
While the memory representation is fixed in this paper, updating the memory along with the query representation should improve the likelihood of choosing the correct fact. However, updating the memory will reduce the precision of the approximate K-MIPS algorithms, since all of them assume that the vectors in the memory are static. Designing efficient dynamic K-MIPS should improve the performance of HMNs even further, a challenge that we hope to address in future work.
3.1 READER WITH CLUSTERING-BASED APPROXIMATE K-MIPS
Clustering-based approximate K-MIPS was proposed in (Auvolat et al., 2015) and it has been shown to outperform various other state-of-the-art data dependent and data independent approximate KMIPS approaches for inference tasks. As we will show in the experiments section, clustering-based MIPS also performs better when used to training HMNs. Hence, we focus our presentation on the clustering-based approach and propose changes that were found to be helpful for learning HMNs.
Following most of the other approximate K-MIPS algorithms, Auvolat et al. (2015) convert MIPS to Maximum Cosine Similarity Search (MCSS) problem:
argmax (K) i∈X qTxi ||q|| ||xi|| = argmax (K) i∈X qTxi ||xi||
(5)
When all the data vectors xi have the same norm, then MCSS is equivalent to MIPS. However, it is often restrictive to have this additional constraint. Instead, Auvolat et al. (2015) append additional dimensions to both query and data vectors to convert MIPS to MCSS. In HMN terminology, this would correspond to adding a few more dimensions to the memory cells and input representations.
The algorithm introduces two hyper-parameters, U < 1 and m ∈ N∗. The first step is to scale all the vectors in the memory by the same factor, such that maxi ||xi||2 = U . We then apply two mappings, P and Q, on the memory cells and on the input vector, respectively. These two mappings simply concatenate m new components to the vectors and make the norms of the data points all roughly the same (Shrivastava & Li, 2015). The mappings are defined as follows:
P (x) = [x, 1/2− ||x||22, 1/2− ||x||42, . . . , 1/2− ||x||2 m
2 ] (6) Q(x) = [x, 0, 0, . . . , 0] (7)
We thus have the following approximation of MIPS by MCSS for any query vector q:
argmax (K) i q >xi ' argmax(K)i Q(q)>P (xi)
||Q(q)||2 · ||P (xi)||2 (8)
Once we convert MIPS to MCSS, we can use spherical K-means (Zhong, 2005) or its hierarchical version to approximate and speedup the cosine similarity search. Once the memory is clustered, then every read operation requires only K dot-products, where K is the number of cluster centroids.
Since this is an approximation, it is error-prone. As we are using this approximation for the learning process, this introduces some bias in gradients, which can affect the overall performance of HMN. To alleviate this bias, we propose three simple strategies.
• Instead of using only the top-K candidates for a single read query, we also add top-K candidates retrieved for every other read query in the mini-batch. This serves two purposes. First, we can do efficient matrix multiplications by leveraging GPUs since all the K-softmax in a minibatch are over the same set of elements. Second, this also helps to decrease the bias introduced by the approximation error.
• For every read access, instead of only using the top few clusters which has a maximum product with the read query, we also sample some clusters from the rest, based on a probability distribution log-proportional to the dot product with the cluster centroids. This also decreases the bias.
• We can also sample random blocks of memory and add it to top-K candidates.
We empirically investigate the effect of these variations in Section 5.5.

4 RELATED WORK
Memory networks have been introduced in (Weston et al., 2015b) and have been so far applied to comprehension-based question answering (Weston et al., 2015a; Sukhbaatar et al., 2015), large scale question answering (Bordes et al., 2015) and dialogue systems (Dodge et al., 2015). While (Weston et al., 2015b) considered supervised memory networks in which the correct supporting fact is given during the training stage, (Sukhbaatar et al., 2015) introduced semi-supervised memory networks that can learn the supporting fact by itself. (Kumar et al., 2015; Xiong et al., 2016) introduced Dynamic Memory Networks (DMNs) which can be considered as a memory network with two types of memory: a regular large memory and an episodic memory. Another related class of model is the Neural Turing Machine (Graves et al., 2014), which uses softmax-based soft attention. Later (Zaremba & Sutskever, 2015) extended NTM to hard attention using reinforcement learning. (Dodge et al., 2015; Bordes et al., 2015) alleviate the problem of the scalability of soft attention by having
an initial keyword based filtering stage, which reduces the number of facts being considered. Our work generalizes this filtering by using MIPS for filtering. This is desirable because MIPS can be applied for any modality of data or even when there is no overlap between the words in a question and the words in facts.
The softmax arises in various situations and most relevant to this work are scaling methods for large vocabulary neural language modeling. In neural language modeling, the final layer is a softmax distribution over the next word and there exist several approaches to achieve scalability. (Morin & Bengio, 2005) proposes a hierarchical softmax based on prior clustering of the words into a binary, or more generally n-ary tree, that serves as a fixed structure for the learning process of the model. The complexity of training is reduced from O(n) to O(log n). Due to its clustering and tree structure, it resembles the clustering-based MIPS techniques we explore in this paper. However, the approaches differ at a fundamental level. Hierarchical softmax defines the probability of a leaf node as the product of all the probabilities computed by all the intermediate softmaxes on the way to that leaf node. By contrast, an approximate MIPS search imposes no such constraining structure on the probabilistic model, and is better thought as efficiently searching for top winners of what amounts to be a large ordinary flat softmax. Other methods such as Noice Constrastive Estimation (Mnih & Gregor, 2014) and Negative Sampling (Mikolov et al., 2013) avoid an expensive normalization constant by sampling negative samples from some marginal distribution. By contrast, our approach approximates the softmax by explicitly including in its negative samples candidates that likely would have a large softmax value. Jean et al. (2015) introduces an importance sampling approach that considers all the words in a mini-batch as the candidate set. This in general might also not include the MIPS candidates with highest softmax values.
(Spring & Shrivastava, 2016) is the only work that we know of, proposing to use MIPS during learning. It proposes hashing-based MIPS to sort the hidden layer activations and reduce the computation in every layer. However, a small scale application was considered and data-independent methods like hashing will likely suffer as dimensionality increases. Rae et al. (2016) have also independently proposed a model called SAM to use approximate search methods for memory access in NTM-like architectures. However, our motivation is different. While Rae et al. (2016) focus on architectures where the memory is written by the controller itself, we focus on handling memory access to large external knowledge bases. While both the models fix the memory access mechanism (HMN uses MIPS and SAM uses NNS), our controller works in a much more constrained setting. Moreover, our experiments suggest that the performance of SAM could be improved using a clustering-based approach as in our work, instead of tree/hash-based approaches for memory search used by SAM.

5 EXPERIMENTS
In this section, we report experiments on factoid question answering using hierarchical memory networks. Specifically, we use the SimpleQuestions dataset Bordes et al. (2015). The aim of these experiments is not to achieve state-of-the-art results on this dataset. Rather, we aim to propose and analyze various approaches to make memory networks more scalable and explore the achieved tradeoffs between speed and accuracy.

5.1 DATASET
We use SimpleQuestions (Bordes et al., 2015) which is a large scale factoid question answering dataset. SimpleQuestions consists of 108,442 natural language questions, each paired with a corresponding fact from Freebase. Each fact is a triple (subject,relation,object) and the answer to the question is always the object. The dataset is divided into training (75910), validation (10845), and test (21687) sets. Unlike Bordes et al. (2015) who additionally considered FB2M (10M facts) or FB5M (12M facts) with keyword-based heuristics for filtering most of the facts for each question, we only use SimpleQuestions, with no keyword-based heuristics. This allows us to do a direct comparison with the full softmax approach in a reasonable amount of time. Moreover, we would like to highlight that for this dataset, keyword-based filtering is a very efficient heuristic since all questions have an appropriate source entity with a matching word. Nevertheless, our goal is to design a general purpose architecture without such strong assumptions on the nature of the data.

5.2 MODEL
Let Vq be the vocabulary of all words in the natural language questions. Let Wq be a |Vq| ∗ m matrix where each row is some m dimensional embedding for a word in the question vocabulary. This matrix is initialized with random values and learned during training. Given any question, we represent it with a bag-of-words representation by summing the vector representation of each word in the question. Let q = {wi}pi=1,
h(q) = p∑ i=1 Wq[wi]
Then, to find the relevant fact from the memory M, we call the K-MIPS-based reader module with h(q) as the query. This uses Equation 3 and 4 to compute the output of the reader Rout. The reader is trained by minimizing the Negative Log Likelihood (NLL) of the correct fact.
Jθ = N∑ i=1 −log(Rout[fi])
where fi is the index of the correct fact in Wm. We are fixing the memory embeddings to the TransE (Bordes et al., 2013) embeddings and learning only the question embeddings.
This model is simpler than the one reported in (Bordes et al., 2015) so that it is esay to analyze the effect of various memory reading strategies.

5.3 TRAINING DETAILS
We trained the model with the Adam optimizer (Kingma & Ba, 2014), with a fixed learning rate of 0.001. We used mini-batches of size 128. We used 200 dimensional embeddings for the TransE entities, yielding 600 dimensional embeddings for facts by concatenating the embeddings of the subject, relation and object. We also experimented with summing the entities in the triple instead of concatenating, but we found that it was difficult for the model to differentiate facts this way. The only learnable parameters by the HMN model are the question word embeddings. The entity distribution in SimpleQuestions is extremely sparse and hence, following Bordes et al. (2015), we also add artificial questions for all the facts for which we do not have natural language questions. Unlike Bordes et al. (2015), we do not add any other additional tasks like paraphrase detection to the model, mainly to study the effect of the reader. We stopped training for all the models when the validation accuracy consistently decreased for 3 epochs.
5.4 EXACT K-MIPS IMPROVES ACCURACY
In this section, we compare the performance of the full soft attention reader and exact K-MIPS attention readers. Our goal is to verify that K-MIPS attention is in fact a valid and useful attention mechanism and see how it fares when compared to full soft attention. For K-MIPS attention, we tried K ∈ 10, 50, 100, 1000. We would like to emphasize that, at training time, along with K candidates for a particular question, we also add the K-candidates for each question in the minibatch. So the exact size of the softmax layer would be higer than K during training. In Table 1, we report the test performance of memory networks using the soft attention reader and K-MIPS attention reader. We also report the average softmax size during training. From the table, it is clear that the K-MIPS attention readers improve the performance of the network compared to soft attention reader. In fact, smaller the value of K is, better the performance. This result suggests that it is better to use a K-MIPS layer instead of softmax layer whenever possible. It is interesting to see that the convergence of the model is not slowed down due to this change in softmax computation (as shown in Figure 1).
This experiment confirms the usefulness of K-MIPS attention. However, exact K-MIPS has the same complexity as a full softmax. Hence, to scale up the training, we need more efficient forms of K-MIPS attention, which is the focus of next experiment.
Model Test Acc. Avg. Softmax Size Full-softmax 59.5 108442
10-MIPS 62.2 1290 50-MIPS 61.2 6180
100-MIPS 60.6 11928 1000-MIPS 59.6 70941 Clustering 51.5 20006 PCA-Tree 32.4 21108 WTA-Hash 40.2 20008
Table 1: Accuracy in SQ test-set and average size of memory used. 10-softmax has high performance while using only smaller amount of memory.
5.5 APPROXIMATE K-MIPS BASED LEARNING
As mentioned previously, designing faster algorithms for K-MIPS is an active area of research. Auvolat et al. (2015) compared several state-of-the-art data-dependent and data-independent methods for faster approximate K-MIPS and it was found that clustering-based MIPS performs significantly better than other approaches. However the focus of the comparison was on performance during the inference stage. In HMNs, K-MIPS must be used at both training stage and inference stages. To verify if the same trend can been seen during learning stage as well, we compared three different approaches:
Clustering: This was explained in detail in section 3.
WTA-Hash: Winner Takes All hashing (Vijayanarasimhan et al., 2014) is a hashing-based K-MIPS algorithm which also converts MIPS to MCSS by augmenting additional dimensions to the vectors. This method used n hash functions and each hash function does p different random permutations of the vector. Then the prefix constituted by the first k elements of each permuted vector is used to construct the hash for the vector.
PCA-Tree: PCA-Tree (Bachrach et al., 2014) is the state-of-the-art tree-based method, which converts MIPS to NNS by vector augmentation. It uses the principal components of the data to construct a balanced binary tree with data residing in the leaves.
For a fair comparison, we varied the hyper-parameters of each algorithm in such a way that the average speedup is approximately the same. Table 1 shows the performance of all three methods, compared to a full softmax. From the table, it is clear that the clustering-based method performs significantly better than the other two methods. However, performances are lower when compared to the performance of the full softmax.
As a next experiment, we analyze various the strategies proposed in Section 3.1 to reduce the approximation bias of clustering-based K-MIPS:
Top-K: This strategy picks the vectors in the top K clusters as candidates.
Sample-K: This strategy samples K clusters, without replacement, based on a probability distribution based on the dot product of the query with the cluster centroids. When combined with the Top-K strategy, we ignore clusters selected by the Top-k strategy for sampling.
Rand-block: This strategy divides the memory into several blocks and uniformly samples a random block as candidate.
We experimented with 1000 clusters and 2000 clusters. While comparing various training strategies, we made sure that the effective speedup is approximately the same. Memory access to facts per query for all the models is approximately 20,000, hence yielding a 5X speedup.
Results are given in Table 2. We observe that the best approach is to combine the Top-K and SampleK strategies, with Rand-block not being beneficial. Interestingly, the worst performances correspond to cases where the Sample-K strategy is ignored.

6 CONCLUSION
In this paper, we proposed a hierarchical memory network that exploits K-MIPS for its attentionbased reader. Unlike soft attention readers, K-MIPS attention reader is easily scalable to larger memories. This is achieved by organizing the memory in a hierarchical way. Experiments on the SimpleQuestions dataset demonstrate that exact K-MIPS attention is better than soft attention. However, existing state-of-the-art approximate K-MIPS techniques provide a speedup at the cost of some accuracy. Future research will investigate designing efficient dynamic K-MIPS algorithms, where the memory can be dynamically updated during training. This should reduce the approximation bias and hence improve the overall performance.
","Memory networks are neural networks with an explicit memory component that can be both read and written to by the network. The memory is often addressed in a soft way using a softmax function, making end-to-end training with backpropagation possible. However, this is not computationally scalable for applications which require the network to read from extremely large memories. On the other hand, it is well known that hard attention mechanisms based on reinforcement learning are challenging to train successfully. In this paper, we explore a form of hierarchical memory network, which can be considered as a hybrid between hard and soft attention memory networks. The memory is organized in a hierarchical structure such that reading from it is done with less computation than soft attention over a flat memory, while also being easier to train than hard attention over a flat memory. Specifically, we propose to incorporate Maximum Inner Product Search (MIPS) in the training and inference procedures for our hierarchical memory network. We explore the use of various state-of-the art approximate MIPS techniques and report results on SimpleQuestions, a challenging large scale factoid question answering task.",ICLR 2017 conference submission,False,,"I find this paper not very compelling.  The basic idea seems to be that we can put a fast neighbor searcher into a memory augmented net to make the memory lookups scalable.  However, this was precisely the point of Rae et al.    There are a  number of standardized neighbor searchers; I don't understand why the authors choose to use their own (which they do not benchmark against the standards).   Moreover, they test on a problem where there is no clear need for (vector based) fast-nn, because one can use hashing on the text.     I also find the repeated distinction between ""mips"" and ""nns"" distracting; most libraries that can do one can do the other, or inputs can be modified  to switch between the problems; indeed the authors do this when they convert to the  ""mcss"" problem.

---

This paper was reviewed by three experts. While they find interesting ideas in the manuscript, all three point to deficiencies (unconvincing results, etc) and unanimously recommend rejection.

---

1. The hierarchical memory is fixed, not learned, and there is no hierarchical in the experimental section, only one layer for softmax layer.
2. It shows the 10-mips > 100-mips > 1000-mips, does it mean 1-mips is the best one we should adopt?
3. Approximated k-mips is worse than even original method. Why does it need exact k-mips? It seems the proposed method is not robust.

---

The paper proposes an algorithm for training memory networks which have very large memories. Training such models in traditional ways, by using soft-attention mechanism over all the memory slots is not only slow, it is also harder to train due to dispersion of gradients. The paper proposes to use the k-mips algorithm over the memories to choose a subset of the memory slots over which the attention is applied. Since the cost of exact k-mips is the same as doing full attention, the authors propose to use approximate k-mips, which while faster to compute, results in inferior performance. An artifact of using k-mips is that one cannot learn the memory slots. Hence they are pre-trained and kept fixed during entire training. The experimental section shows the efficacy of using k-mips using the SimpleQuestions dataset. The exact k-mips results in the same performance as the full attention. The approximate k-mips results in deterioration in performance. The paper is quite clearly written and easy to understand. 

I think the ideas proposed in the paper are not super convincing. I have a number of issues with this paper. 

1. The k-mips algorithm forces the memories to be fixed. This to me is a rather limiting constraint, especially on problems/dataset which will require multiple hops of training to do compounded reasoning. As a results I'm not entirely sure about the usefulness of this technique. 
2. Furthermore, the exact k-mips is the sample complexity as the full attention. The only way to achieve speedup is to use approx k-mips. That, as expected, results in a significant drop in performance. 
3. The paper motivates the ideas by proposing solutions to eliminate heuristics used to prune the memories. However in Section 3.1 the authors themselves end up using multiple heuristics to make the training work. Agreed, that the used heuristics are not data dependent, but still, it feels like they are kicking the can down the road as far as heuristics are concerned. 
4. The experimental results are not very convincing. First there is no speed comparison. Second, the authors do not compare with methods other than k-mips which do fast nearest neighbor search, such as, FLANN.

---

I find this paper not very compelling.  The basic idea seems to be that we can put a fast neighbor searcher into a memory augmented net to make the memory lookups scalable.  However, this was precisely the point of Rae et al.    There are a  number of standardized neighbor searchers; I don't understand why the authors choose to use their own (which they do not benchmark against the standards).   Moreover, they test on a problem where there is no clear need for (vector based) fast-nn, because one can use hashing on the text.     I also find the repeated distinction between ""mips"" and ""nns"" distracting; most libraries that can do one can do the other, or inputs can be modified  to switch between the problems; indeed the authors do this when they convert to the  ""mcss"" problem.

---

I find this paper not very compelling.  The basic idea seems to be that we can put a fast neighbor searcher into a memory augmented net to make the memory lookups scalable.  However, this was precisely the point of Rae et al.    There are a  number of standardized neighbor searchers; I don't understand why the authors choose to use their own (which they do not benchmark against the standards).   Moreover, they test on a problem where there is no clear need for (vector based) fast-nn, because one can use hashing on the text.     I also find the repeated distinction between ""mips"" and ""nns"" distracting; most libraries that can do one can do the other, or inputs can be modified  to switch between the problems; indeed the authors do this when they convert to the  ""mcss"" problem.

---

This paper was reviewed by three experts. While they find interesting ideas in the manuscript, all three point to deficiencies (unconvincing results, etc) and unanimously recommend rejection.

---

1. The hierarchical memory is fixed, not learned, and there is no hierarchical in the experimental section, only one layer for softmax layer.
2. It shows the 10-mips > 100-mips > 1000-mips, does it mean 1-mips is the best one we should adopt?
3. Approximated k-mips is worse than even original method. Why does it need exact k-mips? It seems the proposed method is not robust.

---

The paper proposes an algorithm for training memory networks which have very large memories. Training such models in traditional ways, by using soft-attention mechanism over all the memory slots is not only slow, it is also harder to train due to dispersion of gradients. The paper proposes to use the k-mips algorithm over the memories to choose a subset of the memory slots over which the attention is applied. Since the cost of exact k-mips is the same as doing full attention, the authors propose to use approximate k-mips, which while faster to compute, results in inferior performance. An artifact of using k-mips is that one cannot learn the memory slots. Hence they are pre-trained and kept fixed during entire training. The experimental section shows the efficacy of using k-mips using the SimpleQuestions dataset. The exact k-mips results in the same performance as the full attention. The approximate k-mips results in deterioration in performance. The paper is quite clearly written and easy to understand. 

I think the ideas proposed in the paper are not super convincing. I have a number of issues with this paper. 

1. The k-mips algorithm forces the memories to be fixed. This to me is a rather limiting constraint, especially on problems/dataset which will require multiple hops of training to do compounded reasoning. As a results I'm not entirely sure about the usefulness of this technique. 
2. Furthermore, the exact k-mips is the sample complexity as the full attention. The only way to achieve speedup is to use approx k-mips. That, as expected, results in a significant drop in performance. 
3. The paper motivates the ideas by proposing solutions to eliminate heuristics used to prune the memories. However in Section 3.1 the authors themselves end up using multiple heuristics to make the training work. Agreed, that the used heuristics are not data dependent, but still, it feels like they are kicking the can down the road as far as heuristics are concerned. 
4. The experimental results are not very convincing. First there is no speed comparison. Second, the authors do not compare with methods other than k-mips which do fast nearest neighbor search, such as, FLANN.

---

I find this paper not very compelling.  The basic idea seems to be that we can put a fast neighbor searcher into a memory augmented net to make the memory lookups scalable.  However, this was precisely the point of Rae et al.    There are a  number of standardized neighbor searchers; I don't understand why the authors choose to use their own (which they do not benchmark against the standards).   Moreover, they test on a problem where there is no clear need for (vector based) fast-nn, because one can use hashing on the text.     I also find the repeated distinction between ""mips"" and ""nns"" distracting; most libraries that can do one can do the other, or inputs can be modified  to switch between the problems; indeed the authors do this when they convert to the  ""mcss"" problem.",,,,,,4.666666666666667,,,4.0,,
682,"Authors: ANNEALING GAUSSIAN, LEAKY-RELU RBM, Chun-Liang Li, Siamak Ravanbakhsh, Barnabás Póczos
Source file: 682.pdf

ABSTRACT
Restricted Boltzmann Machine (RBM) is a bipartite graphical model that is used as the building block in energy-based deep generative models. Due to its numerical stability and quantifiability of its likelihood, RBM is commonly used with Bernoulli units. Here, we consider an alternative member of the exponential family RBM with leaky rectified linear units – called leaky RBM. We first study the joint and marginal distributions of the leaky RBM under different leakiness, which leads to interesting interpretation of the leaky RBM model as truncated Gaussian distribution. We then propose a simple yet efficient method for sampling from this model, where the basic idea is to anneal the leakiness rather than the energy; – i.e., start from a fully Gaussian/Linear unit and gradually decrease the leakiness over iterations. This serves as an alternative to the annealing of the temperature parameter and enables numerical estimation of the likelihood that are more efficient and far more accurate than the commonly used annealed importance sampling (AIS). We further demonstrate that the proposed sampling algorithm enjoys relatively faster mixing than contrastive divergence algorithm, which improves the training procedure without any additional computational cost.

1 INTRODUCTION
In this paper, we are interested in deep generative models. One may naively classify these models into a family of directed deep generative models trainable by back-propagation (e.g., Kingma & Welling, 2013; Goodfellow et al., 2014), and deep energy-based models, such as deep belief network (Hinton et al., 2006) and deep Boltzmann machine (Salakhutdinov & Hinton, 2009). The building block of deep energy-based models is a bipartite graphical model called restricted Boltzmann machine (RBM). The RBM model consists of two layers, visible and hidden. The resulting graphical model which can account for higher-order interactions of the visible units (visible layer) using the hidden units (hidden layer). It also makes the inference easier that there are no interactions between the variables in each layer.
The conventional RBM uses Bernoulli units for both the hidden and visible units (Smolensky, 1986). One extension is using Gaussian visible units to model general natural images (Freund & Haussler, 1994). For hidden units, we can also generalize Bernoulli units to the exponential family (Welling et al., 2004; Ravanbakhsh et al., 2016).
Nair & Hinton (2010) propose a variation using Rectified Linear Unit (ReLU) for the hidden layer with a heuristic sampling procedure, which has promising performance in terms of reconstruction error and classification accuracy. Unfortunately, due to its lack of strict monotonicity, ReLU RBM does not fit within the framework of exponential family RBMs (Ravanbakhsh et al., 2016). Instead we study leaky-ReLU RBM (leaky RBM) in this work and address two important issues i) a better training (sampling) algorithm for ReLU RBM and; ii) a better quantification of leaky RBM –i.e., evaluation of its performance in terms of likelihood.
We study some of the fundamental properties of leaky RBM, including its joint and marginal distributions (Section 2). By analyzing these distributions, we show that the leaky RBM is a union of
truncated Gaussian distributions. In this paper, we show that training leaky RBM involves underlying positive definite constraints. Because of this, the training can diverge if these constrains are not satisfied. This is an issue that was previously ignored in ReLU RBM, as it was mainly used for pre-training rather than generative modeling.
Our contribution in this paper is three-fold: I) we systematically identify and address model constraints in leaky RBM (Section 3); II) for the training of leaky RBM, we propose a meta algorithm for sampling, which anneals leakiness during the Gibbs sampling procedure (Section 3) and empirically show that it can boost contrastive divergence with faster mixing (Section 5); III) We demonstrate the power of the proposed sampling algorithm on estimating the partition function. In particular, comparison on several benchmark datasets shows that the proposed method outperforms the conventional AIS (Salakhutdinov & Murray, 2008) in terms of efficiency and accuracy (Section 4). Moreover, we provide an incentive for using leaky RBM by showing that the leaky ReLU hidden units perform better than the Bernoulli units in terms of the model log-likelihood (Section 4).

2 RESTRICTED BOLTZMANN MACHINE AND RELU
The Boltzmann distribution is defined as p(x) = e−E(x)/Z where Z = ∑ x e −E(x) is the partition function. Restricted Boltzmann Machine (RBM) is a Boltzmann distribution with a bipartite structure It is also the building block for many deep models (e.g., Hinton et al., 2006; Salakhutdinov & Hinton, 2009; Lee et al., 2009), which are widely used in numerous applications (Bengio, 2009). The conventional Bernoulli RBM, models the joint probability p(v, h) for the visible units v ∈ [0, 1]I and the hidden units h ∈ [0, 1]J as p(v, h) ∝ exp(−E(v, h)), where E(v, h) = a>v − v>Wh + b>h. The parameters are a ∈ RI , b ∈ RJ and W ∈ RI×J . We can derive the conditional probabilities as
p(vi = 1|h) = σ  J∑ j=1 Wijhj + ai  and p(hj = 1|v) = σ( I∑ i=1 Wijvi + bj ) , (1)
where σ(x) = (1 + e−x)−1 is the sigmoid function.
One extension of Bernoulli RBM is replacing the binary visible units by linear units v ∈ RI with independent Gaussian noise. The energy function in this case is given by
E(v, h) = I∑ i=1 (vi − ai)2 2σ2i − I∑ i=1 J∑ j=1 vi σi Wijhj + b >h.
To simplify the notation, we assume a normalized data so that ai and σi is no longer required. The energy function is accordingly simplified to E(v, h) = ‖v‖ 2
2 − v >Wh + b>h (Note that the
elimination does not influence the discussion and one can easily extend all the results in this paper to the model that includes ai and σi.).
The conditional distributions are as follows:
p(vi|h) = N  J∑ j=1 Wijhj , 1  and p(hj = 1|v) = σ( I∑ i=1 Wijvi + bj ) , (2)
where N (µ, V ) is a Gaussian distribution with mean µ and variance V . To simplify the notation, in the following we define ηj = ∑I i=1Wijvi + bj – that is ηj is the input to the j
th hidden layer neuron and similarly define νi = ∑J j=1Wijhj + ai. Using this notation the conditionals in the (2) are p(vi|νi) = N (νi, 1) and p(hj = 1|ηj) = σ(ηj).

2.1 RELU RBM WITH CONTINUOUS VISIBLE UNITS
From (1) and (2), we can see that the mean of the p(hj |v) is the nonlinearity of the hidden unit at ηj = ∑I i=1Wijvi + bj – e.g., mean of the Bernoulli unit is the sigmoid function. From this perspective, we can extend the sigmoid function to other functions and thus allow RBM to have more expressive power (Ravanbakhsh et al., 2016). In particular, it would be interesting to use rectified linear unit (ReLU) nonlinearity, f(ηj) = max(0, ηj), for generative modeling.
Nair & Hinton (2010) use an RBM with visible Gaussian unit and ReLU hidden activation functions for pretraining. They suggest sampling from max(0, ηj+N (0, σ(ηj)) for conditional sampling from the hidden units (compare to (2)). However, this sampling heuristic does not suggest the parametric form of the joint ReLU-Gaussian distribution. This also means we cannot evaluate it using methods such as Annealed Importance Sampling that require access to this parametric form. In fact, only strictly monotonic activation functions can derive feasible joint and conditional distributions in the exponential familly RBM and ReLU is not strictly monotonic Ravanbakhsh et al. (2016). Similar activation functions that are monotonic are Softplus, f(ηj) = log(1 + eηj ) and leaky ReLU (Maas et al., 2013), defined as f(ηj) = max(cηj , ηj), where c ∈ (0, 1) is the leakiness parameter. In contrast to the ReLU RBM the joint parametric form of these two distributions are available. However, the energy (logarithm of the joint probability) in the case of Softplus activation function contains a polylogarithmic term that requires evaluation of an infinite series; see Table 1 in Ravanbakhsh et al. (2016). For this reason, here we focus on Leaky-ReLU activation function.
By Ravanbakhsh et al. (2016), the conditional probability of the activation, assuming the nonlinearity f(ηj), is generally defined as p(hj |v) = exp (−Df (ηj‖hj) + g(hj)), where Df (ηj‖hj) is the Bregman Divergence associated with f, and g(hj) is the base (or carrier) measure in the exponential family which ensures the distribution is well-defined. The Bergman divergence, for strictly monotonic function f , is Df (ηj‖hj) = −ηjhj + F (ηj) + F ∗(hj), where F with ddηj F (ηj) = f(ηj) is the anti-derivative (integral) of f and F ∗ is the anti-derivative of f−1 (i.e., f−1(f(η)) = η); Note that due to the strict monotonicity of f , f−1 is well-defined, and F and F ∗ are commonly referred to as conjugate duals.
Considering the leaky ReLU activation function f(η) = max(cη, η), using this formalism, the conditional distributions of hidden units in the leaky RBM simplifies to (see Appendix A.1 for details)
p(hj |v) = { N (ηj , 1), if ηj > 0 N (cηj , c), if ηj ≤ 0.
(3)
Since the visible units uses the identity function, the corresponding conditional distribution is a Gaussian1
p(vi|h) = N  J∑ j=1 Wijhj , 1  , (4) Having these two conditional distributions is enough for training a leaky RBM model using contrastive divergence (Hinton, 2002) or some other alternatives (e.g., Tieleman, 2008; Tieleman & Hinton, 2009).

3 TRAINING AND SAMPLING FROM LEAKY RBM
Given the conditional distributions p(v|h) and p(h|v), the joint distribution p(v, h) from the general treatment for MRF model is (Yang et al., 2012; Ravanbakhsh et al., 2016)
p(v, h) ∝ exp v>Wh− I∑ i=1 (F̃ ∗(vi) + g(vi))− J∑ j=1 (F ∗(hj) + g(hj))  , (5) where F̃ ∗(vi) and F ∗(hj) are anti-derivatives of the inverses of the activation functions f̃(vi) and f(hj) for visible units vi and hidden units hj , respectively (see Section 2.1). Assuming f(ηj) = max(cηj , c) and f̃(νi) = νi in leaky-ReLU RBM, the joint distribution above becomes (see Appendix A.2 for details)
p(v, h) ∝ exp v>Wh− ‖v‖2 2 − ∑ ηj>0 ( h2j 2 + log √ 2π ) − ∑ ηj≤0 ( h2j 2c + log √ 2cπ ) + b>h  , 1which can also be written as p(vi|h) = exp ( −Df̃ (νi‖vi) + g(vi) ) , where νi = ∑ j=1Wijhj and
f̃(νi) = νi and Df̃ (νi‖vi) = (νi − vi) 2 and g(vi) = − log
√ 2π.
and the corresponding visible marginal distribution is
p(v) ∝ exp −1 2 v> I −∑ ηj>0 WjW > j − c ∑ ηj≤0 WjW > j  v + ∑ ηj>0 bjW > j v + c ∑ ηj≤0 bjW > j v  . (6)
where Wj is the j-th column of W .

3.1 LEAKY RBM AS UNION OF TRUNCATED GAUSSIAN DISTRIBUTIONS
From (6) we see that the marginal probability is determined by the affine constraints ηj > 0 or ηj ≤ 0 for all hidden units j. By combinatorics, these constraints divide RI (the visible domain) into at most M = ∑I i=1 ( J i ) convex regions R1, · · ·RM . An example with I = 2 and J = 3 is shown in Figure 1. If I > J , then we have at most 2J regions.
We discuss the two types of these regions. For bounded regions, such as R1 in Figure 1, the integration of (6) is also bounded, which results in a valid distribution. Before we discuss the unbounded cases, we define Ω = I − ∑J j=1 αjWjW > j , where αj = 1ηj>0 + c1ηj≤0. For the unbounded region, if Ω ∈ RI×I is a positive definite (PD) matrix, then the probability density is proportional to a multivariate Gaussian distribution with mean µ = Ω−1 (∑J j=1 αjbjWj ) and precision matrix Ω (covariance matrix Ω−1) but over an affine-constrained region. Therefore, the distribution of each unbounded region can be treated as a truncated Gaussian distribution. The marginal distrubution can be treated as a union of truncated Gaussain distribution. Note that leaky RBM is different from Su et al. (2017), which use single truncated Gaussian distribution to model joint (conditional) distributions and require approximated and more complicated sampling algorithms for truncated Gaussian distribution, while leaky RBM only requires to sample from Gaussian distributions.
On the other hand, if Ω is not PD, and the region Ri contains the eigenvectors with negative eigenvalues of Ω, the integration of (6) over Ri is divergent (infinite), which can not result in a valid probability distribution. In practice, with this type of parameter, when we do Gibbs sampling on the conditional distributions, the sampling will diverge. However, it is unfeasible to check exponentially many regions for each gradient update. Theorem 1. If I −WW> is positive definite, then I − ∑ j αjWjW > j is also positive definite, for all αj ∈ [0, 1].
The proof is shown in Appendix 1. From Theorem 1 we can see that if the constraint I −WW> is PD, then one can guarantee that the distribution of every region is a valid truncated Gaussian distribution. Therefore, we introduce the following projection step for each W after the gradient update.
argmin W̃
‖W − W̃‖2F
s.t. I − W̃W̃> 0 (7)
Theorem 2. The above projection step (7) can be done by shrinking the singular values to be less than 1.
The proof is shown in Appendix C. The training algorithm of the leaky RBM is shown in Algorithm 1. By using the projection step (7), we could treat the leaky RBM as the union of truncated Gaussian distributions, which uses weight vectors to divide the space of visible units into several regions and use a truncated Gaussian distribution to model each region. Note that the leaky RBM model is different from Su et al. (2016), which uses a truncated Gaussian distribution to model the conditional distribution p(h|v) instead of the marginal distribution. The empirical study about the divergent values and the necessity of the projection step is shown in Appendix D. Without the projection step, when we run Gibbs sampling for several iterations from the model, the sampled values will diverge because the model does not have a valid marginal distribution p(v). It also implies that we cannot train leaky RBM with larger CD steps without projection, which would result in divergent gradients. The detailed discussion is shown in Appendix D.
Algorithm 1 Training Leaky RBM for t = 1, . . . , T do
Estimate gradient gθ by CD or other algorithms with (13) and (4), where θ = {W,a, b}. θ(t) ← θ(t−1) + ηgθ. Project W (t) by (7).
end for

3.2 SAMPLING FROM LEAKY-RELU RBM
Gibbs sampling is the core procedure for RBM, including training, inference, and estimating the partition function (Fischer & Igel, 2012; Tieleman, 2008; Salakhutdinov & Murray, 2008). For every task, we start from randomly initializing v by an arbitrary distribution q, and iteratively sample from the conditional distributions. Gibbs sampling guarantees the procedure result in the stationary distribution in the long run for any initialized distribution q. However, if q is close to the target distribution p, it can significantly shorten the number of iterations to achieve the stationary distribution. If we set the leakiness c to be 1, then (6) becomes a simple multivariate Gaussian distribution N ( (I −WW>)−1Wb, (I −WW>)−1 ) , which can be easily sampled without Gibbs sampling. Also, the projection step (7) guarantees it is a valid Gaussian distribution. Then we decrease the leakiness with a small , and use samples from the multivariate Gaussian distribution when c = 1 as the initialization to do Gibbs sampling. Note that the distribution of each region is a truncated Gaussian distribution. When we only decrease the leakiness with a small amount, the resulted distribution is a “similar” truncated Gaussian distribution with more concentrated density. From this observation, we could expect the original multivariate Gaussian distribution serves as a good initialization. The one-dimensional example is shown in Figure 2. We then repeat this procedure until we reach the target leakiness. The algorithm can be seen as annealing the leakiness during the Gibbs sampling procedure. The meta algorithm is shown in Algorithm 2. Next, we show the proposed sampling algorithm can help both the partition function estimation and the training of leaky RBM.
Algorithm 2 Meta Algorithm for Sampling from Leaky RBM. Sample v from N ( (I −WW>)−1Wb, (I −WW>)−1 ) = (1− c)/T and c′ = 1 for t = 1, . . . , T do
Decrease c′ = c′ − and perform Gibbs sampling by using (13) and (4) with leakiness c′ end for

4 PARTITION FUNCTION ESTIMATION
It is known that estimating the partition function of RBM is intractable (Salakhutdinov & Murray, 2008). Existing approaches, including Salakhutdinov & Murray (2008); Grosse et al. (2013); Liu et al. (2015); Carlson et al. (2016) focus on using sampling to approximate the partition function of the conventional Bernoulli RBM instead of the RBM with Gaussian visible units and non-Bernoulli hidden units. In this paper, we focus on extending the classic annealed importance sampling (AIS) algorithm (Salakhutdinov & Murray, 2008) to leaky RBM.
Assuming that we want to estimate the partition function Z of p(v) with p(v) = p∗(v)/Z and p∗(v) ∝ ∑ h exp(−E(v, h)), Salakhutdinov & Murray (2008) start from a initial distribution
p0(v) ∝ ∑ h exp(−E0(v, h)), where computing the partition Z0 of p0(v) is tractable and we can draw samples from p0(v). They then use the “geometric path” to anneal the intermediate distribution as pk(v) ∝ p∗k(v) = ∑ h exp (−βkE0(v, h)− (1− βk)E(v, h)), where they grid βk from 1 to 0. If we let β0 = 1, we can draw samples vk from pk(v) by using samples vk−1 from pk−1(v) for k ≥ 1 via Gibbs sampling. The partition function is then estimated via Z = Z0M ∑M i=1 ω (i), where
ω(i) = p∗1(v (i) 0 )
p∗0(v (i) 0 )
p∗2(v (i) 1 ) p∗1(v (i) 1 ) · · · p∗K−1(v (i) K−2) p∗K−2(v (i) K−2) p∗K(v (i) K−1) p∗K−1(v (i) K−1) , and βK = 0.
Salakhutdinov & Murray (2008) use the initial distribution with independent visible units and without hidden units. We consider application of AIS to the leaky-ReLU case with E0(v, h) = ‖v‖2 2 , which results in a multivariate Gaussian distribution p0(v). Compared with the meta algorithm shown in Algorithm 2 which anneals between leakiness, AIS anneals between energy functions.

4.1 STUDY ON TOY EXAMPLES
As we discussed in Section 3.1, leaky RBM with J hidden units is a union of 2J truncated Gaussian distributions. Here we perform a study on the leaky RBM with a small number hidden units. Since in this example the number of hidden units is small, we can integrate out all possible configurations of h. However, integrating a truncated Gaussian distribution with general affine constraints does not have analytical solutions, and several approximations have been developed (e.g., Pakman & Paninski, 2014). To compare our results with the exact partition function, we consider a special case that has the following form:
p(v) ∝ exp −1 2 v> I −∑ ηj>0 WjW > j − c ∑ ηj≤0 WjW > j  v  . (8)
Compared to (6), it is equivalent to the setting where b = 0. Geometrically, everyWj passes through the origin. We further put the additional constraint Wi ⊥ Wj ,∀i 6= j. Therefore. we divide the whole space into 2J equally-sized regions. A three dimensional example is shown in Figure 3. Then the partition function of this special case has the analytical form
Z = 1
2J ∑ αj∈{1,c},∀j (2π)− I 2 ∣∣∣∣∣∣∣ I − J∑ j=1 αjWjW > j − 12 ∣∣∣∣∣∣∣ .
We randomly initialize W and use SVD to make columns orthogonal. Also, we scale ‖Wj‖ to satisfy I −WW> 0. The leakiness parameter is set to be 0.01. For Salakhutdinov & Murray (2008) (AIS-Energy), we use 105 particles with 105 intermediate distributions. For the proposed method (AIS-Leaky), we use only 104 particles with 103 intermediate distributions. In this small problem we study the cases when the model has 5, 10, 20 and 30 hidden units and 3072 visible units. The true log partition function logZ is shown in Table 1 and the difference between logZ and the estimates given by the two algorithms are shown in Table 2.
From Table 1, we observe that AIS-Leaky has significantly better and more stable estimations than AIS-Energy especially and this gap increases as we increase the number of hidden units. AIS-Leaky achieves this with orders magnitude reduced computation –e.g., here it uses ∼.1% of resources used by conventional AIS. For example, when we increase J from 5 to 30, the bias (difference) of AIS-Leaky only increases from 0.02 to 0.13; however, the bias of AIS-Energy increases from 1.76 to 9.6. We further study the implicit connection between the proposed AIS-Leaky and AIS-Energy in Appendix E, which shows AIS-Leaky is a special case of AIS-Energy under certain conditions.

4.2 COMPARISON BETWEEN LEAKY-RELU RBM AND BERNOULLI-GAUSSIAN RBM
It is known that the reconstruction error is not a proper approximation of the likelihood (Hinton, 2012). One commonly adopted way to compare generative models is to sample from the model, and visualize the images to check the quality. However, Theis et al. (2016) show the better visualization does not imply better likelihood. Also, the single layer model cannot adequately model the complicated natural images (the result for Bernoulli-Gaussian RBM has been shown in Ranzato & Hinton (2010)), which makes the visualization comparison difficult (Appendix F has few visualization results).
Fortunately, our accurate estimate of the partition function for leaky RBM can produce a reliable quantitative estimate of the representation power of leaky RBM. We compare the BernoulliGaussian RBM2, which has Bernoulli hidden units and Gaussian visible units. We trained both models with CD-203 and momentum. For both model, we all used 500 hidden units. We initialized W by sampling from Unif(0, 0.01), a = 0, b = 0 and σ = 1. The momentum parameter was 0.9 and the batch size was set to 100. We tuned the learning rate between 10−1 and 10−6. We studied two benchmark data sets, including CIFAR10 and SVHN. The data was normalized to have zero mean and standard deviation of 1 for each pixel. The results of the log-likelihood are reported in Table 3.
From Table 3, leaky RBM outperforms Bernoulli-Gaussian RBM significantly. The unsatisfactory performance of Bernoulli-Gaussian RBM may be in part due to the optimization procedure. If we tune the decay schedule of the learning-rate for each dataset in an ad-hoc way, we observe the performance of Bernoulli-Gaussian RBM can be improved by ∼ 300 nats for both datasets. Also, increasing CD-steps brings slight improvement. The other possibility is the bad mixing during the CD iterations. The advanced algorithms Tieleman (2008); Tieleman & Hinton (2009) may help. Although Nair & Hinton (2010) demonstrate the power of ReLU in terms of reconstruction error and classification accuracy, it does not imply its superior generative capability. Our study confirms leaky RBM could have much better generative performance compared to Bernoulli-Gaussian RBM.

5 BETTER MIXING BY ANNEALING LEAKINESS
In this section, we show the idea of annealing between leakiness benefit the mixing in Gibbs sampling in other settings. A common procedure for comparison of sampling methods for RBM is through visualization. Here, we are interested in more quantitative metrics and the practical benefits of improved sampling. For this, we consider optimization performance as the evaluation metric.
The gradient of the log-likelihood function L(θ|vdata) of general RBM models is
∂L(θ|vdata) ∂θ = Eh|vdata
[ ∂E(v, h)
∂θ
] − Ev,h [ ∂E(v, h)
∂θ
] . (9)
Since the second expectation in (9) is usually intractable, different approximation algorithms are used (Fischer & Igel, 2012).
2Our GPU implementation with gnumpy and cudamat can reproduce the results of http://www.cs.toronto.edu/ tang/code/GaussianRBM.m
3CD-n means that contrastive divergence was run for n steps
In this section, we compare two gradient approximation procedures. The baselines are the conventional contrastive divergence (CD) (Hinton, 2002) and persistent contrastive divergence (Tieleman, 2008) (PCD). The second method is using Algorithm 2 (Leaky) with the same number of mixing steps as CD. The experiment setup is the same as that of Section 4.
The results are shown in Figure 4. The proposed sampling procedure is slightly better than typical CD steps. The reason is we only anneals the leakiness for 20 steps. To get accurate estimation requires thousands of steps as shown in Section 4 when we estimate the partition function. Therefore, the estimated gradient is still inaccurate. However, it still outperforms the conventional CD algorithm. On the other hand, unlike the binary RBM case shown in Tieleman (2008), PCD does not outperform CD with 20 mixing steps for leaky RBM.
The drawback of Algorithm 2 is that sampling v from N ( (I −WW>)−1Wb, (I −WW>)−1 ) requires computing mean, covariance and the Cholesky decomposition of the covariance matrix in every iteration, which are computationally expensive. We study a mixture algorithm by combining CD and the idea of annealing leakiness. The mixture algorithm replaces the sampling from N ( (I −WW>)−1Wb, (I −WW>)−1 ) with sampling from the empirical data distribution. The resulted mix algorithm is almost the same as CD algorithm while it anneals the leakiness over the iterations as Algorithm 2. The results of the mix algorithm is also shown in Figure 4.
The mix algorithm is slightly worse than the original leaky algorithm, but it also outperforms the conventional CD algorithm without additional computation cost. The comparison in terms of CPU time is shown in Appendix F. Annealing the leakiness helps the mix algorithm explore different modes of the distribution, thereby improves the training. The idea could also be combined with more advanced algorithms (Tieleman, 2008; Tieleman & Hinton, 2009)4.

6 CONCLUSION
In this paper, we study the properties of the exponential family distribution produced by leaky RBM. This study relates the leaky RBM model and truncated Gaussian distribution and reveals an underlying positive definite constraint of training leaky RBM. We further proposed a meta sampling algorithm, which anneals between leakiness during the Gibbs sampling procedure. We first demonstrate the proposed sampling algorithm is significantly more effective and efficient in estimating the partition function than the conventional AIS algorithm. Second, we show that the proposed sampling algorithm has comparatively better mixing properties (compared to CD). A few direction are worth further study; in particular we are investigating on speeding up the naive projection step; either using the barrier function as shown in Hsieh et al. (2011) or by eliminating the need for projection by artificially bounding the domain via additional constraints.
4We studied the PCD extension of the proposed sampling algorithm. However, the performance is not as stable as CD.

A DERIVATION OF LEAKY (RELU) RBM
A.1 CONDITIONAL DISTRIBUTIONS
For leaky RBM, the activation function of hidden units is defined as f(ηj) = max(cηj , ηj), where c ∈ (0, 1) and ηj = ∑I i=1Wijvi + bj . The inverse function of f is f
−1(hj) = min(hj , hj/c). Therefore, the anti-derivatives are
F (ηj) =
{ 1 2η 2 j , if ηj > 0
c 2η 2 j , else,
(10)
and
F ∗(hj) =
{ 1 2h 2 j , if ηj > 0
1 2ch 2 j , else.
(11)
The activation function of Gaussian visible units can be treated as the linear unit f̃(νi) = νi, where νi = ∑J j=1Wijhj . Following the similar steps for deriving F and F
∗, we get the anti-derivatives F̃ (νi) = 1 2ν 2 i and F̃ ∗(vi) = 1 2v 2 i .
From Ravanbakhsh et al. (2016), the conditional distribution is defined as
p(hj |ηj) = exp (−ηjhj + F (ηj) + F ∗(hj)) (12)
By plugging F and F ∗ into (12), we get the conditional distribution for leaky RBM
p(hj |v) = { N (ηj , 1)with g(hj) = − log( √ 2π), if ηj > 0
N (cηj , c)with g(hj) = − log( √ 2cπ), if ηj ≤ 0. (13)
Similarly, we have p(vi|νi) = N (νi, 1) with g(vi) = − log( √ 2π).
A.2 JOINT AND MARGINAL DISTRIBUTIONS
Given the conditional distributions p(v|h) and p(h|v), the joint distribution p(v, h) from the general treatment for MRF model given by Yang et al. (2012) is
p(v, h) ∝ exp v>Wh− I∑ i=1 (F̃ ∗(vi) + g(vi))− J∑ j=1 (F ∗(hj) + g(hj))  , (14) By plugging F ∗, F̃ ∗ and g from Section A.1 into (14), we have
p(v, h) ∝ exp v>Wh− ‖v‖2 2 − ∑ ηj>0 ( h2j 2 + log √ 2π ) − ∑ ηj≤0 ( h2j 2c + log √ 2cπ ) + b>h  , Then the marginal distribution is
p(v) ∝ ∫ h p(v, h)dh
∝ ∫ h exp ( −‖v‖ 2 2 ) ∏ ηj>0 exp ( − h2j 2 + ηjhj − log √ 2π ) ∏ ηj≤0 ( − h2j 2c + hjηj − log √ 2cπ ) dh
∝ exp ( −‖v‖ 2
2 ) ∏ ηj>0 exp ( η2j 2 ) ∏ ηj≤0 ( cη2j 2 )
∝ exp −1 2 v> I −∑ ηj>0 WjW > j − c ∑ ηj≤0 WjW > j  v + ∑ ηj>0 bjW > j v + c ∑ ηj≤0 bjW > j v  .

B PROOF OF THEOREM 1
Proof. Since WW>− ∑ j αjWjWj = ∑ j(1−αj)WjW>j 0, we have WW> ∑ j αjWjWj .
Therefore, I − ∑ j αjWjW > j I −WW> 0.

C PROOF OF THEOREM 2
Proof. Let the SVD decomposition of W and W̃ as W = USV > and W̃ = Ũ S̃Ṽ >. Then we have
‖W − W̃‖2F = ‖USV > − Ũ S̃Ṽ >‖2F ≥ I∑ i=1 (Sii − S̃ii)2, (15)
and the constraint I − W̃W̃> 0 can be rewritten as 0 ≤ S̃ii ≤ 1,∀i. The transformed problem has a Lasso-like formulation and we can solve it by S̃ii = min(Sii, 1) (Parikh & Boyd, 2014). Also, the lower bound ∑I i=1(Sii − S̃ii)2 in (15) becomes a tight bound when we set Ũ = U and Ṽ = V , which completes the proof.

D NECESSITY OF THE PROJECTION STEP
We conduct a short comparison to demonstrate the projection step is necessary for the leaky RBM on generative tasks. We train two leaky RBM as follows. The first model is trained by the same setting in Section 4. We use the convergence of log likelihood as the stopping criteria. The second model is trained by CD-1 with weight decay and without the projection step. We stop the training when the reconstruction error is less then 10−2. After we train these two models, we run Gibbs sampling with 1000 independent chains for several steps and output the average value of the visible units. Note that the visible units are normalized to zero mean. The results on SVHN and CIFAR10 are shown in Figure 5.
From Figure 5, the model trained by weight decay without projection step is suffered by the problem of the diverged values. It confirms the study shown in Section 3.1. It also implies that we cannot
train leaky RBM with larger CD steps when we do not do projection; otherwise, we would have the diverged gradients. Therefore, the projection is necessary for training leaky RBM for the generative purpose. However, we also oberseve that the projection step is not necessary for the classification and reconstruction tasks. he reason may be the independency of different evaluation criteria (Hinton, 2012; Theis et al., 2016) or other implicit reasons to be studied.

E EQUIVALENCE BETWEEN ANNEALING THE ENERGY AND LEAKINESS
We analyze the performance gap between AIS-Leaky and AIS-Energy. One major difference is the initial distribution. The intermediate marginal distribution of AIS-Energy has the following form:
pk(v) ∝ exp −1 2 v> I − (1− βk) ∑ ηj>0 WjW > j − (1− βk)c ∑ ηj≤0 WjW > j  v  . (16)
Here we eliminated the bias terms b for simplicity. Compared with Algorithm 2, (16) not only anneals the leakiness (1 − βk)c ∑ ηj≤0WjW > j when ηj ≤ 0, but also in the case (1 −
βk) ∑ ηj>0 WjW > j when ηj > 0, which brings more bias to the estimation. In other words, AIS-Leaky is a one-sided leakiness annealing while AIS-Energy is a two-sided leakiness annealing method.
To address the higher bias problem of AIS-Energy, we replace the initial distribution with the one used in Algorithm 2. By elementary calculation, the marginal distribution becomes
pk(v) ∝ exp −1 2 v> I −∑ ηj>0 WjW > j − (βk + (1− βk)c) ∑ ηj≤0 WjW > j  v  , (17)
which recovers the proposed Algorithm 2. From this analysis, we understand AIS-Leaky is a special case of conventional AIS-Energy with better initialization inspired by the study in Section 3. Also, by this connection between AIS-Energy and AIS-Leaky, we note that AIS-Leaky can be combined with other extensions of AIS (Grosse et al., 2013; Burda et al., 2015) as well.

F MORE EXPERIMENTAL RESULTS FOR SAMPLING
F.1 SAMPLED IMAGES
We show the sampled images from leaky RBM train on CIFAR10 and SVHN datasets. We randomly initialize 20 chains and run Gibbs sampling for 1000 iterations. The sampled results are shown in Figure 6 The results shows that single layer RBM does not adequately model CIFAR10 and SVHN
when compared to multilayer models. The similar results for single layer Bernoulli-Gaussian RBM from Ranzato & Hinton (2010) (in gray scale) is shown in Figure 7. Therefore, we instead focused on quantitative evaluation of the log-likelihood in Table 3.
F.2 COMPUTATIONAL TIME BETWEEN DIFFERENT SAMPLING STRATEGIES
The comparison in terms of CPU time of different sampling algorithms discussed in Section 5 is shown in Figure 8. Please note that the complexity of CD and Mix are the almost the same. Mix only need a few more constant time steps which can be ignored compared with sampling steps. Leaky is more time-consuming because of computing and decomposing the covariance matrix as we discussed in Section 5. We also report the execution time of each step of algorithms in Table 4.
F.3 STUDY ON RELU-BERNOULLI RBM
We study the idea of annealing leakiness on the RBM model with leaky ReLU hidden units and Bernoulli visible units. We create the toy dataset with 20, 25 and 30 visible units as shown in Figure 9. The small datasets allow exact computation of the partition function. For each dataset, we sample 60,000 images for training and 10,000 images for testing. We use 100 hidden units and PCD to train the model. The log likelihood results are shown in Table 5.
Compared to the Gaussian visible units case we study in Section 3, where p(v) is a multi-variate Gaussian distribution when c = 1, the partition function of p(v) in ReLU-Bernoulli when c = 1 does not have the analytical form. Therefore, we do the following two-stage alternative. We first run the standard AIS algorithm, which anneals the energy, to the distribution with leakiness c = 1. We then change to anneals the leakiness from 1 to the target value. For the typical AIS algorithm (AIS-Energy), we use 104 chains with 2 × 104 intermediate distributions. For the proposed twostaged algorithm (AIS-Leaky), we use 104 chains with 104 intermediate distributions for annealing to c = 1 and the other 104 distributions for annealing the leakiness. The results are shown in Table 6.
In Table 6, the standard AIS algorithm (AIS-Energy) has unsatisfactory performance. We show the performance of AIS for estimating the partition function of models with different leakiness on Toy20. We use the 104 independent chains and 2 × 104 intermediate distributions. The results are shown in Table 7. From Table 7, we observe that the AIS performances worse when the leakiness is closer to 0. Although we observed that increasing chains and intermediate distributions could improve the performance, but the improvements are limited. The study demonstrates when the
(a) I = 20
(b) I = 25
non-linearity of the distribution increases (the leakiness value c decreases), the standard AIS cannot effectively estimate the partition function within feasible computational time. On the other hand, it also confirm the proposed idea, annealing the leakiness, can serve as an effective building block for algorithms without enhancing the algorithm complexity. Note that the unsatisfactory performance of AIS may be addressed by Grosse et al. (2013). From Appendix E, the two-stage algorithm used here can also be improved by applying Grosse et al. (2013).
F.3.1 MNIST AND CALTECH DATASETS
We study MNIST and Caltech 101 Silhouettes datasets with 500 hidden units and train the model with CD-25. The results are shown in Table 8 and Table 9. The leaky RBM is better than conventional Bernoulli RBM and some deep models on MNIST data. Although leaky RBM deos not outperform Su et al. (2017), but it enjoys the advantage of the simpler sampling procedure (Gaussian distribution vs truncated Gaussian distribution) in the binary visible unit case.
","Restricted Boltzmann Machine (RBM) is a bipartite graphical model that is used as the building block in energy-based deep generative models. Due to its numerical stability and quantifiability of its likelihood, RBM is commonly used with Bernoulli units. Here, we consider an alternative member of the exponential family RBM with leaky rectified linear units – called leaky RBM. We first study the joint and marginal distributions of the leaky RBM under different leakiness, which leads to interesting interpretation of the leaky RBM model as truncated Gaussian distribution. We then propose a simple yet efficient method for sampling from this model, where the basic idea is to anneal the leakiness rather than the energy; – i.e., start from a fully Gaussian/Linear unit and gradually decrease the leakiness over iterations. This serves as an alternative to the annealing of the temperature parameter and enables numerical estimation of the likelihood that are more efficient and far more accurate than the commonly used annealed importance sampling (AIS). We further demonstrate that the proposed sampling algorithm enjoys relatively faster mixing than contrastive divergence algorithm, which improves the training procedure without any additional computational cost.",ICLR 2017 conference submission,False,,"The authors proposed to use leaky rectified linear units replacing binary units in Gaussian RBM.  A sampling method was presented to train the leaky-ReLU RBM. In the experimental section, AIS estimated likelihood on Cifar10 and SVHN were reported.

 It's interesting for trying different nonlinear hidden units for RBM. However, there are some concerns for the current work.
 1. The author did not explain why the proposed sampling method (Alg. 2) is correct. And the additional computation cost (the inner loop and the projection) should be discussed.
 2. The results (both the resulting likelihood and the generative samples) of Gaussian RBM are much worse than what we have experienced. It seems that the Gaussian RBM were not trained properly.
 3. The representation learned from a good generative model often helps the classification task when there are fewer label samples. Gaussian RBM works well for texture synthesis tasks in which mixing is an important issue. The authors are encouraged to do more experiments in these two direction.

---

This paper identifies a joint distribution for an RBM variant based on leaky-ReLU activations. It also proposes using a sequence of distributions, both as an annealing-based training method, or to estimate log(Z) with AIS.
 
 This paper was borderline. While there is an interesting idea, the reviewers weren't generally as excited by the work as for other papers. One limitation is that unit-variance Gaussian RBMs aren't a strong baseline for comparison, although that is the focus of the main body of the paper. An update to the paper has results for binary visibles in an appendix, although I'm not sure exactly what was done, if the results are comparable, or if there is a large cost of projection here.

---

We thank all reviewers for their careful reading of the paper and constructive feedback. We have extensively revised the paper to address these requests for clarifications and experiments. Here, we explain some of these and point to related updates in our revision. Please check the individual replies below the reviews.

---

The authors proposed to use leaky rectified linear units replacing binary units in Gaussian RBM.  A sampling method was presented to train the leaky-ReLU RBM. In the experimental section, AIS estimated likelihood on Cifar10 and SVHN were reported.

 It's interesting for trying different nonlinear hidden units for RBM. However, there are some concerns for the current work.
 1. The author did not explain why the proposed sampling method (Alg. 2) is correct. And the additional computation cost (the inner loop and the projection) should be discussed.
 2. The results (both the resulting likelihood and the generative samples) of Gaussian RBM are much worse than what we have experienced. It seems that the Gaussian RBM were not trained properly.
 3. The representation learned from a good generative model often helps the classification task when there are fewer label samples. Gaussian RBM works well for texture synthesis tasks in which mixing is an important issue. The authors are encouraged to do more experiments in these two direction.

---

Based on previous work such as the stepped sigmoid units and ReLU hidden units for discriminatively trained supervised models, a Leaky-ReLU model is proposed for generative learning.

Pro: what is interesting is that unlike the traditional way of first defining an energy function and then deriving the conditional distributions, this paper propose the forms of the conditional first and then derive the energy function. However this general formulation is not novel to this paper, but was generalized to exponential family GLMs earlier.

Con: 
Because of the focus on specifying the conditionals, the joint pdf and the marginal p(v) becomes complicated and hard to compute.

On the experiments, it would been nice to see a RBM with binary visbles and leaky ReLu for hiddens. This would demonstrate the superiority of the leaky ReLU hidden units. In addition, there are more results on binary MNIST modeling with which the authors can compare the results to. While the authors is correct that the annealing distribution is no longer Gaussian, perhaps CD-25 or (Faast) PCD experiments can be run to compare agains the baseline RBM trained using (Fast) PCD.

This paper is interesting as it combines new hidden function with the easiness of annealed AIS sampling, However, the baseline comparisons to Stepped Sigmoid Units (Nair &Hinton) or other models like the spike-and-slab RBMs (and others) are missing, without those comparisons, it is hard to tell whether leaky ReLU RBMs are better even in continuous visible domain.

---

The authors propose a novel energy-function for RBMs, using the leaky relu max(cx, x) activation function for the hidden-units. Analogous to ReLU units in feed-forward networks, these leaky relu RBMs split the input space into a combinatorial number of regions, where each region defines p(v) as a truncated Gaussian. A further contribution of the paper is in proposing a novel sampling scheme for the leaky RBM: one can run a much shorter Markov chain by initializing it from a sample of the leaky RBM with c=1 (which yields a standard multi-variate normal over the visibles) and then slowly annealing c. In low-dimension a similar scheme is shown to outperform AIS for estimating the partition function. Experiments are performed on both CIFAR-10 and SVHN.

This is an interesting paper which I believe would be of interest to the ICLR community. The theoretical contributions are strong: the authors not only introduce a proper energy formulation of ReLU RBMs, but also a novel sampling mechanism and an improvement on AIS for estimating their partition function. 

Unfortunately, the experimental results are somewhat limited. The PCD baseline is notably absent. Including (bernoulli visible, leaky-relu hidden) would have allowed the authors to evaluate likelihoods on standard binary RBM datasets. As it stands, performance on CIFAR-10 and SVHN, while improved with leaky-relu, is a far cry from more recent generative models (VAE-based, or auto-regressive models). While this comparison may be unfair, it will certainly limit the wider appeal of the paper to the community. Furthermore, there is the issue of the costly projection method which is required to guarantee that the energy-function remain bounded (covariance matrix over each region be PSD). Again, while it may be fair to leave that for future work given the other contributions, this will further limit the appeal of the paper.

PROS:
Introduces an energy function having the leaky-relu as an activation function
Introduces a novel sampling procedure based on annealing the leakiness parameter
Similar sampling scheme shown to outperform AIS

CONS:
Results are somewhat out of date
Missing experiments on binary datasets (more comparable to prior RBM work)
Missing PCD baseline
Cost of projection method

---

This paper proposed a new variant of RBM, which has a nonlinearity of leaky ReLU, in contrast to the sigmoid function nonlinearity in RBM. By gradually annealing the leakiness coefficient (corresponding to from Gaussian to non-Gaussian model), the authors can sample from their model with a higher mixing rate. With the same idea annealing leakiness, they show they can estimate the partition function of the new model more accurately. 


Main comments:

The proposed model can only account for real-valued data. However, RBM is primarily used to model binary data, real-valued RBM (Gaussian-RBM) is not a well-recognized model for real-valued data. So, to demonstrate the superiority of the model, the author should also include the comparison with binary data. And it is also not enough to only compare two datasets for a newly proposed model.

The claim that the marginal distribution of visible variables is truncated Gaussian is incorrect. For a truncated normal, the values of variables are constrained to be within some region, e.g. requiring variable v from the region a1

---

The authors proposed to use leaky rectified linear units replacing binary units in Gaussian RBM.  A sampling method was presented to train the leaky-ReLU RBM. In the experimental section, AIS estimated likelihood on Cifar10 and SVHN were reported.

 It's interesting for trying different nonlinear hidden units for RBM. However, there are some concerns for the current work.
 1. The author did not explain why the proposed sampling method (Alg. 2) is correct. And the additional computation cost (the inner loop and the projection) should be discussed.
 2. The results (both the resulting likelihood and the generative samples) of Gaussian RBM are much worse than what we have experienced. It seems that the Gaussian RBM were not trained properly.
 3. The representation learned from a good generative model often helps the classification task when there are fewer label samples. Gaussian RBM works well for texture synthesis tasks in which mixing is an important issue. The authors are encouraged to do more experiments in these two direction.

---

This paper identifies a joint distribution for an RBM variant based on leaky-ReLU activations. It also proposes using a sequence of distributions, both as an annealing-based training method, or to estimate log(Z) with AIS.
 
 This paper was borderline. While there is an interesting idea, the reviewers weren't generally as excited by the work as for other papers. One limitation is that unit-variance Gaussian RBMs aren't a strong baseline for comparison, although that is the focus of the main body of the paper. An update to the paper has results for binary visibles in an appendix, although I'm not sure exactly what was done, if the results are comparable, or if there is a large cost of projection here.

---

We thank all reviewers for their careful reading of the paper and constructive feedback. We have extensively revised the paper to address these requests for clarifications and experiments. Here, we explain some of these and point to related updates in our revision. Please check the individual replies below the reviews.

---

The authors proposed to use leaky rectified linear units replacing binary units in Gaussian RBM.  A sampling method was presented to train the leaky-ReLU RBM. In the experimental section, AIS estimated likelihood on Cifar10 and SVHN were reported.

 It's interesting for trying different nonlinear hidden units for RBM. However, there are some concerns for the current work.
 1. The author did not explain why the proposed sampling method (Alg. 2) is correct. And the additional computation cost (the inner loop and the projection) should be discussed.
 2. The results (both the resulting likelihood and the generative samples) of Gaussian RBM are much worse than what we have experienced. It seems that the Gaussian RBM were not trained properly.
 3. The representation learned from a good generative model often helps the classification task when there are fewer label samples. Gaussian RBM works well for texture synthesis tasks in which mixing is an important issue. The authors are encouraged to do more experiments in these two direction.

---

Based on previous work such as the stepped sigmoid units and ReLU hidden units for discriminatively trained supervised models, a Leaky-ReLU model is proposed for generative learning.

Pro: what is interesting is that unlike the traditional way of first defining an energy function and then deriving the conditional distributions, this paper propose the forms of the conditional first and then derive the energy function. However this general formulation is not novel to this paper, but was generalized to exponential family GLMs earlier.

Con: 
Because of the focus on specifying the conditionals, the joint pdf and the marginal p(v) becomes complicated and hard to compute.

On the experiments, it would been nice to see a RBM with binary visbles and leaky ReLu for hiddens. This would demonstrate the superiority of the leaky ReLU hidden units. In addition, there are more results on binary MNIST modeling with which the authors can compare the results to. While the authors is correct that the annealing distribution is no longer Gaussian, perhaps CD-25 or (Faast) PCD experiments can be run to compare agains the baseline RBM trained using (Fast) PCD.

This paper is interesting as it combines new hidden function with the easiness of annealed AIS sampling, However, the baseline comparisons to Stepped Sigmoid Units (Nair &Hinton) or other models like the spike-and-slab RBMs (and others) are missing, without those comparisons, it is hard to tell whether leaky ReLU RBMs are better even in continuous visible domain.

---

The authors propose a novel energy-function for RBMs, using the leaky relu max(cx, x) activation function for the hidden-units. Analogous to ReLU units in feed-forward networks, these leaky relu RBMs split the input space into a combinatorial number of regions, where each region defines p(v) as a truncated Gaussian. A further contribution of the paper is in proposing a novel sampling scheme for the leaky RBM: one can run a much shorter Markov chain by initializing it from a sample of the leaky RBM with c=1 (which yields a standard multi-variate normal over the visibles) and then slowly annealing c. In low-dimension a similar scheme is shown to outperform AIS for estimating the partition function. Experiments are performed on both CIFAR-10 and SVHN.

This is an interesting paper which I believe would be of interest to the ICLR community. The theoretical contributions are strong: the authors not only introduce a proper energy formulation of ReLU RBMs, but also a novel sampling mechanism and an improvement on AIS for estimating their partition function. 

Unfortunately, the experimental results are somewhat limited. The PCD baseline is notably absent. Including (bernoulli visible, leaky-relu hidden) would have allowed the authors to evaluate likelihoods on standard binary RBM datasets. As it stands, performance on CIFAR-10 and SVHN, while improved with leaky-relu, is a far cry from more recent generative models (VAE-based, or auto-regressive models). While this comparison may be unfair, it will certainly limit the wider appeal of the paper to the community. Furthermore, there is the issue of the costly projection method which is required to guarantee that the energy-function remain bounded (covariance matrix over each region be PSD). Again, while it may be fair to leave that for future work given the other contributions, this will further limit the appeal of the paper.

PROS:
Introduces an energy function having the leaky-relu as an activation function
Introduces a novel sampling procedure based on annealing the leakiness parameter
Similar sampling scheme shown to outperform AIS

CONS:
Results are somewhat out of date
Missing experiments on binary datasets (more comparable to prior RBM work)
Missing PCD baseline
Cost of projection method

---

This paper proposed a new variant of RBM, which has a nonlinearity of leaky ReLU, in contrast to the sigmoid function nonlinearity in RBM. By gradually annealing the leakiness coefficient (corresponding to from Gaussian to non-Gaussian model), the authors can sample from their model with a higher mixing rate. With the same idea annealing leakiness, they show they can estimate the partition function of the new model more accurately. 


Main comments:

The proposed model can only account for real-valued data. However, RBM is primarily used to model binary data, real-valued RBM (Gaussian-RBM) is not a well-recognized model for real-valued data. So, to demonstrate the superiority of the model, the author should also include the comparison with binary data. And it is also not enough to only compare two datasets for a newly proposed model.

The claim that the marginal distribution of visible variables is truncated Gaussian is incorrect. For a truncated normal, the values of variables are constrained to be within some region, e.g. requiring variable v from the region a1",,,,,,5.25,,,4.25,,
684,"MULTI-TASK LEARNING WITH DEEP MODEL BASED REINFORCEMENT LEARNING
Authors: Asier Mujika
Source file: 684.pdf

ABSTRACT
In recent years, model-free methods that use deep learning have achieved great success in many different reinforcement learning environments. Most successful approaches focus on solving a single task, while multi-task reinforcement learning remains an open problem. In this paper, we present a model based approach to deep reinforcement learning which we use to solve different tasks simultaneously. We show that our approach not only does not degrade but actually benefits from learning multiple tasks. For our model, we also present a new kind of recurrent neural network inspired by residual networks that decouples memory from computation allowing to model complex environments that do not require lots of memory. The code will be released before ICLR 2017.

1 INTRODUCTION
Recently, there has been a lot of success in applying neural networks to reinforcement learning, achieving super-human performance in many ATARI games (Mnih et al. (2015); Mnih et al. (2016)). Most of these algorithms are based on Q-learning, which is a model free approach to reinforcement learning. This approaches learn which actions to perform in each situation, but do not learn an explicit model of the environment. Apart from that, learning to play multiple games simultaneously remains an open problem as these approaches heavily degrade when increasing the number of tasks to learn.
In contrast, we present a model based approach that can learn multiple tasks simultaneously. The idea of learning predictive models has been previously proposed (Schmidhuber (2015); Santana & Hotz (2016)), but all of them focus on learning the predictive models in an unsupervised way. We propose using the reward as a means to learn a representation that captures only that which is important for the game. This also allows us to do the training in a fully supervised way. In the experiments, we show that our approach can surpass human performance simultaneously on three different games. In fact, we show that transfer learning occurs and it benefits from learning multiple tasks simultaneously.
In this paper, we first discuss why Q-learning fails to learn multiple tasks and what are its drawbacks. Then, we present our approach, Predictive Reinforcement Learning, as an alternative to overcome those weaknesses. In order to implement our model, we present a recurrent neural network architecture based on residual nets that is specially well suited for our task. Finally, we discuss our experimental results on several ATARI games.
2 PREVIOUS WORK: DEEP Q-LEARNING
In recent years, approaches that use Deep Q-learning have achieved great success, making an important breakthrough when Mnih et al. (2015) presented a neural network architecture that was able to achieve human performance on many different ATARI games, using just the pixels in the screen as input.
As the name indicates, this approach revolves around the Q-function. Given a state s and an action a, Q(s, a) returns the expected future reward we will get if we perform action a in state s. Formally, the Q-function is defined in equation 1.
Q(s, a) = Es′ [ r + γmax
a′ Q(s′, a′)|s, a
] (1)
For the rest of this subsection, we assume the reader is already familiar with Deep Q-learning and we discuss its main problems. Otherwise, we recommend skipping to the next section directly as none of the ideas discussed here are necessary to understand our model.
As the true value of the Q-function is not known, the idea of Deep Q-learning is iteratively approximating this function using a neural network1 which introduces several problems.
First, the Q-values depend on the strategy the network is playing. Thus, the target output for the network given a state-action pair is not constant, since it changes as the network learns. This means that apart from learning an strategy, the network also needs to remember which strategy it is playing. This is one of the main problems when learning multiple tasks, as the networks needs to remember how it is acting on each of the different tasks. Rusu et al. (2015) and Parisotto et al. (2015) have managed to successfully learn multiple tasks using Q-learning. Both approaches follow a similar idea: an expert network learns to play a single game, while a multi-tasking network learns to copy the behavior of an expert for each different game. This means that the multi-tasking network does not iteratively approximate the Q-function, it just learns to copy the function that the single-task expert has approximated. That is why their approach works, they manage to avoid the problem of simultaneously approximating all the Q-functions, as this is done by each single task expert.
Apart from that, the network has to change the strategy very slightly at each update as drastically changing the strategy would change the Q-values a lot and cause the approximation process to diverge/slow-down. This forces the model to interact many times with the environment in order to find good strategies. This is not problematic in simulated environments like ATARI games where the simulation can easily be speed up using more computing power. Still, in real world environments, like for example robotics, this is not the case and data efficiency can be an important issue.

3 PREDICTIVE REINFORCEMENT LEARNING
In order to avoid the drawbacks of Deep Q-learning, we present Predictive Reinforcement Learning (PRL). In our approach, we separate the understanding of the environment from the strategy. This has the advantage of being able to learn from different strategies simultaneously while also being able to play strategies that are completely different to the ones that it learns from. We will also argue that this approach makes generalization easier. But before we present it, we need to define what we want to solve.

3.1 PREDICTION PROBLEM
The problem we want to solve is the following: given the current state of the environment and the actions we will make in the future, how is our score going to change through time?
To formalize this problem we introduce the following notation:
• ai: The observation of the environment at time i. In the case of ATARI games, this corresponds to the pixels of the screen.
• ri: The total accumulated reward at time i. In the case of ATARI games, this corresponds to the in-game score.
• ci: The control that was performed at time i. In the case of ATARI games, this corresponds to the inputs of the ATARI controller: up, right, shoot, etc.
1We do not explain the process, but Mnih et al. (2015) give a good explanation on how this is done.
Then, we want to solve the following problem: For a given time i and a positive integer k, let the input to our model be an observation ai and a set of future controls ci+1, . . . ci+k. Then, we want to predict the change in score for the next k time steps, i.e. (ri+1 − ri), . . . , (ri+k − ri). Figure 1 illustrates this with an example.
Observe that, unlike in Q-learning, our predictions do not depend on the strategy being played. The outputs only depend on the environment we are trying to predict. So, the output for a given state-actions pair is always the same or, in the case of non-deterministic environments, it comes from the same distribution.

3.2 MODEL
We have defined what we want to solve but we still need to specify how to implement a model that will do it. We will use neural networks for this and we will divide it into three different networks as follows:
• Perception: This network reads a state ai and converts it to a lower dimensional vector h0 that is used by the Prediction.
• Prediction: For each j ∈ {1, . . . , k}, this network reads the vector hj−1 and the corresponding control ci+j and generates a vector hj that will be used in the next steps of the Prediction and Valuation. Observe that this is actually a recurrent neural network.
• Valuation: For each j ∈ {1, . . . , k}, this network reads the current vector hj of the Prediction and predicts the difference in score between the initial time and the current one, i.e, ri+j − ri.
Figure 2 illustrates the model. Observe that what we actually want to solve is a supervised learning problem. Thus, the whole model can be jointly trained with simple backpropagation. We will now proceed to explain each of the components in more detail.

3.2.1 PERCEPTION
The Perception has to be tailored for the kind of observations the environment returns. For now, we will focus only on vision based Perception. As we said before, the idea of this network is to convert the high dimensional input to a low dimensional vector that contains only the necessary information for predicting the score. In the case of video games, it is easy to see that such vector exists. The input will consists of thousands of pixels but all we care about is the position of a few key objects, like for example, the main character or the enemies. This information can easily be
encoded using very few neurons. In our experiments, we convert an input consisting of 28K pixels into a vector of just 100 real values.
In order to do this, we use deep convolutional networks. These networks have recently achieved super-human performance in very complex image recognition tasks (He et al., 2015). In fact, it has been observed that the upper layers in these models learn lower dimensional abstract representations of the input (Yosinski et al. (2015), Karpathy & Li (2015)). Given this, it seems reasonable to believe that if we use any of the successful architectures for vision, our model will be able to learn a useful representation that can be used by the Prediction.

3.2.2 PREDICTION
For the Prediction network, we present a new kind of recurrent network based on residual neural networks (He et al., 2015), which is specially well suited for our task and it achieved better results than an LSTM (Hochreiter & Schmidhuber, 1997) with a similar number of parameters in our initial tests.
Residual Recurrent Neural Network (RRNN) We define the RRNN in Figure 3 using the following notation: LN is the layer normalization function (Ba et al., 2016) which normalizes the activations to have a median of 0 and standard deviation of 1. ”·” is the concatenation of two vectors. f can be any parameterizable and differentiable function, e.g., a multilayer perceptron.
As in residual networks, instead of calculating what the new state of the network should be, we calculate how it should change (ri). As shown by He et al. (2015) this prevents vanishing gradients or optimization difficulties. LN outputs a vector with mean 0 and standard deviation 1. As we
proof2 in Observation 1, this prevents internal exploding values that may arise from repeatedly adding r to h. It also avoids the problem of vanishing gradients in saturating functions like sigmoid or hyperbolic tangent.
Observation 1. Let x ∈ Rn be a vector with median 0 and standard deviation 1. Then, for all 1 ≤ i ≤ n, we get that xi ≤ √ n.
Proof. Taking into account that the median is 0 and the standard deviation is 1, simply substituting the values in the formula for the standard deviation shows the observation.
σ = √√√√ 1 n n∑ j=1 (xj − µ)2 (4)
1 = √√√√ 1 n n∑ j=1 x2j (5)
√ n = √√√√ n∑ j=1 x2j (6) √ n ≥ xi (7)
The idea behind this network is mimicking how a video game’s logic works. A game has some variables (like positions or speeds of different objects) that are slightly modified at each step. Our intuition is that the network can learn a representation of these variables (h), while f learns how they are transformed at each frame. Apart from that, this model decouples memory from computation allowing to increase the complexity of f without having to increase the number of neurons in h. This is specially useful as the number of real valued neurons needed to represent the state of a game is quite small. Still, the function to move from one frame to the next can be quite complex, as it has to model all the interactions between the objects such as collisions, movements, etc.
Even if this method looks like it may be just tailored for video games, it should work equally well for real world environments. After all, physics simulations that model the real world work in the same way, with some variables that represent the current state of the system and some equations that define how that system evolves over time.

3.2.3 VALUATION
The Valuation network reads the h vector at time i + j and outputs the change in reward for that time step, i.e. ri+j − rj . Still, it is a key part of our model as it allows to decouple the representation learned by the Prediction from the reward function. For example, consider a robot in a real world environment. If the Perception learns to capture the physical properties of all surrounding objects (shape, mass, speed, etc.) and the Prediction learns to make a physical simulation of the environment, this model can be used for any possible task in that environment, only the Valuation would need to be changed.

3.3 STRATEGY
As we previously said, finding an optimal strategy is a very hard problem and this part is the most complicated. So, in order to test our model in the experiments, we opted for hard-coding a strategy. There, we generate a set of future controls uniformly at random and then we pick the one that would maximize our reward, given that the probability of dying is low enough. Because of this, the games we have tried have been carefully selected such that they do not need very sophisticated and long-term strategies.
2The bound is not tight but it is sufficient for our purposes and straightforward to prove.
Still, our approach learns a predictive model that is independent of any strategy and this can be beneficial in two ways. First, the model can play a strategy that is completely different to the ones it learns from. Apart from that, learning a predictive model is a very hard task to over-fit. Consider a game with 10 possible control inputs and a training set where we consider the next 25 time steps. Then, there are 1025 possible control sequences. This means that every sequence we train on is unique and this forces the model to generalize. Unfortunately, there is also a downside. Our approach is not able to learn from good strategies because we test our model with many different ones in order to pick the best. Some of these strategies will be quite bad and thus, the model needs to learn what makes the difference between a good and a bad set of moves.

4 EXPERIMENTS

4.1 ENVIRONMENT
Our experiments have been performed on a computer with a GeForce GTX 980 GPU and an Intel Xeon E5-2630 CPU. For the neural network, we have used the Torch7 framework and for the ATARI simulations, we have used Alewrap, which is a Lua wrapper for the Arcade Learning Environment (Bellemare et al., 2015).
4.2 MODEL
For the Perception, we used a network inspired in deep residual networks (He et al., 2015). Figure 4 shows the architecture. The reason for this, is that even if the Perception is relatively shallow, when unfolding the Prediction network over time, the depth of the resulting model is over 50 layers deep.
For the Prediction, we use a Residual Recurrent Neural Network. Table 1 describes the network used for the f function. Finally, Table 2 illustrates the Valuation network.

4.3 SETUP
In our experiments, we have trained on three different ATARI games simultaneously: Breakout, Pong and Demon Attack.
We preprocess the images following the same technique of Mnih et al. (2015). We take the maximum from the last 2 frames to get a single 84 × 84 black and white image for the current observation. The input to the Perception is a 4×84×84 tensor containing the last 4 observations. This is necessary to be able to use a feed-forward network for the Perception. If we observed a single frame,
it would not be possible to infer the speed and direction of a moving object. Not doing this would force us to use a recurrent network on the Perception, making the training of the whole model much slower.
In order to train the Prediction, we unfold the network over time (25 time steps) and treat the model as a feed-forward network with shared weights. This corresponds to approximately 1.7 seconds.
For our Valuation, network we output two values. First, the probability that our score is higher than in the initial time step. Second, we output the probability of dying. This is trained using cross entropy loss.
To train the model, we use an off-line learning approach for simplicity. During training we alternate between two steps. First, generate and store data and then, train the model off-line on that data.

4.4 GENERATING DATA
In order to generate the data, we store tuples (ai, C = {ci+1, . . . ci+25}, R = {ri+1 − ri, . . . ri+25 − ri}) as we are playing the game. That is, for each time i, we store the following:
• ai: A 4× 84× 84 tensor, containing 4 consecutive black and white frames of size 84× 84 each.
• C: For j ∈ {i+ 1, . . . , i+ 25}, each cj is a 3 dimensional vector that encodes the control action performed at time j. The first dimension corresponds to the shoot action, the second to horizontal actions and the third to vertical actions. For example, [1,−1, 0] represent pressing shoot and left.
• R: For j ∈ {i+1, . . . , i+25}, we store a 2 dimensional binary vector rj . rj1 is 1 if we die between time i and j. rj2 is 1 if we have not lost a life and we also earn a point between time i and j.
Initially, we have an untrained model, so at each time step, we pick an action uniformly at random and perform it. For the next iterations, we pick a k and do the following to play the game:
1. Run the Perception network on the last 4 frames to obtain the initial vector.
2. Generate k − 1 sequences of 25 actions uniformly at random. Apart from that, take the best sequence from the previous time step and also consider it. This gives a total of k sequences. Then, for each sequence, run the Prediction and Valuation networks with the vector obtained in Step 1.
3. Finally, pick a sequence of actions as follows. Consider only the moves that have a low enough probability of dying. From those, pick the one that has the highest probability of earning a point. If none has a high enough probability, just pick the one with the lowest probability of dying.
We start with k = 25 and increase it every few iterations up to k = 200. For the full details check Appendix A. In order to accelerate training, we run several games in parallel. This allows to run the Perception, Prediction and Valuation networks together with the ATARI simulation in parallel, which heavily speeds up the generation of data without any drawback.

4.5 TRAINING
In the beginning, we generate 400K training cases for each of the games by playing randomly, which gives us a total of 1.2M training cases. Then, for the subsequent iterations, we generate 200K additional training cases per game (600K in total) and train again on the whole dataset. That is, at first we have 1.2M training cases, afterwards 1.8M , then 2.4M and so on.
The training is done in a supervised way as depicted in Figure 2b. ai and C are given as input to the network and R as target. We minimize the cross-entropy loss using mini-batch gradient descent. For the full details on the learning schedule check Appendix A.
In order to accelerate the process, instead of training a new network in each iteration, we keep training the model from the previous iteration. This has the effect that we would train much more on the initial training cases while the most recent ones would have an ever smaller effect as the training set grows. To avoid this, we assign a weight to each iteration and sample according to these weights during training. Every three iterations, we multiply by three the weights we assign to them. By doing this, we manage to focus on recent training cases, while still preserving the whole training set.
Observe that we never tell our network which game it is playing, but it learns to infer it from the observation ai. Also, at each iteration, we add cases that are generated using a different neural network. So our training set contains instances generated using many different strategies.

4.6 RESULTS
We have trained a model on the three games for a total of 19 iterations, which correspond to 4M time steps per game (74 hours of play at 60 Hz). Each iteration takes around two hours on our hardware. We have also trained an individual model for each game for 4M time steps. In the individual models, we reduced the length of the training such that the number of parameter updates per game is the same as in the multi-task case. Unless some kind of transfer learning occurs, one would expect some degradation in performance in the multi-task model. Figure 5 shows that not only there is no degradation in Pong and Demon Attack, but also that there is a considerable improvement in Breakout. This confirms our initial belief that our approach is specially well suited for multi-task learning.
We have also argued that our model can potentially play a very different strategy from the one it has observed. Table 3 shows that this is actually the case. A model that has learned only from random play is able to play at least 7 times better.
Demon Attack’s plot in Figure 5c shows a potential problem we mentioned earlier which also happens in the other two games to a lesser extent. Once the strategy is good enough, the agent dies very rarely. This causes the model to ”forget” which actions lead to a death and makes the score oscillate.

5 DISCUSSION
We have presented a novel model based approach to deep reinforcement learning. Despite not achieving state of the art results, this papers opens new lines of research showing that a model based approach can work in environments as complex as ATARI. We have also shown that it can beat human performance in three different tasks simultaneously and that it can benefit from learning multiple tasks.
Still, the model has two areas that can be addressed in future work: long-term dependencies and the instability during training. The first, can potentially be solved by combining our approach with Q-learning based techniques. For the instability, balancing the training set or oversampling hard training cases could alleviate the problem.
Finally, we have also presented a new kind of recurrent network which can be very useful for problems were little memory and a lot of computation is needed.

ACKNOWLEDGMENTS
I thank Angelika Steger and Florian Meier for their hardware support in the final experiments and comments on previous versions of the paper.
","In recent years, model-free methods that use deep learning have achieved great success in many different reinforcement learning environments. Most successful approaches focus on solving a single task, while multi-task reinforcement learning remains an open problem. In this paper, we present a model based approach to deep reinforcement learning which we use to solve different tasks simultaneously. We show that our approach not only does not degrade but actually benefits from learning multiple tasks. For our model, we also present a new kind of recurrent neural network inspired by residual networks that decouples memory from computation allowing to model complex environments that do not require lots of memory. The code will be released before ICLR 2017.",ICLR 2017 conference submission,False,,"This paper proposes a new approach to model based reinforcement learning and
evaluates it on 3 ATARI games. The approach involves training a model that
predicts a sequence of rewards and probabilities of losing a life given a
context of frames and a sequence of actions. The controller samples random
sequences of actions and executes the one that balances the probabilities of
earning a point and losing a life given some thresholds. The proposed system
learns to play 3 Atari games both individually and when trained on all 3 in a
multi-task setup at super-human level.

The results presented in the paper are very encouraging but there are many
ad-hoc design choices in the design of the system. The paper also provides
little insight into the importance of the different components of the system.

Main concerns:
- The way predicted rewards and life loss probabilities are combined is very ad-hoc.
  The natural way to do this would be by learning a Q-value, instead different
  rules are devised for different games.
- Is a model actually being learned and improved? It would be good to see
  predictions for several actions sequences from some carefully chosen start
  states. This would be good to see both on a game where the approach works and
  on a game where it fails. The learning progress could also be measured by
  plotting the training loss on a fixed holdout set of sequences.
- How important is the proposed RRNN architecture? Would it still work without
  the residual connections? Would a standard LSTM also work?

Minor points:
- Intro, paragraph 2 - There is a lot of much earlier work on using models in
  RL. For example, see Dyna and ""Memory approaches to reinforcement learning in
  non-Markovian domains"" by Lin and Mitchell to name just two.
- Section 3.1 - Minor point, but using a_i to represent the observation is
  unusual.  Why not use o_i for observations and a_i for actions?
- Section 3.2.2 - Notation again, r_i was used earlier to represent the
  reward at time i but it is being used again for something else.
- Observation 1 seems somewhat out of place. Citing the layer normalization
  paper for the motivation is enough.
- Section 3.2.2, second last paragraph - How is memory decoupled from
  computation here? Models like neural turning machines accomplish this by using
  an external memory, but this looks like an RNN with skip connections.
- Section 3.3, second paragraph - Whether the model overfits or not depends on
  the data. The approach doesn't work with demonstrations precisely because it
  would overfit.
- Figure 4 - The reference for Batch Normalization should be Ioffe and Szegedy
  instead of Morimoto et al.

Overall I think the paper has some really promising ideas and encouraging
results but is missing a few exploratory/ablation experiments and some polish.

---

The authors have proposed a new method for deep RL that uses model-based evaluation of states and actions and reward/life loss predictions. The evaluation, on just 3 ATari games with no comparisons to state of the art methods, is insufficient, and the method seems ad-hoc and unclear. Design choices are not clearly described or justified. The paper gives no insight as to how the different aspects of the approach relate or contribute to the overall results.

---

This paper proposes a model-based reinforcement learning approach focusing on predicting future rewards given a current state and future actions. This is achieved with a ""residual recurrent neural network"", that outputs the expected reward increase at various time steps in the future. To demonstrate the usefulness of this approach, experiments are conducted on Atari games, with a simple playing strategy that consists in evaluating random sequences of moves and picking the one with highest expected reward (and low enough chance of dying). Interestingly, out of the 3 games tested, one of them exhibits better performance when the agent is trained in a multitask setting (i.e. learning all games simultaneously), hinting that transfer learning is occurring.

This submission is easy enough to read, and the reward prediction architecture looks like an original and sound idea. There are however several points that I believe prevent this work from reaching the ICLR bar, as detailed below.

The first issue is the discrepancy between the algorithm proposed in Section 3 vs its actual implementation in Section 4 (experiments): in Section 3 the output is supposed to be the expected accumulated reward in future time steps (as a single scalar), while in experiments it is instead two numbers, one which is the probability of dying and another one which is the probability of having a higher score without dying. This might work better, but it also means the idea as presented in the main body of the paper is not actually evaluated (and I guess it would not work well, as otherwise why implement it differently?)

In addition, the experimental results are quite limited: only on 3 games that were hand-picked to be easy enough, and no comparison to other RL techniques (DQN & friends). I realize that the main focus of the paper is not about exhibiting state-of-the-art results, since the policy being used is only a simple heuristic to show that the model predictions can ne used to drive decisions. That being said, I think experiments should have tried to demonstrate how to use this model to obtain better reinforcement learning algorithms: there is actually no reinforcement learning done here, since the model is a supervised algorithm, used in a manually-defined hardcoded policy. Another question that could have been addressed (but was not) in the experiments is how good these predictions are (e.g. classification error on dying probability, MSE on future rewards, ...), compared to simpler baselines.

Finally, the paper's ""previous work"" section is too limited, focusing only on DQN and in particular saying very little on the topic of model-based RL. I think a paper like for instance ""Action-Conditional Video Prediction using Deep Networks in Atari Games"" should have been an obvious ""must cite"".

Minor comments:
- Notations are unusual, with ""a"" denoting a state rather than an action, this is potentially confusing and I see no reason to stray away from standard RL notations
- Using a dot for tensor concatenation is not a great choice either, since the dot usually indicates a dot product
- The r_i in 3.2.2 is a residual that has nothing to do with r_i the reward
- c_i is defined as ""The control that was performed at time i"", but instead it seems to be the control performed at time i-1
- There is a recurrent confusion between mean and median in 3.2.2
- x should not be used in Observation 1 since the x from Fig. 3 does not go through layer normalization
- The inequality in Observation 1 should be about |x_i|, not x_i
- Observation 1 (with its proof) takes too much space for such a simple result
- In 3.2.3 the first r_j should be r_i
- The probability of dying comes out of nowhere in 3.3, since we do not know yet it will be an output of the model
- ""Our approach is not able to learn from good strategies"" => did you mean ""*only* from good strategies""?
- Please say that in Fig. 4 ""fc"" means ""fully connected""
- It would be nice also to say how the architecture of Fig. 4 differs from the classical DQN architecture from Mnih et al (2015)
- Please clarify r_j2 as per your answer in OpenReview comments
- Table 3 says ""After one iteration"" but has ""PRL Iteration 2"" in it, which is confusing
- ""Figure 5 shows that not only there is no degradation in Pong and Demon Attack""=> to me it seems to be a bit worse, actually
- ""A model that has learned only from random play is able to play at least 7 times better."" => not clear where this 7 comes from
- ""Demon Attack's plot in Figure 5c shows a potential problem we mentioned earlier"" => where was it mentioned?

---

The term strategy is a bit ambiguous. Could you please explain more in formal terms what is strategy?
Is r the discounted Return at time t, or the reward at time t?
Could the author compare the method to TD learning?
The paper is vague and using many RL terms with different meanings without clarifying those diversions.
""So, the output for a given state-actions pair is always same"". Q function by definition is the value of (state, action). So as long as the policy is deterministic the output would be always same too. How's this different from Q learning?
The model description doesn't specify what is the policy, and it's only being mentioned in data generation part.
Why is it a model based approach?
The learning curves are only for 19 iterations, which does not give any useful information. The final results are clearly nothing comparable to previous works. The model is only being tested on three games.

The paper is vague and using informal language or sometimes misusing the common RL terms. The experiments are very small scale and even in that scenario performing very bad. It's not clear, why it's a model-based approach.

---

This paper proposes a new approach to model based reinforcement learning and
evaluates it on 3 ATARI games. The approach involves training a model that
predicts a sequence of rewards and probabilities of losing a life given a
context of frames and a sequence of actions. The controller samples random
sequences of actions and executes the one that balances the probabilities of
earning a point and losing a life given some thresholds. The proposed system
learns to play 3 Atari games both individually and when trained on all 3 in a
multi-task setup at super-human level.

The results presented in the paper are very encouraging but there are many
ad-hoc design choices in the design of the system. The paper also provides
little insight into the importance of the different components of the system.

Main concerns:
- The way predicted rewards and life loss probabilities are combined is very ad-hoc.
  The natural way to do this would be by learning a Q-value, instead different
  rules are devised for different games.
- Is a model actually being learned and improved? It would be good to see
  predictions for several actions sequences from some carefully chosen start
  states. This would be good to see both on a game where the approach works and
  on a game where it fails. The learning progress could also be measured by
  plotting the training loss on a fixed holdout set of sequences.
- How important is the proposed RRNN architecture? Would it still work without
  the residual connections? Would a standard LSTM also work?

Minor points:
- Intro, paragraph 2 - There is a lot of much earlier work on using models in
  RL. For example, see Dyna and ""Memory approaches to reinforcement learning in
  non-Markovian domains"" by Lin and Mitchell to name just two.
- Section 3.1 - Minor point, but using a_i to represent the observation is
  unusual.  Why not use o_i for observations and a_i for actions?
- Section 3.2.2 - Notation again, r_i was used earlier to represent the
  reward at time i but it is being used again for something else.
- Observation 1 seems somewhat out of place. Citing the layer normalization
  paper for the motivation is enough.
- Section 3.2.2, second last paragraph - How is memory decoupled from
  computation here? Models like neural turning machines accomplish this by using
  an external memory, but this looks like an RNN with skip connections.
- Section 3.3, second paragraph - Whether the model overfits or not depends on
  the data. The approach doesn't work with demonstrations precisely because it
  would overfit.
- Figure 4 - The reference for Batch Normalization should be Ioffe and Szegedy
  instead of Morimoto et al.

Overall I think the paper has some really promising ideas and encouraging
results but is missing a few exploratory/ablation experiments and some polish.

---

This paper proposes a new approach to model based reinforcement learning and
evaluates it on 3 ATARI games. The approach involves training a model that
predicts a sequence of rewards and probabilities of losing a life given a
context of frames and a sequence of actions. The controller samples random
sequences of actions and executes the one that balances the probabilities of
earning a point and losing a life given some thresholds. The proposed system
learns to play 3 Atari games both individually and when trained on all 3 in a
multi-task setup at super-human level.

The results presented in the paper are very encouraging but there are many
ad-hoc design choices in the design of the system. The paper also provides
little insight into the importance of the different components of the system.

Main concerns:
- The way predicted rewards and life loss probabilities are combined is very ad-hoc.
  The natural way to do this would be by learning a Q-value, instead different
  rules are devised for different games.
- Is a model actually being learned and improved? It would be good to see
  predictions for several actions sequences from some carefully chosen start
  states. This would be good to see both on a game where the approach works and
  on a game where it fails. The learning progress could also be measured by
  plotting the training loss on a fixed holdout set of sequences.
- How important is the proposed RRNN architecture? Would it still work without
  the residual connections? Would a standard LSTM also work?

Minor points:
- Intro, paragraph 2 - There is a lot of much earlier work on using models in
  RL. For example, see Dyna and ""Memory approaches to reinforcement learning in
  non-Markovian domains"" by Lin and Mitchell to name just two.
- Section 3.1 - Minor point, but using a_i to represent the observation is
  unusual.  Why not use o_i for observations and a_i for actions?
- Section 3.2.2 - Notation again, r_i was used earlier to represent the
  reward at time i but it is being used again for something else.
- Observation 1 seems somewhat out of place. Citing the layer normalization
  paper for the motivation is enough.
- Section 3.2.2, second last paragraph - How is memory decoupled from
  computation here? Models like neural turning machines accomplish this by using
  an external memory, but this looks like an RNN with skip connections.
- Section 3.3, second paragraph - Whether the model overfits or not depends on
  the data. The approach doesn't work with demonstrations precisely because it
  would overfit.
- Figure 4 - The reference for Batch Normalization should be Ioffe and Szegedy
  instead of Morimoto et al.

Overall I think the paper has some really promising ideas and encouraging
results but is missing a few exploratory/ablation experiments and some polish.

---

The authors have proposed a new method for deep RL that uses model-based evaluation of states and actions and reward/life loss predictions. The evaluation, on just 3 ATari games with no comparisons to state of the art methods, is insufficient, and the method seems ad-hoc and unclear. Design choices are not clearly described or justified. The paper gives no insight as to how the different aspects of the approach relate or contribute to the overall results.

---

This paper proposes a model-based reinforcement learning approach focusing on predicting future rewards given a current state and future actions. This is achieved with a ""residual recurrent neural network"", that outputs the expected reward increase at various time steps in the future. To demonstrate the usefulness of this approach, experiments are conducted on Atari games, with a simple playing strategy that consists in evaluating random sequences of moves and picking the one with highest expected reward (and low enough chance of dying). Interestingly, out of the 3 games tested, one of them exhibits better performance when the agent is trained in a multitask setting (i.e. learning all games simultaneously), hinting that transfer learning is occurring.

This submission is easy enough to read, and the reward prediction architecture looks like an original and sound idea. There are however several points that I believe prevent this work from reaching the ICLR bar, as detailed below.

The first issue is the discrepancy between the algorithm proposed in Section 3 vs its actual implementation in Section 4 (experiments): in Section 3 the output is supposed to be the expected accumulated reward in future time steps (as a single scalar), while in experiments it is instead two numbers, one which is the probability of dying and another one which is the probability of having a higher score without dying. This might work better, but it also means the idea as presented in the main body of the paper is not actually evaluated (and I guess it would not work well, as otherwise why implement it differently?)

In addition, the experimental results are quite limited: only on 3 games that were hand-picked to be easy enough, and no comparison to other RL techniques (DQN & friends). I realize that the main focus of the paper is not about exhibiting state-of-the-art results, since the policy being used is only a simple heuristic to show that the model predictions can ne used to drive decisions. That being said, I think experiments should have tried to demonstrate how to use this model to obtain better reinforcement learning algorithms: there is actually no reinforcement learning done here, since the model is a supervised algorithm, used in a manually-defined hardcoded policy. Another question that could have been addressed (but was not) in the experiments is how good these predictions are (e.g. classification error on dying probability, MSE on future rewards, ...), compared to simpler baselines.

Finally, the paper's ""previous work"" section is too limited, focusing only on DQN and in particular saying very little on the topic of model-based RL. I think a paper like for instance ""Action-Conditional Video Prediction using Deep Networks in Atari Games"" should have been an obvious ""must cite"".

Minor comments:
- Notations are unusual, with ""a"" denoting a state rather than an action, this is potentially confusing and I see no reason to stray away from standard RL notations
- Using a dot for tensor concatenation is not a great choice either, since the dot usually indicates a dot product
- The r_i in 3.2.2 is a residual that has nothing to do with r_i the reward
- c_i is defined as ""The control that was performed at time i"", but instead it seems to be the control performed at time i-1
- There is a recurrent confusion between mean and median in 3.2.2
- x should not be used in Observation 1 since the x from Fig. 3 does not go through layer normalization
- The inequality in Observation 1 should be about |x_i|, not x_i
- Observation 1 (with its proof) takes too much space for such a simple result
- In 3.2.3 the first r_j should be r_i
- The probability of dying comes out of nowhere in 3.3, since we do not know yet it will be an output of the model
- ""Our approach is not able to learn from good strategies"" => did you mean ""*only* from good strategies""?
- Please say that in Fig. 4 ""fc"" means ""fully connected""
- It would be nice also to say how the architecture of Fig. 4 differs from the classical DQN architecture from Mnih et al (2015)
- Please clarify r_j2 as per your answer in OpenReview comments
- Table 3 says ""After one iteration"" but has ""PRL Iteration 2"" in it, which is confusing
- ""Figure 5 shows that not only there is no degradation in Pong and Demon Attack""=> to me it seems to be a bit worse, actually
- ""A model that has learned only from random play is able to play at least 7 times better."" => not clear where this 7 comes from
- ""Demon Attack's plot in Figure 5c shows a potential problem we mentioned earlier"" => where was it mentioned?

---

The term strategy is a bit ambiguous. Could you please explain more in formal terms what is strategy?
Is r the discounted Return at time t, or the reward at time t?
Could the author compare the method to TD learning?
The paper is vague and using many RL terms with different meanings without clarifying those diversions.
""So, the output for a given state-actions pair is always same"". Q function by definition is the value of (state, action). So as long as the policy is deterministic the output would be always same too. How's this different from Q learning?
The model description doesn't specify what is the policy, and it's only being mentioned in data generation part.
Why is it a model based approach?
The learning curves are only for 19 iterations, which does not give any useful information. The final results are clearly nothing comparable to previous works. The model is only being tested on three games.

The paper is vague and using informal language or sometimes misusing the common RL terms. The experiments are very small scale and even in that scenario performing very bad. It's not clear, why it's a model-based approach.

---

This paper proposes a new approach to model based reinforcement learning and
evaluates it on 3 ATARI games. The approach involves training a model that
predicts a sequence of rewards and probabilities of losing a life given a
context of frames and a sequence of actions. The controller samples random
sequences of actions and executes the one that balances the probabilities of
earning a point and losing a life given some thresholds. The proposed system
learns to play 3 Atari games both individually and when trained on all 3 in a
multi-task setup at super-human level.

The results presented in the paper are very encouraging but there are many
ad-hoc design choices in the design of the system. The paper also provides
little insight into the importance of the different components of the system.

Main concerns:
- The way predicted rewards and life loss probabilities are combined is very ad-hoc.
  The natural way to do this would be by learning a Q-value, instead different
  rules are devised for different games.
- Is a model actually being learned and improved? It would be good to see
  predictions for several actions sequences from some carefully chosen start
  states. This would be good to see both on a game where the approach works and
  on a game where it fails. The learning progress could also be measured by
  plotting the training loss on a fixed holdout set of sequences.
- How important is the proposed RRNN architecture? Would it still work without
  the residual connections? Would a standard LSTM also work?

Minor points:
- Intro, paragraph 2 - There is a lot of much earlier work on using models in
  RL. For example, see Dyna and ""Memory approaches to reinforcement learning in
  non-Markovian domains"" by Lin and Mitchell to name just two.
- Section 3.1 - Minor point, but using a_i to represent the observation is
  unusual.  Why not use o_i for observations and a_i for actions?
- Section 3.2.2 - Notation again, r_i was used earlier to represent the
  reward at time i but it is being used again for something else.
- Observation 1 seems somewhat out of place. Citing the layer normalization
  paper for the motivation is enough.
- Section 3.2.2, second last paragraph - How is memory decoupled from
  computation here? Models like neural turning machines accomplish this by using
  an external memory, but this looks like an RNN with skip connections.
- Section 3.3, second paragraph - Whether the model overfits or not depends on
  the data. The approach doesn't work with demonstrations precisely because it
  would overfit.
- Figure 4 - The reference for Batch Normalization should be Ioffe and Szegedy
  instead of Morimoto et al.

Overall I think the paper has some really promising ideas and encouraging
results but is missing a few exploratory/ablation experiments and some polish.",,,,,,3.3333333333333335,,,4.333333333333333,,
689,"TENSORIAL MIXTURE MODELS
Authors: Or Sharir, Ronen Tamari, Nadav Cohen
Source file: 689.pdf

ABSTRACT
We introduce a generative model, we call Tensorial Mixture Models (TMMs) based on mixtures of basic component distributions over local structures (e.g. patches in an image) where the dependencies between the local-structures are represented by a ”priors tensor” holding the prior probabilities of assigning a component distribution to each local-structure. In their general form, TMMs are intractable as the priors tensor is typically of exponential size. However, when the priors tensor is decomposed it gives rise to an arithmetic circuit which in turn transforms the TMM into a Convolutional Arithmetic Circuit (ConvAC). A ConvAC corresponds to a shallow (single hidden layer) network when the priors tensor is decomposed by a CP (sum of rank-1) approach and corresponds to a deep network when the decomposition follows the Hierarchical Tucker (HT) model. The ConvAC representation of a TMM possesses several attractive properties. First, the inference is tractable and is implemented by a forward pass through a deep network. Second, the architectural design of the model follows the deep networks community design, i.e., the structure of TMMs is determined by just two easily understood factors: size of pooling windows and number of channels. Finally, we demonstrate the effectiveness of our model when tackling the problem of classification with missing data, leveraging TMMs unique ability of tractable marginalization which leads to optimal classifiers regardless of the missingness distribution.

1 INTRODUCTION
Generative models have played a crucial part in the early development of the field of Machine Learning. However, in recent years they were mostly cast aside in favor of discriminative models, lead by the rise of ConvNets (LeCun et al., 2015), which were found to perform equally well or better than classical generative counter-parts on almost any task. Despite the increased interest in unsupervised learning, many of the recent studies on generative models choose to focus solely on the generation capabilities of these models (Goodfellow et al., 2014; Gregor et al., 2015; van den Oord et al., 2016; Dinh et al., 2016; Tran et al., 2016; Chen et al., 2016; Kingma et al., 2016; Kim and Bengio, 2016). There is much less emphasis on leveraging generative models to solve actual tasks, e.g. semi-supervised learning (Kingma et al., 2014; Springenberg, 2016; Maaløe et al., 2016; Forster et al., 2015; Salimans et al., 2016), image restoration (Dinh et al., 2014; Bengio et al., 2014; van den Oord et al., 2016; Zoran and Weiss, 2011; Rosenbaum and Weiss, 2015; Sohl-Dickstein et al., 2015; Theis and Bethge, 2015) or unsupervised feature representation (Radford et al., 2016; Coates et al., 2011). Nevertheless, work on generative models for solving actual problems are yet to show a meaningful advantage over competing discriminative models.
On the most fundamental level, the difference between a generative model and a discriminative one is simply the difference between learning P (X,Y ) and learning P (Y |X), respectively. While it is always possible to infer P (Y |X) given P (X,Y ), it might not be immediately apparent why the generative objective is preferred over the discriminative one. In Ng and Jordan (2002), this question was studied w.r.t. the sample complexity, proving that under some cases it can be significantly lesser in favor of the generative classifier. However, their analysis was limited only to specific pairs of discriminative and generative classifiers, and they did not present a general case where the the generative method is undeniably preferred. We wish to highlight one such case, where learning
P (X,Y ) is provenly better regardless of the models in question, by examining the problem of classification with missing data. Despite the artificially well-behave nature of the typical classification benchmarks presented in current publications, real-world data is usually riddled with noise and missing values – instead of observing X we only have a partial observation X̂ – a situation that tends to be ignored in modern research. Discriminative models have no natural mechanisms to handle missing data and instead must rely on data imputation, i.e. filling missing data by a preprocessing step prior to prediction. Unlike the discriminative approaches, generative models are naturally fitted to handle missing data by simply marginalizing over the unknown values in P (X,Y ), from which we can attain P (Y |X̂) by an application of Bayes Rule. Moreover, under mild assumptions which apply to many real-world settings, this method is proven to be optimal regardless of the process by which values become missing (see sec. 5 for a more detailed discussion).
While almost all generative models can represent P (X,Y ), only few can actually infer its exact value efficiently. Models which possess this property are said to have tractable inference. Many studies specifically address the hard problem of learning generative models that do not have this property. Notable amongst those are works based on Variational Inference (Kingma and Welling, 2014; Kingma et al., 2014; Blei et al., 2003; Wang and Grimson, 2007; Makhzani et al., 2015; Kingma et al., 2016), which only provide approximated inference, and ones based on Generative Adversarial Networks (Goodfellow et al., 2014; Radford et al., 2016; Springenberg, 2016; Chen et al., 2016; Salimans et al., 2016; Makhzani et al., 2015), which completely circumvent the inference problem by restructuring the learning problem as a two-player game of discriminative objectives – both of these approaches are incapable of tractable inference.
There are several advantages to models with tractable inference (e.g. they could be simpler to train), and as we have shown above, this property is also a requirement for proper handling of missing data in the form of marginalization. In practice, to marginalize over P (X,Y ) means to perform integration on it, thus, even if it is tractable to compute P (X,Y ), it still might not be tractable to compute every possible marginalization. Models which are capable of this are said to have tractable marginalization. Mixture Models (e.g. Gaussian Mixture Models) are the classical example of a generative model with tractable inference, as well as tractable marginalization. Though they are simple to understand, easy to train and even known to be universal – can approximate any distribution given sufficient capacity – they do not scale well to high-dimensional data. The Gaussian Mixture Model is an example of a shallow model – containing just a single latent variable – with limited expressive efficiency. More generally, Graphical Models are deep and exponentially more expressive, capable of representing intricate relations between many latent variables. While not all kinds of Graphical Models are tractable, many are, e.g. Latent Tree Models (Zhang, 2004; Mourad et al., 2013) and Sum-Product Networks (Poon and Domingos, 2011). The main issue with generic graphical models is that by virtue of being too general they lack the inductive bias needed to efficiently model unstructured data, e.g. images or text. Despite the success of structure learning algorithms (Huang et al., 2015; Gens and Domingos, 2013; Adel et al., 2015) on structured datasets, such as discovering a hierarchy among diseases in patients health records, there are no similar results on unstructured datasets. Indeed some recent works on the subject have failed to solve even simple handwritten digit classification tasks (Adel et al., 2015). Thus deploying graphical models on such cases requires experts to manually design the model. Other attempts which harness neural networks blocks (Dinh et al., 2014; 2016) offer tractable inference, but not tractable marginalization.
To summarize, most generative models do not have tractable inference, and of the few models which do, they all possess one or more of the following shortcomings: (i) they do not possess the expressive capacity to model high-dimensional data (e.g. images), (ii) they require explicitly designing all the dependencies of the data, or (iii) they do not have tractable marginalization.
We present in this paper a family of generative models we call Tensorial Mixture Models (TMMs), which aim to address the above shortcomings of alternative models. Under TMMs, we assume that the data generated by our model is composed of a sequence of local-structures (e.g. patches in an image), where each local-structure is generated from a small set of simple component distributions (e.g. Gaussian), and the dependencies between the local-structures are represented by a prior tensor holding the prior probabilities of assigning a component distribution to each local-structure. In their general form, TMMs are intractable as the prior tensor is typically of exponential size. However, by decomposing the prior tensor, inference of TMMs becomes realizable by Convolutional Arithmetic Circuits (ConvACs) – a recently proposed (Cohen et al., 2016a) ConvNet architecture based on two
operations, weighted sum and product pooling – which enables both tractable inference as well as tractable marginalization. While Graphical Models are typically hard to design, ConvACs follow the same design conventions of modern ConvNets, which reduces the task of designing a model to simply choosing the number of channels at each layer, and size of pooling windows. ConvACs were also the subject of several theoretical studies on its expressive capacity (Cohen et al., 2016a; Cohen and Shashua, 2016b) and comparing them to ConvNets (Cohen and Shashua, 2016a), showing they are especially suitable for high-dimensional natural data (images, audio, etc.) with a non-negligible advantage over standard ConvNets. Sum-Product Networks are another kind of Graphical Model realizable by Arithmetic Circuits, but they do not posses the same theoretical guarantees, nor do they provide a simple method to design efficient and expressive models.
The rest of the article is organized as follows. In sec. 2 we briefly review mathematical background on tensors required in order to follow our work. This is followed by sec. 3 which presents our generative model and its theoretical properties. How our model is trained is covered in sec. 4, and a thorough discussion on the importance of marginalization and its implications on our model is given in sec. 5. We conclude the article by presenting our experiments on classification with missing data in sec. 6, and revisit the main points of the article and future research in sec. 7.

2 PRELIMINARIES
We begin by establishing the minimal background in the field of tensor analysis required for following our work (see app. A for a more detailed review of the subject). A tensor is best thought of as a multi-dimensional array Ad1,...,dN ∈ R, where ∀i ∈ [N ], di ∈ [Mi] and N is referred to as the order of the tensor. For our purposes we typically assume that M1 = . . . = MN = M , and denote it as A ∈ (RM )⊗N . It is immediately apparent that performing operations with tensors, or simply storing them, quickly becomes intractable due to their exponential size of MN . That is one of the primary motivations behind tensor decomposition, which can be seen as a generalization of low-rank matrix factorization.
The relationship between tensor decomposition and networks arises from the simple observation, that through decomposition one can tradeoff storage complexity with computation, where the type of computation consists of sums and products. Specifically, the decompositions could be described by a compact representation coupled with a decoding algorithm of polynomial complexity to retrieve the entries of the tensor. Most tensor decompositions have a decoding algorithm representable via computation graphs of products and weighted sums, also known as Arithmetic Circuits (Shpilka and Yehudayoff, 2010) or Sum-Product Networks (Poon and Domingos, 2011). More specifically, these circuits take as input N indicator vectors δ1, . . . , δN , representing the coordinates (d1, . . . , dN ), where δi = 1[j=di], and output the value ofAd1,...,dN , where the weights of these circuits form the compact representation of tensors.
Applying this perspective to two of the most common decomposition formats, CANDECOMP/PARFAC (CP) and Hierarchical Tucker (HT), give rise to a shared framework for representing their decoding circuits by convolutional networks as illustrated in fig. 1, where a shallow network with one hidden layer corresponds to the CP decomposition, and a deep network with log2(N) hidden layers corresponds to the HT decomposition. The networks consists of just product pooling and 1×1 conv layers. Having no point-wise activations between the layers, the non-linearity of the models stems from the product pooling operation itself. The pooling layers also control the depth of the network by the choice of the size and the shape of pooling windows. The conv operator is not unlike the standard convolutional layer of ConvNets, with the sole difference being that it may operate without coefficient sharing, i.e. the filters that generate feature maps by sliding across the
previous layer may have different coefficients at different spatial locations. This is often referred to in the deep learning community as a locally-connected operator (Taigman et al., 2014).
Arithmetic Circuits constructed from the above conv and product pooling layers are called Convolutional Arithmetic Circuits, or ConvACs for short, first suggested by Cohen et al. (2016a) as a theoretical framework for studying standard convolutional networks, sharing many of the defining traits of the latter, most noteworthy, the locality, sharing and pooling properties of ConvNets. Unlike general circuits, the structure of the network is determined solely by two parameters, the number of channels of each conv layer and the size of pooling windows, which indirectly controls the depth of the network. Any decomposition that corresponds to a ConvAC can represent any tensor, given sufficient number of channels, though deeper circuits result in more efficient representations (Cohen et al., 2016a).
Finally, since we are dealing with generative models, the tensors we study are non-negative and sum to one, i.e. the vectorization of A (rearranging its entries to the shape of a vector), denoted by vec(A), is constrained to lie in the multi-dimensional simplex, denoted by:
4k := { x ∈ Rk+1|
∑k+1 i=1 xi = 1,∀i ∈ [k + 1] : xi ≥ 0 }
(1)

3 TENSORIAL MIXTURE MODELS
We represent the input signal X by a sequence of low-dimensional local structures
X = (x1, . . . ,xN ) ∈ (Rs)N This representation is quite natural for many high-dimensional input domains such as images – where the local structures represent patches consisting of s pixels – voice through spectrograms, and text through words.
A well-known observation, which has been verified in several empirical studies (e.g. by Zoran and Weiss (2011)), is that the distributions of local structures typically found in natural data could be sufficiently modeled by a mixture model consisting of only few components (on the order of 100) of simple distributions (e.g. Gaussian). Assuming the above holds for X ∈ (Rs)N and let {P (x|d; θd)}Md=1 be the mixing components, parameterized by θ1, . . . , θM , from which local structures are generated, i.e. for all i ∈ [N ] there exist di ∈ [M ] such that xi ∼ P (x|di; θdi), where di is a hidden variable specifying the matching component for the i-th local structure, then the probability density of sampling X is fully described by:
P (X) = ∑M
d1,...,dN=1 P (d1, . . . , dN ) ∏N i=1
P (xi|di; θdi) (2) where P (d1, . . . , dN ) represents the prior probability of assigning components d1, . . . , dN to their respective local structures x1, . . . ,xN . Even though we had to make an assumption on X to derive eq. 2, it is important to note that if we allow M to become unbounded, then any distribution with support in (Rs)N could be approximated by this equation. The argument follows from the universality property of the common parametric families of distributions (Gaussian, Laplacian, etc.), where any distribution can be approximated given sufficient number of components from these families, and thus the assumption always holds to some degree (see app. B for the complete proof).
The prior probabilities P (d1, . . . , dN ) can also be represented by a tensorA ∈ (RM )⊗N of orderN , given that the vectorization of A is constrained to the simplex, i.e. vec(A) ∈ 4(MN−1) (see eq. 1). Thus, we refer to eq. 2 as a Tensorial Mixture Model (TMM) with priors tensor A and mixing components P (x|d1; θ1), . . . , P (x|dN ; θN ). Notice that if N = 1 then we obtain the standard mixture model, whereas for a general N it is equivalent to a mixture model with tensorised mixing weights and conditionally independent mixing components.
Unlike standard mixture models, we cannot perform inference directly from eq. 2, nor can we even store the priors tensor directly given its exponential size of MN entries. Therefore the TMM as presented by eq. 2 is not tractable. The way to make the TMM tractable is to replace the tensor Ad1,...,dN by a tensor decomposition and, as described in the previous section, this gives rise to arithmetic circuits. But before we present our approach for tractable TMMs through tensor decompositions, it is worth examining some of the TMM special cases and how they relate to other known generative models.

3.1 SPECIAL CASES
We have already shown that TMMs can be thought of as a special case of mixture models, but it is important to also note that diagonal Gaussian Mixture Models (GMMs), probably the most common type of mixture models, are a strict subset of TMMs. Assume M = N ·K, as well as:
P (d1, . . . , dN ) = { wk ∀i ∈ [N ], di=N ·(k−1)+i 0 Otherwise
P (x|d; θd) = N (x;µki, diag(σ2ki)), d=N ·(k−1)+i then eq. 2 reduces to:
P (X) = ∑K k=1 wk ∏N i=1 N (x;µki, diag(σ2ki)) = ∑K k=1
wkN (x; µ̃k, diag(σ̃2k)) µ̃k = (µ T k1, . . . ,µ T kN ) T σ̃2k = ((σ 2 k1) T , . . . , (σ2kN ) T )T
which is equivalent to a diagonal GMM with mixing weights w ∈ 4K−1 and Gaussian mixture components with means {µ̃k}Kk=1 and covariances {diag(σ̃2k)}Kk=1. While the previous example highlights another connection between TMMs and mixture models, it does not take full advantage of the priors tensor, setting most of its entries to zero. Perhaps the simplest assumption we could make about the priors tensor, without it becoming degenerate, would be to assume that that the hidden variables d1, . . . , dN are statistically independent, i.e. P (d1, . . . , dN )= ∏N i=1 P (di). Then rearranging eq. 2 will result in a product of mixture models:
P (X) = ∏N
i=1 ∑M d=1 P (di = d)P (xi|di = d; θd)
If we also assume that the priors are identical in addition to being independent, i.e. P (d1 = d) = . . . = P (dN = d), then this model becomes a bag-of-words model, where the components {P (x|d; θd)}Md=1 define a soft dictionary for translating local-structures into ”words”, as is often done when applying bag-of-words models to images. Despite this familiar setting, had we subscribed to only using independent priors, we would lose the universality property of the general TMM model – it would not be capable of modeling dependencies between the local-structures.

3.2 DECOMPOSING THE PRIORS TENSOR
We have just seen that TMMs could be made tractable through constraints on the priors tensor, but it was at the expense of either not taking advantage of its tensor structure, or losing its universality property. Our approach for tractable TMMs is to apply tensor decompositions to the priors tensor, which is the conventional method for tackling the exponential size of high-order tensors.
We have already mentioned in sec. 2 that any decomposition representable by ConvACs, including the well-known CP and HT decompositions, can represent any tensor, and thus applying them would not limit the expressivity of our model. Fixing a ConvAC representing the priors tensor, i.e. ΦΘ(δ1, . . . , δN ) = Ad1,...,dN where Θ are the parameters of the ConvAC and {δi}Ni=1 are the indicators representation of {di}Ni=1, and simply rearranging the terms of eq. 2 after substituting the entries of the priors tensor with the sums and products expression of ΦΘ(δ1, . . . , δN ) results in:
P (X) = ΦΘ(q 1, . . . ,qN ) ∀i ∈ [N ]∀d ∈ [M ], qid = P (xi|di = d) (3)
which is nearly equivalent to how the ConvAC is used for computing the entries of the priors tensor, differing only in the way the input vectors are defined. Namely, eq. 3 is a result of
replacing indicator vectors δi with probability vectors qi, which could be interpreted as a soft variant of indicator vectors. Viewed as a network, it begins with a representation layer, mapping the local structures to the likelihood probabilities of belonging to each mixing component, i.e. {xi}Ni=1→{P (xi|di=d; θd)}N,Mi=1,d=1. Following the representation layer is the same ConvAC described by ΦΘ(·, . . . , ·). The complete network is illustrated by fig. 2. Unlike general tensors, for a TMM to represent a valid distribution, the priors tensor is constrained to the simplex and thus not every choice of parameters for the decomposition would result in a tensor holding this constraint. By restricting ourselves to non-negative decomposition parameters, i.e. use positive weights in the 1×1 conv layers, it guarantees the resulting tensors would be nonnegative as well. Additionally, normalizing the non-negative tensor is equivalent to requiring the parameters to be restricted to the simplex, i.e. for every layer l and spatial position j the weight vector wl,j ∈ 4rl−1−1 of the respective 1×1 conv kernel is normalized to sum to one. Under these constraints we refer to it as a generative decomposition. Notice that restricting ourselves to generative decompositions does not limit the expressivity of our model, as we can still represent any non-negative tensor and thus any distribution that the original TMM could represent. In discussing the above, it helps to distinguish between the two extreme cases of generative decompositions representable by ConvACs, namely, the shallow Generative CP decomposition referred to as the GCP-model, and the deep Generative HT decomposition referred to as the GHT-model.
Non-negative matrix and tensor decompositions have a long history together with the development of corresponding generative models, e.g., pLSA (Hofmann, 1999) which uses non-negative matrix decompositions for text analysis, which was later extended for images with the help of “visual words” (Li and Perona, 2005). The non-negative variant of the CP decomposition presented above is related to the more general Latent Class Models (Zhang, 2004), which could be seen as a multi-dimensional pLSA. Likewise, the non-negative HT decomposition is related to the Latent Tree Model (Zhang, 2004; Mourad et al., 2013) with the structure of a complete binary tree. Thus both the GCP and GHT models can be represented as a two-level graphical model, where the top level is either an LCM or an LTM, and the bottom level represent the local structures which are conditionally sampled from the mixing components of the TMM.
To conclude, the application of ConvACs to decompose the priors tensor leads to tractable TMMs with inference implemented by convolutional networks, has deep roots to classical use of nonnegative factorizations of generative models, and given sufficient resources does not limit expressivity. However, practical considerations raise the question on the extent of the expressive capacity of our models when the size of the ConvAC is polynomial with respect to the number of local structures and mixing components. This question was thoroughly studied in a series of works analyzing the importance of depth (Cohen et al., 2016a), compared them to the expressive capacity of ConvNets (Cohen and Shashua, 2016a), showing the latter is less capable than ConvACs, and the ability of ConvACs to model the dependency structure typically found in natural data (Cohen and Shashua, 2016b). We prove in app. D that their main results are not hindered by the introduction of simplex constraints to ConvACs as we did above. Together these results give us a detailed understanding of how the number of channels and size of pooling windows control the expressivity of the model. A more in depth overview of their results and its application to our models can be found in app. C.

3.3 COMPARISON TO SUM-PRODUCT NETWORKS
Sum-Product Networks (SPNs) are a related class of generative models which are also realized by Arithmetic Circuits, though not strictly convolutional circuits as defined above. While SPNs can realize any ConvAC and thus are universal and posses tractable inference, their lack of structure puts them at a disadvantage.
Picking the right SPN structure from the infinite possible combinations of sum and product nodes could be perplexing even for experts in the field. Indeed Poon and Domingos (2011); Gens and Domingos (2012) had to hand-engineer complex structures for each dataset guided by prior knowledge and heuristics, and while their results were impressive for their time, they are poor by current measures. This lead to many works studying the task of learning the structure directly from the data itself (Peharz et al., 2013; Gens and Domingos, 2013; Adel et al., 2015; Rooshenas and Lowd, 2014), which indeed improved upon manually designed SPNs on some tasks. Nevertheless, when
compared in absolute terms compared to other models, and not just average log-likelihood, they do not perform well even on simple handwritten digit classification datasets (Adel et al., 2015).
As opposed to SPNs, TMMs implemented with ConvACs have an easily designed architecture with only two set of parameters, size of pooling windows and number of channels, both of which can be directly related to the expressivity of the model as detailed in app. C. Additionally, while SPNs are typically trained using special EM-type algorithms, TMMs are trained using the stochastic gradient descent type algorithms as is common in training neural networks (see sec. 4 for details), thereby benefiting from the shared experience of a large and growing community.

4 CLASSIFICATION AND LEARNING WITH TMMS
Until this point we presented the TMM as a generative model for high-dimensional data, which is universal, and whose structure is tightly coupled to that of convolutional networks. We have yet to incorporate classification and learning into our framework. This is the purpose of the current section.
The common way to introduce object classes into a generative framework is to consider a class variable Y , and the distributions P (X|Y ) of the instanceX conditioned on Y . Under our model this is equivalent to having shared mixing components, but different priors tensors P (d1, . . . , dN |Y=y) for each class. Though it is possible to decompose each priors tensor separately, it is much more efficient to employ the concept of joint tensor decomposition, and use a shared ConvAC instead. This results in a single ConvAC computing inference, where instead of a single scalar output, multiple outputs are driven by the network – one for each class – as illustrated through the network in fig. 3.
Heading on to predicting the class of a given instance, we note that in practice, naı̈ve implementation of ConvACs is not numerically stable, the reason being that high degree polynomials (as computed by such networks) are easily susceptible to numerical underflow or overflow. The conventional method for tackling this issue is to perform all computations in log-space. This transforms ConvACs into SimNets, a recently introduced deep learning architecture (Cohen and Shashua, 2014; Cohen et al., 2016b). Finally, prediction is carried by returning the most likely class, which in the common setting of uniform class priors (PΘ(Y=y)≡1/K), translates to simply predicting the class for which the corresponding network output is maximal, in accordance with standard neural network practice:
Ŷ (X) = argmaxy P (Y = y|X) = argmaxy logP (X|Y = y)
Suppose now that we are given a training set S = {(X(i)∈(Rs)N , Y (i)∈[K])}|S|i=1 of instances and labels, and would like to fit the parameters Θ of multi-class TMM according to the Maximum Likelihood method. Equivalently, we minimize the Negative Log-Likelihood (NLL) loss function: L(Θ) = E[− logPΘ(X,Y )], which can be factorized into two separate loss functions:
L(Θ) = E[− logPΘ(Y |X)] + E[− logPΘ(X)] where E[− logPΘ(Y |X)] is commonly known as the cross-entropy loss, which we refer to as the discriminative loss, while E[− logPΘ(X)] corresponds to maximizing the prior likelihood P (X), and has no analogy in standard discriminative neural networks. It is this term that captures the generative nature of our model, and we accordingly refer to it as the generative loss. Now, let NΘ(X (i); y):= logPΘ(X (i)|Y=y) stand for the y’th output of the SimNet (ConvAC in log-space) realizing the TMM with parameters Θ, then in the case of uniform class priors, the empirical estimation of L(Θ) may be written as:
L(Θ;S) = − 1|S| ∑|S| i=1 log eNΘ(X (i);Y (i))
∑K y=1 e
NΘ(X(i);y) − 1|S| ∑|S| i=1 log ∑K y=1 eNΘ(X (i);y) (4)
Maximum likelihood training of generative models is oftentimes based on dedicated algorithms such as Expectation-Maximization, which are typically difficult to apply at scale. We leverage the resemblance between our objective (eq. 4) and that of standard neural networks, and apply the same optimization procedures used for the latter, which have proven to be extremely effective for training classifiers at scale. Whereas other works have used tensor decompositions for the optimization of probabilistic models (Song et al., 2013; Anandkumar et al., 2014), we employ them strictly for modeling and instead make use of conventional methods. In particular, our implementation of TMMs is based on the SimNets extension of Caffe toolbox (Cohen et al., 2016b; Jia et al., 2014), and uses standard Stochastic Gradient Descent-type methods for optimization (see sec. 6 for more details).

5 CLASSIFICATION WITH MISSING DATA THROUGH MARGINALIZATION
A major advantage of generative models over discriminative ones lies in the ability to cope with missing data, specifically in the context of classification. By and large, discriminative methods either attempt to complete missing parts of the data before classification, known as data imputation, or learn directly to classify data with missing values (Little and Rubin, 2002). The first of these approaches relies on the quality of data completion, a much more difficult task than the original one of classification with missing data. Even if the completion was optimal, the resulting classifier is known to be sub-optimal (see app. E). The second approach does not make this assumption, but nonetheless assumes that the distribution of missing values at train and test times are similar, a condition which often does not hold in practice. Indeed, Globerson and Roweis (2006) coined the term “nightmare at test time” to refer to the common situation where a classifier must cope with missing data whose distribution is different from that encountered in training.
As opposed to discriminative methods, generative models are endowed with a natural mechanism for classification with missing data. Namely, a generative model can simply marginalize over missing values, effectively classifying under all possible completions, weighing each completion according to its probability. This, however, requires tractable inference and marginalization. We have already shown in sec. 3 that TMM support the former, and will show in sec. 5.1 bring forth marginalization which is just as efficient. Beforehand, we lay out the formulation of classification with missing data.
Let X be a random vector in Rs representing an object, and Y be a random variable in [K]:={1, . . . ,K} representing its label. Denote byD(X ,Y) the joint distribution of (X ,Y), and by (x∈Rs, y∈[K]) specific realizations thereof. Assume that after sampling a specific instance (x, y), a random binary vectorM is drawn conditioned on X=x. More concretely, we sample a binary mask m∈{0, 1}s (realization ofM) according to a distributionQ(·|X=x). xi is considered missing ifmi is equal to zero, and observed otherwise. Formally, we consider the vector x m, whose i’th coordinate is defined to hold xi if mi=1, and the wildcard ∗ if mi=0. The classification task is then to predict y given access solely to x m. Following the works of Rubin (1976); Little and Rubin (2002), we consider three cases for the missingness distribution Q(M=m|X=x): missing completely at random (MCAR), where M is independent of X , i.e. Q(M=m|X=x) is a function of m but not of x; missing at random (MAR), whereM is independent of the missing values in X , i.e. Q(M=m|X=x) is a function of both m and x, but is not affected by changes in xi if mi=0; and missing not at random (MNAR), covering the rest of the distributions for whichM depends on missing values in X , i.e.Q(M=m|X=x) is a function of both m and x, which at least sometimes is sensitive to changes in xi when mi=0.
Let P be the joint distribution of the object X , label Y , and missingness maskM: P(X=x,Y=y,M=m) = D (X=x,Y=y) · Q(M=m|X=x)
For given x ∈ Rs and m ∈ {0, 1}s, denote by o(x,m) the event where the random vector X coincides with x on the coordinates i for which mi = 1. For example, if m is an all-zero vector o(x,m) covers the entire probability space, and if m is an all-one vector o(x,m) corresponds to the event X = x. With these notations in hand, we are now in a position to characterize the optimal predictor in the presence of missing data: Claim 1. For any data distribution D and missingness distribution Q, the optimal classification rule in terms of 0-1 loss is given by:
h∗(x m) = argmaxy P(Y=y|o(x,m))P(M=m|o(x,m),Y=y)
Proof. See app. E.
When the distributionQ is MAR (or MCAR), the classifier admits a simpler form, referred to as the marginalized Bayes predictor: Corollary 1. Under the conditions of claim 1, if the distributionQ is MAR (or MCAR), the optimal classification rule may be written as:
h∗(x m) = argmaxy P(Y=y|o(x,m)) (5)
Proof. See app. E.
Corollary 1 indicates that in the MAR setting, which is frequently encountered in practice, optimal classification does not require prior knowledge regarding the missingness distribution Q. As long as one is able to realize the marginalized Bayes predictor (eq. 5), or equivalently, to compute the likelihoods of observed values conditioned on labels (P(o(x,m)|Y=y)), classification with missing data is guaranteed to be optimal, regardless of the corruption process taking place. This is in stark contrast to discriminative methods, which require access to the missingness distribution during training, and thus are not able to cope with unknown conditions at test time.
Most of this section dealt with the task of prediction given an input with missing data, where we assumed we had access to a complete and uncorrupted training set, and only faced missingness during prediction. However, many times we wish to tackle the reverse problem, where the training set itself is riddled with missing data. Generative methods can once again leverage their natural ability to handle missing data in the form of marginalization during the learning stage. Generative models are typically learned through the Maximum Likelihood principle. When it comes to learning from missing data, the marginalized likelihood objective is used instead. Under the MAR assumption, this method results in an unbiased classifier (Little and Rubin, 2002).

5.1 EFFICIENT MARGINALIZATION WITH TMMS
As discussed above, with generative models optimal classification with missing data (in the MAR setting) is oblivious to the specific missingness distribution. However, it requires tractable computation of the likelihood of observed values conditioned on labels, i.e. tractable marginalization over missing values. The plurality of generative models that have recently gained attention in the deep learning community (Goodfellow et al., 2014; Kingma and Welling, 2014; Dinh et al., 2014; 2016) do not meet this requirement, and thus are not suitable for classification with missing data. TMMs on the other hand bring forth extremely efficient marginalization, requiring only a single forward pass through the corresponding network. Details follow.
Recall from sec. 3 and 4 that a multi-class TMM realizes the following form:
P (x1, . . . ,xN |Y=y) = ∑M
d1,...,dN P (d1, . . . , dN |Y=y) ∏N i=1
P (xi|di; θdi) (6) Suppose now that only the local structures xi1 . . .xiV are observed, and we would like to marginalize over the rest. Integrating eq. 6 gives:
P (xi1 , . . . ,xiV |Y=y) = ∑M
d1,...,dN P (d1, . . . , dN |Y=y) ∏V v=1
P (xiv |div ; θdiv ) from which it is evident that the same ConvAC used to compute P (x1, . . . ,xN |Y=y), can be used to compute P (xi1 , . . . ,xiV |Y=y) – all it requires is a slight adaptation of the representation layer. Namely, the latter would represent observed values through the usual likelihoods, whereas missing (marginalized) values would now be represented via constant ones:
rep(i, d) = {
1 , xi is missing (marginalized) P (xi|d; Θ) , xi is visible (not marginalized)
To conclude, with TMMs marginalizing over missing values is just as efficient as plain inference – requires only a single pass through the corresponding ConvAC. Accordingly, the marginalized Bayes predictor (eq. 5) is realized efficiently, and classification with missing data (in the MAR setting) is optimal, regardless of the missingness distribution. This capability is not provided by discriminative methods, which rely on the distribution of missing values being know at training, and by contemporary generative models, which do not bring forth tractable marginalization.

6 EXPERIMENTS
We demonstrate the properties of our models through both qualitative and quantitative experiments. In subsec. 6.1 we present our state-of-the-art results on image classification with missing data, with robustness to various missingness distributions. In app. G we show visualizations produced by our models, which gives us insight into its inner workings. Our experiments were conducted on the MNIST digit classification dataset, consisting of 60000 grayscale images of single digit numbers, as well as the small NORB 3D object recognition dataset, consisting of 48600 grayscale stereo images of toys belonging to 5 categories: four-legged animals, human figures, airplanes, trucks, and cars
In all our experiments we use either the GCP or GHT model with Gaussian mixing components. The weights of the conv layers are partially shared as described in sec 3.2, and are represented in log-space. For the case of the GHT model, we use 2× 2 pooling windows for all pooling layers. We train our model according to the loss described in sec. 4, using the Adam (Kingma and Ba, 2015) variant of SGD and decaying learning rates. We apply L2-regularization to the weights while taking into account they are stored in log-space. Additionally, we also adapt a probabilistic interpretation of dropout (?) by introducing random marginalization layers, that randomly select spatial locations in the input and marginalize over them. We provide a complete and detailed description of our experiments in app. F.
Our implementation, which is based on Caffe (Jia et al., 2014) and MAPS (Ben-Nun et al., 2015), as well as other code for reproducing our experiments, is available through our Github repository: https://github.com/HUJI-Deep/TMM.

6.1 IMAGE CLASSIFICATION WITH MISSING DATA
We demonstrate the effectiveness of our method for classification with missing data of unknown missingness distribution (see sec. 5), by conducting three kinds of experiments on the MNIST dataset, and an additional experiment on the NORB dataset. We begin by following the protocol of Globerson and Roweis (2006) – the binary classification problem of digit pairs with feature deletion noise – where we compare our method to the best known result on that benchmark (Dekel and Shamir, 2008). For our main experiment, we move to the harder multi-class digit classification under two different MAR missingness distributions, comparing against other methods which do not assume a specific missingness distribution. We repeat this experiment on the NORB dataset as well. Finally, our last experiment demonstrates the failure of purely discriminative methods to adapt to previously unseen missingness distributions, underlining the importance of the generative approach to missing data. We do wish to emphasize that missing data is not typically found in most image data, nevertheless, experiments on images with missing data are very common, for both classification and inpainting tasks. Additionally, there is nothing about our method, nor the methods we compare it against, that is very specific to the image domain, and thus any conclusion drawn should not be limited to the chosen datasets, but be taken in the broader context of the missing data problem.
The problem of learning classifiers which are robust to unforeseen missingness distributions at test time was first proposed by Globerson and Roweis (2006). They suggested missing values could be denoted by values which were deleted, i.e. their values were changed to zero, and a robust classifier would have to assume that any of its zero-value inputs could be the result of such a deletion process, and must be treated as missing. Their solution was to train a linear classifier and formulate the optimization as a quadric program under the constraint that N of its features could be deleted. In Dekel and Shamir (2008), this solution was improved upon and generalized to other kinds of corruption beyond deletion as well as to an adversarial setting.
We follow the central experiment of these articles, conducted on binary classification of digits pairs from the MNIST dataset, where N non-zero pixels are deleted with uniform probability over the set of N non-zero pixel locations of the given image. We compare our method, using the deep GHT-
Model, solely against the LP-based algorithm of Dekel and Shamir (2008), which is the previous state-of-the-art on this task. Due to the limited computational resources at the time, the original experiments were limited to training sets of just 50 images per digit. We have repeated their experiment, using the implementation kindly supplied to us by the authors, and increased the limit to 300 images per digit, which is the maximal amount possible with our current computational resources. Though it is possible to train our own models using much larger training sets, we have trained them under the same limitations. Despite the fact that missingness distribution of this experiment is of the MNAR type, which our method was not guarantied to be optimal under, the test results (see table 1) clearly show the large gap between our method and theirs. Additionally, whereas our method uses a single model trained once and with no prior knowledge on the missingness distribution, their method requires training special classifiers for each value of N , chosen through a cross-validation process, disqualifying it from being truly blind to the missingness distribution.
We continue to our main experiments on multi-class blind classification with missing data, where the missingness distribution is completely unknown during test time, and a single classifier must handle all possible distributions. We simulate two kinds of MAR missingness distributions: (i) an i.i.d. mask with a fixed probability p ∈ [0, 1] of missing each pixel, and (ii) a mask composed of the union of N possibly overlapping rectangles of width and height equal to W , each with a randomly assigned position in the image, distributed uniformly. We evaluate both our shallow GCP-Model as well as the deep GHT-Model against the most widely used methods for blind classification with missing data. We repeat these experiments on the MNIST and NORB datasets, the results of which are presented in fig. 4.
As a baseline for our results, we use K-Nearest Neighbors (KNN) to vote on the most likely class of a given example. We extend KNN to missing data by comparing distances using only the observed entries, i.e. for a corrupted instance x m, and a clean image from the training set x̃, we compute: d(x̃,x m)=∑mij=1(x̃ij−xij)2. Though it scores better than the majority of modern methods we have compared, in practice KNN is very inefficient, even more so for missing data, which prevents most common memory and runtime optimizations typically employed to reduce its inefficiency. Additionally, KNN does not generalize well for more complex datasets, as is evident by its poor performance on the clean test set of the NORB dataset.
As discusses in sec. 5, data-imputation is the most common method to handle missing data of unknown missingness distributions. Despite the popularity of this method, high quality data imputations are very hard to produce, amplified by the fact that classification algorithms are known to be highly sensitive to even a small noise applied to their inputs (?). Even if we assume the dataimputation step was done optimally, it would still not give optimal performance under all MAR missingness distributions, and under some settings could produce results which are only half as good as our method (see app. E for such a case). In our experiments, we have applied several data-imputations methods to complete the missing data, followed by classifying its outputs using a standard ConvNet fitted to the fully-observed training set. We first tested naive heuristics, filling missing values with zeros or the mean pixel value computed over all the images in the dataset. We then tested three generative models: GSN (Bengio et al., 2014), NICE (Dinh et al., 2014) and DPM (Sohl-Dickstein et al., 2015), which are known to work well for inpainting. GSN was omitted from the NORB experiments as we have not manage to properly train it on that dataset. Though the data-imputation methods are competitive when only few of the pixels are missing, they all fall far behind our models above a certain threshold, with more than 50 percentage points separating our GHT-model from the best data-imputation method under some of the cases. Additionally, all the generative models require very long runtimes, which prevents from using them in most real-world applications. While we tried to be as comprehensive as possible when choosing which inpainting methods to use, some of the most recent studies on the subject, e.g. the works of van den Oord et al. (2016) and Pathak et al. (2016), have either not yet published their code or only partially published it. We have also ruled out inpainting algorithms which are made specifically for images, as we did not want to limit the implications of these experiments solely to images.
We have also compared ourselves to the published results of the MPDBM model (Goodfellow et al., 2013). Unlike the previous generative models we tested, MPDBM is a generative classifier similar to our method. However, unlike our model, MPDBM does not posses the tractable marginalization nor the tractable inference properties, and uses approximations instead. Its lesser performance underlines the importance of these properties for achieving optimality under missing data. An additional factor might also be their training method, which includes randomly picking a subset of variables to act as missing, which might have introduced a bias to the specific missingness distribution used during their training.
In order to demonstrate the ineffectiveness of purely discriminative models, we trained ConvNets directly on randomly corrupted instances according to pre-selected missingness distributions on the MNIST dataset. Unlike the previous experiments, we do allow prior knowledge about the missingness distribution during training time. We found that the best results are achieved when replacing missing values with zeros, and adding as an extra input channel the mask of missing values (known as flag data-imputation). The results (see fig. 5) unequivocally show the effectiveness of this method when tested on the same distribution it was trained on, achieving a high accuracy even when only 10% of the pixels are visible. However, when tested on different distributions, whether on a completely different kind or even on the same kind but with different parameters, the accuracy drops by a large factor, at times by more than 35 percentage points. This illustrate the disadvantage of the discriminative method, as it necessarily incorporates bias towards the corruption process it had seen during training, which makes it fail on other distributions. One might wonder whether it is
possible for a single network to be robust on more than a single distribution. We found out that the latter is true, and if we train a network on multiple different missingness distributions1, then the network will achieve good performance on all such distributions, though at some cases not reaching the optimal performance. However, though it is possible to train a network to be robust on more than one distribution, the type of missingness distributions are rarely known in advance, and there is no known method to train a neural network against all possible distributions, limiting the effectivity of this method in practice.
Unlike all the above methods, our GHT-model, which is trained only once on the clean dataset, match or sometimes even surpass the performance of ConvNets that are trained and tested on the same distribution, showing it is achieving near optimal performance – as much as possible on any given distribution. Additionally, note that similar to ConvNets and according to the theory in app. C, the deep GHT-model is decidedly superior to the shallow GCP-model. Experimenting on more complex datasets is left for further research. Progress on optimization and regularization of networks based on product pooling (even in log-space) is required, and ways to incorporate larger b×b convolutional operations with overlaps would be useful before we venture into larger and complex datasets. Nevertheless, our preliminary results demonstrate an overwhelming advantage of our TMM models compared to competing methods, both in terms of robustness to different types of missing data, as well as in terms of raw performance, with very wide gaps in absolute accuracy than the next best method, at times as large as 50 percentage points more than the next best method.

7 SUMMARY
We have introduced a new family of probabilistic models, which we call Tensorial Mixture Models. TMMs are based on a simple assumption on the data, which stems from known empirical results on natural images, that gives rise to mixture models with tensorial structure represented by the priors tensor. When the priors tensor is decomposed it gives rise to an arithmetic circuit which in turn transforms the TMM into a Convolutional Arithmetic Circuit (ConvAC). A ConvAC corresponds to a shallow (single hidden layer) network when the priors tensor is decomposed by a CP (sum of rank-1) approach and corresponds to a deep network when the decomposition follows the Hierarchical Tucker (HT) model.
The ConvAC representation of a TMM possesses several attractive properties. First, the inference is tractable and is implemented by a forward pass through a deep network. Second, the architectural design of the model follows the deep networks community design, i.e., the structure of TMMs is determined by just two easily understood factors: size of pooling windows and number of channels. Finally, we have demonstrated the effectiveness of our model when tackling the problem of classification with missing data, leveraging TMMs unique ability of tractable marginalization which leads to optimal classifiers regardless of the missingness distribution.
There are several avenues for future research on TMMs which we are currently looking at, including other problems which TMMs could solve (e.g. semi-supervised learning), experimenting with other ConvACs architectures (e.g. through different decompositions), and further progress on optimization and regularization of networks with product pooling.

A BACKGROUND ON TENSOR DECOMPOSITIONS AND CONVOLUTIONAL ARITHMETIC CIRCUITS
We begin by establishing the minimal background in the field of tensor analysis required for following our work. A tensor is best thought of as a multi-dimensional array Ad1,...,dN ∈ R, where ∀i ∈ [N ], di ∈ [Mi]. The number of indexing entries in the array, which are also called modes, is referred to as the order of the tensor. The number of values an index of a particular mode can take is referred to as the dimension of the mode. The tensor A ∈ RM1⊗...⊗MN mentioned above is thus of order N with dimension Mi in its i-th mode. For our purposes we typically assume that M1 = . . . = MN = M , and simply denote it as A ∈ (RM )⊗N .
The fundamental operator in tensor analysis is the tensor product. The tensor product operator, denoted by ⊗, is a generalization of outer product of vectors (1-ordered vectors) to any pair of tensors. Specifically, letA and B be tensors of order P and Q respectively, then the tensor product A⊗ B results in a tensor of order P +Q, defined by: (A⊗ B)d1,...,dP+Q = Ad1,...,dP · BdP+1,...,dP+Q .
The main concept from tensor analysis we use in our work is that of tensor decompositions. The most straightforward and common tensor decomposition format is the rank-1 decomposition, also known as a CANDECOMP/PARAFAC decomposition, or in short, a CP decomposition. The CP decomposition is a natural extension of low-rank matrix decomposition to general tensors, both built upon the concept of a linear combination of rank-1 elements. Similarly to matrices, tensors of the form v(1) ⊗ · · · ⊗ v(N), where v(i) ∈ RMi are non-zero vectors, are regarded as N -ordered rank-1 tensors, thus the rank-Z CP decomposition of a tensor A is naturally defined by:
A = Z∑ z=1 aza z,1 ⊗ · · · ⊗ az,N
⇒ Ad1,...,dN = Z∑ z=1 az N∏ i=1 az,idi (7)
where {az,i ∈ RMi}N,Zi=1,z=1 and a ∈ R Z are the parameters of the decomposition. As mentioned above, for N = 2 it is equivalent to low-order matrix factorization. It is simple to show that any tensor A can be represented by the CP decomposition for some Z, where the minimal such Z is known as its tensor rank.
Another decomposition we will use in this paper is of a hierarchical nature and known as the Hierarchical Tucker decomposition (Hackbusch and Kühn, 2009), which we will refer to as HT decomposition. While the CP decomposition combines vectors into higher order tensors in a single step, the HT decomposition does that more gradually, combining vectors into matrices, these matrices into 4th ordered tensors and so on recursively in a hierarchically fashion. Specifically, the following describes the recursive formula of the HT decomposition2
2 More precisely, we use a special case of the canonical HT decomposition as presented in Hackbusch and Kühn (2009). In the terminology of the latter, the matrices Al,j,γ are diagonal and equal to diag(al,j,γ) (using the notations from eq. 8).
for a tensor A ∈ (RM )⊗N where N = 2L, i.e. N is a power of two3:
φ1,j,γ = r0∑ α=1 a1,j,γα a 0,2j−1,α ⊗ a0,2j,α
· · ·
φl,j,γ = rl−1∑ α=1
al,j,γα φ l−1,2j−1,α︸ ︷︷ ︸ order 2l−1 ⊗φl−1,2j,α︸ ︷︷ ︸ order 2l−1
· · ·
φL−1,j,γ = rL−2∑ α=1 aL−1,j,γα φ L−2,2j−1,α︸ ︷︷ ︸
order N 4
⊗φL−2,2j,α︸ ︷︷ ︸ order N
4 A = rL−1∑ α=1
aLα φ L−1,1,α︸ ︷︷ ︸ order N
2
⊗φL−1,2,α︸ ︷︷ ︸ order N
2
(8)
where the parameters of the decomposition are the vectors {al,j,γ∈Rrl−1}l∈{0,...,L−1},j∈[N/2l],γ∈[rl] and the top level vector aL ∈ RrL−1 , and the scalars r0, . . . , rL−1 ∈ N are referred to as the ranks of the decomposition. Similar to the CP decomposition, any tensor can be represented by an HT decomposition. Moreover, any given CP decomposition can be converted to an HT decomposition by only a polynomial increase in the number of parameters.
The relationship between tensor decomposition and networks arises from the simple observation that through decomposition one can tradeoff storage complexity with computation where the type of computation consists of sums and products. Specifically, tensor decompositions could be seen as a mapping, that takes a tensor of exponential size and converts it into a polynomially sized representation, coupled with a decoding algorithm of polynomial runtime complexity to retrieve the original entries of tensor – essentially trading off space complexity for computational complexity. Examining the decoding algorithms for the CP and HT decompositions, i.e. eq. 7 and eq. 8, respectively, reveal a shared framework for representing these algorithms via computation graphs of products and weighted sums, also known as Arithmetic Circuits (Shpilka and Yehudayoff, 2010) or Sum-Product Networks (Poon and Domingos, 2011). More specifically, these circuits take as inputN indicator vectors δ1, . . . , δN , representing the coordinates (d1, . . . , dN ), where δi = 1[j=di], and output the value of Ad1,...,dN . In the case of the CP decomposition, the matching decoding circuit is defined by eq. 9 below:
az,idi = M∑ d=1 az,id δid ⇒ Ad1,...,dN = Z∑ z=1 az N∏ i=1 M∑ d=1 az,id δid (9)
The above formula is better represented by the network illustrated in fig. 6, beginning with an input layer of√ N × √ N M -dimensional indicator vectors arranged in a 3D array, followed by a 1 × 1 conv operator, a global product pooling layer, and ends with a dense linear layer outputtingAd1,...,dN . The conv operator is not unlike the standard convolutional layer of ConvNets, with the sole difference being that it may operate without coefficient sharing, i.e. the filters that generate feature maps by sliding across the previous layer may have different coefficients at different spatial locations. This is often referred to in the deep learning community as a locally-connected operator (Taigman et al., 2014). Similarly to the CP decomposition, retrieving the entries of a tensor from its HT decomposition can be computed by the circuit represented in fig. 7, where instead of a single pair of conv and pooling layers there are log2 N such pairs, with pooling windows of size 2. Though the canonical HT decomposition dictates size 2 pooling windows, any pooling structure used in practice still results in a valid HT decomposition.
Arithmetic Circuits constructed from the above conv and product pooling layers are called Convolutional Arithmetic Circuits, or ConvACs for short, first suggested by Cohen et al. (2016a) as a theoretical framework for studying standard convolutional networks, sharing many of the defining traits of the latter, most noteworthy, the locality, sharing and pooling properties of ConvNets. Unlike general circuits, the structure of the network is determined solely by two parameters, the number of channels of each conv layer and the size of pooling windows, which indirectly controls the depth of the network.
3The requirement for N to be a power of two is solely for simplifying the definition of the HT decomposition. More generally, instead of defining it through a complete binary tree describing the order of operations, the canonical decomposition can use any balanced binary tree.

B THE UNIVERSALITY OF TENSORIAL MIXTURE MODELS
In this section we prove the universality property of TMMs, as discussed in sec. 3. We begin by taking note from functional analysis and define a new property called PDF total set, which is similar in concept to a total set, followed by proving that this property is invariant under the cartesian product of functions, which entails the universality of TMMs as a corollary. Definition 1. Let F be a set of PDFs over Rs. F is PDF total iff for any PDF h(x) over Rs and for all > 0 there exists M ∈ N, {f1(x), . . . , fM (x)} ⊂ F and w ∈ 4M−1 s.t. ∥∥∥h(x)−∑Mi=1 wifi(x)∥∥∥ 1 < . In other words, a set is a PDF total set if its convex span is a dense set under L1 norm.
Claim 2. Let F be a set of PDFs over Rs and let F⊗N = { ∏N i=1 fi(x)|∀i, fi(x) ∈ F} be a set of PDFs over the product space (Rs)N . If F is a PDF total set then F⊗N is PDF total set.
Proof. If F is the set of Gaussian PDFs over Rs with diagonal covariance matrices, which is known to be a PDF total set, then F⊗N is the set of Gaussian PDFs over (Rs)N with diagonal covariance matrices and the claim is trivially true.
Otherwise, let h(x1, . . . ,xN ) be a PDF over (Rs)N and let > 0. From the above, there exists K ∈ N, w ∈ 4M1−1 and a set of diagonal Gaussians {gij(x)}i∈[M1],j∈[N ] s.t.∥∥∥∥∥g(x)− M1∑ i=1 wi N∏ j=1 gij(xj) ∥∥∥∥∥ 1 < 2 (10) Additionally, since F is a PDF total set then there exists M2 ∈ N, {fk(x)}k∈[M2] ⊂ F and {wij ∈ 4M2−1}i∈[M1],j∈[N ] s.t. for all i ∈ [M1], j ∈ [N ] it holds that ∥∥∥gij(x)−∑M2k=1 wijkfk(x)∥∥∥ 1 < 2N , from which it is trivially proven using a telescopic sum and the triangle inequality that:∥∥∥∥∥ M1∑ i=1 wi N∏ j=1 gij(x)− M1∑ i=1 wi N∏ j=1 M2∑ k=1 wijkfk(xj) ∥∥∥∥∥ 1 < 2 (11) From eq. 10, eq. 11 the triangle inequality it holds that:∥∥∥∥∥∥g(x)− M2∑
k1,...,kN=1
Ak1,...,kN N∏ j=1 fkj (xj) ∥∥∥∥∥∥ 1 <
where Ak1,...,kN = ∑M1 i=1 wi ∏N j=1 wijkj which holds ∑M2 k1,...,kN=1 Ak1,...,kN = 1. Taking M = M N 2 ,
{ ∏N j=1 fkj (xj)}k1∈[M2],...,kN∈[M2] ⊂ F ⊗N and w = vec(A) completes the proof.
Corollary 2. Let F be a PDF total set of PDFs over Rs, then the family of TMMs with mixture components from F can approximate any PDF over (Rs)N arbitrarily well, given arbitrarily many components.

C OVERVIEW ON THE EXPRESSIVE CAPACITY OF CONVOLUTIONAL ARITHMETIC CIRCUITS AND ITS AFFECT ON TENSORIAL MIXTURE MODELS
The expressiveness of ConvACs has been extensively studied, and specifically the non-generative variants of our models, named CP-model and HT-model respectively. In Cohen et al. (2016a) it was shown that ConvACs
posses the property known as complete depth efficiency. Namely, almost all functions4 realized by an HT-model of polynomial size, for them to be realized (or approximated) by a CP-model, require it to be of exponential size. In other words, the expressiveness borne out of depth is exponentially stronger than a shallow network, almost always. It is worth noting that in the followup paper (Cohen and Shashua, 2016a), the authors have shown that the same result does not hold for standard ConvNets – while there are specific instances where depth efficiency holds, it is not complete, i.e. there is a non-zero probability that a function realized by a polynomially sized deep ConvNet can also be realized by a polynomially sized shallow ConvNet. Despite the additional simplex constraints put on the parameters, complete depth efficiency does hold for the generative ConvACs of our work, proof of which can be found in app. D, which shows the advantage of the deeper GHT-model over the shallow GCP-model. Additionally, this illustrates how the two factors controlling the architecture – number of channels and size of pooling windows – control the expressive capacity of the GHT-model. While the above shows why the deeper GHT-model is preferred over the shallow GCP-model, there is still the question of whether a polynomially sized GHT-model is sufficient for describing the complexities of natural data. Though a complete and definite answer is unknown as of yet, there are some strong theoretical evidence that it might. One aspect of being sufficient for modeling natural data is the ability of the model to describe the dependency structures typically found in the data. In Cohen and Shashua (2016b), the authors studied the separation rank – a measure of correlation, which for a given input partition, measures how far a function is from being separable – and found that a polynomially sized HT-model is capable of exponential separation rank for interleaved partitions, i.e. that it can model high correlations in local areas in the input. Additionally, for non-contiguous partitions, the separation rank can be at most polynomial, i.e. it can only model a limited correlation between far away areas in the input. These two results combined suggest that the HT-model, and thus also our GHT-model, is especially fit for modeling the type of correlations typically found in natural images and audio, even if it is only of polynomial size. Finally, from an empirical perspective, convolutional hierarchical structures have shown great success on multitude of different domains and tasks. Our models leverage these structures, taking them to a probabilistic setting, which leads us to believe that they will be able to effectively model distributions in practice – a belief we verify by experiments.

D PROOF FOR THE DEPTH EFFICIENCY OF GENERATIVE CONVOLUTIONAL ARITHMETIC CIRCUITS
In this section we prove that the depth efficiency property of ConvACs proved in Cohen et al. (2016a) applies also to the Generative ConvACs we have introduced in sec. 3.2. More specifically, we prove the following theorem, which is the generative analog of theorem 1 from (Cohen et al., 2016a): Theorem 1. Let Ay be a tensor of order N and dimension M in each mode, generated by the recursive formulas in eq. 8, under the simplex constraints introduced in sec. 3.2. Define r := min{r0,M}, and consider the space of all possible configurations for the parameters of the decomposition – {al,j,γ ∈ 4rl−1−1}l,j,γ . In this space, the generated tensorAy will have CP-rank of at least rN/2 almost everywhere (w.r.t. the product measure of simplex spaces). Put differently, the configurations for which the CP-rank of Ay is less than rN/2 form a set of measure zero. The exact same result holds if we constrain the composition to be “shared”, i.e. set al,j,γ ≡ al,γ and consider the space of {al,γ ∈ 4rl−1−1}l,γ configurations.
The only differences between ConvACs and their generative counter-parts are the simplex constraints applied to the parameters of the models, which necessitate a careful treatment to the measure theoretical arguments of the original proof. More specifically, while the k-dimensional simplex4k is a subset of the k+ 1-dimensional space Rk+1, it has a zero measure with respect to the Lebesgue measure over Rk+1. The standard method to define a measure over 4k is by the Lebesgue measure over Rk of its projection to that space, i.e. let λ : Rk → R be the Lebesgue measure over Rk, p : Rk+1 → Rk, p(x) = (x1, . . . , xk)T be a projection, and A ⊂ 4k be a subset of the simplex, then the latter’s measure is defined as λ(p(A)). Notice that p(4k) has a positive measure, and moreover that p is invertible over the set p(4k), and that its inverse is given by p−1(x1, . . . , xk) = (x1, . . . , xk, 1 − ∑k i=1 xi). In our case, the parameter space is the cartesian product of several simplex spaces of different dimensions, for each of them the measure is defined as above, and the measure over their cartesian product is uniquely defined by the product measure. Though standard, the choice of the projection function p above could be seen as a limitation, however, the set of zero measure sets in 4k is identical for any reasonable choice of a projection π (e.g. all polynomial mappings). More specifically, for any projection π : Rk+1 → Rk that is invertible over π(4k), π−1 is differentiable, and the Jacobian of π−1 is bounded over π(4k), then a subset A ⊂ 4k is of measure zero w.r.t. the projection π iff it is of measure zero w.r.t. p (as defined above). This implies that if we sample the weights of the generative decomposition (eq. 8 with simplex constraints) by a continuous distribution, a property that holds with probability 1 under the standard parameterization (projection p), will hold with probability 1 under any reasonable parameterization.
4”Almost all functions” in this context means, that for any continuous distribution over the parameters of the HT-model, with probability one the following statement is true for a function realized by an HT-model with sampled parameters.
We now state and prove a lemma that will be needed for our proof of theorem 1.
Lemma 1. Let M,N,K ∈ N, 1 ≤ r ≤ min{M,N} and a polynomial mapping A : RK → RM×N (i.e. for every i ∈ [M ], j ∈ [N ] then Aij : Rk → R is a polynomial function). If there exists a point x ∈ RK s.t. rank (A(x)) ≥ r, then the set {x ∈ RK |rank (A(x)) < r} has zero measure.
Proof. Remember that rank (A(x)) ≥ r iff there exits a non-zero r × r minor of A(x), which is polynomial in the entries of A(x), and so it is polynomial in x as well. Let c = ( M r ) · ( N r ) be the number of minors in A,
denote the minors by {fi(x)}ci=1, and define the polynomial function f(x) = ∑c i=1 fi(x)
2. It thus holds that f(x) = 0 iff for all i ∈ [c] it holds that fi(x) = 0, i.e. f(x) = 0 iff rank (A(x)) < r.
Now, f(x) is a polynomial in the entries of x, and so it either vanishes on a set of zero measure, or it is the zero polynomial (see Caron and Traynor (2005) for proof). Since we assumed that there exists x ∈ RK s.t. rank(A(x)) ≥ r, the latter option is not possible.
Following the work of Cohen et al. (2016a), our main proof relies on following notations and facts:
• We denote by [A] the matricization of an N -order tensor A (for simplicity, N is assumed to be even), where rows and columns correspond to odd and even modes, respectively. Specifically, if A ∈ RM1×···MN , the matrix [A] has M1 ·M3 · . . . ·MN−1 rows and M2 ·M4 · . . . ·MN columns, rearranging the entries of the tensor such that Ad1,...,dN is stored in row index 1 + ∑N/2 i=1(d2i−1 −
1) ∏N/2 j=i+1 M2j−1 and column index 1 + ∑N/2 i=1(d2i − 1) ∏N/2 j=i+1 M2j . Additionally, the matricization is a linear operator, i.e. for all scalars α1, α2 and tensors A1,A2 with the order and dimensions in every mode, it holds that [α1A1 + α2A2] = α1[A1] + α2[A2].
• The relation between the Kronecker product (denoted by ) and the tensor product (denoted by ⊗) is given by [A⊗ B] = [A] [B].
• For any two matrices A and B, it holds that rank (A B) = rank (A) · rank (B). • Let Z be the CP-rank of A, then it holds that rank ([A]) ≤ Z (see (Cohen et al., 2016a) for proof).
Proof of theorem 1. Stemming from the above stated facts, to show that the CP-rank of Ay is at least rN/2, it is sufficient to examine its matricization [Ay] and prove that rank ([Ay]) ≥ rN/2.
Notice from the construction of [Ay], according to the recursive formula of the HT-decomposition, that its entires are polynomial in the parameters of the decomposition, its dimensions are MN/2 each and that 1 ≤ rN/2 ≤ MN/2. In accordance with the discussion on the measure of simplex spaces, for each vector parameter al,j,γ ∈ 4rl−1−1, we instead examine its projection ãl,j,γ = p(al,j,γ) ∈ Rrl−1−1, and notice that p−1(ãl,j,γ) is a polynomial mapping5 w.r.t. ãl,j,γ . Thus, [Ay] is a polynomial mapping w.r.t. the projected parameters {ãl,j,γ}l,j,γ , and using lemma 1 it is sufficient to show that there exists a set of parameters for which rank ([Ay]) ≥ rN/2.
Denoting for convenience φL,1,1 := Ay and rL = 1, we will construct by induction over l = 1, ..., L a set of parameters, {al,j,γ}l,j,γ , for which the ranks of the matrices {[φl,j,γ ]}j∈[N/2l],γ∈[rl] are at least r
2l/2, while enforcing the simplex constraints on the parameters. More so, we’ll construct these parameters s.t. al,j,γ = al,γ , thus proving both the ”unshared” and ”shared” cases.
For the case l = 1 we have:
φ1,j,γ = r0∑ α=1 a1,j,γα a 0,2j−1,α ⊗ a0,2j,α
and let a1,j,γα = 1α≤r r and a0,j,αi = 1α=i for all i, j, γ and α ≤ M , and a 0,j,α i = 1i=1 for all i and α > M , and so
[φ1,j,γ ]i,j = { 1/r i = j ∧ i ≤ r 0 Otherwise
which means rank ( [φ1,j,γ ] ) = r, while preserving the simplex constraints, which proves our inductive hypothesis for l = 1.
5As we mentioned earlier, p is invertible only over p(4k), for which its inverse is given by p−1(x1, . . . , xk) = (x1, . . . , xk, 1 − ∑k i=1 xi). However, to simplified the proof and notations, we use p −1 as defined here over the entire range Rk−1, even where it does not serve as the inverse of p.
Assume now that rank ( [φl−1,j ′,γ′ ] ) ≥ r2
l−1/2 for all j′ ∈ [N/2l−1] and γ′ ∈ [rl−1]. For some specific choice of j ∈ [N/2l] and γ ∈ [rl] we have:
φl,j,γ = rl−1∑ α=1 al,j,γα φ l−1,2j−1,α ⊗ φl−1,2j,α
=⇒ [φl,j,γ ] = rl−1∑ α=1 al,j,γα [φ l−1,2j−1,α] [φl−1,2j,α]
Denote Mα := [φl−1,2j−1,α] [φl−1,2j,α] for α = 1, ..., rl−1. By our inductive assumption, and by the general property rank (A B) = rank (A) · rank (B), we have that the ranks of all matricesMα are at least r 2l−1/2 · r2 l−1/2 = r 2l/2. Writing [φl,j,γ ] = ∑rl−1 α=1 a l,j,γ α ·Mα, and noticing that {Mα} do not depend on al,j,γ , we simply pick al,j,γα = 1α=1, and thus φl,j,γ = M1, which is of rank r 2l/2. This completes the proof of the theorem.
From the perspective of TMMs, theorem 1 leads to the following corollary:
Corollary 3. Assume the mixing componentsM = {fi(x) ∈ L2(R2)∩L1(Rs)}Mi=1 are square integrable6 probability density functions, which form a linearly independent set. Consider a deep GHT-model of polynomial size whose parameters are drawn at random by some continuous distribution. Then, with probability 1, the distribution realized by this network requires an exponential size in order to be realized (or approximated w.r.t. the L2 distance) by the shallow GCP-model. The claim holds regardless of whether the parameters of the deep GHT-model are shared or not.
Proof. Given a coefficient tensorA, the CP-rank ofA is a lower bound on the number of channels (denoted by Z in the body of the article) required to represent that tensor by the ConvAC following the CP decomposition as introduced in sec. 2. Additionally, since the mixing components are linearly independent, their products { ∏N i=1 fi(xi)|fi ∈ M} are linearly independent as well, which entails that any distribution representable by the TMM with mixing components M has a unique coefficient tensor A. From theorem 1, the set of parameters of a polynomial GHT-model with a coefficient tensor of a polynomial CP-rank, the requirement for a polynomial GCP-model realizing that distribution exactly, forms a set of measure zero.
It is left to prove, that not only is it impossible to exactly represent a distribution with an exponential coefficient tensor by a GCP-model, it is also impossible to approximate it. This follows directly from lemma 7 in appendix B of Cohen et al. (2016a), as our case meets the requirement of that lemma.

E PROOF FOR THE OPTIMALITY OF MARGINALIZED BAYES PREDICTOR
In this section we give short proofs for the claims from sec. 5, on the optimality of the marginalized Bayes predictor under missing-at-random (MAR) distribution, when the missingness mechanism is unknown, as well as the general case when we do not add additional assumptions. In addition, we will also present a counter example proving data imputation results lead to suboptimal classification performance. We begin by introducing several notations that augment the notations already introduced in the body of the article.
Given a specific mask realization m ∈ {0, 1}s, we use the following notations to denote partial assignments to the random vector X . For the observed indices of X , i.e. the indices for which mi = 1, we denote a partial assignment by X \m = xo, where xo ∈ Rdo is a vector of length do equal to the number of observed indices. Similarly, we denote by X ∩ m = xm a partial assignment to the missing indices according to m, where xm ∈ Rdm is a vector of length dm equal to the number of missing indices. As an example of the notation, for given realizations x ∈ Rs and m ∈ {0, 1}s, we defined in sec. 5 the event o(x,m), which using current notation is marked by the partial assignment X \m = xo where xo matches the observed values of the vector x according to m.
With the above notations in place, we move on to prove claim 1, which describes the general solution to the optimal prediction rule given both the data and missingness distributions, and without adding any additional assumptions.
6It is important to note that most commonly used distribution functions are square integrable, e.g. most members of the exponential family such as the Gaussian distribution.
Proof of claim 1. Fix an arbitrary prediction rule h. We will show that L(h∗) ≤ L(h), where L is the expected 0-1 loss.
1− L(h)=E(x,m,y)∼(X ,M,Y)[1h(x m)=y] = ∑
m∈{0,1}s ∑ y∈[k] ∫ Rs P(M=m,X=x,Y=y)1h(x m)=ydx
= ∑
m∈{0,1}s ∑ y∈[k] ∫ Rdo ∫ Rdm P(M=m,X\m=xo,X∩m=xm,Y=y)1h(x⊗m)=ydxodxm
=1 ∑
m∈{0,1}s ∑ y∈[k] ∫ Rdo 1h(x m)=ydxo ∫ Rdm P(M=m,X\m=xo,X∩m=xm,Y=y)dxm
=2 ∑
m∈{0,1}s ∑ y∈[k] ∫ Rdo 1h(x m)=yP(M=m,X\m=xo,Y=y)dxo
=3 ∑
m∈{0,1}s
∫ Rdo P(X\m=xo) ∑ y∈[k] 1h(x m)=yP(Y=y|X\m=xo)P(M=m|X\m=xo,Y=y)dxo
≤4 ∑
m∈{0,1}s
∫ Rdo P(X\m=xo) ∑ y∈[k] 1h∗(x m)=yP(Y=y|X\m=xo)P(M=m|X\m=xo,Y=y)dxo
=1− L(h∗) Where (1) is because the output of h(x m) is independent of the missing values, (2) by marginalization, (3) by conditional probability definition and (4) because by definition h∗(x m) maximizes the expression P(Y=y|X\m=xo)P(M=m|X\m=xo,Y=y) w.r.t. the possible values of y for fixed vectors m and xo. Finally, by replacing integrals with sums, the proof holds exactly the same when instances (X ) are discrete.
We now continue and prove corollary 1, a direct implication of claim 1 which shows that in the MAR setting, the missingness distribution can be ignored, and the optimal prediction rule is given by the marginalized Bayes predictor.
Proof of corollary 1. Using the same notation as in the previous proof, and denoting by xo the partial vector containing the observed values of x m, the following holds:
P(M=m|o(x,m),Y=y) := P(M=m|X\m=xo,Y=y)
= ∫ Rdm P(M=m,X ∩m=xm|X\m=xo,Y=y)dxm
= ∫ Rdm P(X∩m=xm|X\m=xo,Y=y) · P(M=m|X∩m=xm,X\m=xo,Y=y)dxm
=1 ∫ Rdm P(X∩m=xm|X\m=xo,Y=y) · P(M=m|X∩m=xm,X\m=xo)dxm
=2 ∫ Rdm P(X∩m=xm|X\m=xo,Y=y) · P(M=m|X\m=xo)dxm
=P(M=m|X\m=xo) ∫ Rdm P(X∩m=xm|X\m=xo,Y=y)dxm
=P(M=m|o(x,m)) Where (1) is due to the independence assumption of the events Y = y andM = m conditioned on X = x, while noting that (X \m = xo) ∧ (X ∩m = xm) is a complete assignment of X . (2) is due to the MAR assumption, i.e. that for a given m and xo it holds for all xm ∈ Rdm :
P(M=m|X\m=xo,X∩m=xm) = P(M=m|X\m=xo) We have shown that P(M=m|o(x,m),Y = y) does not depend on y, and thus does not affect the optimal prediction rule in claim 1. It may therefore be dropped, and we obtain the marginalized Bayes predictor.
Having proved that in the MAR setting, classification through marginalization leads to optimal performance, we now move on to show that the same is not true for classification through data-imputation. Though there are many methods to perform data-imputation, i.e. to complete missing values given the observed ones, all of these methods can be seen as the solution of the following optimization problem, or more typically its approximation:
g(x m) = argmax x′∈Rs∧∀i:mi=1→x′i=xi
P(X = x′)
Where g(x m) is the most likely completion of x m. When data-imputation is carried out for classification purposes, one is often interested in data-imputation conditioned on a given class Y = y, i.e.:
g(x m; y) = argmax x′∈Rs∧∀i:mi=1→x′i=xi
P(X = x′|Y = y)
Given a classifier h : Rs → [K] and an instance x with missing values according to m, classification through data-imputation is simply the result of applying h on the output of g. When h is the optimal classifier for complete data, i.e. the Bayes predictor, we end up with one of the following prediction rules:
Unconditional: h(x m) = argmax y P(Y = y|X = g(x m))
Conditional: h(x m) = argmax y P(Y = y|X = g(x m; y))
Claim 3. There exists a data distribution D and MAR missingness distribution Q s.t. the accuracy of classification through data-imputation is almost half the accuracy of the optimal marginalized Bayes predictor, with an absolute gap of more than 33 percentage points.
Proof. For simplicity, we will give an example for a discrete distribution over the binary set X ×Y = {0, 1}2 × {0, 1}. Let 1> > 0 be some small positive number, and we defineD according to table 2, where each triplet (x1, x2, y) ∈ X×Y is assigned a positive weight, which through normalization defines a distribution over X×Y . The missingness distribution Q is defined s.t. PQ(M1 = 1,M2 = 0|X = x) = 1 for all x ∈ X , i.e. X1 is always observed andX2 is always missing, which is a trivial MAR distribution. Given the above data distribution D, we can easily calculate the exact accuracy of the optimal data-imputation classifier and the marginalized Bayes predictor under the missingness distribution Q, as well as the standard Bayes predictor under full-observability. First notice that whether we apply conditional or unconditional data-imputation, and whether X1 is equal to 0 or 1, the completion will always be X2 = 1 and the predicted class will always be Y = 1. Since the data-imputation classifiers always predict the same class Y = 1 regardless of their input, the probability of success is simply the probability P (Y = 1) = 1+
3 (for = 10−4 it equals approximately
33.337%). Similarly, the marginalized Bayes predictor always predicts Y = 0 regardless of its input, and so its probability of success is P (Y = 0) = 2−
3 (for = 10−4 it equals approximately 66.663%), which is
almost double the accuracy achieved by the data-imputation classifier. Additionally, notice that the marginalized Bayes predictor achieves almost the same accuracy as the Bayes predictor under full-observability, which equals exactly 2
3 .

F DETAILED DESCRIPTION OF THE EXPERIMENTS
Experiments are meaningful only if they could be reproduced by other proficient individuals. Providing sufficient details to enable others to replicate our results is the goal of this section. We hope to accomplish this by making our code public, as well as documenting our experiments to a sufficient degree allowing for their reproduction from scratch. Our complete implementation of the models presented in this paper, as well as our modifications to other open-source projects and scripts used in the process of conducting our experiments, are available at our Github repository: https://github.com/HUJI-Deep/TMM. We additionally wish to invite readers to contact the authors, if they deem the following details insufficient in their process to reproduce our results.

F.1 DESCRIPTION OF METHODS
In the following we give concise descriptions of each classification method we have used in our experiments. The results of the experiment on MP-DBM (Goodfellow et al., 2013) were taken directly from the paper and
were not conducted by us, hence we do not cover it in this section. We direct the reader to that article for exact details on how to reproduce their results.

F.1.1 ROBUST LINEAR CLASSIFIER
In Dekel and Shamir (2008), binary linear classifiers were trained by formulating their optimization as a quadric program under the constraint that some of its features could be deleted, i.e. their original value was changed to zero. While the original source code was never published, the authors have kindly agreed to share with us their code, which we used to reproduced their results, but on larger datasets. The algorithm has only a couple hyper-parameters, which were chosen by a grid-search through a cross-validation process. For details on the exact protocol for testing binary classifiers on missing data, please see sec. F.2.1.

F.1.2 K-NEAREST NEIGHBORS
K-Nearest Neighbors (KNN) is a classical machine learning algorithm used for both regression and classification tasks. Its underlying mechanism is finding the k nearest examples (called neighbors) from the training set, (x1, y1), . . . , (xk, yk) ∈ S, according to some metric function d(·, ·) : X × X → R+, after which a summarizing function f is applied to the targets of the k nearest neighbors to produce the output y∗ = f(y1, . . . , yk). When KNN is used for classification, f is typically the majority voting function, returning the class found in most of the k nearest neighbors.
In our experiments we use KNN for classification with missing data, where the training set consists of complete examples with no missing data, but at classification time the inputs have missing values. Given an input with missing values x m and an example x′ from the training set, we use a modified Euclidean distance metric, where we compare the distance only against the non-missing coordinates of x, i.e. the metric is defined by d(x′,x m) = ∑ i:mi=1 (x′i − xi) 2. Through a process of cross-validation we have chosen k = 5 for all of our experiments. Our implementation of KNN is based on the popular scikit-learn python library (Pedregosa et al., 2011).

F.1.3 CONVOLUTIONAL NEURAL NETWORKS
The most widespread and successful discriminative method nowadays are Convolutional Neural Networks (ConvNets). Standard ConvNets are represented by a computational graph consisted of different kinds of nodes, called layers, with a convolutional-like operators applied to their inputs, followed by a non-linear point-wise activation function, e.g. max(0, x) known as ReLU.
For our experiments on MNIST, both with and without missing data, we have used the LeNeT ConvNet architecture (LeCun et al., 1998) that is bundled with Caffe (Jia et al., 2014), trained for 20,000 iterations using SGD with 0.9 momentum and 0.01 base learning rate, which remained constant for 10,000 iterations, followed by a linear decrease to 0.001 for another 5,000 iterations, followed by a linear decrease to 0 learning rate for the remaining 5,000 iterations. The model also used l2-regularization (also known as weight decay), which was chosen through cross-validation for each experiment separately. No other modifications were made to the model or its training procedure.
For our experiments on NORB, we have used an ensemble of 3 ConvNets, each using the following architecture: 5×5 convolution with 128 output channels, 3×3 max pooling with stride 2, ReLU activation, 5×5 convolution with 128 output channels, ReLU activation, dropout layer with probability 0.5, 3×3 average pooling with stride 2, 5×5 convolution with 256 output channels, ReLU activation, dropout layer with probability 0.5, 3×3 average pooling with stride 2, fully-connected layer with 768 output channels, ReLU activation, dropout layer with probability 0.5, and ends with fully-connected layer with 5 output channels. The stereo images were represented as a two-channel input image when fed to the network. During training we have used data augmentation consisting of randomly scaling and rotation transforms. The networks were trained for 40,000 iterations using SGD with 0.99 momentum and 0.001 base learning rate, which remained constant for 30,000 iterations, followed by a linear decrease to 0.0001 for 6000 iterations, followed by a linear decrease to 0 learning rate for the remaining 4,000 iterations. The model also used 0.0001 weight decay for additional regularization.
When ConvNets were trained on images containing missing values, we passed the network the original image with missing values zeroed out, and an additional binary image as a separate channel, containing 1 for missing values at the same spatial position, and 0 otherwise – this missing data format is sometimes known as flag data imputation. Other formats for representing missing values were tested (e.g. just using zeros for missing values), however, the above scheme performed significantly better than other formats. In our experiments, we assumed that the training set was complete and missing values were only present in the test set. In order to design ConvNets that are robust against specific missingness distributions, we have simulated missing values during training, sampling a different mask of missing values for each image in each mini-batch. As covered in sec. 6, the results of training ConvNets directly on simulated missingness distributions resulted in classifiers
which were biased towards the specific distribution used in training, and performed worse on other distributions compared to ConvNets trained on the same distribution.
In addition to training ConvNets directly on missing data, we have also used them as the classifier for testing different data imputation methods, as describe in the next section.

F.1.4 CLASSIFICATION THROUGH DATA IMPUTATION
The most common method for handling missing data, while leveraging available discriminative classifiers, is through the application of data imputation – an algorithm for the completion of missing values – and then passing the results to a classifier trained on uncorrupted dataset. We have tested five different types of data imputation algorithms:
• Zero data imputation: replacing every missing value by zero. • Mean data imputation: replacing every missing value by the mean value computed over the dataset. • Generative data imputation: training a generative model and using it to complete the missing values
by finding the most likely instance that coincides with the observed values, i.e. solving the following
g(x m) = argmax x′∈Rs∧∀i,mi=1→x′i=xi
P (X = x′)
We have tested the following generative models:
– Generative Stochastic Networks (GSN) (Bengio et al., 2014): We have used their original source code from https://github.com/yaoli/GSN, and trained their example model on MNIST for 1000 epochs. Whereas in the original article they have tested completing only the left or right side of a given image, we have modified their code to support general masks. Our modified implementation can be found at https://github.com/HUJI-Deep/GSN.
– Non-linear Independent Components Estimation (NICE) (Dinh et al., 2014): We have used their original source code from https://github.com/laurent-dinh/nice, and trained it on MNIST using their example code without changes. Similarly to our modification to the GSN code, here too we have adapted their code to support general masks over the input. Additionally, their original inpainting code required 110,000 iterations, which we have reduced to just 8,000 iterations, since the effect on classification accuracy was marginal. For the NORB dataset, we have used their CIFAR10 example, with lower learning rate of 10−4. Our modified code can be found at https://github.com/HUJI-Deep/nice.
– Diffusion Probabilistic Models (DPM) (Sohl-Dickstein et al., 2015): We have user their original source code from https://github.com/Sohl-Dickstein/ Diffusion-Probabilistic-Models, and trained it on MNIST using their example code without changes. Similarly to our modifications to GSN, we have add support for a general mask of missing values, but other than that kept the rest of the parameters for inpainting unchanged. For NORB we have used the same model as MNIST. We have tried using their CIFAR10 example for NORB, however, it produced exceptions during training. Our modified code can be found at https://github.com/HUJI-Deep/Diffusion-Probabilistic-Models.

F.1.5 TENSORIAL MIXTURE MODELS
For a complete theoretical description of our model please see the body of the article. Our models were implemented by performing all intermediate computations in log-space, using numerically aware operations. In practiced, that meant our models were realized by the SimNets architecture (Cohen and Shashua, 2014; Cohen et al., 2016b), which consists of Similarity layers representing gaussian distributions, MEX layers representing weighted sums performed on log-space input and outputs, as well as standard pooling operations. The learned parameters of the MEX layers are called offsets, which represents the weights of the weighted sum, but saved in log-space. The parameters of the MEX layers can be optionally shared between spatial regions, or alternatively left with no parameter sharing at all. Additionally, when used to implement our generative models, the offsets are normalized to have a soft-max (i.e., log (∑ i exp(xi) ) ) of zero.
The network architectures we have tested in this article, consists of M different Gaussian mixture components with diagonal covariance matrices, over non-overlapping patches of the input of size 2× 2, which were implemented by a similarity layer as specified by the SimNets architecture, but with an added gaussian normalization term.
We first describe the architectures used for the MNIST dataset. For the GCP-model, we used M = 800, and following the similarity layer is a 1 × 1 MEX layer with no parameter sharing over spatial regions and 10 output channels. The model ends with a global sum pooling operation, followed by another 1 × 1 MEX layer
with 10 outputs, one for each class. The GHT-model starts with the similarity layer with M = 32, followed by a sequence of four pairs of 1 × 1 MEX layer followed by 2 × 2 sum pooling layer, and after the pairs and additional 1 × 1 MEX layer lowering the outputs of the model to 10 outputs as the number of classes. The number of output channels for each MEX layer are as follows 64-128-256-512-10. All the MEX layers in this network do not use parameter sharing, except the first MEX layer, which uses a repeated sharing pattern of 2 × 2 offsets, that analogous to a 2 × 2 convolution layer with stride 2. Both models were trained with the losses described in sec. 4, using the Adam SGD variant for optimizing the parameters, with a base learning rate of 0.03, and β1 = β2 = 0.9. The models were trained for 25,000 iterations, where the learning rate was dropped by 0.1 after 20,000 iterations.
For the NORB dataset, we have trained only the GHT-model with M = 128 for the similarity layer. The MEX layers use the same parameter sharing scheme as the one for MNIST, and the number of output channels for each MEX layer are as follows: 256-256-256-512-5. Training was identical to the MNIST models, with the exception of using 40,000 iterations instead of just 25,000. Additionally, we have used an ensemble of 4 models trained separately, each trained using a different generative loss weight (see below for more information). We have also used the same data augmentation methods (scaling and rotation) which were used in training the ConvNets for NORB used in this article.
The standard L2 weight regularization (sometimes known as weight decay) did not work well on our models, which lead us to adapt it to better fit to log-space weights, by minimizing λ ∑ i (exp (xi))
2 instead of λ||x||2 = λ ∑ i x 2 i , where the parameter λ was chosen through cross-validation. Additionally, since even with large values of λ our model was still overfitting, we have added another form of regularization in the form of random marginalization layers. A random marginalization layer, is similar in concept to dropout, but instead of zeroing activations completely in random, it choses spatial locations at random, and then zero out the activations at those locations for all the channels. Under our model, zeroing all the activations in a layer at a specific location, is equivalent to marginalizing over all the inputs for the receptive field for that respective location. We have used random marginalization layers in between all our layers during training, where the probability for zeroing out activations was chosen through cross-validation for each layer separately. Though it might raise concern that random marginalization layers could lead to biased results toward the missingness distributions we have tested it on, in practice the addition of those layers only helped improve our results under cases where only few pixels where missing.
Finally, we wish to discuss a few optimization tricks which had a minor effects compared to the above, but were nevertheless very useful in achieving slightly better results. First, instead of optimizing directly the objective defined by eq. 4, we add smoothing parameter β between the two terms, as follows:
Θ∗ = argmin Θ − |S|∑ i=1 log eNΘ(X (i);Y (i))∑K y=1 e NΘ(X (i);y) − β |S|∑ i=1 log K∑ y=1 eNΘ(X (i);y)
setting β too low diminish the generative capabilities of our models, while setting it too high diminish the discriminative performance. Through cross-validation, we decided on the value β = 0.01 for the models trained on MNIST, while for NORB we have used a different value of β for each of the models, ranging in {0.01, 0.1, 0.5, 1}. Second, we found that performance increased if we normalized activations before applying the 1 × 1 MEX operations. Specifically, we calculate the soft-max over the channels for each spatial location which we call the activation norm, and then subtract it from every respective activation. After applying the MEX operation, we add back the activation norm. Though might not be obvious at first, subtracting a constant from the input of a MEX operation and adding it to its output is equivalent does not change the mathematical operation. However, it does resolve the numerical issue of adding very large activations to very small offsets, which might result in a loss of precision. Finally, we are applying our model in different translations of the input and then average the class predictions. Since our model can marginalize over inputs, we do not need to crop the original image, and instead mask the unknown parts after translation as missing. Applying a similar trick to standard ConvNets on MNIST does not seem to improve their results. We believe this method is especially fit to our model, is because it does not have a natural treatment of overlapping patches like ConvNets do, and because it is able to marginalize over missing pixels easily, not limiting it just to crop translation as is typically done.

F.2 DESCRIPTION OF EXPERIMENTS
In this section we will give a detailed description of the protocol we have used during our experiments.

F.2.1 BINARY DIGIT CLASSIFICATION WITH FEATURE DELETION MISSING DATA
This experiment focuses on the binary classification problem derived from MNIST, by limiting the number of classes to two different digits at a time. We use the same non-zero feature deletion distribution as suggested by Globerson and Roweis (2006), i.e. for a given image we uniformly sample a set of N non-zero pixels from the
image (if the image has less than N non-zero pixels then they are non-zero pixels are chosen), and replace their values with zeros. This type of missingness distribution falls under the MNAR type defined in sec.5.
We test values of N in {0, 25, 50, 75, 100, 125, 150}. For a given value of N , we train a separate classifier on each digit pair classifier on a randomly picked subset of the dataset containing 300 images per digit (600 total). During training we use a fixed validation set with 1000 images per digit. After picking the best classifier according to the validation set, the classifier is tested against a test set with a 1000 images per digits with a randomly chosen missing values according to the value of N . This experiment is repeated 10 times for each digit pair, each time using a different subset for the training set, and a new corrupted test set. After conducting all the different experiments, all the accuracies are averaged for each value of N , which are reported in table 1.

F.2.2 MULTI-CLASS DIGIT CLASSIFICATION WITH MAR MISSING DATA
This experiment focuses on the complete multi-class digit classification of the MNIST dataset, in the presence of missing data according to different missingness distributions. Under this setting, only the test set contains missing values, whereas the training set does not. We test two kinds of missingness distributions, which both fall under the MAR type defined in sec.5. The first kind, which we call i.i.d. corruption, each pixel is missing with a fixed probability p. the second kind, which we call missing rectangles corruption, The positions of N rectangles of widthW or chosen uniformly in the picture, where the rectangles can overlap one another. During the training stage, the models to be tested are not to be biased toward the specific missingness distributions we have chosen, and during the test stage, the same classifier is tested against all types of missingness distributions, and without supplying it with the parameters or type of the missingness distribution it is tested against. This rule prevent the use of ConvNets trained on simulated missingness distributions. To demonstrate that the latter lead to biased classifiers, we have conducted a separate experiment just for ConvNets, where the previous rule is ignored, and we train a separate ConvNet classifier on each type and parameter of the missingness distributions we have used. We then tested each of those ConvNets on all other missingness distributions, the results of which are in fig. 5, which confirmed our hypothesis.
G IMAGE GENERATION AND NETWORK VISUALIZATION
Following the graphical model perspective of our models allows us to not only generate random instances from the distribution, but to also generate the most likely patches for each neuron in the network, effectively explaining its role in the classification process. We remind the reader that every neuron in the network corresponds to a possible assignment of a latent variable in the graphical model. By looking for the most likely assignments for each of its child nodes in the graphical tree model, we can generate a patch that describes that neuron. Unlike similar suggested methods to visualize neural networks (Zeiler and Fergus, 2014), often relying on brute-force search or on solving some optimization problem to find the most likely image, our method emerges naturally from the probabilistic interpretation of our model.
In fig. 8, we can see conditional samples generates for each digit, while in fig. 9 we can see a visualization of the top-level layers of network, where each small patch matches a different neuron in the network. The common wisdom of how ConvNets work is by assuming that simple low-level features are composed together to create more and more complex features, where each subsequent layer denotes features of higher abstraction – the visualization of our network clearly demonstrate this hypothesis to be true for our case, showing small strokes iteratively being composed into complete digits.

H RAW RESULTS OF EXPERIMENTS
For both presentational and page layout reasons we have chosen to present most of results in the form of charts in the body or the article. Considering that exact results are important for both reproducibility as well as future comparisons to our work, we provide below the raw results of our experiments in the form of detailed tables. For completeness, some of the tables we did include in the body of the article are duplicated to here as well.
","We introduce a generative model, we call Tensorial Mixture Models (TMMs) based on mixtures of basic component distributions over local structures (e.g. patches in an image) where the dependencies between the local-structures are represented by a ”priors tensor” holding the prior probabilities of assigning a component distribution to each local-structure. In their general form, TMMs are intractable as the priors tensor is typically of exponential size. However, when the priors tensor is decomposed it gives rise to an arithmetic circuit which in turn transforms the TMM into a Convolutional Arithmetic Circuit (ConvAC). A ConvAC corresponds to a shallow (single hidden layer) network when the priors tensor is decomposed by a CP (sum of rank-1) approach and corresponds to a deep network when the decomposition follows the Hierarchical Tucker (HT) model. The ConvAC representation of a TMM possesses several attractive properties. First, the inference is tractable and is implemented by a forward pass through a deep network. Second, the architectural design of the model follows the deep networks community design, i.e., the structure of TMMs is determined by just two easily understood factors: size of pooling windows and number of channels. Finally, we demonstrate the effectiveness of our model when tackling the problem of classification with missing data, leveraging TMMs unique ability of tractable marginalization which leads to optimal classifiers regardless of the missingness distribution.",ICLR 2017 conference submission,False,,"The authors have recently made several connections between deep learning and tensor algebra. While their earlier works dealt with supervised learning, the current work analyzes generative models through the lens of tensor algebra. 
 The authors show propose a tensorial mixture model over local structures where the mixture components are expressed as tensor decompositions. They show that hierarchical tensor decomposition is exponentially more expressive compared to the shallow models. 
 The paper makes original contributions in terms of establishing expressivity of deep generative models. The connections with tensor algebra could lead to further innovations, e.g. in training algorithms.
 However, the paper can be improved in two aspects: 
 (1) It will be nice if the authors make a connection between the algebraic view presented here with the geometric view presented by:

---

This paper proposes a generative model for mixtures of basic local structures where the dependency between local structures is a tensor. They use tensor decomposition and the result of their earlier paper on expressive power of CNNs along with hierarchical Tucker to provide an inference mechanism. However, this is conditioned on the existence of decomposition. The authors do not discuss how applicable their method is for a general case, what is the subspace where this decomposition exists/is efficient/has low approximation error. Their answer to this question is that in deep learning era these theoretical analysis is not needed. While this claim is subjective, I need to emphasize that the paper does not clarify this claim and does not mention the restrictions. Hence, from theoretical perspective, the paper has flaws and the claims are not justified completely. Some claims cannot be justified with the  current results in tensor literature as the authors also mentioned in the discussions. Therefore, they should have corrected their claims in the paper and made the clarifications that this approach is restricted to a clear subclass of tensors.

If we ignore the theoretical aspect and only consider the paper from empirical perspective, the experiments the appear in the paper are not enough to accept the paper. MNIST and CIFAR-10 are very simple baselines and more extensive experiments are required. Also, the experiments for missing data are not covering real cases and are too synthetic. Also, the paper lacks the extension beyond images. Since the authors repeatedly mention that their approach goes beyond images, and since the theory part is not complete, those experiments are essential for acceptance of this paper.

---

This paper uses Tensors to build generative models.  The main idea is to divide the input into regions represented with mixture models, and represent the joint distribution of the mixture components with a tensor.  Then, by restricting themselves to tensors that have an efficient decomposition, they train convolutional arithmetic circuits to generate the probability of the input and class label, providing a generative model of the input and labels.  

This approach seems quite elegant.  It is not completely clear to me how the authors choose the specific architecture for their model, and how these choices relate to the class of joint distributions that they can represent, but even if these choices are somewhat heuristic, the overall framework provides a nice way of controlling the generality of the distributions that are represented.

The experiments are on simple, synthetic examples of missing data.  This is somewhat of a limitation, and the paper would be more convincing if it could include experiments on a real-world problem that contained missing data.  One issue here is that it must be known which elements of the input are missing, which somewhat limits applicability.  Could experiments be run on problems relating to the Netflix challenge, which is the classic example of a prediction problem with missing data?  In spite of these limitations, the experiments provide appropriate comparisons to prior work, and form a reasonable initial evaluation.

I was a little confused about how the input of missing data is handled experimentally.  From the introductory discussion my impression was that the generative model was built over region patches in the image.  This led me to believe that they would marginalize over missing regions.  However, when the missing data consists of IID randomly missing pixels, it seems that every region will be missing some information.  Why is it appropriate to marginalize over missing pixels?  Specifically, $x_i$ in Equation 6 represents a local region, and the ensuing discussion shows how to marginalize over missing regions.  How is this done when only a subset of a region is missing?  It also seems like the summation in the equation following Equation 6 could be quite large.  What is the run time of this? 

The paper is also a bit schizophrenic about the extent to which the results are applicable beyond images.  The motivation for the probabilistic model is mostly in terms of images.  But in the experiments, the authors state that they do not use state-of-the-art inpainting algorithms because their method is not limited to images and they want to compare to methods that are restricted to images.  This would be more convincing if there were experiments outside the image domain.

It was also not clear to me how, if at all, the proposed network makes use of translation invariance.  It is widely assumed that much of the success of CNNs comes from their encoding of translation invariance through weight sharing.   Is such invariance built into the authors’ network?  If not, why would we expect it to work well in challenging image domains?

As a minor point, the paper is not carefully proofread.  To just give a few examples from the first page or so:

“significantly lesser” -> “significantly less”

“the the”

“provenly” -> provably

---

The paper provides an interesting use of generative models to address the classification with missing data problem. The tensorial mixture models proposed take into account the general problem of dependent samples. This is an nice extension of current mixture models where samples are usually considered as independent. Indeed the TMM model is reduced to the conventional latent variable models. As much as I love the ideas behind the paper, I feel pitiful about the sloppiness of the presentation (such as missing notations) and flaws in the technical derivations.  Before going into the technical details, my high level concerns are as follows:
(1) The joint density over all samples is modeled as a tensorial mixture generative model. The interpretation of the CP decomposition or HT decomposition on the prior density tensor is not clear. The authors have an interpretation of TMM as product of mixture models when samples are independent, however their interpretation seems flawed to me, and I will elaborate on this in the detailed technical comments below. 
(2) The authors employ convolution operators to compute an inner product. It is realizable by zero padding, but the invariance structure, which is the advantage of CNN compared to feed-forward neural network, will be lost. However, I am not sure how much this would affect the performance in practice. 
(3) The author could comment in the paper a little bit on the sample complexity of this method given the complexity of the model. 

Because I liked the ideas of the paper so much, and the ICLR paper submitted didn't present the technical details well due to sloppiness of notations, so I read the technical details in the arXiv version the authors pointed out. There are a few technical typos that I would like to point out (my reference to equations are to the ones in the arXiv paper). 
(1) The generative model as in figure (5) is flawed. P(x_i|d_i;\theta_{d_i}) are vectors of length s, there the product of vectors is not well defined. It is obvious that the dimensions of the terms between two sides of the equation are not equal. In fact, this should be a tucker decomposition instead of multiplication. It should be P(X) = \sum_{d1,\ldots,d_N} P(d_1,\ldots,d_N) (P(x_1|d_1;theta_{d_1},P(x_2|d_2;theta_{d_2},\ldots,P(x_N|d_N;theta_{d_N}), which means a sum of multi-linear operation on tensor P(d_1,\ldots,d_N), and each mode is projected onto P(x_i|d_i;theta_{d_i}. 
(2) I suspect the special case for diagonal Gaussian Mixture Models has some typos as I couldn't derive the third last equation on page 6. But it might be just I didn't understand this example. 
(3) The claim that TMM reduces to product of mixture model is not accurate. The first equation on page 7 is only right when ""sum of product"" operation is equal to ""product of sum"" operation. Similarly, in equation (6), the second equality doesn't hold unless in some special cases. However, this is not true. This might be just a typo, but it is good if the authors could fix this. I also suspect that if the authors correct this typo,the performance on MNIST might be improved.

Overall, I like the ideas behind this paper very much. I suggest the authors fix the technical typos if the paper is accepted.

---

The authors have recently made several connections between deep learning and tensor algebra. While their earlier works dealt with supervised learning, the current work analyzes generative models through the lens of tensor algebra. 
 The authors show propose a tensorial mixture model over local structures where the mixture components are expressed as tensor decompositions. They show that hierarchical tensor decomposition is exponentially more expressive compared to the shallow models. 
 The paper makes original contributions in terms of establishing expressivity of deep generative models. The connections with tensor algebra could lead to further innovations, e.g. in training algorithms.
 However, the paper can be improved in two aspects: 
 (1) It will be nice if the authors make a connection between the algebraic view presented here with the geometric view presented by:

---

This paper proposes a generative model for mixtures of basic local structures where the dependency between local structures is a tensor. They use tensor decomposition and the result of their earlier paper on expressive power of CNNs along with hierarchical Tucker to provide an inference mechanism. However, this is conditioned on the existence of decomposition. The authors do not discuss how applicable their method is for a general case, what is the subspace where this decomposition exists/is efficient/has low approximation error. Their answer to this question is that in deep learning era these theoretical analysis is not needed. While this claim is subjective, I need to emphasize that the paper does not clarify this claim and does not mention the restrictions. Hence, from theoretical perspective, the paper has flaws and the claims are not justified completely. Some claims cannot be justified with the  current results in tensor literature as the authors also mentioned in the discussions. Therefore, they should have corrected their claims in the paper and made the clarifications that this approach is restricted to a clear subclass of tensors.

If we ignore the theoretical aspect and only consider the paper from empirical perspective, the experiments the appear in the paper are not enough to accept the paper. MNIST and CIFAR-10 are very simple baselines and more extensive experiments are required. Also, the experiments for missing data are not covering real cases and are too synthetic. Also, the paper lacks the extension beyond images. Since the authors repeatedly mention that their approach goes beyond images, and since the theory part is not complete, those experiments are essential for acceptance of this paper.

---

This paper uses Tensors to build generative models.  The main idea is to divide the input into regions represented with mixture models, and represent the joint distribution of the mixture components with a tensor.  Then, by restricting themselves to tensors that have an efficient decomposition, they train convolutional arithmetic circuits to generate the probability of the input and class label, providing a generative model of the input and labels.  

This approach seems quite elegant.  It is not completely clear to me how the authors choose the specific architecture for their model, and how these choices relate to the class of joint distributions that they can represent, but even if these choices are somewhat heuristic, the overall framework provides a nice way of controlling the generality of the distributions that are represented.

The experiments are on simple, synthetic examples of missing data.  This is somewhat of a limitation, and the paper would be more convincing if it could include experiments on a real-world problem that contained missing data.  One issue here is that it must be known which elements of the input are missing, which somewhat limits applicability.  Could experiments be run on problems relating to the Netflix challenge, which is the classic example of a prediction problem with missing data?  In spite of these limitations, the experiments provide appropriate comparisons to prior work, and form a reasonable initial evaluation.

I was a little confused about how the input of missing data is handled experimentally.  From the introductory discussion my impression was that the generative model was built over region patches in the image.  This led me to believe that they would marginalize over missing regions.  However, when the missing data consists of IID randomly missing pixels, it seems that every region will be missing some information.  Why is it appropriate to marginalize over missing pixels?  Specifically, $x_i$ in Equation 6 represents a local region, and the ensuing discussion shows how to marginalize over missing regions.  How is this done when only a subset of a region is missing?  It also seems like the summation in the equation following Equation 6 could be quite large.  What is the run time of this? 

The paper is also a bit schizophrenic about the extent to which the results are applicable beyond images.  The motivation for the probabilistic model is mostly in terms of images.  But in the experiments, the authors state that they do not use state-of-the-art inpainting algorithms because their method is not limited to images and they want to compare to methods that are restricted to images.  This would be more convincing if there were experiments outside the image domain.

It was also not clear to me how, if at all, the proposed network makes use of translation invariance.  It is widely assumed that much of the success of CNNs comes from their encoding of translation invariance through weight sharing.   Is such invariance built into the authors’ network?  If not, why would we expect it to work well in challenging image domains?

As a minor point, the paper is not carefully proofread.  To just give a few examples from the first page or so:

“significantly lesser” -> “significantly less”

“the the”

“provenly” -> provably

---

The paper provides an interesting use of generative models to address the classification with missing data problem. The tensorial mixture models proposed take into account the general problem of dependent samples. This is an nice extension of current mixture models where samples are usually considered as independent. Indeed the TMM model is reduced to the conventional latent variable models. As much as I love the ideas behind the paper, I feel pitiful about the sloppiness of the presentation (such as missing notations) and flaws in the technical derivations.  Before going into the technical details, my high level concerns are as follows:
(1) The joint density over all samples is modeled as a tensorial mixture generative model. The interpretation of the CP decomposition or HT decomposition on the prior density tensor is not clear. The authors have an interpretation of TMM as product of mixture models when samples are independent, however their interpretation seems flawed to me, and I will elaborate on this in the detailed technical comments below. 
(2) The authors employ convolution operators to compute an inner product. It is realizable by zero padding, but the invariance structure, which is the advantage of CNN compared to feed-forward neural network, will be lost. However, I am not sure how much this would affect the performance in practice. 
(3) The author could comment in the paper a little bit on the sample complexity of this method given the complexity of the model. 

Because I liked the ideas of the paper so much, and the ICLR paper submitted didn't present the technical details well due to sloppiness of notations, so I read the technical details in the arXiv version the authors pointed out. There are a few technical typos that I would like to point out (my reference to equations are to the ones in the arXiv paper). 
(1) The generative model as in figure (5) is flawed. P(x_i|d_i;\theta_{d_i}) are vectors of length s, there the product of vectors is not well defined. It is obvious that the dimensions of the terms between two sides of the equation are not equal. In fact, this should be a tucker decomposition instead of multiplication. It should be P(X) = \sum_{d1,\ldots,d_N} P(d_1,\ldots,d_N) (P(x_1|d_1;theta_{d_1},P(x_2|d_2;theta_{d_2},\ldots,P(x_N|d_N;theta_{d_N}), which means a sum of multi-linear operation on tensor P(d_1,\ldots,d_N), and each mode is projected onto P(x_i|d_i;theta_{d_i}. 
(2) I suspect the special case for diagonal Gaussian Mixture Models has some typos as I couldn't derive the third last equation on page 6. But it might be just I didn't understand this example. 
(3) The claim that TMM reduces to product of mixture model is not accurate. The first equation on page 7 is only right when ""sum of product"" operation is equal to ""product of sum"" operation. Similarly, in equation (6), the second equality doesn't hold unless in some special cases. However, this is not true. This might be just a typo, but it is good if the authors could fix this. I also suspect that if the authors correct this typo,the performance on MNIST might be improved.

Overall, I like the ideas behind this paper very much. I suggest the authors fix the technical typos if the paper is accepted.",,,,,,5.333333333333333,,,3.3333333333333335,,
728,"REINFORCEMENT LEARNING THROUGH NEURAL ENCODING
Authors: Nir Baram, Tom Zahavy
Source file: 728.pdf

ABSTRACT
Recent progress in the field of Reinforcement Learning (RL) has enabled to tackle bigger and more challenging tasks. However, the increasing complexity of the problems, as well as the use of more sophisticated models such as Deep Neural Networks (DNN), has impeded the ability to understand the behavior of trained policies. In this work, we present the Semi-Aggregated Markov Decision Process (SAMDP) model. The purpose of the SAMDP modeling is to analyze trained policies by identifying temporal and spatial abstractions. In contrast to other modeling approaches, SAMDP is built in a transformed state-space that encodes the dynamics of the problem. We show that working with the right state representation mitigates the problem of finding spatial and temporal abstractions. We describe the process of building the SAMDP model by observing trajectories of a trained policy and give examples for using it in a toy problem and complicated DQN agents. Finally, we show how using the SAMDP we can monitor the trained policy and make it more robust.

INTRODUCTION
In the early days of RL, understanding the behavior of trained policies could be done rather easily (Sutton, 1990). Researchers focused on simpler problems (Peng and Williams, 1993), and policies were built using lighter models than today (Tesauro, 1994). As a result, a meaningful analysis of policies was possible even by working with the original state representation and relating to primitive actions. However, in recent years research has made a huge step forward. Fancier models such as Deep Neural Networks (DNNs) have become a commodity (Mnih et al., 2015), and the RL community tackles bigger and more challenging problems (Silver et al., 2016). Artificial agents are even expected to be used in autonomous systems such as self-driving cars. The need to reason the behavior of trained agents, and understand the mechanisms that govern its choice of actions is pressing more than ever.
Analyzing a trained policy modeled by a DNN (either graphically using the state-action diagram, or by any other mean) is practically impossible. A typical problem consists of an immense number of states, and policies often rely on skills (Mankowitz, Mann, and Mannor, 2014), creating more than a single level of planning. The resulting Markov reward processes induced by such policies are too complicated to comprehend through observation. Simplifying the behavior requires finding a suitable representation of the state space; a long-standing problem in machine learning, where extensive research has been conducted over the years (Boutilier, Dean, and Hanks, 1999). There, the goal is to come up with a transformation of the state space φ : s → ŝ, that can facilitate learning.
∗These authors contributed equally
In the field of RL, where problems are sequential in nature, this problem is exacerbated since the representation of a state needs to account for the dynamics of the problem as well.
Finding a suitable state representation can be phrased as a learning problem itself (Ng, 2011; Lee et al., 2006). DNNs are very useful in this context since they automatically build a hierarchy of representations with an increasing level of abstraction along the layers. In this work, we show that the state representation that is learned automatically by DNNs is suitable for building abstractions in RL. To this end, we introduce the SAMDP model; a modeling approach that creates abstractions both in space and time. Contrary to other modeling approaches, SAMDP is built in a transformed state space, where the problem of creating spatial abstractions (i.e., state aggregation), and temporal abstractions (i.e., identifying skills) is facilitated using spatiotemporal clustering. We provide an example for building an SAMDP model for a basic gridworld problem where φ(s) is hand-crafted. However, the real strength of the model is demonstrated on challenging Atari2600 games solved using DQNs (Mnih et al., 2015). There, we set φ(s) to be the state representation automatically learned by the DNN (i.e. the last hidden layer). We continue by presenting methods for evaluating the fitness of the SAMDP model to the trained policy at hand. Finally, we describe a method for using the SAMDP as a monitor that alerts when the policy’s performance weakens, and provide initial results showing how the SAMDP model is useful for shared autonomy systems.

BACKGROUND
We briefly review the standard reinforcement learning framework of discrete-time finite Markov decision processes (MDPs). An MDP is defined by a five-tuple < S,A, P,R, γ >. At time t an agent observes a state st ∈ S, selects an action at ∈ A, and receives a reward rt. Following the agent’s action choice, it transitions to the next state st+1 ∈ S according to a Markovian probability matrix Pa ∈ P . The cumulative return at time t is given by Rt = ∑∞ t′=t γ
t′−trt, where γ ∈ [0, 1] is the discount factor. In this framework, the goal of an RL agent is to maximize the expected return by learning a policy π : S → ∆A; a mapping from states s ∈ S to a probability distribution over actions. The action-value function Qπ(s, a) = E[Rt|st = s, at = a, π] represents the expected return after observing state s, taking action a and then following policy π. The optimal action-value function obeys a fundamental recursion known as the optimal Bellman Equation: Q∗(st, at) = E [ rt + γmax
a′ Q∗(st+1, a
′) ] .
Skills, Options (Sutton, Precup, and Singh, 1999) are temporally extended control structures, denoted by σ. A skill is defined by a triplet: σ =< I, π, β >, where I defines the set of states where the skill can be initiated, π is the intra-skill policy, and β is the set of termination probabilities determining when a skill will stop executing. β is typically either a function of state s or time t. Any MDP with a fixed set of skills is a Semi-MDP (SMDP). Planning with skills can be performed by learning for each state the value of choosing each skill. More formally, an SMDP is defined by a five-tuple < S,Σ, P,R, γ >. S is the set of states, Σ is the set of skills, P is the SMDP transition
matrix, γ is the discount factor and the SMDP reward is defined by:
Rσs = E[rσs ] = E[rt+1 + γrt+2 + · · ·+ γk−1rt+k|st = s, σ]. (1)
The Skill Policy µ : S → ∆Σ is a mapping from states to a distribution over skills. The action-value function Qµ(s, σ) = E[ ∑∞ t=0 γ
tRt|(s, σ), µ] represents the value of choosing skill σ ∈ Σ at state s ∈ S, and thereafter selecting skills according to policy µ. The optimal skill value function is given by: Q∗Σ(s, σ) = E[Rσs + γkmax
σ′∈Σ Q∗Σ(s ′, σ′)] (Stolle and Precup, 2002).

THE SEMI AGGREGATED MDP
Reinforcement Learning problems are typically modeled using the MDP formulation. Given an MDP, a variety of algorithms have been proposed to find an optimal policy. However, when one wishes to analyze a trained policy, MDP may not be the best modeling choice due to the size of the state space and the length of the planning horizon. In this section, we present the SMDP and Aggregated MDP (AMDP) models which can simplify the analysis by using temporal and spatial abstractions respectively. We also introduce the new Semi-Aggregated MDP (SAMDP) model, that combines SMDP and AMDP models in a novel way which leverages the abstractions made in each modeling approach. SMDP (Sutton, Precup, and Singh, 1999), can simplify the analysis of a trained policy by using temporal abstractions. The model extends the MDP action space A to allow the agent to plan with temporally extended actions Σ (i.e., skills). Analyzing policies using the SMDP model shortens the planning horizon and simplifies the analysis. However, there are two problems with this approach. First, one still faces the high complexity of the state space, and second, the SMDP model requires to identify skills.
Skill identification is an ill-posed problem that can be addressed in many ways, and for which extensive research has been done over the years. The popular approaches are to identify bottlenecks in the state space (McGovern and Barto, 2001),or to search for common behavior trajectories, or common state region policies (McGovern, 2002). A different approach can be to build a graphical model of the agent’s interaction with the environment and to use betweenness centrality measures to identify subtasks (Şimşek and Barreto, 2009). No matter what the method is, identifying skills solely by observing an agent play is a challenging task.
Alternative approach to SMDP modeling is to analyze a policy using spatial abstractions in the state space. If there is a reason to believe that groups of states share common attributes such as similar policy or value function, it is possible to use State Aggregation (Moore, 1991). State Aggregation is a well-studied problem that typically involves identifying clusters as the new states of an Aggregated MDP, where the set of clusters C replaces the MDP states S. Applying RL on aggregated states is potentially advantageous because the dimensions of the transition probability matrix P , the reward signal R and the policy π are decreased (Singh, Jaakkola, and Jordan, 1995). However, the AMDP modeling approach has two drawbacks. First, the action space A is not modified, and therefore the planning horizon remains intractable, and second, AMDPs are not necessarily Markovian (Bai, Srivastava, and Russell, 2016). In this paper, we propose a model that combines the advantages of the SMDP and AMDP approaches and denote it by SAMDP. Under SAMDP modeling, aggregation defines both the states and the set of skills, allowing analysis with spatiotemporal abstractions (the state-space dimensions and the planning horizon are reduced). However, SAMDPs are still not necessarily Markovian. We summarize the different modeling approaches in Figure 1. The rest of this section is devoted to explaining the five stages of building an SAMDP model: (0) Feature selection, (1) State Aggregation, (2) Skill identification, (3) Inference, and (4) Model Selection. (0) Feature selection. We define the mapping from MDP states to features, by a mapping function φ : s → s′ ⊂ Rm. The features may be raw (e.g., spatial coordinates, frame pixels) or higher level abstractions (e.g., the last hidden layer of an NN). The feature representation has a significant effect on the quality of the resulting SAMDP model and vice versa; a good model can point out a good feature representation. (1) Aggregation via Spatio-temporal clustering. The goal of Aggregation is to find a mapping (clustering) from the MDP feature space S′ ⊂ Rm to the AMDP state space C. Clustering algorithms typically assume that data is drawn from an i.i.d distribution. However, in our problem data
is generated from an MDP which violates this assumption. We alleviate this problem using two different approaches. First, we decouple the clustering step from the SAMDP model, by creating an ensemble of clustering candidates and building an SAMDP model for each (following stages 2 and 3). In stage 4, we will explain how to run a non-analytic outer optimization loop to choose between these candidates based on spatiotemporal evaluation criteria. Second, we introduce a novel extension of the celebrated K-means algorithm (MacQueen and others, 1967), which enforces temporal coherency along trajectories. In the vanilla K-means algorithm, a point xt is assigned to cluster ci with mean µi if µi is the closest cluster center to xt (for further details please see the supplementary material). We modified this step as follows:
c(xt) = { ci : ∥∥Xt − µi∥∥2F ≤ ∥∥Xt − µj∥∥2F ,∀j ∈ [1,K]}, where F stands for the Frobenius norm, K is the number of clusters, t is the time index of xt, and Xt is a set of 2w+ 1 centered at xt from the same trajectory: { xj ∈ Xt ⇐⇒ j ∈ [t−w, t+w] } . The dimensions of µ correspond to a single point, but is expanded to the dimensions of Xt. In this way, we enforce temporal coherency since a point xt is assigned to a cluster ci if its neighbors in time along the trajectory are also close to µi. We have also experimented with other clustering methods such as spectral clustering, hierarchical agglomerative clustering and entropy minimization (please refer to the supplementary material for more details). (2) Skill identification. We define an SAMDP skill σi,j ∈ Σ uniquely by a single initiation state ci ∈ C and a single termination state cj ∈ C : σij =< ci, πi,j , cj > . More formally, at time t the agent enters an AMDP state ci at an MDP state st ∈ ci. It chooses a skill according to its SAMDP policy and follows the skill policy πi,j for k time steps until it reaches a state st+k ∈ cj , s.t i 6= j. We do not define the skill length k apriori nor the skill policy but infer the skill length from the data. As for the skill policies, our model does not define them explicitly, but we will observe later that our model successfully identifies skills that are localized in time and space.
(3) Inference. Given the SAMDP states and skills, we infer the skill length, the SAMDP reward and the SAMDP probability transition matrix from observations. The skill length, is inferred for a skill σi,j by averaging the number of MDP states visited since entering SAMDP state ci until leaving for SAMDP state cj . The skill reward is inferred similarly using Equation 1. The inference of the SAMDP transition matrices is a bit more puzzling, since the probability of seeing the next SAMDP state depends both on the MDP dynamics and the agent policy in the MDP state space. We now turn to discuss how to infer these matrices by observing transitions in the MDP state space. Our goal is to infer two quantities: (a) The SAMDP transition probability matrices PΣ : P σ∈Σ i,j = Pr(cj |ci, σ), measures the probability of moving from state ci to cj given that skill σ is chosen. These matrices are defined uniquely by our definition of skills as deterministic probability matrices. (b) The probability of moving from state ci to cj given that skill σ is chosen according to the agent SAMDP policy: Pπi,j = Pr(cj |ci, σ = π(ci)). This quantity involves both the SAMDP transition probability matrices and the agent policy. However, since SAMDP transition probability matrices are deterministic, this is equivalent to the agent policy in the SAMDP state space. Therefore by inferring transitions between SAMDP states, we directly infer the agent’s SAMDP policy. Given an MDP with a deterministic environment and an agent with a nearly deterministic MDP policy (e.g., a deterministic policy that uses an -greedy exploration ( 1)), it is intuitive to assume that we would observe a nearly deterministic SAMDP policy. However, there are two mechanisms that cause stochasticity in the SAMDP policy: (1) Stochasticity that is accumulated along skill trajectories. (2) Approximation errors in the aggregation process. A given SAMDP state may contain more than one ”real” state and therefore more than one skill. Performing inference in this setup, we might observe a stochastic policy that chooses randomly between skills. Therefore, it is very likely to infer a stochastic SAMDP transition matrix, even though the SAMDP transition probability matrices and the MDP environment are deterministic, and the MDP policy is nearly deterministic. (4) Model selection. So far we have explained how to build an SAMDP from observations. In this stage, we’ll explain how to choose between different SAMDP model candidates. There are two advantages of choosing between multiple SAMDPs. First, there are different hyperparameters to tune: two examples are the number of SAMDP states (K) and the window size (w) for the clustering algorithm. Second, there is randomness in the aggregation step. Hence, clustering multiple times and picking the best result will potentially yield better models.
We developed, therefore, evaluation criteria that allow us to select the best model, motivated by Hallak, Di-Castro, and Mannor (2013). We follow the Occams Razor principle and aim to find the simplest model which best explains the data. (i) Value Mean Square Error(VMSE), measures the consistency of the model with the observations. The estimator is given by
VMSE = ‖v−vSAMDP ‖/‖v‖, (2)
where v stands for the SAMDP value function of the given policy, and vSAMDP is given by: VSAMDP = (I + γ
kP )−1r, where P is measured under the SAMDP policy. (ii) Inertia, the Kmeans algorithm objective function, is given by : I = ∑n i=0 minµj∈C(||xj − µi||2). Inertia measures the variance inside clusters and encourages spatial coherency. Motivated by Ncut and spectral clustering (Von Luxburg, 2007), we define (iii) The Intensity Factor as the fraction of out/in cluster transitions. However, we define edges between states that are connected along the trajectory (a transition between them was observed) and give them equal weights (instead of defining the edges by euclidean distances as in spectral clustering). Minimizing the intensity factor encourages longer duration skills. (iv)had been Entropy, is defined on the SAMDP probability transition matrix as follows: e = − ∑ i{|Ci| · ∑ j Pi,j logPi,j}. Low entropy encourages clusters to have less skills, i.e., clusters that are localized both in time and space.

SAMDP FOR GRIDWORLD
We first illustrate the advantages of SAMDP in a basic gridworld problem (Figure 2). In this task, an agent is placed at the origin (marked in X), where the goal is to reach the green ball and return. The state s ∈ R3 is given by: s = {x, y, b}, where (x, y) are the coordinates and b ∈ {0, 1} indicates whether the agent has reached the ball or not. The policy is trained to find skills following the algorithm of Mankowitz, Mann, and Mannor (2014). We are given trajectories of the trained agent, and wish to analyze its behavior by building the state-action graph for all four modeling approaches. For clarity, we plot the graphs on the maze using the coordinates of the state. The MDP graph (Figure 2(a)), consists of a vast number of states. It is also difficult to understand what skills the agent is using. In the SMDP graph (Figure 2(b)), the number of states remain high, however
coloring the edges by the skills, helps to understand the agent’s behavior. Unfortunately, producing this graph is seldom possible because we rarely receive information about the skills. On the other hand, abstracting the state space can be done more easily using state aggregation. However, in the AMDP graph (Figure 2(c)), the clusters are not aligned with the played skills because the routes leading to and from the ball overlap. For building the SAMDP model (Figure 2(d)), we transform the state space in a way that disentangles the routes:
φ(x, y) = { (x, y), if b is 0 (2L− x, y), if b is 1 ,
where L is the maze width. The transformation φ flip and translate the states where b = 1. Now that the routes to and from the ball are disentangled, the clusters are perfectly aligned with the skills. Understanding the behavior of the agent is now possible by examining inter-cluster and intra-cluster transitions.

SAMDPS FOR DQNS
Feature extraction: We evaluate a pre-trained DQN agent for multiple trajectories with an -greedy policy on three Atari2600 games, Pacman (a game where DQN performs very well), Seaquest (for the opposite reason) and Breakout (for its popularity). We let the trained agent play 120k game states, and record the neural activations of the last hidden layer as well as the Q values. We also keep the time index of each state to be able to find temporal neighbors. Features from other layers can also be used. However, we rely on the results from Zahavy, Zrihem, and Mannor (2016) that showed that the features learned in the last hidden layer capture a spatiotemporal hierarchy and therefore make a good candidate for state aggregation. We then apply t-SNE on the neural activations data, a non-linear dimensionality reduction method that is particularly good at creating a single map that reveals structure at many different scales. We use the two coordinates of the t-SNE map and the value estimation as the MDP state features. Each coordinate is normalized to have zero mean and
unit variance. We have experimented with other configurations such as using the activations without t-SNE as well as different normalization. However, we found that this configuration results in better SAMDP models. We also use two approximations in the inference stage which we found to work well: 1) overlooking transitions with a small skill length (shorter than 2) and 2) truncating transitions with probability less than 0.1. We only present results for the Breakout game and refer the reader to the supplementary material for results on Pacman and Seaquest. Model Selection: We perform a grid search on two parameters: i) number of clusters K ∈ [15, 25]. ii) window size w ∈ [1, 7]. We found that models larger (smaller) than that are too cumbersome (simplistic) to analyze. We select the best model in the following way: we first sort all models by the four evaluation criteria (SAMDP Section, stage 4) from best to worst. Then, we iteratively intersect the p-prefix of all sets (i.e., the first p elements of each set) starting with 1-prefix. We stop when the intersection is nonempty and choose the configuration at the intersection. The resulted SAMDP model for Breakout can be seen in Figure 3. We also measure the p-value of the chosen model. For the null hypothesis, we take the SAMDP model constructed with random clusters. We tested 10000 random SAMDP models, none of which scored better than the chosen model (for all the evaluation criteria). Qualitative Evaluation: Examining the resulting SAMDP (Figure 3) it is interesting to note the sparsity of transitions, which implies low entropy. Inspecting the mean image of each cluster reveals insights about the nature of the skills hiding within and uncovers the policy hierarchy as described in Zahavy, Zrihem, and Mannor (2016). The agent begins to play in low value (blue) clusters (e.g., 1,5,8,9,13,16,18,19). These clusters are well connected between them and are disconnected from other clusters. Once the agent transitions to the ”tunnel-digging” option in clusters 4,12,14, it stays in there until it finishes to curve the tunnel, then it transitions to cluster 11. From cluster 11 the agent progresses through the ”left banana” and hops between clusters 2,21,5,10,0,7 and 3 in that order. Model Evaluation: We first measure the VMSE criterion, as defined in Equation 2 (Figure 4, top). We infer v by averaging the DQN value estimates in each cluster: vDQN (cj) =
1 |Cj | ∑ i:si∈cj v
DQN (si), and evaluate VSAMDP as defined above. Since the Atari environment is deterministic, the vSAMDP estimate is accurate with respect to the DQN policy. Therefore, the VMSE criterion measures how well the SAMDP model approximates the true MDP. In practice, we observe that the DQN and SAMDP values are very similar; indicating that the SAMDP model fits the data well. Second, we evaluate the greedy policy with respect to the SAMDP value function by: πgreedy(ci) = argmax
j {Rσi,j + γ kσi,j vSAMDP (cj)}. We then measure the correlation between the greedy policy decisions and the trajectory reward. For a given trajectory j we measure P ji : the empirical distribution of choosing the greedy policy at state ci and the cumulative reward Rj . Finally, we present the correlation between these two measures in each state: corri = corr(P j i , R j) in (Figure 4, center). A
positive correlation indicates that following the greedy policy leads to high reward. Indeed for most of the states, we observe positive correlation, supporting the consistency of the model. The third evaluation is close in spirit to the second one. We partition the data to a train and test sets. We evaluate the greedy policy based on the train set and create two transition matrices T+, T− using the k top and bottom rewarded trajectories respectively from the test set. We measure the correlation of the greedy policy TG with each of the transition matrices for different values of k (Figure 4 bottom). As clearly seen, the correlation of the greedy policy and the top trajectories is higher than the correlation with the bottom trajectories. Eject Button: The motivation for this experiment stems from the idea of shared autonomy Pitzer et al. (2011). There are domains where errors are dreadful, and performance must be as high as possible. The idea of shared autonomy, is to allow an operator to intervene in the decision loop at critical times. For example, in 20% of commercial flights, the auto-pilot returns the control to the human pilots. In the following experiment, we show how the SAMDP model can help to identify where the agent’s behavior deteriorates. Setup. (a) Evaluate a DQN agent, create a trajectory data set, and evaluate the features for each state (stage 0). (b) Divide the data into two groups: train (100 trajectories) and test (60). then build an SAMDP model (stages 1-4) on the train data. (c) Split the train data to k top and bottom rewarded trajectories T+, T− and re-infer the model parameters separately for each (stage 3). (d) Project the test data on the SAMDP model (mapping each state to the nearest SAMDP state). (e) Eject when the transitions of the agent are more likely under the T− matrix rather then under T+ (inspired by the idea of option interruption Sutton, Precup, and Singh (1999)). (f) We average the trajectory reward on (i) the entire test set, and (ii) the un-ejected trajectories sub set. We measure 36% ± 7.7%, 20% ± 8.0%, and 4.7% ± %1.2 performance gain for Breakout Seaquest and Pacman, respectively. The eject experiment indicates that the SAMDP model can be used to make a given DQN policy robust by identifying when the agent is not going to perform well and return control to a human operator or some other AI agent. Other eject mechanisms are also possible. For example, ejecting by looking at MDP values. However, the Q value is not monotonically decreasing along the trajectory as expected (See Figure 3). The solution we propose is to eject by monitoring transitions and not state values, which makes the MDP impractical in this case because it’s state-action diagram is too large to construct, and too expensive to process.

DISCUSSION
SAMDP modeling offers a way to present a trained policy in a concise way by creating abstractions that relate to the spatiotemporal structure of the problem. We showed that by using the right representation, time-aware state aggregation could be used to identify skills. It implies that the crucial step in building an SAMDP is the state aggregation phase. The aggregation depends on the state features and the clustering algorithm at hand.
In this work, we presented a basic K-means variant that relies on temporal information. However, other clustering approaches are possible. We also experimented with agglomerative methods but found them to be significantly slower without providing any benefit. We believe that clustering methods that better relate to the topology, such as spectral clustering, would produce the best results. Regarding the state features; in the DQN example, we used the 2D t-SNE map. This map, however, is built under the i.i.d assumption that overlooks the temporal dimension of the problem. An interesting line of future work will be to modify the t-SNE algorithm to take into account temporal distances as well as spatial ones. A tSNE algorithm of this kind may produce 2D maps with even lower entropy which will decrease the aggregation artifacts that affect the quality of the SAMDP model.
In this work we analyzed discrete-action policies, however SAMDP can also be applied for continuous-action policies that maintain a value function (since our algorithm depends on it for construction and evaluation), as in the case of actor-critic methods. Another issue we wish to investigate is the question of consistency in re-building an SAMDP. We would like the SAMDP to be unique for a given problem. However, there are several aspects of randomness that may cause divergence. For instance, when using a DQN, randomness exists in the creation of the t-SNE map, and in the clustering phase. From our experience, though, different models built for the same problem are reasonably consistent. In future work, we wish to address the same problem by laying out an optimization problem that will directly account for all of the performance criteria introduced here. It would be interesting to see what clustering method will be drawn out of this process and to compare the principled solution with our current approach.
","Recent progress in the field of Reinforcement Learning (RL) has enabled to tackle bigger and more challenging tasks. However, the increasing complexity of the problems, as well as the use of more sophisticated models such as Deep Neural Networks (DNN), has impeded the ability to understand the behavior of trained policies. In this work, we present the Semi-Aggregated Markov Decision Process (SAMDP) model. The purpose of the SAMDP modeling is to analyze trained policies by identifying temporal and spatial abstractions. In contrast to other modeling approaches, SAMDP is built in a transformed state-space that encodes the dynamics of the problem. We show that working with the right state representation mitigates the problem of finding spatial and temporal abstractions. We describe the process of building the SAMDP model by observing trajectories of a trained policy and give examples for using it in a toy problem and complicated DQN agents. Finally, we show how using the SAMDP we can monitor the trained policy and make it more robust.",ICLR 2017 conference submission,False,,"The paper combines semi-Markov models with state aggregation models to define a Semi-Aggregated Markov Decision Process (SAMDP). As per the reviews, the need for spatio-temporal representations is recognized as an important problem. Where the work currently falls short is the loose description of framework, which should be formalized for clarity and analysis, and in terms of its contributions and benefits with respect to the broader literature on hierarchical RL methods.
 The anonymous reviewers (and the additional public review/comment) provide a number of clear directions to pursue for the next iteration of this work; indeed, there is strong general enthusiasm for the greater goals (as shared by the relevant body of work in the research community), but the work is seen as requiring further clarity in formalization and presentation.

---

This work is interesting and one of the few papers out there to address the problem of autonomous skill acquisition in Deep RL. The importance of this problem cannot be overstated since Hierarchical RL is not only about using a meta-controller to appropriately use a specific skill from a repertoire of previously learned subgoals, but also about devising a set of subgoals through experience. Subgoals are inherently tied to the topology of the underlying problem and hence clustering approaches make sense. 

I would like to point out that a very similar attempt has been done in one of the past works in this area -

---

The paper starts by pointing out the need for methods that perform both state and temporal representation learning for RL and which allow gaining insight into what is being learned (perhaps in order to allow a human operator to intervene if necessary). This is a very important goal from a practical point of view, and it is great to see research in this direction. For this reason, I would like to encourage the authors to pursue this further. However, I am not at all convinced that the current incarnation of this work is the right answer. Part of the issues are more related to presentation, part may require rethinking.
In order to get the ""interpretability"", the authors opt for some fairly specific ways of performing abstraction. For example, their skills always start In a single skill initiation state, and likewise end in one state. This seems unnecessarily restrictive, and it is not clear why this restriction is needed (other than convenience). Similarly, clustering is the basis for forming the higher level states, and there is a specific kind of clustering used here. Again, it is not clear why this has to be done via clustering as opposed to other methods. Ensuring temporal coherence in the particular form employed also seems restrictive. There is a reference to supplementary material where some of these choices are explained, but I could not find this in the posted version of the paper. The authors should either explain clearly why these specific choices are necessary, or (even better) try to think if they can be relaxed while still keeping interpretability. 
From a presentation point of view, the paper would benefit from formal definitions of AMDP and SAMDP, as well as from formal descriptions of the algorithms employed in constructing these representations (eg Bellman equations for the models, and update rules for the algorithms learning them). While intuitions are given, the math is not precisely stated. The overhead of constructing an SAMDP (computation time and space) should be clarified as well.
The experiments are well carried out and it is nice to have both gridworld experiments, where visualization are easy to perform and understand, as well as Atari games (gridworld still have their place despite what other reviewers might say). The results are positive, but because the proposed approach has many moving parts which rely on specific choices, significance and general ease of use are unclear at this point. Perhaps having the complete supplementary would have helped in this respect.
Small comment: The two lines after Eq 2 contain typos in the notation and a wrong sign in the equation.

---

The framework of semi-Markov decision processes (SDMPs) has been long used to model skill learning and temporal abstraction in reinforcement learning. This paper proposes a variant of such a model called a semi-aggregated MDP model. 

The formalism of SAMDP is not defined clearly enough to merit serious attention. The approach is quasi heuristic and explained through examples rather than clear definition. The work also lacks sufficient theoretical rigor. 

Simple experiments are proposed using 2D grid worlds to demonstrate skills. Grid worlds have served their purpose long enough in reinforcement learning, and it is time to retire them. More realistic domains are now routinely used and should be used in this paper as well.

---

The paper presents a method for visualization and analysis of policies from observed trajectories that the policies produce. The method infers higher level skills and clusters states. The result is a simplified, discrete higher-order state and action transition matrix. This model can be used for analysis, modeling, and interpretation. To construct semi-aggregated MDP the authors propose combining ideas for creating semi-MPDs and agregrated-MDPs. The method consists of choosing features, state clustering, skill inference, reward and skill length inference, and model selection. The method was demonstrated on a small grid-world problem, and DQN-trained agent for playing Atari games.

The authors correctly identify that tools and means for interpretibility of RL methods are important for analysis, and deployment of such methods for real-world applications. This is particularly true in robotics and high-consequence systems.

The end-result of the presented method is a high-level transition matrix. There is a big body of literature looking into hierarchical RL methods where lower level skills are combined with higher level policies. The presented method has the similar result, but the advantage of the presented method is that it comes up with a structure and analyzes already trained agent, which is very interesting. The paper would benefit from emphasizing this difference, and contrasting with the broader body of literature.

To build the model, the authors propose combining the ideas from two existing ideas, semi-MPDs and agregrated-MDPs with using modified k-means for state clustering. It appears that the novelty of the presented method is limited. The paper would have been stronger if the authors explicitly stated the contributions over combining existing methods, and better highlighted the practical utility of the method. 

The evaluation section would be made stronger with more analytical results and precise evaluation, showing full strength of the method.

The paper is difficult to read. To improve readability:
- The Semi-Aggregated MDP section should include more precise description of the methods. The narrative that builds intuition is welcome. In addition to the existing narrative, algorithms and formulas where applicable should be included as well. 
- The paper should be self-contained. For example, more background on Occams Razor principle should be included.
- Reduce the number of acronyms, in particular similarly sounding acronyms. Define acronyms before using. 
- Be more clear on the contributions, contrast with relevant literature, and the specific benefits of the presented method.
- Fix typos, formatting mistakes etc., as they can be distracting for reading. 

The approach of reverse engineering the hierarchy, and learning high-level transition matrix is very interesting and promising. Perhaps the method can be used to outperform single network approach by using the model as an input to more specialized hierarchical trainers and learn complex behaviors more optimally then possible with one large network approach. Unfortunately, the paper falls short in the novelty, precision, and clarity.

---

The paper combines semi-Markov models with state aggregation models to define a Semi-Aggregated Markov Decision Process (SAMDP). As per the reviews, the need for spatio-temporal representations is recognized as an important problem. Where the work currently falls short is the loose description of framework, which should be formalized for clarity and analysis, and in terms of its contributions and benefits with respect to the broader literature on hierarchical RL methods.
 The anonymous reviewers (and the additional public review/comment) provide a number of clear directions to pursue for the next iteration of this work; indeed, there is strong general enthusiasm for the greater goals (as shared by the relevant body of work in the research community), but the work is seen as requiring further clarity in formalization and presentation.

---

This work is interesting and one of the few papers out there to address the problem of autonomous skill acquisition in Deep RL. The importance of this problem cannot be overstated since Hierarchical RL is not only about using a meta-controller to appropriately use a specific skill from a repertoire of previously learned subgoals, but also about devising a set of subgoals through experience. Subgoals are inherently tied to the topology of the underlying problem and hence clustering approaches make sense. 

I would like to point out that a very similar attempt has been done in one of the past works in this area -

---

The paper starts by pointing out the need for methods that perform both state and temporal representation learning for RL and which allow gaining insight into what is being learned (perhaps in order to allow a human operator to intervene if necessary). This is a very important goal from a practical point of view, and it is great to see research in this direction. For this reason, I would like to encourage the authors to pursue this further. However, I am not at all convinced that the current incarnation of this work is the right answer. Part of the issues are more related to presentation, part may require rethinking.
In order to get the ""interpretability"", the authors opt for some fairly specific ways of performing abstraction. For example, their skills always start In a single skill initiation state, and likewise end in one state. This seems unnecessarily restrictive, and it is not clear why this restriction is needed (other than convenience). Similarly, clustering is the basis for forming the higher level states, and there is a specific kind of clustering used here. Again, it is not clear why this has to be done via clustering as opposed to other methods. Ensuring temporal coherence in the particular form employed also seems restrictive. There is a reference to supplementary material where some of these choices are explained, but I could not find this in the posted version of the paper. The authors should either explain clearly why these specific choices are necessary, or (even better) try to think if they can be relaxed while still keeping interpretability. 
From a presentation point of view, the paper would benefit from formal definitions of AMDP and SAMDP, as well as from formal descriptions of the algorithms employed in constructing these representations (eg Bellman equations for the models, and update rules for the algorithms learning them). While intuitions are given, the math is not precisely stated. The overhead of constructing an SAMDP (computation time and space) should be clarified as well.
The experiments are well carried out and it is nice to have both gridworld experiments, where visualization are easy to perform and understand, as well as Atari games (gridworld still have their place despite what other reviewers might say). The results are positive, but because the proposed approach has many moving parts which rely on specific choices, significance and general ease of use are unclear at this point. Perhaps having the complete supplementary would have helped in this respect.
Small comment: The two lines after Eq 2 contain typos in the notation and a wrong sign in the equation.

---

The framework of semi-Markov decision processes (SDMPs) has been long used to model skill learning and temporal abstraction in reinforcement learning. This paper proposes a variant of such a model called a semi-aggregated MDP model. 

The formalism of SAMDP is not defined clearly enough to merit serious attention. The approach is quasi heuristic and explained through examples rather than clear definition. The work also lacks sufficient theoretical rigor. 

Simple experiments are proposed using 2D grid worlds to demonstrate skills. Grid worlds have served their purpose long enough in reinforcement learning, and it is time to retire them. More realistic domains are now routinely used and should be used in this paper as well.

---

The paper presents a method for visualization and analysis of policies from observed trajectories that the policies produce. The method infers higher level skills and clusters states. The result is a simplified, discrete higher-order state and action transition matrix. This model can be used for analysis, modeling, and interpretation. To construct semi-aggregated MDP the authors propose combining ideas for creating semi-MPDs and agregrated-MDPs. The method consists of choosing features, state clustering, skill inference, reward and skill length inference, and model selection. The method was demonstrated on a small grid-world problem, and DQN-trained agent for playing Atari games.

The authors correctly identify that tools and means for interpretibility of RL methods are important for analysis, and deployment of such methods for real-world applications. This is particularly true in robotics and high-consequence systems.

The end-result of the presented method is a high-level transition matrix. There is a big body of literature looking into hierarchical RL methods where lower level skills are combined with higher level policies. The presented method has the similar result, but the advantage of the presented method is that it comes up with a structure and analyzes already trained agent, which is very interesting. The paper would benefit from emphasizing this difference, and contrasting with the broader body of literature.

To build the model, the authors propose combining the ideas from two existing ideas, semi-MPDs and agregrated-MDPs with using modified k-means for state clustering. It appears that the novelty of the presented method is limited. The paper would have been stronger if the authors explicitly stated the contributions over combining existing methods, and better highlighted the practical utility of the method. 

The evaluation section would be made stronger with more analytical results and precise evaluation, showing full strength of the method.

The paper is difficult to read. To improve readability:
- The Semi-Aggregated MDP section should include more precise description of the methods. The narrative that builds intuition is welcome. In addition to the existing narrative, algorithms and formulas where applicable should be included as well. 
- The paper should be self-contained. For example, more background on Occams Razor principle should be included.
- Reduce the number of acronyms, in particular similarly sounding acronyms. Define acronyms before using. 
- Be more clear on the contributions, contrast with relevant literature, and the specific benefits of the presented method.
- Fix typos, formatting mistakes etc., as they can be distracting for reading. 

The approach of reverse engineering the hierarchy, and learning high-level transition matrix is very interesting and promising. Perhaps the method can be used to outperform single network approach by using the model as an input to more specialized hierarchical trainers and learn complex behaviors more optimally then possible with one large network approach. Unfortunately, the paper falls short in the novelty, precision, and clarity.",,,,,,4.0,,,4.666666666666667,,
732,"GENERATIVE PARAGRAPH VECTOR
Authors: Ruqing Zhang, Jiafeng Guo, Yanyan Lan, Jun Xu, Xueqi Cheng
Source file: 732.pdf

ABSTRACT
The recently introduced Paragraph Vector is an efficient method for learning highquality distributed representations for pieces of texts. However, an inherent limitation of Paragraph Vector is lack of ability to infer distributed representations for texts outside of the training set. To tackle this problem, we introduce a Generative Paragraph Vector, which can be viewed as a probabilistic extension of the Distributed Bag of Words version of Paragraph Vector with a complete generative process. With the ability to infer the distributed representations for unseen texts, we can further incorporate text labels into the model and turn it into a supervised version, namely Supervised Generative Paragraph Vector. In this way, we can leverage the labels paired with the texts to guide the representation learning, and employ the learned model for prediction tasks directly. Experiments on five text classification benchmark collections show that both model architectures can yield superior classification performance over the state-of-the-art counterparts.

1 INTRODUCTION
A central problem in many text based applications, e.g., sentiment classification (Pang & Lee, 2008), question answering (Stefanie Tellex & Marton., 2003) and machine translation (I. Sutskever & Le, 2014), is how to capture the essential meaning of a piece of text in a fixed-length vector. Perhaps the most popular fixed-length vector representations for texts is the bag-of-words (or bag-of-ngrams) (Harris, 1954). Besides, probabilistic latent semantic indexing (PLSI) (Hofmann, 1999) and latent Dirichlet allocation (LDA) (Blei & Jordan, 2003) are two widely adopted alternatives.
A recent paradigm in this direction is to use a distributed representation for texts (T. Mikolov & Dean, 2013a). In particular, Le and Mikolov (Quoc Le, 2014; Andrew M.Dai, 2014) show that their method, Paragraph Vector (PV), can capture text semantics in dense vectors and outperform many existing representation models. Although PV is an efficient method for learning high-quality distributed text representations, it suffers a similar problem as PLSI that it provides no model on text vectors: it is unclear how to infer the distributed representations for texts outside of the training set with the learned model (i.e., learned text and word vectors). Such a limitation largely restricts the usage of the PV model, especially in those prediction focused scenarios.
Inspired by the completion and improvement of LDA over PLSI, we first introduce the Generative Paragraph Vector (GPV) with a complete generation process for a corpus. Specifically, GPV can be viewed as a probabilistic extension of the Distributed Bag of Words version of Paragraph Vector (PVDBOW), where the text vector is viewed as a hidden variable sampled from some prior distributions, and the words within the text are then sampled from the softmax distribution given the text and word vectors. With a complete generative process, we are able to infer the distributed representations of new texts based on the learned model. Meanwhile, the prior distribution over text vectors also acts as a regularization factor from the view of optimization, thus can lead to higher-quality text representations.
More importantly, with the ability to infer the distributed representations for unseen texts, we now can directly incorporate labels paired with the texts into the model to guide the representation learning, and turn the model into a supervised version, namely Supervised Generative Paragraph Vector (SGPV). Note that supervision cannot be directly leveraged in the original PV model since it has no
generalization ability on new texts. By learning the SGPV model, we can directly employ SGPV to predict labels for new texts. As we know, when the goal is prediction, fitting a supervised model would be a better choice than learning a general purpose representations of texts in an unsupervised way. We further show that SGPV can be easily extended to accommodate n-grams so that we can take into account word order information, which is important in learning semantics of texts.
We evaluated our proposed models on five text classification benchmark datasets. For the unsupervised GPV, we show that its superiority over the existing counterparts, such as bag-of-words, LDA, PV and FastSent (Felix Hill, 2016). For the SGPV model, we take into comparison both traditional supervised representation models, e.g. MNB (S. Wang, 2012), and a variety of state-of-the-art deep neural models for text classification (Kim, 2014; N. Kalchbrenner, 2014; Socher & Potts, 2013; Irsoy & Cardie, 2014). Again we show that the proposed SGPV can outperform the baseline methods by a substantial margin, demonstrating it is a simple yet effective model.
The rest of the paper is organized as follows. We first review the related work in section 2 and briefly describe PV in section 3. We then introduce the unsupervised generative model GPV and supervised generative model SGPV in section 4 and section 5 respectively. Experimental results are shown in section 6 and conclusions are made in section 7.

2 RELATED WORK
Many text based applications require the text input to be represented as a fixed-length feature vector. The most common fixed-length representation is bag-of-words (BoW) (Harris, 1954). For example, in the popular TF-IDF scheme (Salton & McGill, 1983), each document is represented by tfidf values of a set of selected feature-words. However, the BoW representation often suffers from data sparsity and high dimension. Meanwhile, due to the independent assumption between words, BoW representation has very little sense about the semantics of the words.
To address this shortcoming, several dimensionality reduction methods have been proposed, such as latent semantic indexing (LSI) (S. Deerwester & Harshman, 1990), Probabilistic latent semantic indexing (PLSI) (Hofmann, 1999) and latent Dirichlet allocation (LDA) (Blei & Jordan, 2003). Both PLSI and LDA have a good statistical foundation and proper generative model of the documents, as compared with LSI which relies on a singular value decomposition over the term-document cooccurrence matrix. In PLSI, each word is generated from a single topic, and different words in a document may be generated from different topics. While PLSI makes great effect on probabilistic modeling of documents, it is not clear how to assign probability to a document outside of the training set with the learned model. To address this issue, LDA is proposed by introducing a complete generative process over the documents, and demonstrated as a state-of-the-art document representation method. To further tackle the prediction task, Supervised LDA (David M.Blei, 2007) is developed by jointly modeling the documents and the labels.
Recently, distributed models have been demonstrated as efficient methods to acquire semantic representations of texts. A representative method is Word2Vec (Tomas Mikolov & Dean, 2013b), which can learn meaningful word representations in an unsupervised way from large scale corpus. To represent sentences or documents, a simple approach is then using a weighted average of all the words. A more sophisticated approach is combing the word vectors in an order given by a parse tree (Richard Socher & Ng, 2012). Later, Paragraph Vector (PV) (Quoc Le, 2014) is introduced to directly learn the distributed representations of sentences and documents. There are two variants in PV, namely the Distributed Memory Model of Paragraph Vector (PV-DM) and the Distributed Bag of Words version of Paragraph Vector (PV-DBOW), based on two different model architectures. Although PV is a simple yet effective distributed model on sentences and documents, it suffers a similar problem as PLSI that it provides no model on text vectors: it is unclear how to infer the distributed representations for texts outside of the training set with the learned model.
Besides these unsupervised representation learning methods, there have been many supervised deep models with directly learn sentence or document representations for the prediction tasks. Recursive Neural Network (RecursiveNN) (Richard Socher & Ng, 2012) has been proven to be efficient in terms of constructing sentence representations. Recurrent Neural Network (RNN) (Ilya Sutskever & Hinton, 2011) can be viewed as an extremely deep neural network with weight sharing across time. Convolution Neural Network (CNN) (Kim, 2014) can fairly determine discriminative phrases in a
text with a max-pooling layer. However, these deep models are usually quite complex and thus the training would be time-consuming on large corpus.

3 PARAGRAPH VECTOR
Since our model can be viewed as a probabilistic extension of the PV-DBOW model with a complete generative process, we first briefly review the PV-DBOW model for reference.
In PV-DBOW, each text is mapped to a unique paragraph vector and each word is mapped to a unique word vector in a continuous space. The paragraph vector is used to predict target words randomly sampled from the paragraph as shown in Figure 1. More formally, Let D={d1, . . . ,dN} denote a corpus of N texts, where each text dn = (wn1 , w n 2 , . . . , w n ln
), n ∈ 1, 2, . . . , N is an lnlength word sequence over the word vocabulary V of size M . Each text d ∈ D and each word w ∈ V is associated with a vector ~d ∈ RK and ~w ∈ RK , respectively, where K is the embedding dimensionality. The predictive objective of the PV-DBOW for each word wnl ∈ dn is defined by the softmax function
p(wni |dn) = exp(~wni · ~dn)∑ w′∈V exp(~w ′ · ~dn) (1)
The PV-DBOW model can be efficiently trained using the stochastic gradient descent (Rumelhart & Williams, 1986) with negative sampling (T. Mikolov & Dean, 2013a).
As compared with traditional topic models, e.g. PLSI and LDA, PV-DBOW conveys the following merits. Firstly, PV-DBOW using negative sampling can be interpretated as a matrix factorization over the words-by-texts co-occurrence matrix with shifted-PMI values (Omer Levy & Ramat-Gan, 2015). In this way, more discriminative information (i.e., PMI) can be modeled in PV as compared with the generative topic models which learn over the words-by-texts co-occurrence matrix with raw frequency values. Secondly, PV-DBOW does not have the explicit “topic” layer and allows words automatically clustered according to their co-occurrence patterns during the learning process. In this way, PV-DBOW can potentially learn much finer topics than traditional topic models given the same hidden dimensionality of texts. However, a major problem with PV-DBOW is that it provides no model on text vectors: it is unclear how to infer the distributed representations for unseen texts.

4 GENERATIVE PARAGRAPH VECTOR
In this section, we introduce the GPV model in detail. Overall, GPV is a generative probabilistic model for a corpus. We assume that for each text, a latent paragraph vector is first sampled from some prior distributions, and the words within the text are then generated from the normalized exponential (i.e. softmax) distribution given the paragraph vector and word vectors. In our work, multivariate normal distribution is employed as the prior distribution for paragraph vectors. It could
be replaced by other prior distributions and we will leave this as our future work. The specific generative process is as follows:
For each text dn ∈D, n = 1, 2, . . . , N : (a) Draw paragraph vector ~dn ∼ N (µ,Σ) (b) For each word wni ∈ dn, i = 1, 2, . . . , ln :
Draw word wni ∼ softmax(~dn ·W )i
where W denotes a k ×M word embedding matrix with W∗j = ~wj , and softmax(~dn ·W )i is the softmax function defined the same as in Equation (1). Figure 2 (Left) provides the graphical model of this generative process. Note that GPV differs from PV-DBOW in that the paragraph vector is a hidden variable generated from some prior distribution, which allows us to infer the paragraph vector over future texts given the learned model. Based on the above generative process, the probability of the whole corpus can be written as follows:
p(D)= N∏ n=1 ∫ p(~dn|µ,Σ) ∏ wni ∈dn p(wni |W, ~dn)d~dn
To learn the model, direct maximum likelihood estimation is not tractable due to non-closed form of the integral. We approximate this learning problem by using MAP estimates for ~dn, which can be formulated as follows:
(µ∗,Σ∗,W ∗) = arg max µ,Σ,W
∏ p(d̂n|µ,Σ) ∏ wni ∈dn p(wni |W, d̂n)
where d̂n denotes the MAP estimate of ~dn for dn, (µ∗,Σ∗,W ∗) denotes the optimal solution. Note that for computational simplicity, in this work we fixed µ as a zero vector and Σ as a identity matrix. In this way, all the free parameters to be learned in our model are word embedding matrix W . By taking the logarithm and applying the negative sampling idea to approximate the softmax function, we obtain the final learning problem
L= N∑ n=1 ( −1 2 ||d̂n||2+ ∑ wni ∈dn ( log σ(~wni ·d̂n)+k·Ew′∼Pnw log σ(− ~w′ · d̂n) )) where σ(x) = 1/(1 + exp(−x)), k is the number of “negative” samples, w′ denotes the sampled word and Pnw denotes the distribution of negative word samples. As we can see from the final objective function, the prior distribution over paragraph vectors actually act as a regularization term. From the view of optimization, such regularization term could constrain the learning space and usually produces better paragraph vectors.
For optimization, we use coordinate ascent, which first optimizes the word vectors W while leaving the MAP estimates (d̂) fixed. Then we find the new MAP estimate for each document while leaving the word vectors fixed, and continue this process until convergence. To accelerate the learning, we adopt a similar stochastic learning framework as in PV which iteratively updates W and estimates ~d by randomly sampling text and word pairs.
At prediction time, given a new text, we perform an inference step to compute the paragraph vector for the input text. In this step, we freeze the vector representations of each word, and apply the same MAP estimation process of ~d as in the learning phase. With the inferred paragraph vector of the test text, we can feed it to other prediction models for different applications.

5 SUPERVISED GENERATIVE PARAGRAPH VECTOR
With the ability to infer the distributed representations for unseen texts, we now can incorporate the labels paired with the texts into the model to guide the representation learning, and turn the model into a more powerful supervised version directly towards prediction tasks. Specifically, we introduce an additional label generation process into GPV to accommodate text labels, and obtain the Supervised Generative Paragraph Vector (SGPV) model. Formally, in SGPV, the n-th text dn and the corresponding class label yn ∈ {1, 2, . . . , C} arise from the following generative process:
For each text dn ∈D, n = 1, 2, . . . , N : (a) Draw paragraph vector ~dn ∼ N (µ,Σ) (b) For each word wni ∈ dn, i = 1, 2, . . . , ln :
Draw word wni ∼ softmax(~dn ·W )i (c) Draw label yn|~dn, U, b ∼ softmax(U · ~dn+b)
where U is a C ×K matrix for a dataset with C output labels, and b is a bias term. The graphical model of the above generative process is depicted in Figure 2 (Right). SGPV defines the probability of the whole corpus as follows
p(D)= N∏ n=1 ∫ p(~dn|µ,Σ) ( ∏ wni ∈dn p(wni |W, ~dn) ) p(yn|~dn, U, b)d~dn
We adopt a similar learning process as GPV to estimate the model parameters. Since the SGPV includes the complete generative process of both paragraphs and labels, we can directly leverage it to predict the labels of new texts. Specifically, at prediction time, given all the learned model parameters, we conduct an inference step to infer the paragraph vector as well as the label using MAP estimate over the test text.
The above SGPV may have limited modeling ability on text representation since it mainly relies on uni-grams. As we know, word order information is often critical in capturing the meaning of texts. For example, “machine learning” and “learning machine” are totally different in meaning with the same words. There has been a variety of deep models using complex architectures such as convolution layers or recurrent structures to help capture such order information at the expense of large computational cost.
Here we propose to extend SGPV by introducing an additional generative process for n-grams, so that we can incorporate the word order information into the model and meanwhile keep its simplicity in learning. We name this extension as SGPV-ngram. Here we take the generative process of SGPVbigram as an example.
For each text dn ∈D, n = 1, 2, . . . , N : (a) Draw paragraph vector ~dn ∼ N (µ,Σ) (b) For each word wni ∈ dn, i = 1, 2, . . . , ln :
Draw word wni ∼ softmax(~dn ·W )i
(c) For each bigram gni ∈ dn, i = 1, 2, . . . , sn : Draw bigram gni ∼ softmax(~dn ·G)i
(d) Draw label yn|~dn, U, b ∼ softmax(U · ~dn+b)
where G denotes a K × S bigram embedding matrix with G∗j = ~gj , and S denotes the size of bigram vocabulary. The joint probability over the whole corpus is then defined as
p(D)= N∏ n=1 ∫ p(~dn|µ,Σ) ( ∏ wni ∈dn p(wni |W, ~dn) )( ∏ gni ∈dn p(gni |G, ~dn) ) p(yn|~dn, U, b)d~dn

6 EXPERIMENTS
In this section, we introduce the experimental settings and empirical results on a set of text classification tasks.

6.1 DATASET AND EXPERIMENTAL SETUP
We made use of five publicly available benchmark datasets in comparison.
TREC: The TREC Question Classification dataset (Li & Roth, 2002)1 which consists of 5, 452 train questions and 500 test questions. The goal is to classify a question into 6 different types depending on the answer they seek for.
Subj: Subjectivity dataset (Pang & Lee, 2004) which contains 5, 000 subjective instances and 5, 000 objective instances. The task is to classify a sentence as being subjective or objective.
MR: Movie reviews (Pang & Lee, 2005) 2 with one sentence per review. There are 5, 331 positive sentences and 5, 331 negative sentences. The objective is to classify each review into positive or negative category.
SST-1: Stanford Sentiment Treebank (Socher & Potts, 2013) 3. SST-1 is provided with train/dev/test splits of size 8, 544/1, 101/2, 210. It is a fine-grained classification over five classes: very negative, negative, neutral, positive, and very positive.
SST-2: SST-2 is the same as SST-1 but with neutral reviews removed. We use the standard train/dev/test splits of size 6, 920/872/1, 821 for the binary classification task.
Preprocessing steps were applied to all datasets: words were lowercased, non-English characters and stop words occurrence in the training set are removed. For fair comparison with other published results, we use the default train/test split for TREC, SST-1 and SST-2 datasets. Since explicit split of train/test is not provided by subj and MR datasets, we use 10-fold cross-validation instead.
In our model, text and word vectors are randomly initialized with values uniformly distributed in the range of [-0.5, +0.5]. Following the practice in (Tomas Mikolov & Dean, 2013b) , we set the noise distributions for context and words as pnw(w) ∝ #(w)0.75. We adopt the same linear learning rate strategy where the initial learning rate of our models is 0.025. For unsupervised methods, we use support vector machines (SVM) 4 as the classifier.

6.2 BASELINES
We adopted both unsupervised and supervised methods on text representation as baselines.

6.2.1 UNSUPERVISED BASELINES
Bag-of-word-TFIDF and Bag-of-bigram-TFIDF. In the bag-of-word-TFIDF scheme (Salton & McGill, 1983) , each text is represented as the tf-idf value of chosen feature-words. The bag-of-
1http://cogcomp.cs.illinois.edu/Data/QA/QC/ 2https://www.cs.cornell.edu/people/pabo/movie-review-data/ 3http://nlp.stanford.edu/sentiment/ 4http://www.csie.ntu.edu.tw/˜cjlin/libsvm/
bigram-TFIDF model is constructed by selecting the most frequent unigrams and bigrams from the training subset. We use the vanilla TFIDF in the gensim library5.
LSI (S. Deerwester & Harshman, 1990) and LDA (Blei & Jordan, 2003). LSI maps both texts and words to lower-dimensional representations in a so-called latent semantic space using SVD decomposition. In LDA, each word within a text is modeled as a finite mixture over an underlying set of topics. We use the vanilla LSI and LDA in the gensim library with topic number set as 100.
cBow (Tomas Mikolov & Dean, 2013b). Continuous Bag-Of-Words model. We use average pooling as the global pooling mechanism to compose a sentence vector from a set of word vectors.
PV (Quoc Le, 2014). Paragraph Vector is an unsupervised model to learn distributed representations of words and paragraphs.
FastSent (Felix Hill, 2016). In FastSent, given a simple representation of some sentence in context, the model attempts to predict adjacent sentences.
Note that unlike LDA and GPV, LSI, cBow, and FastSent cannot infer the representations of unseen texts. Therefore, these four models need to fold-in all the test data to learn representations together with training data, which makes it not efficient in practice.

6.2.2 SUPERVISED BASELINES
NBSVM and MNB (S. Wang, 2012). Naive Bayes SVM and Multinomial Naive Bayes with unigrams and bi-grams.
DAN (Mohit Iyyer & III, 2015). Deep averaging network uses average word vectors as the input and applies multiple neural layers to learn text representation under supervision.
CNN-multichannel (Kim, 2014). CNN-multichannel employs convolutional neural network for sentence modeling.
DCNN (N. Kalchbrenner, 2014). DCNN uses a convolutional architecture that replaces wide convolutional layers with dynamic pooling layers.
MV-RNN (Richard Socher & Ng, 2012). Matrix-Vector RNN represents every word and longer phrase in a parse tree as both a vector and a matrix.
DRNN (Irsoy & Cardie, 2014). Deep Recursive Neural Networks is constructed by stacking multiple recursive layers.
Dependency Tree-LSTM (Kai Sheng Tai & Manning, 2015). The Dependency Tree-LSTM based on LSTM structure uses dependency parses of each sentence.

6.3 PERFORMANCE OF GENERATIVE PARAGRAPH VECTOR
We first evaluate the GPV model by comparing with the unsupervised baselines on the TREC, Subj and MR datasets. As shown in table 1, GPV works better than PV over the three tasks. It demonstrates the benefits of introducing a prior distribution (i.e., regularization) over the paragraph vectors. Moreover, GPV can also outperform almost all the baselines on three tasks except Bow-TFIDF and Bigram-TFIDF on the TREC collection. The results show that for unsupervised text representation, bag-of-words representation is quite simple yet powerful which can beat many embedding models. Meanwhile, by using a complete generative process to infer the paragraph vectors, our model can achieve the state-of-the-art performance among the embedding based models.

6.4 PERFORMANCE OF SUPERVISED GENERATIVE PARAGRAPH VECTOR
We compare SGPV model to supervised baselines on all the five classification tasks. Empirical results are shown in Table 2. We can see that SGPV achieves comparable performance against other deep learning models. Note that SGPV is much simpler than these deep models with significantly less parameters and no complex structures. Moreover, deep models with convolutional layers or recurrent structures can potentially capture compositional semantics (e.g., phrases), while SGPV only
5http://radimrehurek.com/gensim/
relies on uni-gram. In this sense, SGPV is quite effective in learning text representation. Meanwhile, if we take Table 1 into consideration, it is not surprising to see that SGPV can consistently outperform GPV on all the three classification tasks. This also demonstrates that it is more effective to directly fit supervised representation models than to learn a general purpose representation in prediction scenarios.
By introducing bi-grams, SGPV-bigram can outperform all the other deep models on four tasks. In particular, the improvements of SGPV-bigram over other baselines are significant on SST-1 and SST-2. These results again demonstrated the effectiveness of our proposed SGPV model on text representations. It also shows the importance of word order information in modeling text semantics.

7 CONCLUSIONS
In this paper, we introduce GPV and SGPV for learning distributed representations for pieces of texts. With a complete generative process, our models are able to infer vector representations as well as labels over unseen texts. Our models keep as simple as PV models, and thus can be efficiently learned over large scale text corpus. Even with such simple structures, both GPV and SGPV can produce state-of-the-art results as compared with existing baselines, especially those complex deep models. For future work, we may consider other probabilistic distributions for both paragraph vectors and word vectors.
","The recently introduced Paragraph Vector is an efficient method for learning highquality distributed representations for pieces of texts. However, an inherent limitation of Paragraph Vector is lack of ability to infer distributed representations for texts outside of the training set. To tackle this problem, we introduce a Generative Paragraph Vector, which can be viewed as a probabilistic extension of the Distributed Bag of Words version of Paragraph Vector with a complete generative process. With the ability to infer the distributed representations for unseen texts, we can further incorporate text labels into the model and turn it into a supervised version, namely Supervised Generative Paragraph Vector. In this way, we can leverage the labels paired with the texts to guide the representation learning, and employ the learned model for prediction tasks directly. Experiments on five text classification benchmark collections show that both model architectures can yield superior classification performance over the state-of-the-art counterparts.",ICLR 2017 conference submission,False,,"This work reframes paragraph vectors from a generative point of view and in so doing, motivates the existing method of inferring paragraph vectors as well as applying a L2 regularizer on the paragraph embeddings. The work also motivates joint learning of a classifier on the paragraph vectors to perform text classification.

The paper has numerous citation issues both in formatting within the text and the formatting of the bibliography, e.g. on some occasions including first names, on others not. I suggest the authors use a software package like BibTex to have a more consistent bibliography. There seems to be little novelty in this work. 

The authors claim that there is no proposed method for inferring unseen documents for paragraph vectors. This is untrue. In the original paragraph vector paper, the authors show that to get a new vector, the rest of the model parameters are held fixed and gradient descent is performed on the new paragraph vector. This means the original dataset is not needed when inferring a paragraph vector for new text. This work seems to be essentially doing the same thing when finding the MAP estimate for a new vector. Thus the only contribution from the generative paragraph vector framing is the regularization on the embedding matrix.

The supervised generative paragraph vector amounts to jointly training a linear classifier on the paragraph vectors, while inference for the paragraph vector is unchanged. For the n-gram based approach, the authors should cite Li et al., 2015.

In the experiments, table 1 and 2 are badly formatted with .0 being truncated. The authors also do not state the size of the paragraph vector. Finally the SGPV results are actually worse than that reported in the original paragraph vector paper where SST-1 got 48.7 and SST-2 got 86.3.

Bofang Li, Tao Liu, Xiaoyong Du, Deyuan Zhang, Zhe Zhao, Learning Document Embeddings by Predicting N-grams for Sentiment Classification of Long Movie Reviews, 2015.

---

The contribution of this paper generally boils down to adding a prior to the latent representations of the paragraph in the Paragraph Vector model. An especially problematic point about this paper is the claim that the original paper considered only the transductive setting (i.e. it could not induce representations of new documents). It is not accurate, they also used gradient descent at test time. Though I agree that regularizing the original model is a reasonable thing to do, I share the reviewers' feeling that the contribution is minimal. There are also some serious issues with presentation (as noted by the reviewers), I am surprised that the authors have not addressed them during the review period.

---

While this paper has some decent accuracy numbers, it is hard to argue for acceptance given the following:

1) motivation based on the incorrect assumption that the Paragraph Vector wouldn't work on unseen data
2) Numerous basic formatting and Bibtex citation issues.

Lack of novelty of yet another standard directed LDA-like bag of words/bigram model.

---

It feels that this paper is structured around a shortcoming of the original paragraph vectors paper, namely an alleged inability to infer representation for text outside of the training data. I am reasonably sure that this is not the case. Unfortunately on that basis, the premise for the work presented here no longer holds, which renders most of the subsequent discussion void.

While I recommend this paper be rejected, I encourage the authors to revisit the novel aspects of the idea presented here and see if that can be turned into a different type of paper going forward.

---

This work reframes paragraph vectors from a generative point of view and in so doing, motivates the existing method of inferring paragraph vectors as well as applying a L2 regularizer on the paragraph embeddings. The work also motivates joint learning of a classifier on the paragraph vectors to perform text classification.

The paper has numerous citation issues both in formatting within the text and the formatting of the bibliography, e.g. on some occasions including first names, on others not. I suggest the authors use a software package like BibTex to have a more consistent bibliography. There seems to be little novelty in this work. 

The authors claim that there is no proposed method for inferring unseen documents for paragraph vectors. This is untrue. In the original paragraph vector paper, the authors show that to get a new vector, the rest of the model parameters are held fixed and gradient descent is performed on the new paragraph vector. This means the original dataset is not needed when inferring a paragraph vector for new text. This work seems to be essentially doing the same thing when finding the MAP estimate for a new vector. Thus the only contribution from the generative paragraph vector framing is the regularization on the embedding matrix.

The supervised generative paragraph vector amounts to jointly training a linear classifier on the paragraph vectors, while inference for the paragraph vector is unchanged. For the n-gram based approach, the authors should cite Li et al., 2015.

In the experiments, table 1 and 2 are badly formatted with .0 being truncated. The authors also do not state the size of the paragraph vector. Finally the SGPV results are actually worse than that reported in the original paragraph vector paper where SST-1 got 48.7 and SST-2 got 86.3.

Bofang Li, Tao Liu, Xiaoyong Du, Deyuan Zhang, Zhe Zhao, Learning Document Embeddings by Predicting N-grams for Sentiment Classification of Long Movie Reviews, 2015.

---

This work reframes paragraph vectors from a generative point of view and in so doing, motivates the existing method of inferring paragraph vectors as well as applying a L2 regularizer on the paragraph embeddings. The work also motivates joint learning of a classifier on the paragraph vectors to perform text classification.

The paper has numerous citation issues both in formatting within the text and the formatting of the bibliography, e.g. on some occasions including first names, on others not. I suggest the authors use a software package like BibTex to have a more consistent bibliography. There seems to be little novelty in this work. 

The authors claim that there is no proposed method for inferring unseen documents for paragraph vectors. This is untrue. In the original paragraph vector paper, the authors show that to get a new vector, the rest of the model parameters are held fixed and gradient descent is performed on the new paragraph vector. This means the original dataset is not needed when inferring a paragraph vector for new text. This work seems to be essentially doing the same thing when finding the MAP estimate for a new vector. Thus the only contribution from the generative paragraph vector framing is the regularization on the embedding matrix.

The supervised generative paragraph vector amounts to jointly training a linear classifier on the paragraph vectors, while inference for the paragraph vector is unchanged. For the n-gram based approach, the authors should cite Li et al., 2015.

In the experiments, table 1 and 2 are badly formatted with .0 being truncated. The authors also do not state the size of the paragraph vector. Finally the SGPV results are actually worse than that reported in the original paragraph vector paper where SST-1 got 48.7 and SST-2 got 86.3.

Bofang Li, Tao Liu, Xiaoyong Du, Deyuan Zhang, Zhe Zhao, Learning Document Embeddings by Predicting N-grams for Sentiment Classification of Long Movie Reviews, 2015.

---

The contribution of this paper generally boils down to adding a prior to the latent representations of the paragraph in the Paragraph Vector model. An especially problematic point about this paper is the claim that the original paper considered only the transductive setting (i.e. it could not induce representations of new documents). It is not accurate, they also used gradient descent at test time. Though I agree that regularizing the original model is a reasonable thing to do, I share the reviewers' feeling that the contribution is minimal. There are also some serious issues with presentation (as noted by the reviewers), I am surprised that the authors have not addressed them during the review period.

---

While this paper has some decent accuracy numbers, it is hard to argue for acceptance given the following:

1) motivation based on the incorrect assumption that the Paragraph Vector wouldn't work on unseen data
2) Numerous basic formatting and Bibtex citation issues.

Lack of novelty of yet another standard directed LDA-like bag of words/bigram model.

---

It feels that this paper is structured around a shortcoming of the original paragraph vectors paper, namely an alleged inability to infer representation for text outside of the training data. I am reasonably sure that this is not the case. Unfortunately on that basis, the premise for the work presented here no longer holds, which renders most of the subsequent discussion void.

While I recommend this paper be rejected, I encourage the authors to revisit the novel aspects of the idea presented here and see if that can be turned into a different type of paper going forward.

---

This work reframes paragraph vectors from a generative point of view and in so doing, motivates the existing method of inferring paragraph vectors as well as applying a L2 regularizer on the paragraph embeddings. The work also motivates joint learning of a classifier on the paragraph vectors to perform text classification.

The paper has numerous citation issues both in formatting within the text and the formatting of the bibliography, e.g. on some occasions including first names, on others not. I suggest the authors use a software package like BibTex to have a more consistent bibliography. There seems to be little novelty in this work. 

The authors claim that there is no proposed method for inferring unseen documents for paragraph vectors. This is untrue. In the original paragraph vector paper, the authors show that to get a new vector, the rest of the model parameters are held fixed and gradient descent is performed on the new paragraph vector. This means the original dataset is not needed when inferring a paragraph vector for new text. This work seems to be essentially doing the same thing when finding the MAP estimate for a new vector. Thus the only contribution from the generative paragraph vector framing is the regularization on the embedding matrix.

The supervised generative paragraph vector amounts to jointly training a linear classifier on the paragraph vectors, while inference for the paragraph vector is unchanged. For the n-gram based approach, the authors should cite Li et al., 2015.

In the experiments, table 1 and 2 are badly formatted with .0 being truncated. The authors also do not state the size of the paragraph vector. Finally the SGPV results are actually worse than that reported in the original paragraph vector paper where SST-1 got 48.7 and SST-2 got 86.3.

Bofang Li, Tao Liu, Xiaoyong Du, Deyuan Zhang, Zhe Zhao, Learning Document Embeddings by Predicting N-grams for Sentiment Classification of Long Movie Reviews, 2015.",,,,,,3.0,,,4.333333333333333,,
733,"HOST-BASED INTRUSION DETECTION SYSTEMS
Authors: Gyuwan Kim, Hayoon Yi, Jangho Lee, Yunheung Paek, Sungroh Yoon
Source file: 733.pdf

ABSTRACT
In computer security, designing a robust intrusion detection system is one of the most fundamental and important problems. In this paper, we propose a system-call language-modeling approach for designing anomaly-based host intrusion detection systems. To remedy the issue of high false-alarm rates commonly arising in conventional methods, we employ a novel ensemble method that blends multiple thresholding classifiers into a single one, making it possible to accumulate ‘highly normal’ sequences. The proposed system-call language model has various advantages leveraged by the fact that it can learn the semantic meaning and interactions of each system call that existing methods cannot effectively consider. Through diverse experiments on public benchmark datasets, we demonstrate the validity and effectiveness of the proposed method. Moreover, we show that our model possesses high portability, which is one of the key aspects of realizing successful intrusion detection systems.

In computer security, designing a robust intrusion detection system is one of the most fundamental and important problems. In this paper, we propose a system-call language-modeling approach for designing anomaly-based host intrusion detection systems. To remedy the issue of high false-alarm rates commonly arising in conventional methods, we employ a novel ensemble method that blends multiple thresholding classifiers into a single one, making it possible to accumulate ‘highly normal’ sequences. The proposed system-call language model has various advantages leveraged by the fact that it can learn the semantic meaning and interactions of each system call that existing methods cannot effectively consider. Through diverse experiments on public benchmark datasets, we demonstrate the validity and effectiveness of the proposed method. Moreover, we show that our model possesses high portability, which is one of the key aspects of realizing successful intrusion detection systems.

1 INTRODUCTION
An intrusion detection system (IDS) refers to a hardware/software platform for monitoring network or system activities to detect malicious signs therefrom. Nowadays, practically all existing computer systems operate in a networked environment, which continuously makes them vulnerable to a variety of malicious activities. Over the years, the number of intrusion events is significantly increasing across the world, and intrusion detection systems have already become one of the most critical components in computer security. With the explosive growth of logging data, the role of machine learning in effective discrimination between malicious and benign system activities has never been more important.
A survey of existing IDS approaches needs a multidimensional consideration. Depending on the scope of intrusion monitoring, there exist two main types of intrusion detection systems: networkbased (NIDS) and host-based (HIDS). The network-based intrusion detection systems monitor communications between hosts, while the host-based intrusion detection systems monitor the activity on a single system. From a methodological point of view, intrusion detection systems can also be classified into two classes (Jyothsna et al., 2011): signature-based and anomaly-based. The signaturebased approaches match the observed behaviors against templates of known attack patterns, while the anomaly-based techniques compare the observed behaviors against an extensive baseline of normal behaviors constructed from prior knowledge, declaring each of anomalous activities to be an attack. The signature-based methods detect already known and learned attack patterns well but have an innate difficulty in detecting unfamiliar attack patterns. On the other hand, the anomaly-based methods can potentially detect previously unseen attacks but may suffer from making a robust baseline of normal behavior, often yielding high false alarm rates. The ability to detect a ‘zero-day’ attack (i.e., vulnerability unknown to system developers) in a robust manner is becoming an important requirement of an anomaly-based approach. In terms of this two-dimensional taxonomy, we can classify our proposed method as an anomaly-based host intrusion detection system.
∗To whom correspondence should be addressed.
It was Forrest et al. (1996) who first started to use system-call traces as the raw data for hostbased anomaly intrusion detection systems, and system-call traces have been widely used for IDS research and development since their seminal work (Forrest et al., 2008). Recently, Creech & Hu (2014) proposed to use neural networks on top of a sequence of system calls in the context of HIDS. System calls represent low-level interactions between programs and the kernel in the system, and many researchers consider system-call traces as the most accurate source useful for detecting intrusion in an anomaly-based HIDS. From a data acquisition point of view, system-call traces are easy to collect in a large quantity in real-time. Our approach described in this paper also utilizes system-call traces as input data.
For nearly two decades, various research has been conducted based on analyzing system-call traces. Most of the existing anomaly-based host intrusion detection methods typically aim to identify meaningful features using the frequency of individual calls and/or windowed patterns of calls from sequences of system calls. However, such methods have limited ability to capture call-level features and phrase-level features simultaneously. As will be detailed shortly, our approach tries to address this limitation by generating a language model of system calls that can jointly learn the semantics of individual system calls and their interactions (that can collectively represent a new meaning) appearing in call sequences.
In natural language processing (NLP), a language model represents a probability distribution over sequences of words, and language modeling has been a very important component of many NLP applications, including machine translation (Cho et al., 2014; Bahdanau et al., 2014), speech recognition (Graves et al., 2013), question answering (Hermann et al., 2015), and summarization (Rush et al., 2015). Recently, deep recurrent neural network (RNN)-based language models are showing remarkable performance in various tasks (Zaremba et al., 2014; Jozefowicz et al., 2016). It is expected that such neural language models will be applicable to not only NLP applications but also signal processing, bioinformatics, economic forecasting, and other tasks that require effective temporal modeling.
Motivated by this performance advantage and versatility of deep RNN-based language modeling, we propose an application of neural language modeling to host-based introduction detection. We consider system-call sequences as a language used for communication between users (or programs) and the system. In this view, system calls and system-call sequences correspond to words and sentences in natural languages, respectively. Based on this system-call language model, we can perform various tasks that comprise our algorithm to detect anomalous system-call sequences: e.g., estimation of the relative likelihood of different words (i.e., system calls) and phrases (i.e., a window of system calls) in different contexts.
The idea of using artificial neural networks for IDSs has been popular (Debar et al., 1992; Ryan et al., 1998; Mukkamala et al., 2002; Wang et al., 2010; Creech & Hu, 2014). For more recent deep learning-based techniques, there exists an example that utilized LSTM for improving intrusion detection performance (Staudemeyer & Omlin, 2013; Staudemeyer, 2015). However, the work by Staudemeyer & Omlin (2013); Staudemeyer (2015) was in essence a feature-based supervised classifier (rather than an anomaly detector) requiring heavy annotation efforts to create labels. As such, their work required explicitly labeled attack data and possessed an inherent limitation that it could not detect new types of attacks. In addition, their approach was not an end-to-end framework and needed careful feature engineering to extract salient features for the classification task. Only one binary label was given per sequence to train their model, unlike our proposed method that is trained to predict the next call, effectively capturing contextual information needed for classification.
Our specific contributions can be summarized as follows: First, to model sequences of system calls, we propose a neural language modeling technique that utilizes long short-term memory (LSTM) (Hochreiter & Schmidhuber, 1997) units for enhanced long-range dependence learning. The present work is one of the first end-to-end frameworks to model system-call sequences as a natural language for effectively detecting anomalous patterns therefrom. Second, to reduce false-alarm rates of anomaly-based intrusion detection, we propose a leaky rectified linear units (ReLU) (Maas et al., 2013) based ensemble method that constructs an integrative classifier using multiple (relatively weak) thresholding classifiers. Each of the component classifiers is trained to detect different types of ‘highly normal’ sequences (i.e., system call sequences with very high probability of being normal), and our ensemble method blends them to produce a robust classifier that delivers significantly lower false-alarm rates than other commonly used ensemble methods. As shown in Figure 1, these
two aspects of our contributions can seamlessly be combined into a single framework. Note that the ensemble method we propose is not limited to our language-model based front-end but also applicable to other types of front-ends.
In the rest of this paper, we will explain more details of our approach and then present our experimental results that demonstrate the effectiveness of our proposed method.

2 PROPOSED METHOD
Figure 1 shows the overview of our proposed approach to designing an intrusion detection system. Our method consists of two parts: the front-end is for language modeling of system calls in various settings, and the back-end is for anomaly prediction based on an ensemble of thresholding classifiers derived from the front-end. In this section, we describe details of each component in our pipeline.

2.1 LANGUAGE MODELING OF SYSTEM CALLS
Figure 2 illustrates the architecture of our system-call language model. The system call language model estimates the probability distribution of the next call in a sequence given the sequence of previous calls. We assume that the host system generates a finite number of system calls. We index each system call by using an integer starting from 1 and denote the fixed set of all possible system calls in the system as S = {1, · · · ,K}. Let x = x1x2 · · ·xl(xi ∈ S) denote a sequence of l system calls.
At the input layer, the call at each time step xi is fed into the model in the form of one-hot encoding, in other words, a K dimensional vector with all elements zero except position xi. At the embedding layer, incoming calls are embedded to continuous space by multiplying embedding matrix W , which should be learned. At the hidden layer, the LSTM unit has an internal state, and this state is updated recurrently at each time step. At the output layer, a softmax activation function is used to produce the estimation of normalized probability values of possible calls coming next in the sequence, P (xi|x1:i−1). According to the chain rule, we can estimate the sequence probability by the following formula:
P (x) = l∏ i=1 P (xi|x1:i−1) (1)
Given normal training system call sequence data, we can train this LSTM-based system call language model using the back-propagation through time (BPTT) algorithm. The training criterion minimizes the cross-entropy loss, which is equivalent to maximizing the likelihood of the system call sequence. A standard RNN often suffers from the vanishing/exploding gradient problem, and when training with BPTT, gradient values tend to blow up or vanish exponentially. This makes it difficult to learn long-term dependency in RNNs (Bengio et al., 1994). LSTM, a well-designed RNN architecture component, is equipped with an explicit memory cell and tends to be more effective to cope with this problem, resulting in numerous successes in recent RNN applications.
(a) language model architecture (b) estimation of sequence probability
embedding layer
hidden layer
output layer
input layer 𝟎 ∙ ∙ ∙ 𝟎 𝟏 𝟎 ∙ ∙ ∙ 𝟎
⋯
𝑃(𝑥1) 𝑃(𝑥2|𝑥1) 𝑃(𝑥3|𝑥1:2) 𝑃(𝑥𝑛|𝑥1:𝑛−1)
𝑥1
fork
𝑥2
setgid
𝑥𝑛−1
ioctl
𝑥𝑛
close
[GO]
Figure 2: System-call language model.
Because typical processes in the system execute a long chain of system calls, the number of system calls required to fully understand the meaning of a system-call sequence is quite large. In addition, the system calls comprising a process are intertwined with each other in a complicated way. The boundaries between system-call sequences are also vague. In this regard, learning long-term dependence is crucial for devising effective intrusion detection systems.
Markov chains and hidden Markov models are widely used probabilistic models that can estimate the probability of the next call given a sequence of previous calls. There has been previous work on using Markov models in intrusion detection systems (Hofmeyr et al., 1998; Hoang et al., 2003; Hu et al., 2009; Yolacan et al., 2014). However, these methods have an inherent limitation in that the probability of the next call is decided by only a finite number of previous calls. Moreover, LSTM can model exponentially more complex functions than Markov models by using continuous space representations. This property alleviates the data sparsity issue that occurs when a large number of previous states are used in Markov models. In short, the advantages of LSTM models compared to Markov models are two folds: the ability to capture long-term dependency and enhanced expressive power.
Given a new query system-call sequence, on the assumption that abnormal call patterns deviate from learned normal patterns, yielding significantly lower probabilities than those of normal call patterns, a sequence with an average negative log-likelihood above a threshold is classified as abnormal, while a sequence with an average negative log-likelihood below the threshold is classified as normal. By changing the threshold value, we can draw a receiver operating characteristic (ROC) curve, which is the most widely used measure to evaluate intrusion detection systems.
Commonly, IDS is evaluated by the ROC curve rather than a single point corresponding to a specific threshold on the curve. Sensitivity to the threshold is shown on the curve. The x-axis of the curve represents false alarm rates, and the y-axis of the curve represents detection rates.1 If the threshold is too low, the IDS is able to detect attacks well, but users would be annoyed due to false alarms. Conversely, if the threshold is too high, false alarm rates becomes lower, but it is easy for IDS to miss attacks. ROC curves closer to (0, 1) means a better classifier (i.e., a better intrusion detection system). The area under curve (AUC) summarizes the ROC curve into a single value in the range [0, 1] (Bradley, 1997).

2.2 ENSEMBLE METHOD TO MINIMIZE FALSE ALARM RATES
Building a ‘strong normal’ model (a model representing system-call sequences with high probabilities of being normal) is challenging because of over-fitting issues. In other words, a lower training loss does not necessarily imply better generalization performance. We can consider two reasons for encountering this issue.
First, it is possible that only normal data were used for training the IDS without any attack data. Learning discriminative features that can separate normal call sequences from abnormal sequences is thus hard without seeing any abnormal sequences beforehand. This is a common obstacle for
1A false alarm rate is the ratio of validation normal data classified as abnormal. A detection rate is the ratio of detected attacks in the real attack data.
almost every anomaly detection problem. In particular, malicious behaviors are frequently hidden and account for only a small part of all the system call sequences.
Second, in theory, we need a huge amount of data to cover all possible normal patterns to train the model satisfactorily. However, doing so is often impossible in a realistic situation because of the diverse and dynamic nature of system call patterns. Gathering live system-call data is harder than generating synthetic system-call data. The generation of normal training data in an off-line setting can create artifacts, because these data are made in fixed conditions for the sake of convenience in data generation. This setting may cause normal patterns to have some bias.
All these situations make it more difficult to choose a good set of hyper-parameters for LSTM architecture. To cope with this challenge, we propose a new ensemble method. Due to the lack of data, different models with different parameters capture slightly different normal patterns. If function f ∈ S∗ 7→ R, which maps a system call sequence to a real value, is given, we can define a thresholding classifier as follows:
Cf (x; θ) = { normal forf(x) ≤ θ; abnormal otherwise.
(2)
Most of the intrusion detection algorithms, including our proposed method, employ a thresholding classifier. For the sake of explanation, we define a term ‘highly normal’ sequence for the classifier Cf as a system call sequence having an extremely low f value so it will be classified as normal even when the threshold θ is sufficiently low to discriminate true abnormals. Highly normal sequences are represented as a flat horizontal line near (1, 1) in the ROC curve. The more the classifier finds highly normal sequences, the longer this line is. Note that a highly normal sequence is closely related to the false alarm rate.
Our goal is to minimize the false alarm rate through the composition of multiple classifiers Cf1 , Cf2 , . . . , Cfm into a single classifier Cf , resulting in accumulated ‘highly normal’ data (here m is the number of classifiers used in the ensemble). This is due to the fact that a low false alarm rate is an important requisite in computer security, especially in intrusion detection systems. Our ensemble method can be represented by a simple formula:
f(x) = m∑ i=1 wiσ(fi(x)− bi). (3)
As activation function σ, we used a leaky ReLU function, namely σ(x) = max(x, 0.001x). Intuitively, the activation function forces potential ‘highly normal’ sequences having f values lower than bi to keep their low f values to the final f value. If we use the regular ReLU function instead, the degree of ‘highly normal’ sequences could not be differentiated. We set the bias term bi to the median of f values of the normal training data. In (3), wi indicates the importance of each classifier fi. Because we do not know the performance of each classifier before evaluation, we set wi to 1/m. Mathematically, this appears to be a degenerated version of a one-layer neural network. The basic philosophy of the ensemble method is that when the classification results from various classifiers are slightly different, we can make a better decision by composing them well. Still, including bad classifiers could degrade the overall performance. By choosing classifiers carefully, we can achieve satisfactory results in practice, as will be shown in Section 3.2.

2.3 BASELINE CLASSIFIERS
Deep neural networks are an excellent representation learning method. We exploit the sequence representation learned from the final state vector of the LSTM layer after feeding all the sequences of calls. For comparison with our main classifier, we use two baseline classifiers that are commonly used for anomaly detection exploiting vectors corresponding to each sequence: k-nearest neighbor (kNN) and k-means clustering (kMC). Examples of previous work for mapping sequences into vectors of fixed-dimensional hand-crafted features include normalized frequency and tf-idf (Liao & Vemuri, 2002; Xie et al., 2014).
Let T be a normal training set, and let lstm(x) denotes a learned representation of call sequence x from the LSTM layer. kNN classifiers search for k nearest neighbors in T of query sequence x
on the embedded space and measure the minimum radius to cover them all. The minimum radius g(x; k) is used to classify query sequence x. Alternatively, we can count the number of vectors within the fixed radius, g(x; r). In this paper, we used the former. Because the computational cost of a kNN classifier is proportional to the size of T , using a kNN classifier would be intolerable when the normal training dataset becomes larger.
g(x; k) = min r s.t. ∑ y∈T [ d(lstm(x), lstm(y)) ≤ r ] ≥ k (4)
g(x; r) = 1− 1 |T | ∑ y∈T [ d(lstm(x), lstm(y)) ≤ r ] (5)
The kMC algorithm partitions T on the new vector space into k clusters G1, G2, . . . , Gk in which each vector belongs to the cluster with the nearest mean so as to minimize the within-cluster sum of squares. They are computed by Lloyd’s algorithm and converge quickly to a local optimum. The minimum distance from each center of clusters µi, h(x; k), is used to classify the new query sequence.
h(x; k) = min i=1,··· ,k d(lstm(x), µi) (6)
The two classifiers Cg and Ch are closely related in that the kMC classifier is equivalent to the 1-nearest neighbor classifier on the set of centers. In both cases of kNN and kMC, we need to choose parameter k empirically, depending on the distribution of vectors. In addition, we need to choose a distance metric on the embedding space; we used the Euclidean distance measure in our experiments.

3 EXPERIMENTAL RESULTS AND DISCUSSION

3.1 DATASETS
Though system call traces themselves might be easy to acquire, collecting or generating a sufficient amount of meaningful traces for the evaluation of intrusion detection systems is a nontrivial task. In order to aid researchers in this regard, the following datasets were made publicly available from prior work: ADFA-LD (Creech & Hu, 2013), KDD98 (Lippmann et al., 2000) and UNM (of New Mexico, 2012). The KDD98 and UNM datasets were released in 1998 and 2004, respectively. Although these two received continued criticism about their applicability to modern systems (Brown et al., 2009; McHugh, 2000; Tan & Maxion, 2003), we include them as the results would show how our model fares against early works in the field, which were mostly evaluated on these datasets. As the ADFALD dataset was generated around 2012 to reflect contemporary systems and attacks, we have done our evaluation mainly on this dataset.
The ADFA-LD dataset was captured on an x86 machine running Ubuntu 11.04 and consists of three groups: normal training traces, normal validation traces, and attack traces. The KDD98 dataset was audited on a Solaris 2.5.1 server. We processed the audit data into system call traces per session. Each session trace was marked as normal or attack depending on the information provided in the accompanied bsm.list file, which is available alongside the dataset. Among the UNM process set, we tested our model with lpr that was collected from SunOS 4.1.4 machines. We merged the live lpr set and the synthetic lpr set. This combined dataset is further categorized into two groups: normal traces and attack traces. To maintain consistency with ADFA-LD, we divided the normal data of KDD98 and UNM into training and validation data in a ratio of 1:5, which is the ratio of the ADFA-LD dataset. The numbers of system-call sequences in each dataset we used are summarized in Table 1.

3.2 PERFORMANCE EVALUATION
We used ADFA-LD and built three independent system-call language models by changing the hyperparameters of the LSTM layer: (1) one layer with 200 cells, (2) one layer with 400 cells, and (3) two layers with 400 cells. We matched the number of cells and the dimension of the embedding
vector. Our parameters were uniformly initialized in [−0.1, 0.1]. For computational efficiency, we adjusted all system-call sequences in a mini-batch to be of similar lengths. We used the Adam optimizer (Kingma & Ba, 2014) for stochastic gradient descent with a learning rate of 0.0001. The normalized gradient was rescaled whenever its norm exceeded 5 (Pascanu et al., 2013), and we used dropout (Srivastava et al., 2014) with probability 0.5. We show the ROC curves obtained from the experiment in Figure 3.
For the two baseline classifiers, we used the Euclidean distance measure. Changing the distance measure to another metric did not perform well on average. In case of kNN, using k = 11 achieved the best performance empirically. For kMC, using k = 1 gave the best performance. Increasing the value of k produced similar but poorer results. We speculate the reason why a single cluster suffices as follows: learned representation vectors of normal training sequence are symmetrically distributed. The kNN classifier Cg and the kMC classifier Ch achieved similar performance. Compared to Liao & Vemuri (2002); Xie et al. (2014), our baseline classifiers easily returned ‘highly normal’ calls. This result was leveraged by the better representation obtained from the proposed system-call language modeling.
As shown in the left plot of Figure 3, three LSTM classifiers performed better than Cg and Ch. We assume that the three LSTM classifiers we trained are strong enough by themselves, and their classification results would be different from each other. By applying ensemble methods, we would expect to improve the performance. The first one was averaging, the second one was voting, and lastly we used our ensemble method as we explained in Section 2.2. The proposed ensemble method gave a better AUC value (0.928) with a large margin than that of the averaging ensemble method (0.890) and the voting ensemble method (0.859). Moreover, the curve obtained from the proposed ensemble method was placed above individual single curves, while other ensemble methods did not show this property.
In the setting of anomaly detection where attack data are unavailable, learning ensemble parameters is infeasible. If we exploit partial attack data, the assumption breaks down and the zero-day attack issue remains. Our ensemble method is appealing in that it performs remarkably well without learning.
To be clear, we applied ensemble methods to three LSTM classifiers learned independently using different hyper-parameters, not with the baseline classifiers, Cg or Ch. Applying ensemble methods to each type of baseline classifier gave unsatisfactory results since changing parameters or initialization did not result in complementary and reasonable classifiers that were essential for ensemble methods. Alternatively, we could do ensemble our LSTM classifiers and baseline classifiers together. However, this would also be a wrong idea because their f values differ in scale. The value of f in our LSTM classifier is an average negative log-likelihood, whereas g and h indicate distances in a continuous space.
According to Creech & Hu (2014), the extreme learning machine (ELM) model, sequence timedelay embedding (STIDE), and the hidden Markov model (HMM) (Forrest et al., 1996; Warrender et al., 1999) achieved about 13%, 23%, and 42% false alarm rates (FAR) for 90% detection rate (DR), respectively. We achieved 16% FAR for 90% DR, which is comparable to the result of ELM and outperforms those of STIDE and HMM. The ROC curves for ELM, HMM, and STIDE can be found, but we could not draw those curves on the same plot with ours because the authors provided no specific details of their results. Creech & Hu (2014) classified ELM as a semantic approach and other two as syntactic approaches which treat each call as a basic unit. To be fair, our proposed method should be compared with those approaches that use system calls only as a basic unit in that we watch the sequence call-by-call. Furthermore, our method is end-to-end while ELM relies on hand-crafted features.
In Creech & Hu (2014), the authors reported that there was significant overhead for training the models mentioned above, and the overhead would inevitably increase for handling larger data. Longer phrases tend to be more informative, but handling them typically requires larger dictionaries. For this reason, Creech & Hu (2014) had to put an empirical upper bound to limit the lengths of phrases, which then might lower the performance of the models to handle various attacks. By contrast, our approach can learn in continuous space semantically meaningful representations of calls, phrases, and sequences of arbitrary lengths. Moreover, our method can relieve the burden of preprocessing (potentially massive) logging data. We expect that incorporating prior knowledge into our model can further boost its performance.

3.3 PORTABILITY EVALUATION
We carried out experiments similar to those presented in Section 3.2 using the KDD98 dataset and the UNM dataset. First, we trained our system-call language model with LSTM having one layer of 200 cells and built our classifier using the normal training traces of the KDD98 dataset. The same model was used to evaluate the UNM dataset to examine the portability of the LSTM models trained with data from a different but similar system. The results of our experiments are represented in Figure 4. For comparison, we display the ROC curve of the UNM dataset by using the model from training the normal traces therein. To examine portability, the system calls in test datasets need to be included or matched to those of training datasets. UNM was generated using an earlier version of OS than that of KDD98, but ADFA-LD was audited on a fairly different OS. This made our experiments with other combinations difficult.
Through a quantitative analysis, for the KDD98 dataset, we earned an almost perfect ROC curve with an AUC value of 0.994 and achieved 2.3% FAR for 100% DR. With the same model, we tested the UNM datset and obtained a ROC curve with an AUC value of 0.969 and 5.5% FAR for 99.8% DR. This result was close to the result earned by using the model trained on normal training traces of the UNM dataset itself, as shown in the right plot of Figure 4.
This result is intriguing because it indicates that system-call language models have a strong portability. In other words, after training one robust and extensive model, the model can then be deployed to other similar host systems. By doing so, we can mitigate the burden of training cost. This paradigm is closely related to the concept of transfer learning, or zero-shot learning. It is well known that neural networks can learn abstract features and that they can be used successfully for unseen data.

3.4 VISUALIZATION OF LEARNED REPRESENTATIONS
It is well-known that neural network based-language models can learn semantically meaningful embeddings to continuous space (Bengio et al., 2003; Mikolov et al., 2013; Cho et al., 2014). We
expected to see a similar characteristic with the proposed system-call language model. The 2D projection of the calls using the embedding matrix W learned from the system-call language model was done by t-SNE (Van der Maaten & Hinton, 2008) and shown in Figure 5. Just as the natural language model, we can expect that calls having similar co-occurrence patterns are positioned in similar locations in the embedded space after training the system call language model. We can clearly see that calls having alike functionality are clustered with each other.
The first obvious cluster would be the read-write call pair and the open-close pair. The calls of each pair were located in close proximity in the space, meaning that our model learned to associate them together. At the same time, the difference between the calls of each pair appears to be almost the same in the space, which in turn would mean our model learned that the relationship of each pair somewhat resembles.
Another notable cluster would be the group of select, pselect6, ppoll, epoll wait and nanosleep. The calls select, pselect6 and ppoll all have nearly identical functions in that they wait for some file descriptors to become ready for some class of I/O operation or for signals. The other two calls also have similar characteristics in that they wait for a certain event or signal as well. This could be interpreted as our model learning that these ‘waiting’ calls share similar characteristics.
Other interesting groups would be: readlink and lstat64 which are calls related to symbolic links; fstatat64 and fstat64 which are calls related to stat calls using file descriptors; pipe and pipe2 which are nearly identical and appear almost as one on the embedding layer. These cases show that our model is capable of learning similar characteristics among the great many system calls.
Similarly to the call representations, we expected that attack sequences with the same type would cluster to each other, and we tried to visualize them. However, for various reasons including the lack of data, we were not able to observe this phenomenon. Taking the fact that detecting abnormal patterns from normal patterns well would be sufficiently hard into consideration, learning representation to separate different abnormal patterns with only seen normal patterns would also be an extremely difficult task.

4 CONCLUSION
Our main contributions for designing intrusion detection systems as described in this paper have two parts: the introduction of a system-call language modeling approach and a new ensemble method. To the best of the authors’ knowledge, our method is the first to introduce the concept of a language model, especially using LSTM, to anomaly-based IDS. The system-call language model can capture the semantic meaning of each call and its relation to other system calls. Moreover, we proposed an innovative and simple ensemble method that can better fit to IDS design by focusing on lowering false alarm rates. We showed its outstanding performance by comparing it with existing state-of-theart methods and demonstrated its robustness and generality by experiments on diverse benchmarks.
As discussed earlier, the proposed method also has excellent portability. In contrast to alternative methods, our proposed method incurs significant smaller training overhead because it does not need to build databases or dictionaries to keep a potentially exponential amount of patterns. Our method is compact and light in that the size of the space required to save parameters is small. The overall training and inference processes are also efficient and fast, as our methods can be implemented using efficient sequential matrix multiplications.
As part of our future work, we are planning to tackle the task of detecting elaborate contemporary attacks including mimicry attacks (Wagner & Soto, 2002; Shu et al., 2015) by more advanced methods. Our proposed method allows us to estimate the likelihood of arbitrary sections in a given system-call sequence, which may be helpful for analyzing the capability of handling mimicry attacks. For instance, it is possible to determine if there exists a sufficiently long section (rather than the whole sequence) with the average log-likelihood below the threshold. In addition, we are considering designing a new framework to build a robust model in on-line settings by collecting large-scale data generated from distributed environments. For optimization of the present work, we would be able to alter the structure of RNNs used in our system-call language model and ensemble algorithm. Finally, we anticipate that a hybrid method that combines signature-based approaches and feature engineering will allow us to create more accurate intrusion detection systems.

ACKNOWLEDGMENTS
This work was supported by BK21 Plus Project in 2016 (Electrical and Computer Engineering, Seoul National University).
","In computer security, designing a robust intrusion detection system is one of the most fundamental and important problems. In this paper, we propose a system-call language-modeling approach for designing anomaly-based host intrusion detection systems. To remedy the issue of high false-alarm rates commonly arising in conventional methods, we employ a novel ensemble method that blends multiple thresholding classifiers into a single one, making it possible to accumulate ‘highly normal’ sequences. The proposed system-call language model has various advantages leveraged by the fact that it can learn the semantic meaning and interactions of each system call that existing methods cannot effectively consider. Through diverse experiments on public benchmark datasets, we demonstrate the validity and effectiveness of the proposed method. Moreover, we show that our model possesses high portability, which is one of the key aspects of realizing successful intrusion detection systems.",ICLR 2017 conference submission,False,,"In this paper a novel approach for anomaly detection is considered for the task of intrusion detection based on system call sequence.
The system call sequence is regarded as a language, and multiple lstm-rnn language models are trained and ensembled. 
Diversity in the ensemble is achieved by choosing different hyper parameters for each lstm-LM.
The combination of the LMs is done by averaging transformations of the likelihoods. 

I really like the fact that no attack data is used during training, and I like the LM and ensemble approach. 
The only high level drawbacks I have are the following, which might have a simple answer as I'm not an expert in this field:

- Relaying of system calls seems weak: If the attacker has access to some ""normal"" sequences of system calls, all she can fool the system by interleaving its malicious system calls with normal ones, in a way that will artificially raise the likelihood of the sequence. 
- A few lines covering other anomaly detection tasks, where RNNs are used, can be added to the introduction, to give a better idea about the novelty of the approach.

---

This is a pure application paper: an application of LSTMs to host intrusion detection systems based upon observed system calls. And from the application standpoint, I don't believe this is a bad paper, the authors seem to achieve reasonable results from the method (though admittedly, I can't really judge the quality of these results without a lot more familiarity based upon current work in intrusion detection systems). However, in terms of the ICLR audience, I simply don't believe there is enough here to warrant substantial interest. As an example, the authors highlighted the distinction between network intrusion detection systems (NIDS) and host intrusion detection systems (HIDS), and felt that one review was completely unsuitable because they didn't realize this distinction when reading the paper. This is of course a crucial distinction from the security application side, but from the algorithmic/ML side, it's simply not that important, and the fact that there _has_ been previous work exactly on LSTMs for NIDS makes this paper unlikely to have a huge impact in the ICLR community. It would be much better suited to a security conference, where the application could be judged on its own merits, and the community would likely understand much better how significant these results were.
 
 Pros:
 + Nice application of LSTMs to HIDS task
 
 Cons:
 - Nothing really novel from the algorithmic/ML side
 - The significance of the results are difficult to assess without more formal understanding of the problem domains
 
 The PCs have thus decided that this paper isn't ready to appear at the conference.

---

In order to reflect the comments from reviewers, the revised manuscript now includes additional references to related work (Introduction) and more in-depth comparison with existing methods (Section 3.2) as well as more details of our future work (Conclusion).

---

This paper presents an anomaly-based host intrusion detection method. LSTM RNN is used to model the system-call sequences and the averaged sequence likelihood is then used to determine anomaly, which is the attack. This paper also compares an ensemble method with two baselines as classification model.
+This is is well written and more of ideas are clearly presented.
+It demonstrates an interesting application of LSTM sequential modeling to HIDS problem
-The overall novelty is limited considering the major technical components like LSTM RNN and ensemble method are already established. 
-The contribution of the proposed ensemble method needs further evaluation because it is also possible to use ensemble ideas in kNN and kMC baselines.

---

The authors propose using an LSTM on a sequence of system calls to perform network intrusion detection (NIDS). The idea of using neural networks (in general) for NIDS is old [1]. The idea of using some sort of NN on top of a sequence of system calls for NIDS is published [2]. The idea of using LSTMs for NIDS is published [2]. The paper in [2] operates on counts of N-grams of system calls, rather than on the raw sequence, but that pre-processing does not seem ""heavy"" to me.

Overall, the proposed system works as well as other proposed NIDS system, and the paper checks portability (which is good).

But, on the con side, I don't see this paper as adding a lot to the state-of-the-art in NIDS. Nor does is the paper well-matched to ICLR. I didn't learn a lot about representations from this paper: many people have thrown LSTM at sequence problems.

Therefore, I think it's below threshold for ICLR. The authors may wish to submit to a security conference.

References:
1. Debar, Herve, Monique Becker, and Didier Siboni. ""A neural network component for an intrusion detection system."" Research in Security and Privacy, 1992. Proceedings., 1992 IEEE Computer Society Symposium on. IEEE, 1992.
2. Creech, Gideon, and Jiankun Hu. ""A semantic approach to host-based intrusion detection systems using contiguousand discontiguous system call patterns."" IEEE Transactions on Computers 63.4 (2014): 807-819.
3. Staudemeyer, Ralf C. ""Applying long short-term memory recurrent neural networks to intrusion detection."" South African Computer Journal 56.1 (2015).

---

In this paper a novel approach for anomaly detection is considered for the task of intrusion detection based on system call sequence.
The system call sequence is regarded as a language, and multiple lstm-rnn language models are trained and ensembled. 
Diversity in the ensemble is achieved by choosing different hyper parameters for each lstm-LM.
The combination of the LMs is done by averaging transformations of the likelihoods. 

I really like the fact that no attack data is used during training, and I like the LM and ensemble approach. 
The only high level drawbacks I have are the following, which might have a simple answer as I'm not an expert in this field:

- Relaying of system calls seems weak: If the attacker has access to some ""normal"" sequences of system calls, all she can fool the system by interleaving its malicious system calls with normal ones, in a way that will artificially raise the likelihood of the sequence. 
- A few lines covering other anomaly detection tasks, where RNNs are used, can be added to the introduction, to give a better idea about the novelty of the approach.

---

In this paper a novel approach for anomaly detection is considered for the task of intrusion detection based on system call sequence.
The system call sequence is regarded as a language, and multiple lstm-rnn language models are trained and ensembled. 
Diversity in the ensemble is achieved by choosing different hyper parameters for each lstm-LM.
The combination of the LMs is done by averaging transformations of the likelihoods. 

I really like the fact that no attack data is used during training, and I like the LM and ensemble approach. 
The only high level drawbacks I have are the following, which might have a simple answer as I'm not an expert in this field:

- Relaying of system calls seems weak: If the attacker has access to some ""normal"" sequences of system calls, all she can fool the system by interleaving its malicious system calls with normal ones, in a way that will artificially raise the likelihood of the sequence. 
- A few lines covering other anomaly detection tasks, where RNNs are used, can be added to the introduction, to give a better idea about the novelty of the approach.

---

This is a pure application paper: an application of LSTMs to host intrusion detection systems based upon observed system calls. And from the application standpoint, I don't believe this is a bad paper, the authors seem to achieve reasonable results from the method (though admittedly, I can't really judge the quality of these results without a lot more familiarity based upon current work in intrusion detection systems). However, in terms of the ICLR audience, I simply don't believe there is enough here to warrant substantial interest. As an example, the authors highlighted the distinction between network intrusion detection systems (NIDS) and host intrusion detection systems (HIDS), and felt that one review was completely unsuitable because they didn't realize this distinction when reading the paper. This is of course a crucial distinction from the security application side, but from the algorithmic/ML side, it's simply not that important, and the fact that there _has_ been previous work exactly on LSTMs for NIDS makes this paper unlikely to have a huge impact in the ICLR community. It would be much better suited to a security conference, where the application could be judged on its own merits, and the community would likely understand much better how significant these results were.
 
 Pros:
 + Nice application of LSTMs to HIDS task
 
 Cons:
 - Nothing really novel from the algorithmic/ML side
 - The significance of the results are difficult to assess without more formal understanding of the problem domains
 
 The PCs have thus decided that this paper isn't ready to appear at the conference.

---

In order to reflect the comments from reviewers, the revised manuscript now includes additional references to related work (Introduction) and more in-depth comparison with existing methods (Section 3.2) as well as more details of our future work (Conclusion).

---

This paper presents an anomaly-based host intrusion detection method. LSTM RNN is used to model the system-call sequences and the averaged sequence likelihood is then used to determine anomaly, which is the attack. This paper also compares an ensemble method with two baselines as classification model.
+This is is well written and more of ideas are clearly presented.
+It demonstrates an interesting application of LSTM sequential modeling to HIDS problem
-The overall novelty is limited considering the major technical components like LSTM RNN and ensemble method are already established. 
-The contribution of the proposed ensemble method needs further evaluation because it is also possible to use ensemble ideas in kNN and kMC baselines.

---

The authors propose using an LSTM on a sequence of system calls to perform network intrusion detection (NIDS). The idea of using neural networks (in general) for NIDS is old [1]. The idea of using some sort of NN on top of a sequence of system calls for NIDS is published [2]. The idea of using LSTMs for NIDS is published [2]. The paper in [2] operates on counts of N-grams of system calls, rather than on the raw sequence, but that pre-processing does not seem ""heavy"" to me.

Overall, the proposed system works as well as other proposed NIDS system, and the paper checks portability (which is good).

But, on the con side, I don't see this paper as adding a lot to the state-of-the-art in NIDS. Nor does is the paper well-matched to ICLR. I didn't learn a lot about representations from this paper: many people have thrown LSTM at sequence problems.

Therefore, I think it's below threshold for ICLR. The authors may wish to submit to a security conference.

References:
1. Debar, Herve, Monique Becker, and Didier Siboni. ""A neural network component for an intrusion detection system."" Research in Security and Privacy, 1992. Proceedings., 1992 IEEE Computer Society Symposium on. IEEE, 1992.
2. Creech, Gideon, and Jiankun Hu. ""A semantic approach to host-based intrusion detection systems using contiguousand discontiguous system call patterns."" IEEE Transactions on Computers 63.4 (2014): 807-819.
3. Staudemeyer, Ralf C. ""Applying long short-term memory recurrent neural networks to intrusion detection."" South African Computer Journal 56.1 (2015).

---

In this paper a novel approach for anomaly detection is considered for the task of intrusion detection based on system call sequence.
The system call sequence is regarded as a language, and multiple lstm-rnn language models are trained and ensembled. 
Diversity in the ensemble is achieved by choosing different hyper parameters for each lstm-LM.
The combination of the LMs is done by averaging transformations of the likelihoods. 

I really like the fact that no attack data is used during training, and I like the LM and ensemble approach. 
The only high level drawbacks I have are the following, which might have a simple answer as I'm not an expert in this field:

- Relaying of system calls seems weak: If the attacker has access to some ""normal"" sequences of system calls, all she can fool the system by interleaving its malicious system calls with normal ones, in a way that will artificially raise the likelihood of the sequence. 
- A few lines covering other anomaly detection tasks, where RNNs are used, can be added to the introduction, to give a better idea about the novelty of the approach.",,,,,,6.0,,,3.3333333333333335,,
734,"DEEP VARIATIONAL CANONICAL CORRELATION ANALYSIS
Authors: Weiran Wang, Xinchen Yan, Honglak Lee, Karen Livescu
Source file: 734.pdf

ABSTRACT
We present deep variational canonical correlation analysis (VCCA), a deep multiview learning model that extends the latent variable model interpretation of linear CCA (Bach and Jordan, 2005) to nonlinear observation models parameterized by deep neural networks (DNNs). Computing the marginal data likelihood, as well as inference of the latent variables, are intractable under this model. We derive a variational lower bound of the data likelihood by parameterizing the posterior density of the latent variables with another DNN, and approximate the lower bound via Monte Carlo sampling. Interestingly, the resulting model resembles that of multiview autoencoders (Ngiam et al., 2011), with the key distinction of an additional sampling procedure at the bottleneck layer. We also propose a variant of VCCA called VCCA-private which can, in addition to the “common variables” underlying both views, extract the “private variables” within each view. We demonstrate that VCCA-private is able to disentangle the shared and private information for multi-view data without hard supervision.

1 INTRODUCTION
In the multi-view representation learning setting, we have multiple views/measurements of the same underlying signal, and the goal is to learn useful features of each view using complementary information contained in the views. The intuition underlying this setting is that the learned features can help uncover the common sources of variation in the views, which can be helpful for exploratory analysis or for downstream tasks.
A classical approach in this setting is canonical correlation analysis (CCA, Hotelling, 1936) and its nonlinear extensions, including the kernel extension (Lai and Fyfe, 2000; Akaho, 2001; Melzer et al., 2001; Bach and Jordan, 2002) and the deep neural network (DNN) extension (Andrew et al., 2013; Wang et al., 2015b). CCA projects two random vectors x ∈ Rdx and y ∈ Rdy into a lowerdimensional subspace so that the projections are maximally correlated. There is a probabilistic latent variable model interpretation of linear CCA (Bach and Jordan, 2005) as shown in Figure 1 (left). Assume that x and y are linear functions of some lower-dimensional random variable z ∈ Rdz , where dz ≤ min(dx, dy). When the prior distribution of the latent variable p(z) and the conditional distributions p(x|z) and p(y|z) are Gaussian, Bach and Jordan (2005) showed that E[z|x] (resp. E[z|y]) lives in the same space as the linear CCA projection for x (resp. y). This generative interpretation of CCA is often lost in nonlinear extensions of CCA. For example, in deep CCA (DCCA, (Andrew et al., 2013)), to extend CCA to nonlinear mappings with greater representation power, one extracts nonlinear features from the original inputs of each view using two DNNs, f for x and g for y, so that the canonical correlation of the DNN outputs (measured by a linear CCA with projection matrices U and V) is maximized. Formally, given a dataset of N pairs of observations (x1,y1), . . . , (xN ,yN ) of the random vectors (x,y), DCCA optimizes
max Wf ,Wg
U,V
tr ( U>f(X)g(Y)>V ) s.t. U> ( f(X)f(X)> ) U = V> ( g(Y)g(Y)> ) V = NI, (1)
where f(X) = [f(x1), . . . , f(xN )] and g(Y) = [g(y1), . . . ,g(yN )], and Wf denotes all weight parameters of the DNN f (and similarly for g).
DCCA has achieved good performance in the multi-view representation learning setting across different domains (Wang et al., 2015b,a; Lu et al., 2015; Yan and Mikolajczyk, 2015). However, a disadvantage of DCCA is that it directly looks for DNNs that can map inputs into the low-dimensional space, without a model for generating samples from the latent space. Although Wang et al. (2015b)’s deep canonically correlated autoencoders (DCCAE) model optimizes the combination of the autoencoder objective (reconstruction errors) and the canonical correlation objective, the authors found that in practice, the canonical correlation term tends to dominate the reconstruction error terms in the DCCAE objective when tuning performance for a downstream task (especially when the inputs are noisy), and as a result the inputs are not reconstructed well. At the same time, optimization of the DCCA and DCCAE objectives is challenging due to the constraints that couple all training samples.
The main contribution of this paper is the proposal of a new deep multi-view learning model named deep variational CCA (VCCA), which extends the latent variable model interpretation of linear CCA to nonlinear observation models parameterized by DNNs. Computing the marginal data likelihood, as well as inference of the latent variables, are intractable under this model. Inspired by variational autoencoders (VAE, Kingma and Welling, 2014), we parameterize the posterior distribution of the latent variables with another DNN, and derive a variational lower bound of the data likelihood as the objective of VCCA, which is further approximated by Monte Carlo sampling. With the reparameterization trick, sampling for the Monte Carlo approximation is trivial and all DNN weights in VCCA can be optimized jointly via stochastic gradient descent, using unbiased gradient estimates from small minibatches. Interestingly, VCCA is related to multi-view autoencoders (Ngiam et al., 2011), with the key distinctions of additional regularization on the posterior distribution and the sampling procedure at the bottleneck layer.
We also propose a variant of VCCA called VCCA-private that can, in addition to the “common variables” underlying both views, extract the “private variables” within each view. We demonstrate that VCCA-private is able to disentangle the shared and private information for multi-view data without hard supervision. Last but not least, as generative models, VCCA and VCCA-private enable us to obtain high-quality samples for the input of each view.

2 VARIATIONAL CCA
The probabilistic latent variable model of CCA (Bach and Jordan, 2005) defines the following joint distribution over the random variables (x,y):
p(x,y, z) = p(z)p(x|z)p(y|z), p(x,y) = ∫ p(x,y, z)dz. (2)
The assumption underlying this model is that, conditioned on the latent variables z ∈ Rdz , the two views x and y are independent. However, linear observation models (p(x|z) and p(y|z) as shown in Figure 1 (left)) have limited representation power. In this paper, we consider nonlinear
observation models pθ(x|z;θx) and pθ(y|z;θy), parameterized by θx and θy respectively, which can be the collections of weights of DNNs. In this case, the marginal likelihood pθ(x,y) does not have a closed form. In addition, the inference problem pθ(z|x)—the problem of inferring the latent variables given one of the views—is also intractable.
Inspired by Kingma and Welling (2014)’s work on variational autoencoders (VAE), we approximate pθ(z|x) with the conditional density qφ(z|x;φz), where φz is the collection of parameters of another DNN.1 We can derive a lower bound on the marginal data likelihood using qφ(z|x):
log pθ(x,y) = log pθ(x,y) ∫ qφ(z|x)dz = ∫ log pθ(x,y)qφ(z|x)dz
= ∫ qφ(z|x) ( log
qφ(z|x) pθ(z|x,y) + log pθ(x,y, z) qφ(z|x)
) dz
= DKL(qφ(z|x)||pθ(z|x,y)) + Eqφ(z|x) [ log pθ(x,y, z)
qφ(z|x) ] ≥ Eqφ(z|x) [ log pθ(x,y, z)
qφ(z|x)
] =: L(x,y;θ,φ) (3)
where we used the fact that KL divergence is nonnegative in the last step. As a result, L(x,y;θ,φ) is a lower bound on the data log-likelihood logθ p(x,y). Substituting (2) into (3), we have
L(x,y;θ,φ) = ∫ qφ(z|x) ( log p(z)
qφ(z|x) + log pθ(x|z) + log pθ(y|z)
) dz
= −DKL(qφ(z|x)||p(z)) + Eqφ(z|x) [log pθ(x|z) + log pθ(y|z)] . (4) VCCA maximizes this variational lower bound on the data likelihood on the training set:
max θ,φ
1
N N∑ i=1 L(xi,yi;θ,φ). (5)
The first term in (4) measures the KL divergence between the approximate posterior distribution and the prior distribution of the latent variables z. When the parameterization qφ(z|x) is chosen properly, this term can be computed exactly in closed form. As a concrete example, let the variational approximate posterior be a multivariate Gaussian with diagonal covariance. That is, for a sample pair (xi,yi), we have
log qφ(zi|xi) = logN (zi;µi,Σi), Σi = diag ( σ2i1, . . . , σ 2 idz ) , (6)
where the mean µi and covariance Σi are outputs of an encoding DNN f (and thus [µi,Σi] = f(xi;φz) are deterministic nonlinear functions of xi). In this case, we have
DKL(qφ(zi|xi)||p(zi)) = − 1
2 dz∑ j=1 ( 1 + log σ2ij − σ2ij − µ2ij ) .
The second term of (4) corresponds to the expected complete data likelihood under the approximate posterior distribution. Though still intractable, this term can be approximated by Monte Carlo sampling. In particular, we draw L samples z(l)i ∼ qφ(zi|xi):
z (l) i = µi + Σi (l), where (l) ∼ N (0, I), for l = 1, . . . , L, (7) and have
Eqφ(zi|xi) [log pθ(xi|zi) + log pθ(yi|zi)] ≈ 1
L L∑ l=1 log pθ ( xi|z(l)i ) + log pθ ( yi|z(l)i ) . (8)
Notice that we parameterized qφ(zi|xi) above to obtain the VCCA objective; this is useful when the first view is available for downstream tasks, in which case we can directly apply qφ(zi|xi) to obtain its projection (as features). One could also derive likelihood lower bounds by parameterizing the approximate posteriors qφ(zi|yi) and qφ(zi|xi,yi), and optimize their convex combinations for training. We give a sketch of VCCA in Figure 1 (right).
1For notational simplicity, we denote by θ the collection of parameters associated with the model probabilities pθ(·), and φ the collection of parameters associated with the variational approximate probabilities qφ(·), and often omit specific parameters inside the probabilities.
Connection to multi-view autoencoder (MVAE) If we use the Gaussian observation models
log pθ(x|z) = logN (gx(z;θx), I), log pθ(y|z) = logN (gy(z;θy), I), we observe that log pθ ( xi|z(l)i ) and log pθ ( yi|z(l)i ) measure the reconstruction errors of each
view’s inputs from samples z(l)i using the two DNNs gx and gy respectively. In this case, maximizing L(x,y;θ,φ) is equivalent to
min θ,φ
1
N N∑ i=1 DKL(qφ(zi|xi)||p(zi)) + 1 2NL N∑ i=1 L∑ l=1 ∥∥∥xi − gx (z(l)i ;θx)∥∥∥2 + ∥∥∥yi − gy (z(l)i ;θy)∥∥∥2 (9)
s.t. z(l)i = µi + Σi (l), where (l) ∼ N (0, I), l = 1, . . . , L.
Now, consider the case of Σi → 0, for i = 1, . . . , N , and we have z(l)i → µi which is a deterministic function of x (and there is no need for sampling). In the limit, the second term of (9) reduces to
1
2N N∑ i=1 ‖xi − gx(f(xi;φz);θx)‖ 2 + ‖yi − gy(f(xi;φz);θy)‖ 2 , (10)
which is the objective of the multi-view autoencoder (MVAE, Ngiam et al., 2011). Note, however, that Σi → 0 is prevented by the VCCA objective as it results in a large penalty in DKL(qφ(zi|xi)||p(zi)). Compared with the MVAE objective, in the VCCA objective we are creating L different “noisy” versions of the latent representation and enforce that these versions reconstruct the original inputs well. The “noise” distribution (the variances Σi) are also learned and regularized by the KL divergence DKL(qφ(zi|xi)||p(zi)). Using the VCCA objective, we expect to learn different representations from those of MVAE, due to these regularization effects.

2.1 EXTRACTING PRIVATE VARIABLES
So far, VCCA aims at extracting only the latent variables z that are common to both views. A potential disadvantage of this model is that it assumes the common variables are sufficient by themselves to generate the views, which can be too restrictive in practice. Consider the example of audio and articulatory measurements as two views for speech. Although the transcription is a common variable behind the views, it combines with the physical environment and the vocal tract anatomy to generate the individual views. In other words, there might be large variations in the input space that can not be explained by the common variables, making the objective (4) hard to optimize. It may then be beneficial to explicitly model the private variables within each view.
We therefore propose a new probabilistic graphical model, shown in Figure 2, that we refer to as VCCA-private. We introduce two sets of hidden variables hx ∈ Rdhx and hy ∈ Rdhy to explain the aspects of x and y not captured by the common variables z. Under this model, the data likelihood
is defined by
pθ(x,y, z,hx,hy) = p(z)p(hx)p(hy)pθ(x|z,hx;θx)pθ(y|z,hy;θy), (11)
pθ(x,y) = ∫ ∫ ∫ pθ(x,y, z,hx,hy)dz dhx dhy.
To obtain tractable inference, we introduce the following factored variational posterior
qφ(z,hx,hy|x,y) = qφ(z|x;φz)qφ(hx|x;φx)qφ(hy|y;φy), (12)
where each factor is parameterized by a different DNN. Similarly to VCCA, we can derive a variational lower bound on the data likelihood for VCCA-private as
log pθ(x,y) ≥ ∫ ∫ ∫ qφ(z,hx,hy|x,y) log pθ(x,y, z,hx,hy)
qφ(z,hx,hy|x,y) dz dhx dhy
= ∫ ∫ ∫ qφ(z,hx,hy|x,y) [ log p(z)
qφ(z|x) + log
p(hx)
qφ(hx|x) + log
p(hy)
qφ(hy|y) + log pθ(x|z,hx) + log pθ(y|z,hy) ] dz dhx dhy
= −DKL(qφ(z|x)||p(z))−DKL(qφ(hx|x)||p(hx))−DKL(qφ(hy|y)||p(hy))
+ ∫ ∫ qφ(z|x)qφ(hx|x) log pθ(x|z,hx)dz dhx + ∫ ∫ qφ(z|x)qφ(hy|y) log pθ(y|z,hy)dz dhy
=: Lprivate(x,y;θ,φ). (13)
As in VCCA, the last two terms of (14) can be approximated by Monte Carlo sampling. For example, we draw samples of z and hx from their corresponding approximate posteriors, and concatenate their samples as inputs to the DNN parameterizing pθ(x|z,hx). In this paper, we use simple Gaussian prior distributions for the private variables, i.e., hx ∼ N (0, I) and hy ∼ N (0, I). We leave to future work to examine the effect of more sophisticated prior distributions for the latent variables.
VCCA-private maximizes this lower bound on the training set, i.e.,
max θ,φ
1
N N∑ i=1 Lprivate(xi,yi;θ,φ). (14)
Optimization The objectives (5) and (14) decouple over the training samples and can be trained efficiently using stochastic gradient descent. Enabled by the reparameterization trick, unbiased gradient estimates are obtained by Monte Carlo sampling and the standard backpropagation procedure on minibatches of training samples. We apply the ADAM algorithm (Kingma and Ba, 2015) for optimizing our objectives.

3 RELATED WORK
Recently, there has been much interest in unsupervised deep generative models (Kingma and Welling, 2014; Rezende et al., 2014; Goodfellow et al., 2014; Gregor et al., 2015; Makhzani et al., 2016; Burda et al., 2016; Alain et al., 2016). A common motivation behind these models is that, with the expressive power of DNNs, the generative models can capture distributions for complex inputs. Additionally, if we are able to generate realistic samples from the learned distribution, we can infer that we have discovered the underlying structure of the data, which may allow us to reduce the sample complexity for learning for downstream tasks. These previous models have mostly focused on single-view data. Here we focus on the multi-view setting where multiple views of the data are present for feature extraction but only one view is available at test time (in downstream tasks).
Some recent work has explored deep generative models for (semi-)supervised learning. Kingma et al. (2014) built a generative model based on variational autoencoders (VAEs) for semi-supervised classification, where the authors model the input distribution with two set of latent variables: the class label (if it is missing) and another set that models the intra-class variabilities (styles). Sohn
et al. (2015) proposed a conditional generative model for structured output prediction, where the authors explicitly model the uncertainty in the input/output using Gaussian latent variables. While there are two set of observations (input and output labels) in these work, their graphical models are different from that of VCCA.
Our work is also related to the deep multi-view probabilistic models based on restricted Boltzmann machines (Srivastava and Salakhutdinov, 2014; Sohn et al., 2014). We note that these are undirected graphical models for which both inference and learning are difficult, and one typically resorts to carefully designed variational approximation and Gibbs sampling procedures for training such models. In contrast, our models only require sampling from simple, standard distributions (such as Gaussians), and all parameters can be learned end-to-end by standard stochastic gradient methods. Therefore, our models are more scalable than the previous multi-view probabilistic models.
On the other hand, there is a rich literature in modeling multi-view data using the same or similar graphical models behind VCCA/VCCA-private (Wang, 2007; Jia et al., 2010; Salzmann et al., 2010; Virtanen et al., 2011; Memisevic et al., 2012; Klami et al., 2013). Our methods differ from previous work in parameterizing the probability distributions using DNNs. This makes the model more powerful, while still having tractable objectives and efficient end-to-end training using the local reparameterization technique. We note that, unlike earlier work on probabilistic models of linear CCA (Bach and Jordan, 2005), VCCA does not optimize the same criterion, nor produce the same solution, as any linear or nonlinear CCA. However, we retain the terminology in order to clarify the connection with earlier work on probabilistic models for CCA, which we are extending with DNN models for the observations and for the variational posterior distribution approximation.

4 EXPERIMENTAL RESULTS
In this section, we compare different multi-view representation learning algorithms on three tasks involving several domains: image-image, speech-articulation, and image-text. The algorithms we choose to compare below are closely related to the proposed model or have been shown to have strong empirical performance under similar settings.
• Linear CCA: its probabilistic interpretation motivates this work.
• Deep CCA (DCCA) (Andrew et al., 2013): see its objective in (1).
• Deep canonically correlated autoencoders (DCCAE) (Wang et al., 2015b): combination of the DCCA objective and the reconstruction errors of each view.
• Multi-view autoencoder (MVAE) (Ngiam et al., 2011): see its objective in (10).
• Multi-view contrastive loss (Hermann and Blunsom, 2014): based on the intuition that the distance between embeddings of paired examples x+ and y+ should be smaller than the distance between embeddings of x+ and an unmatched negative example y− by a margin:
min f,g Lcontrast :=
1
N N∑ i max ( 0, m+ dis ( f(x+i ), g(y + i ) ) − dis ( f(x+i ), g(y − i ) )) ,
where y−i is a randomly sampled view 2 example, and m is a margin hyperparameter. We use the cosine distance dis (a,b) = 1− 〈
a ‖a‖ , b ‖b‖
〉 .

4.1 NOISY MNIST DATASET
We first demonstrate our algorithms on the noisy MNIST dataset used by Wang et al. (2015b). The dataset is generated using the MNIST dataset (LeCun et al., 1998), which consists of 28 × 28 grayscale digit images, with 60K/10K images for training/testing. We first linearly rescale the pixel values to the range [0, 1]. Then, we randomly rotate the images at angles uniformly sampled from [−π/4, π/4] and the resulting images are used as view 1 inputs. For each view 1 image, we randomly select an image of the same identity (0-9) from the original dataset, add independent random noise uniformly sampled from [0, 1] to each pixel, and truncate the pixel final values to [0, 1] to obtain the corresponding view 2 sample. Selection of input images are given in Figure 3 (left). The original
training set is further split into training/tuning sets of size 50K/10K. The data generation process ensures that the digit identity is the only common variable underlying both views.
To evaluate the amount of class information extracted by different methods, after unsupervised learning of latent representations, we reveal the labels and train a linear SVM on the projected view 1 training data (using the one-versus-all scheme), and use it to classify the projected test set. This experiment simulates the typical usage of multi-view learning methods, which is to extract useful representations for downstream discriminative tasks.
Note that this synthetic dataset perfectly satisfies the multi-view assumption that the two views are independent given the class label, so the latent representation should contain precisely the class information. This is indeed achieved by CCA-based and contrastive loss-based multi-view approaches. In Figure 3 (right), we show 2D t-SNE (van der Maaten and Hinton, 2008) visualizations of the original view 1 inputs and view 1 projections by various deep multi-view methods.
We use DNNs with 3 hidden layers of 1024 rectified linear units (ReLUs, Nair and Hinton, 2010) each to parameterize the distributions: qφ(z|x), pθ(x|z), pθ(y|z) in VCCA, and additionally qφ(hx|x) and qφ(hy|y) in VCCA-private. The capacities of these networks are the same as those of their counterparts in DCCA and DCCAE from Wang et al. (2015b). The reconstruction networks pθ(x|z) or pθ(x|z,hx) model each pixel of x as an independent Bernoulli variable and parameterize its mean (using a sigmoid activation); pθ(y|z) and pθ(y|z,hy) model y with diagonal Gaussians and parameterize the mean (using a sigmoid activation) and standard deviation for each pixel dimension. We tune the dimensionality dz over {10, 20, 30, 40, 50}, and fix dhx = dhy = 30 for VCCA-private. We select the hyperparameter combination that yields the best SVM classification accuracy on the projected tuning set, and report the corresponding accuracy on the projected test set.
The effect of dropout We add dropout (Srivastava et al., 2014) to all intermediate layers and the input layers and find it to be very useful in our models, with most of the gain coming from dropout applied to the samples of z, hx and hy . This is because dropout encourages each latent dimension to reconstruct the inputs well in the absence of other dimensions, and therefore avoids learning coadapted features. Intuitively, in VCCA-private dropout also helps to prevent the degenerate situation where the pathways x → hx → x and y → hy → y achieve good reconstruction while ignoring z (e.g., by setting it to a constant). We use the same dropout rate for all layers and tune it over {0, 0.1, 0.2, 0.3, 0.4}. We show the 2D t-SNE embeddings of the common variables z learned by VCCA and VCCA-private on test set in Figure 4. We observe that in general, VCCA/VCCA-private tend to separate the classes
in the projection well; dropout significantly improves the performance of both VCCA and VCCAprivate, with the latter slightly outperforming the former. While such class separation can also be achieved by DCCA/contrastive loss as well, these methods can not naturally generate samples in the input space. On the other hand, such separation is not achieved by multi-view autoencoders.
The effect of private variables on reconstructions We show sample reconstructions (mean and standard deviation) by VCCA for the view 2 images from the test set in Figure 5 (columns 2 and 3). We observe that for each input, the mean reconstruction of yi by VCCA is a prototypical image of the same digit, regardless of the individual style in yi. This is to be expected, as yi contains an arbitrary image of the same digit as xi, and the variation in background noise in yi does not appear in xi and can not be reflected in qφ(z|x); thus the best way for pθ(y|z) to model yi is to output a prototypical image of that class to achieve on average small reconstruction error. On the other hand, since yi contains little rotation of the digits, this variation is suppressed to a large extent in qφ(z|x) (it is no longer the major variation in z as in the original inputs).
We show sample reconstructions by VCCA-private for the same set of view 2 images in Figure 5 (columns 4 and 5). With the help of private variables hy (as part of the input to pθ(y|z,hy)), the model does a much better job in reconstructing the styles of y. And by disentangling the private variables from the shared variables, qφ(z|x) achieves even better class separation than VCCA does.
We also note that the standard deviation of the reconstruction is low within the digit and high outside the digit, implying that pθ(y|z,hy) is able to separate the background noise from the digit image.
Disentanglement of private/shared variables In Figure 6 (in Appendix) we provide the 2D tSNE embeddings of the shared variables z (top row) and the private variables hx (bottom row) learned by VCCA-private. In the embedding of hx, digits with different identities but the same rotation are mapped close together, and the rotation varies smoothly from left to right, confirming that the private variables contain little class information but mainly style information.
Finally, we give the test error rates of linear SVMs applied to the features learned with different models in Table 1. VCCA-private is comparable in performance to the best previous approach (DCCAE), while having the advantage that it can also generate.

4.2 XRMB SPEECH-ARTICULATION DATASET
We now consider the task of learning acoustic features for speech recognition. We use data from the Wisconsin X-ray microbeam (XRMB) corpus (Westbury, 1994), which contains simultaneously recorded speech and articulatory measurements from 47 American English speakers. We follow the setup of Wang et al. (2015a,b) and use the learned features for speaker-independent phonetic recognition.2 The two input views are standard 39D acoustic features (13 mel frequency cepstral coefficients (MFCCs) and their first and second derivatives) and 16D articulatory features (horizontal/vertical displacement of 8 pellets attached to several parts of the vocal tract), each then concatenated over a 7-frame window around each frame to incorporate context. The speakers are split into disjoint sets of 35/8/2/2 speakers for feature learning/recognizer training/tuning/testing. The 35 speakers for feature learning are fixed; the remaining 12 are used in a 6-fold experiment (recognizer training on 8 speakers, tuning on 2 speakers, and testing on the remaining 2 speakers). Each speaker has roughly 50K frames. We remove the per-speaker mean and variance of the articulatory measurements for each training speaker, and remove the mean of the acoustic measurements for each utterance. All learned feature types are used in a “tandem” speech recognizer (Hermansky et al., 2000), i.e., they are appended to the original 39D features and used in a standard hidden Markov model (HMM)-based recognizer with Gaussian mixture observation distributions.
Each algorithm uses up to 3 ReLU hidden layers, each of 1500 units, for the projection and reconstruction mappings. For VCCA/VCCA-private, we use Gaussian observation models as the inputs are real-valued. In contrast to the MNIST experiments, we do not learn the standard deviations of each output dimension on training data, as this leads to poor downstream task performance. Instead, we use isotropic covariances for each view, and tune the standard deviations by grid search. The best model uses a smaller standard deviation (0.1) for the view 2 than for view 1 (1.0), effectively putting more emphasis on the reconstruction of articulatory measurements. Our best performing VCCA model uses dz = 70, while the best performing VCCA-private model uses dz = 70 and dhx = dhy = 10.
2As in Wang and Livescu (2016), we use the Kaldi toolkit (Povey et al., 2011) for feature extraction and recognition with hidden Markov models. Our results do not match Wang et al. (2015a,b) (who instead used the HTK toolkit (Young et al., 1999)) for the same types of features, but the relative results are consistent.
The mean phone error rates (PER) over 6 folds obtained by different algorithms are given in Table 1. Our methods achieve competitive performance in comparison to previous deep multi-view methods.

4.3 MIR-FLICKR DATASET
Finally, we consider the task of learning cross-modality features for topic classification on the MIRFlickr database (Huiskes and Lew, 2008). The Flickr database contains 1 million images accompanied by user tags, among which 25000 images are labeled with 38 topic classes (each image may be categorized as multiple topics). We use the same image and text features as in previous work (Srivastava and Salakhutdinov, 2014; Sohn et al., 2014): the image feature is 3857 dimensional real-valued vector, composed of Pyramid Histogram of Words (PHOW) (Bosch et al., 2007), GIST (Oliva and Torralba, 2001), and MPEG-7 descriptors (Manjunath et al., 2001), while the text feature is a 2000-dimensional binary vector of frequent tags.
Following the same protocol as Sohn et al. (2014), we train multi-view representations using the unlabelled data,3 and use projected image features of the labeled data (further divided into splits of 10000/5000/10000 samples for training/tuning/testing) for training and evaluating a classifier that predicts the topic labels, corresponding to the unimodal query task in Srivastava and Salakhutdinov (2014); Sohn et al. (2014). For each algorithm, we select the model which achieves the highest mean average precision (mAP) on the validation set, and report its performance on the test set.
Each algorithm uses up to 4 ReLU hidden layers, each of 1024 units, for the projection and reconstruction mappings. For VCCA/VCCA-private, we use Gaussian observation models with isotropic covariance for image features, with standard deviation tuned by grid search, and a Bernoulli model for text features. In this experiment, we also found it helpful to tune an additional trade-off parameter for the text-view likelihood (cross-entropy); the best VCCA/VCCA-private models prefer a large trade-off parameter of the level 104, emphasizing the reconstruction of the sparse text-view inputs. Our best performing VCCA model uses dz = 1024, while the best performing VCCA-private model uses dz = 1024 and dhx = dhy = 16.
As shown in Table 1, VCCA/VCCA-private achieve significantly higher mAPs than other methods considered here. Being much easier to train, the performance of our methods are competitive with the previous state-of-the-art mAP result of 0.607 achieved by the multi-view RBMs of Sohn et al. (2014) under the same setting.

5 CONCLUSIONS
We have proposed variational canonical correlation analysis (VCCA), a deep generative method for multi-view representation learning. Our method embodies a natural idea for multi-view learning: the multiple views can be generated from a small set of shared latent variables. VCCA is parameterized by DNNs and can be trained efficiently by backpropagation, and is therefore scalable. We have also shown that, by modeling the private variables that are specific to each view, the VCCA-private variant can disentangle shared/private variables and provide higher-quality reconstructions.
In the future, we will explore other prior distributions such as mixtures of Gaussians or discrete random variables, which may enforce clustering in the latent space and in turn work better for discriminative tasks. We will also explore other observation models, including replacing the autoencoder objective with that of adversarial networks (Goodfellow et al., 2014; Makhzani et al., 2016; Chen et al., 2016). Another direction is to explicitly incorporate the structure of the inputs, such as the sequence structure of speech and text and the spatial structure of images.
ACKOWLEDGEMENTS
This research was supported by NSF grant IIS-1321015. The opinions expressed in this work are those of the authors and do not necessarily reflect the views of the funding agency. This research used GPUs donated by NVIDIA Corporation.
3As in Sohn et al. (2014), we exclude about 250000 samples which contain fewer than two tags.

A ADDITIONAL T-SNE VISUALIZATION OF NOISY MNIST
","We present deep variational canonical correlation analysis (VCCA), a deep multiview learning model that extends the latent variable model interpretation of linear CCA (Bach and Jordan, 2005) to nonlinear observation models parameterized by deep neural networks (DNNs). Computing the marginal data likelihood, as well as inference of the latent variables, are intractable under this model. We derive a variational lower bound of the data likelihood by parameterizing the posterior density of the latent variables with another DNN, and approximate the lower bound via Monte Carlo sampling. Interestingly, the resulting model resembles that of multiview autoencoders (Ngiam et al., 2011), with the key distinction of an additional sampling procedure at the bottleneck layer. We also propose a variant of VCCA called VCCA-private which can, in addition to the “common variables” underlying both views, extract the “private variables” within each view. We demonstrate that VCCA-private is able to disentangle the shared and private information for multi-view data without hard supervision.",ICLR 2017 conference submission,False,,"7

Summary:
This paper describes the use of variational autoencoders for multi-view representation learning as an alternative to canonical correlation analysis (CCA), deep CCA (DCCA), and multi-view autoencoders (MVAE). Two variants of variational autoencoders (which the authors call VCCA and VCCA-private) are investigated. The method’s performances are compared on a synthetic MNIST dataset, the XRMB speech-articulation dataset, and the MIR-Flickr dataset.

Review:
Variational autoencoders are widely used and their performance for multi-view representation learning should be of interest to the ICLR community. The paper is well written and clear. The experiments are thorough. It is interesting that the performance of MVAE and VCCA is quite different given the similarity of their objective functions. I further find the analyses of the effects of dropout and private variables useful.

As the authors point out, “VCCA does not optimize the same criterion, nor produce the same solution, as any linear or nonlinear CCA”. It would have been interesting to discuss the differences of a linear variant of VCCA and linear CCA, and to compare it quantitatively. While it might not make sense to use variational inference in the linear case, it would nevertheless help to understand the differences better.

The derivations in Equation 3 and Equation 13 seem unnecessarily detailed given that VCCA and VCCA-p are special cases of VAE, only with certain architectural constraints. Perhaps move to the Appendix?

In Section 3 the authors claim that “if we are able to generate realistic samples from the learned distribution, we can infer that we have discovered the underlying structure of the data”. This is not correct, a model which hasn’t learned a thing can have perfectly realistic samples (see Theis et al., 2016). Please remove or revise the sentence.

Minor:
In the equation between Equation 8 and 9, using notation N(x; g(z, theta), I) as in Equation 6 would make it clearer.

---

The application of VAEs to multiview settings, snd the general problem of learning representations over multiple modalities is of increasing importance. After discussion and reviewing the rebuttals, the reviewers felt that the paper is not yet ready. The application is strong, but that more can be done in terms of praticality, generalisation using other posteriors, and the comparisons made. For this reason, the paper is unfortunately not yet ready for inclusion in this year's proceedings.

---

Dear reviewers,

We have modified our response to your review, acknowledging the related work pointed out by several reviewers, and clarifying our main contributions. 

Thanks!

---

This paper considers the case where multiple views of data are learned through a probabilistic deep neural network formulation. This makes the model non-linear (unlike e.g. CCA) but makes inference difficult. Therefore, the VAE framework is invoked for inference.

In [Ref 1] the authors show that maximum likelihood estimation based on their linear latent model leads to the canonical correlation directions. But in the non-linear case with DNNs it's not clear (at least with the present analysis) what the solution is wrt to the canonical directions. There's no such analysis in the paper, hence I find it a stretch to refer to this model as a CCA type of model. In contrast, e.g. DCCA / DCCAE are taking the canonical correlation between features into account inside the objective and provide interpretations.

[Ref 1] F. R. Bach and M. I. Jordan. A probabilistic interpretation of canonical correlation analysis. Technical Report 688, 2005.

There is also a significant body of very related work on non-linear multi-view models which is not discussed in this paper. For example, there's been probabilistic non-linear multi-view models [Ref 2, 3], also extended to the Bayesian case with common/private spaces [Ref 4] and the variational / deep learning case [Ref 5].

[Ref 2] Ek et al. Gaussian process latent variable models for human pose estimation. MLMI, 2007.
[Ref 3] Shon et al. Learning shared latent structure for image synthesis and robotic imitation. NIPS, 2006.
[Ref 4] Damianou et al. Manifold relevance determination. ICML, 2012.
[Ref 5] Damianou and Lawrence. Deep Gaussian processes. AISTATS, 2013.

I can see the utility of this model as bringing together two elements: multi-view modeling and VAEs. This seems like an obvious idea but to the best of my knowledge it hasn't been done before and is actually a potentially very useful model.

However, the question is, what is the proper way of extending VAE to multiple views? The paper didn't convince me that VAE can work well with multiple views using the shown straightforward construction. Specifically, VCCA doesn't seem to promote the state of the art in terms of results (it actually is overall below the SOA), while the VCCA-private seems a quite ill-posed model: the dimensionalities d have to be manually tuned with exhaustive search; further, the actual model does not provide a consinstent way of encouraging the private and common variables to avoid learning redundant information. Relying only on dropout for this seems a quite ad-hoc solution (in fact, from Fig. 4 (ver2) it seems that the dropout rate is quite crucial). Perhaps good performance might be achieved with a lot of tuning (which might be why the FLICKR results got better in ver2 without changing the model), but it seems quite difficult to optimize for the above reasons. From a purely experimental point of view, VCCA-private doesn't seem to promote the SOA either. Of course one wouldn't expect any new published paper to beat all previous baselines, but it seems that extension of VAE to multiple views is a very interesting idea which deserves some more investigation of how to do it efficiently.

Another issue is the approximate posterior being parameterized only from one of the views. This makes the model less useful as a generic multi-view model, since it will misbehave in tasks other than classification. But if classification is the main objective, then one should compare to a proper classification model, e.g. a feedforward neural network.

The plots of Fig. 8 are very nice. Overall, the paper convinced me that there is merit in attaching multiple views to VAE. However, it didn't convince me a) that the proposed way to achieve this is practical b) that there is a connection to CCA (other than being a method for multiple views). The bottom line is that, although the paper is interesting, it needs a little more work.

---

UPDATE: I have read the replies on this thread. My opinion has not changed.

The authors propose deep VCCA, a deep version of the probabilistic CCA model by using likelihoods parameterized by nonlinear functions (neural nets). Variational inference is applied with an inference network and reparameterization gradients. An additional variant, termed VCCA-private, is also introduced, which includes local latent variables for each data point (view). A connection to the multi view auto encoder is also shown.

Since the development of black box variational inference and variational auto-encoders, the methodology in model-specific papers like this one are arguably not very interesting. The model is a straightforward extension of probabilistic CCA with neural net parameterized likelihoods. Inference is mechanically the same as any black box approach using the reparameterization gradient and inference networks. The approach also uses a mean-field approximation, which is quite old given the many recent developments in more expressive approximations (see, e.g., Rezende and Mohamed (2015); Tran et al. (2016)).

The connection to multi-view auto encoders is at first insightful, but no more than the difference between MAP and variational inference. This is a well-known insight: in the abstract, the authors argue that the key distinction is the additional sampling, but ultimately what matters is the KL regularizer. Even with noisy samples, the variances of a normal variational approximation would collapse to zero and thus become a point mass approximation, equivalent to optimizing a point estimate from the MVAE objective. (I suspect the authors know this to some degree due to their remarks in the paper, but it is unclear.)

That said, I think the paper has strong merits in application. The experiments are strong, comparing to alternative multi-view approaches under a number of interesting data sets. While the use of ""private variables"" is simple, they demonstrate how it can successfully disentangle the per-view latent representation from the shared view. It would have been preferable to compare to methods using probabilistic inference, such as full Bayes for the linear CCA. 

There are also a number of approximations taken to almost be standard in the paper which may not be necessary, such as the use of a mean-field family or the use of an inference network. To separate out how much the approximate inference is influencing the fit of the model, I strongly recommend using MCMC and non-amortized variational inference on at least one experiment. 

+ Rezende, D. J., & Mohamed, S. (2015). Variational Inference with Normalizing Flows. Presented at the International Conference on Machine Learning.
+ Tran, D., Ranganath, R., & Blei, D. M. (2016). The Variational Gaussian Process. Presented at the International Conference on Learning Representations.

---

7

Summary:
This paper describes the use of variational autoencoders for multi-view representation learning as an alternative to canonical correlation analysis (CCA), deep CCA (DCCA), and multi-view autoencoders (MVAE). Two variants of variational autoencoders (which the authors call VCCA and VCCA-private) are investigated. The method’s performances are compared on a synthetic MNIST dataset, the XRMB speech-articulation dataset, and the MIR-Flickr dataset.

Review:
Variational autoencoders are widely used and their performance for multi-view representation learning should be of interest to the ICLR community. The paper is well written and clear. The experiments are thorough. It is interesting that the performance of MVAE and VCCA is quite different given the similarity of their objective functions. I further find the analyses of the effects of dropout and private variables useful.

As the authors point out, “VCCA does not optimize the same criterion, nor produce the same solution, as any linear or nonlinear CCA”. It would have been interesting to discuss the differences of a linear variant of VCCA and linear CCA, and to compare it quantitatively. While it might not make sense to use variational inference in the linear case, it would nevertheless help to understand the differences better.

The derivations in Equation 3 and Equation 13 seem unnecessarily detailed given that VCCA and VCCA-p are special cases of VAE, only with certain architectural constraints. Perhaps move to the Appendix?

In Section 3 the authors claim that “if we are able to generate realistic samples from the learned distribution, we can infer that we have discovered the underlying structure of the data”. This is not correct, a model which hasn’t learned a thing can have perfectly realistic samples (see Theis et al., 2016). Please remove or revise the sentence.

Minor:
In the equation between Equation 8 and 9, using notation N(x; g(z, theta), I) as in Equation 6 would make it clearer.

---

Dear reviewers,

We just uploaded a slightly modified version. This version 

-- contains the updated results on Flickr and the proposed methods significantly outperform others
-- reorganized the figures and tables to make the paper more compact

Thanks!

---

7

Summary:
This paper describes the use of variational autoencoders for multi-view representation learning as an alternative to canonical correlation analysis (CCA), deep CCA (DCCA), and multi-view autoencoders (MVAE). Two variants of variational autoencoders (which the authors call VCCA and VCCA-private) are investigated. The method’s performances are compared on a synthetic MNIST dataset, the XRMB speech-articulation dataset, and the MIR-Flickr dataset.

Review:
Variational autoencoders are widely used and their performance for multi-view representation learning should be of interest to the ICLR community. The paper is well written and clear. The experiments are thorough. It is interesting that the performance of MVAE and VCCA is quite different given the similarity of their objective functions. I further find the analyses of the effects of dropout and private variables useful.

As the authors point out, “VCCA does not optimize the same criterion, nor produce the same solution, as any linear or nonlinear CCA”. It would have been interesting to discuss the differences of a linear variant of VCCA and linear CCA, and to compare it quantitatively. While it might not make sense to use variational inference in the linear case, it would nevertheless help to understand the differences better.

The derivations in Equation 3 and Equation 13 seem unnecessarily detailed given that VCCA and VCCA-p are special cases of VAE, only with certain architectural constraints. Perhaps move to the Appendix?

In Section 3 the authors claim that “if we are able to generate realistic samples from the learned distribution, we can infer that we have discovered the underlying structure of the data”. This is not correct, a model which hasn’t learned a thing can have perfectly realistic samples (see Theis et al., 2016). Please remove or revise the sentence.

Minor:
In the equation between Equation 8 and 9, using notation N(x; g(z, theta), I) as in Equation 6 would make it clearer.

---

The application of VAEs to multiview settings, snd the general problem of learning representations over multiple modalities is of increasing importance. After discussion and reviewing the rebuttals, the reviewers felt that the paper is not yet ready. The application is strong, but that more can be done in terms of praticality, generalisation using other posteriors, and the comparisons made. For this reason, the paper is unfortunately not yet ready for inclusion in this year's proceedings.

---

Dear reviewers,

We have modified our response to your review, acknowledging the related work pointed out by several reviewers, and clarifying our main contributions. 

Thanks!

---

This paper considers the case where multiple views of data are learned through a probabilistic deep neural network formulation. This makes the model non-linear (unlike e.g. CCA) but makes inference difficult. Therefore, the VAE framework is invoked for inference.

In [Ref 1] the authors show that maximum likelihood estimation based on their linear latent model leads to the canonical correlation directions. But in the non-linear case with DNNs it's not clear (at least with the present analysis) what the solution is wrt to the canonical directions. There's no such analysis in the paper, hence I find it a stretch to refer to this model as a CCA type of model. In contrast, e.g. DCCA / DCCAE are taking the canonical correlation between features into account inside the objective and provide interpretations.

[Ref 1] F. R. Bach and M. I. Jordan. A probabilistic interpretation of canonical correlation analysis. Technical Report 688, 2005.

There is also a significant body of very related work on non-linear multi-view models which is not discussed in this paper. For example, there's been probabilistic non-linear multi-view models [Ref 2, 3], also extended to the Bayesian case with common/private spaces [Ref 4] and the variational / deep learning case [Ref 5].

[Ref 2] Ek et al. Gaussian process latent variable models for human pose estimation. MLMI, 2007.
[Ref 3] Shon et al. Learning shared latent structure for image synthesis and robotic imitation. NIPS, 2006.
[Ref 4] Damianou et al. Manifold relevance determination. ICML, 2012.
[Ref 5] Damianou and Lawrence. Deep Gaussian processes. AISTATS, 2013.

I can see the utility of this model as bringing together two elements: multi-view modeling and VAEs. This seems like an obvious idea but to the best of my knowledge it hasn't been done before and is actually a potentially very useful model.

However, the question is, what is the proper way of extending VAE to multiple views? The paper didn't convince me that VAE can work well with multiple views using the shown straightforward construction. Specifically, VCCA doesn't seem to promote the state of the art in terms of results (it actually is overall below the SOA), while the VCCA-private seems a quite ill-posed model: the dimensionalities d have to be manually tuned with exhaustive search; further, the actual model does not provide a consinstent way of encouraging the private and common variables to avoid learning redundant information. Relying only on dropout for this seems a quite ad-hoc solution (in fact, from Fig. 4 (ver2) it seems that the dropout rate is quite crucial). Perhaps good performance might be achieved with a lot of tuning (which might be why the FLICKR results got better in ver2 without changing the model), but it seems quite difficult to optimize for the above reasons. From a purely experimental point of view, VCCA-private doesn't seem to promote the SOA either. Of course one wouldn't expect any new published paper to beat all previous baselines, but it seems that extension of VAE to multiple views is a very interesting idea which deserves some more investigation of how to do it efficiently.

Another issue is the approximate posterior being parameterized only from one of the views. This makes the model less useful as a generic multi-view model, since it will misbehave in tasks other than classification. But if classification is the main objective, then one should compare to a proper classification model, e.g. a feedforward neural network.

The plots of Fig. 8 are very nice. Overall, the paper convinced me that there is merit in attaching multiple views to VAE. However, it didn't convince me a) that the proposed way to achieve this is practical b) that there is a connection to CCA (other than being a method for multiple views). The bottom line is that, although the paper is interesting, it needs a little more work.

---

UPDATE: I have read the replies on this thread. My opinion has not changed.

The authors propose deep VCCA, a deep version of the probabilistic CCA model by using likelihoods parameterized by nonlinear functions (neural nets). Variational inference is applied with an inference network and reparameterization gradients. An additional variant, termed VCCA-private, is also introduced, which includes local latent variables for each data point (view). A connection to the multi view auto encoder is also shown.

Since the development of black box variational inference and variational auto-encoders, the methodology in model-specific papers like this one are arguably not very interesting. The model is a straightforward extension of probabilistic CCA with neural net parameterized likelihoods. Inference is mechanically the same as any black box approach using the reparameterization gradient and inference networks. The approach also uses a mean-field approximation, which is quite old given the many recent developments in more expressive approximations (see, e.g., Rezende and Mohamed (2015); Tran et al. (2016)).

The connection to multi-view auto encoders is at first insightful, but no more than the difference between MAP and variational inference. This is a well-known insight: in the abstract, the authors argue that the key distinction is the additional sampling, but ultimately what matters is the KL regularizer. Even with noisy samples, the variances of a normal variational approximation would collapse to zero and thus become a point mass approximation, equivalent to optimizing a point estimate from the MVAE objective. (I suspect the authors know this to some degree due to their remarks in the paper, but it is unclear.)

That said, I think the paper has strong merits in application. The experiments are strong, comparing to alternative multi-view approaches under a number of interesting data sets. While the use of ""private variables"" is simple, they demonstrate how it can successfully disentangle the per-view latent representation from the shared view. It would have been preferable to compare to methods using probabilistic inference, such as full Bayes for the linear CCA. 

There are also a number of approximations taken to almost be standard in the paper which may not be necessary, such as the use of a mean-field family or the use of an inference network. To separate out how much the approximate inference is influencing the fit of the model, I strongly recommend using MCMC and non-amortized variational inference on at least one experiment. 

+ Rezende, D. J., & Mohamed, S. (2015). Variational Inference with Normalizing Flows. Presented at the International Conference on Machine Learning.
+ Tran, D., Ranganath, R., & Blei, D. M. (2016). The Variational Gaussian Process. Presented at the International Conference on Learning Representations.

---

7

Summary:
This paper describes the use of variational autoencoders for multi-view representation learning as an alternative to canonical correlation analysis (CCA), deep CCA (DCCA), and multi-view autoencoders (MVAE). Two variants of variational autoencoders (which the authors call VCCA and VCCA-private) are investigated. The method’s performances are compared on a synthetic MNIST dataset, the XRMB speech-articulation dataset, and the MIR-Flickr dataset.

Review:
Variational autoencoders are widely used and their performance for multi-view representation learning should be of interest to the ICLR community. The paper is well written and clear. The experiments are thorough. It is interesting that the performance of MVAE and VCCA is quite different given the similarity of their objective functions. I further find the analyses of the effects of dropout and private variables useful.

As the authors point out, “VCCA does not optimize the same criterion, nor produce the same solution, as any linear or nonlinear CCA”. It would have been interesting to discuss the differences of a linear variant of VCCA and linear CCA, and to compare it quantitatively. While it might not make sense to use variational inference in the linear case, it would nevertheless help to understand the differences better.

The derivations in Equation 3 and Equation 13 seem unnecessarily detailed given that VCCA and VCCA-p are special cases of VAE, only with certain architectural constraints. Perhaps move to the Appendix?

In Section 3 the authors claim that “if we are able to generate realistic samples from the learned distribution, we can infer that we have discovered the underlying structure of the data”. This is not correct, a model which hasn’t learned a thing can have perfectly realistic samples (see Theis et al., 2016). Please remove or revise the sentence.

Minor:
In the equation between Equation 8 and 9, using notation N(x; g(z, theta), I) as in Equation 6 would make it clearer.

---

Dear reviewers,

We just uploaded a slightly modified version. This version 

-- contains the updated results on Flickr and the proposed methods significantly outperform others
-- reorganized the figures and tables to make the paper more compact

Thanks!",,,,,,5.666666666666667,,,4.0,,
738,"Authors: GRADIENT DESCENT, Maohua Zhu, Yuan Xie, Minsoo Rhu, Jason Clemons, Stephen W. Keckler
Source file: 738.pdf

ABSTRACT
Prior work has demonstrated that exploiting the sparsity can dramatically improve the energy efficiency and reduce the memory footprint of Convolutional Neural Networks (CNNs). However, these sparsity-centric optimization techniques might be less effective for Long Short-Term Memory (LSTM) based Recurrent Neural Networks (RNNs), especially for the training phase, because of the significant structural difference between the neurons. To investigate if there is possible sparsity-centric optimization for training LSTM-based RNNs, we studied several applications and observed that there is potential sparsity in the gradients generated in the backward propagation. In this paper, we investigate why the sparsity exists and propose a simple yet effective thresholding technique to induce further more sparsity during the LSTM-based RNN training. The experimental results show that the proposed technique can increase the sparsity of linear gate gradients to more than 80% without loss of performance, which makes more than 50% multiply-accumulate (MAC) operations redundant for the entire LSTM training process. These redundant MAC operations can be eliminated by hardware techniques to improve the energy efficiency and the training speed of LSTM-based RNNs.

Prior work has demonstrated that exploiting the sparsity can dramatically improve the energy efficiency and reduce the memory footprint of Convolutional Neural Networks (CNNs). However, these sparsity-centric optimization techniques might be less effective for Long Short-Term Memory (LSTM) based Recurrent Neural Networks (RNNs), especially for the training phase, because of the significant structural difference between the neurons. To investigate if there is possible sparsity-centric optimization for training LSTM-based RNNs, we studied several applications and observed that there is potential sparsity in the gradients generated in the backward propagation. In this paper, we investigate why the sparsity exists and propose a simple yet effective thresholding technique to induce further more sparsity during the LSTM-based RNN training. The experimental results show that the proposed technique can increase the sparsity of linear gate gradients to more than 80% without loss of performance, which makes more than 50% multiply-accumulate (MAC) operations redundant for the entire LSTM training process. These redundant MAC operations can be eliminated by hardware techniques to improve the energy efficiency and the training speed of LSTM-based RNNs.

1 INTRODUCTION
Deep neural networks have achieved state-of-the-art performance in many different tasks, such as computer vision (Krizhevsky et al., 2012) (Simonyan & Zisserman, 2015), speech recognition, and natural language processing (Karpathy et al., 2016). The underlying representational power of these neural networks comes from the huge parameter space, which results in an extremely large amount of computation operations and memory footprint. To reduce the memory usage and accelerate the training process, the research community has strived to eliminate the redundancy in the deep neural networks (Han et al., 2016b). Exploiting the sparsity in both weights and activations of Convolutional Neural Networks (CNNs), sparsity-centric optimization techniques (Han et al., 2016a) (Albericio et al., 2016) have been proposed to improve the speed and energy efficiency of CNN accelerators.
These sparsity-centric approaches can be classified into two categories: (1) pruning unimportant weight parameters and (2) skipping zero values in activations to eliminate multiply-accumulate (MAC) operations with zero operands. Although both categories have achieved promising results for CNNs, it remains unclear if they are applicable to training other neural networks, such as LSTMbased RNNs. The network pruning approach is not suitable for training because it only benefits the inference phase of neural networks by iteratively pruning and re-training. The approach that exploits the sparsity in the activations can be used for training because the activations are involved in both
the forward propagation and the backward propagation. But there are still some issues if we directly apply it to LSTM-based RNNs.
The sparsity in CNN activations mostly comes from the Rectified Linear Unit (ReLU) activation function, which sets all negative values to zero. However, Long Short-Term Memory, one of the most popular RNN cells, does not adopt the ReLU function. Therefore, LSTM should exhibit much less sparsity in activations than CNNs, intuitively. Furthermore, the structure of an LSTM cell is much more complicated than neurons in convolutional layers or fully connected layers of a CNN.
To explore additional opportunities to apply sparsity-centric optimization to LSTM-based RNNs, we conducted an application characterization on several LSTM-based RNN applications, including character-based language model, image captioning, and machine translation. Although the experimental results of the application characterization show that there is little sparsity in the activations, we observed potential sparsity in backward propagation of the LSTM training process. The activation values of the gates (input gate, forget gate, and output gate) and the new cell state exhibit a skewed distribution due to their functionality. That is, a large fraction of the activation values of these Sigmoid-based gates are either close to 1 or close to 0 (for the Tanh-based new cell activations, values are close to -1 or 1). This skewed distribution will lead to a considerable amount of very small values in the LSTM backward propagation since there is a term σ(x)(1− σ(x)) in the gradients of the Sigmoid-based gates (tanh(x)(1− tanh(x)) for the gradients of the new cell gradients), which will be zero given σ(x) = 0 or σ(x) = 1 (tanh(x) = −1 or tanh(x) = 1 for the new cell gradients). In real-world implementations, these very small values might be clamped to zero as they are in the form of floating-point numbers, of which the precision is limited. Therefore, there is potential sparsity in the gradients of the backward propagation of LSTM training.
To ensure that there is non-trivial amount of sparsity for hardware designers to exploit, we propose “sparsified” SGD, a rounding to zero technique to induce more sparsity in the gradients. This approach can be seen as a stochastic gradient descent (SGD) learning algorithm with sparsifying, which strips the precision of floating point numbers for unimportant small gradients. Experiment results show that with proper thresholds, we can make 80% of the gradients of the gate inputs to zero without performance loss for all applications and datasets we tested so far. As the sparse gradients of the gate inputs are involved in 67% matrix multiplications, more than 50% MAC operations are redundant in the entire LSTM training process. Eliminating these ineffectual MAC operations with hardware techniques, the energy efficiency and training speed of LSTM-based RNNs will be improved significantly.

2 BACKGROUND AND MOTIVATION
In this section, we first review some of the prior work on sparsity-centric optimization techniques for neural networks, and then illustrate the application characterization example as the motivation for our research.

2.1 SPARSITY-CENTRIC OPTIMIZATION FOR NEURAL NETWORKS
It has been demonstrated that there is significant redundancy in the parameterization of deep neural networks (Denil et al., 2013). Consequently, the over-sized parameter space results in sparsity in the weight parameters of a neural network. Besides the parameters, there is also sparsity in the activations of each layer in a network, which comes from two sources: (1) the sparsity in weight parameters and (2) the activation function of neurons, such as ReLU.
As the sparsity in weight parameters do not depend on the input data, it is often referred to as static sparsity. On the other hand, the sparsity in the activations depend on not only the weight values but also the input data. Therefore, we refer to the sparsity in the activations as dynamic sparsity.
Exploiting sparsity can dramatically reduce the network size and thus improve the computing performance and energy efficiency. For example, Deep Compression (Han et al., 2016b) applied network pruning to CNNs to significantly reduce the footprint of the weights, which enables us to store all the weights on SRAM. However, the static sparsity can only help the inference phase but not training because weight parameters are adjusted during training. Fortunately, leveraging the dynamic sparsity can benefit both inference and training of neural networks. Recent publications (Han et al.,
2016a) (Albericio et al., 2016) have proposed various approaches to eliminate ineffectual MAC operations with zero operands. Although these sparsity-centric optimization approaches have achieved promising results on CNNs, much less attention has been paid to LSTM-based RNNs, because there is a common belief that the major source of sparsity is the ReLU function, which is widely used in the convolutional layers but not in LSTM-based RNNs. To accelerate LSTM-based RNNs and improve the energy efficiency, we investigate opportunities to exploit sparsity in the LSTM-based RNN training process. As an initial step, in this paper we focus on the basic LSTM cell without peephole or other advanced features, as shown in Figure 1.

2.2 APPLICATION CHARACTERIZATION
To reveal if there is sparsity in LSTM training, we conduct an application characterization study. We start with a character-based language model as described in (Karpathy et al., 2016). This characterbased language model takes a sequence of characters as input and predicts the next character of this sequence. The characters are represented in one-hot vectors, which are transformed into distributed vectors by a word2vec layer. Then the distributed vectors feed into an RNN model based on LSTM cells, followed by a linear classifier.
The LSTM cells used in this character-based language model are all basic LSTM cells. For each cell, the forward propagation flow is as below:
it = σ(W ixt + U iht−1 + b i)
ft = σ(W fxt + U fht−1 + b f )
ot = σ(W oxt + U oht−1 + b o)
gt = tanh(W gxt + U ght−1 + b g)
ct = ft ◦ ct−1 + it ◦ gt ht = ot ◦ tanh(ct)
As shown in Figure 1, it, ft, and ot stand for input gate, forget gate, and output gate, respectively. These sigmoid-based gates (σ stands for sigmoid) are used to prevent irrelevant input from affecting the memory cell (ct). The new cell state (gt) is a preliminary summary of the current input from the previous layer and the previous status of current layer. The final hidden status ht is the output of the LSTM cell if it is seen as a black box.
Since the gates are introduced to prevent irrelevant inputs from affecting the memory cell ct, we have a hypothesis that a large fraction of the activations of these gates should be either close to 1 or close to 0, representing the control signal on or off, respectively. Similarly, the tanh-based new cell status is active if its activation is 1 or inactive if it is -1. There should also be a considerable portion of the activations close to 1 or -1.
To validate our hypothesis, we extracted the activations of the sigmoid-based gates and tanh-based new cell state from several model snapshots during training the character-based language model. Figure 2 shows the histogram of the activation values of the gates and the new cell. The red curves represent the activation values generated by a snapshot model which is 0.5% trained (in terms of total number of iterations) while the bars represent the activation values generated by a fully trained
model. We can observe skewed distributions from each gate (and new cell) for both the 0.5% trained snapshot model and the fully trained model. Furthermore, the fully trained model shows a distribution that is more skewed to the leftmost and the rightmost. Additionally, other un-shown snapshots demonstrate that the distribution becomes consistently more skewed as the training process goes on. We also observed that after 10% of the training process, the distribution becomes steady, almost the same as the fully trained model.
Besides the character-based language model, we also conducted the same characterization to the image captioning task described in (Karpathy & Li, 2015). The activation values of the RNN layer in the image captioning task exhibit the skewed distribution too. Even though we did not observe sparsity in the gate activations, the skewed distribution indicates potential sparsity in the LSTMbased RNN backward propagation, which will be shown in the next section.

3 SPARSIFIED STOCHASTIC GRADIENT DESCENT FOR LSTM
In this section, we first show how the skewed distribution of gate values leads to potential sparsity in the LSTM backward propagation, and then we propose the “sparsified” SGD to induce more sparsity in LSTM training.

3.1 POTENTIAL SPARSITY IN LSTM BACKWARD PROPAGATION
To show how the skewed distribution in the gate activations results in potential sparsity in the LSTMbased RNN backward propagation, we need to review the forward and backward propagation at first. We can re-write the forward propagation equations as
net(i)t =W ixt + U iht−1 + b i
net(f)t =W fxt + U fht−1 + b f
net(o)t =W oxt + U oht−1 + b o
net(g)t =W gxt + U ght−1 + b g
it = σ(net(i)t)
ft = σ(net(f)t)
ot = σ(net(o)t)
gt = tanh(net(g)t)
ct = ft ◦ ct−1 + it ◦ gt ht = ot ◦ tanh(ct)
Here we introduce variables net(i), net(f), net(o) and net(g) to represent the linear part of the gates and the new cell state. In GPU implementations such as cuDNN v5 (Appleyard et al., 2016), these linear gates (including new cell state from now on) are usually calculated in one step since they share the same input vectors xt and ht−1. Therefore we can use a uniform representation for the four linear gates, that is
nett =Wxt + Uht−1 + b
The matrix W here stands for the combination of the matrices W i, W f , W o and W g and the matrix U stands for the combination of the matrices U i, Uf , Uo and Ug .
With these denotations, we can express the backward propagation as
dot = dht ◦ tanh(ct)
dct = dht ◦ (1− tanh2(ct)) ◦ ot + ft ◦ ct+1 dnet(g)t = dct ◦ it ◦ (1− g2t )
dnet(o)t = dht ◦ tanh(ct) ◦ (1− ot) ◦ ot dnet(f)t = dct ◦ ct−1 ◦ (1− ft) ◦ ft dnet(i)t = dct ◦ gt ◦ (1− it) ◦ it
dxt = dnettW T
dht−1 = dnettU T
dW+ = xtdnett
dU+ = ht−1dnett
In the equations of the backward propagation, we use dnet to denote the gradient of the linear gates.
From these equations we can see that for each linear gate gradient there is one term introduced by the sigmoid function or the tanh function, e.g. (1−g2t ) in dnet(g)t and (1−ot)◦ot in dnet(o)t. As we observed in the application characterization results, the activation values of these gates exhibit skewed distribution, which means a large fraction of ot, ft and it are close to 0 or 1 (gt close to -1 or 1). The skewed distribution makes a large fraction of the linear gate gradients close to zero because (1 − g2t ), (1 − ot) ◦ ot, (1 − ft) ◦ ft and (1 − it) ◦ it are mostly close to zero given the skewed distribution of the gate activations.
When implementing the LSTM-based RNNs, we usually use 32-bit floating point numbers to represent the gradients. Due to the precision limit, floating point numbers will round extremely small values to zero. Therefore, there is potential sparsity in dnet since a large fraction of the linear gate gradients are close to zero.

3.2 INDUCING MORE SPARSITY
In the previous section we showed how the skewed distribution in gate activations results in potential sparsity in linear gate gradients theoretically. However, from mathematical perspective, there will be no sparsity in linear gate gradients if the floating point numbers in computers have infinite precision since they are only close to zero rather than be zero. Even the precision of 32-bit floating point numbers is not infinite, the 8-bit exponential part can still accommodate an extremely large dynamic range, which makes the sparsity less interesting to hardware accelerator designers. Fortunately, recent attempts to train neural networks with 16-bit floating points (Gupta et al., 2015) and fixed points (Lin et al., 2015) have shown acceptable performance with smaller dynamic range. This inspires us to induce more sparsity by rounding very small linear gate gradients to zero, which is similar to replace 32-bit floating points with 16-bit floating points or fixed points.
The intuition behind this “rounding to zero” approach is that pruning CNNs will not affect the overall training performance. Similarly, thresholding very small gradient (dnet) values to zero is likely not to affect the overall training accuracy. Therefore, we propose a simple static thresholding approach which sets small dnet values below a threshold t to zero. By doing this, we can increase the sparsity in dnet even further than the original sparsity caused by limited dynamic range of floating
point numbers. With our static thresholding technique, the backward propagation of LSTM training becomes as below:
dot = dht ◦ tanh(ct) dct = dht ◦ (1− tanh2(ct)) ◦ ot + ft ◦ ct+1
dnet(g)t = dct ◦ it ◦ (1− g2t ) dnet(o)t = dht ◦ tanh(ct) ◦ (1− ot) ◦ ot dnet(f)t = dct ◦ ct−1 ◦ (1− ft) ◦ ft dnet(i)t = dct ◦ gt ◦ (1− it) ◦ it dnett = (dnett > t)?dnett : 0
dxt = dnettW T
dht−1 = dnettU T
dW+ = xtdnett
dU+ = ht−1dnett
In this “sparsified” SGD backward propagation, a new hyper-parameter t is introduced to control the sparsity we would like to induce in dnet. Clearly, the optimal threshold t is the highest one that has no impact on the training performance since it can induce the highest sparsity in dnet. Therefore, to select the threshold, we need to monitor the impact on the gradients. As the SGD only uses the gradients of the weights (dW ) to update the weights, dW is the only gradients we need to care about. From the equations of the backward propagation we can see that dW is computed based on dnet, which is sparsified by our approach. Although sparsifying dnet affects dW , we can control the change of dW by setting the threshold. To determine the largest acceptable threshold, we conducted an evaluation of the impact caused by different thresholds on one single step in LSTM training. The application here is the same as the one in the application characterization.
Figure 3 shows the evaluation result. We measure the change of dW by the normalized inner product of sparsified dW and the original dW without sparisifying (the baseline shown in Figure 3). If we denote the original weight gradient as dW0, the correlation between sparsified dW and dW0 can be measured by normalized inner product
correlation = dW · dW0
||dW || · ||dW0||
If the correlation is 1, it means dW is exactly the same to dW0. If the correlation is 0, it means dW is orthogonal to dW0. The higher the correlation is, the less impact the sparsification has on this single step backward propagation. From Figure 3 we can see that even without our thresholding technique, the dnet still exhibits approximately 10% sparsity. These zero values are resulted from the limited dynamic range of floating point numbers, in which extremely small values are rounded to zero. By applying the thresholds to dnet, we can induce more sparsity shown by the bars. Even with a low threshold (10−8), the sparsity in dnet is increased to about 45%. With a relatively high threshold (10−6), the sparsity can be increased to around 80%. Although the sparsity is high, the correlation between the sparsified dW and dW0 is close to 1 even with the high threshold. Therefore, we can hypothesize that we can safely induce a considerable amount of sparsity with an appropriate threshold. It is straightforward to understand that the threshold cannot be arbitrarily large since we need to contain the information of the gradients. For example, if we increase the threshold even further to 10−5, the correlation will drop to 0.26, which is far from the original dW0 and not acceptable.
We have demonstrated that we can induce more sparsity by rounding small dnet to zero while maintaining the information in dW . However, this is only an evaluation on one single iteration of training. To show the generality of our static thresholding approach, we applied the thresholds to the entire training process.

4 AN ENTIRE LSTM TRAINING WITH SPARSIFIED SGD
In this section, we first present the sparsity induced by applying our sprsified SGD to an entire training process, and then discuss the generality of our approach.

4.1 CHARACTER-BASED LANGUAGE MODEL
To validate our proposed static thresholding approach, we apply it to the entire LSTM-based RNN training process. We first conducted an experiment on training a character-based language model. The language model consists of one word2vec layer, three LSTM-based RNN layers, and one linear classifier layer. The number of LSTM cells per RNN layer is 256. We feed the network with sequences of 100 characters each. The training dataset is a truncated Wikipedia dataset. We apply a fixed threshold to all dnet gradients for every iteration during the whole training process.
Figure 4 shows the sparsity of the linear gate gradients (dnet) of each layer during the whole training process. In the baseline configuration, the training method is standard SGD without sparsifying (zero
threshold). The baseline configuration exhibits about 20% sparsity in dnet. By applying only a low threshold (10−7), the sparsity is increased to around 70%. And we can consistently increase the sparsity further by raising the threshold. However, we have to monitor the impact of the threshold on the overall training performance to check if the threshold is too large to use.
Figure 5 shows the validation loss of each iteration. We observe that up to the medium threshold (10−6), the validation loss of the model trained with sparsified SGD keeps close to the baseline. However, if we continues raising the threshold to 10−5, the validation loss becomes unacceptably higher than the baseline. Although the validation loss with the 10−5 threshold is consistently decreasing as the training goes on, we conservatively do not pick this configuration to train the LSTM network. So combining Figure 4 with Figure 5, we can choose the threshold 10−6 to train the character-based language model to achieve about 80% sparsity in dnet.
Since the linear gate gradients dnet are involved in all the four matrix multiplications in the backward propagation, there are 80% MAC operations in these matrix multiplications have zero operands. Furthermore, there are six matrix multiplications (all of them are of the same amount of computation) in one LSTM training iteration and four out of them (67%) are sparse. So there are more than 50% MAC operations will have zero operands introduced by our sparsified SGD in one LSTM training iteration. The MAC operations with zero operands produce zero output and thus make no contribution to the final results. These redundant MAC operations can be eliminated by hardware techniques similar to (Han et al., 2016a) (Albericio et al., 2016) to improve the energy efficiency of LSTM training.

4.2 SENSITIVITY TEST
Our static thresholding approach can induce more than 80% sparsity in linear gate gradients of the character-based language model training. To demonstrate the generality of our approach, we then changed the topology of the RNN layers in the character-based language model with several different LSTM-based RNNs for a sensitivity test. The network topologies used in the sensitivity test are shown below.
• Number of layers: 2, 3, 6, 9; • Number of LSTM cells per layer: 128, 256, 512; • Sequence length: 25, 50, 100.
We also trained the network with other datasets, such as the tiny-Shakespear dataset and the novel War and Peace. For all the data points we collected from the sensitivity test, we can always achieve
more than 80% sparsity in dnet with less than 1% loss of performance in terms of validation loss with respect to the baseline.
Moreover, we also validated our approach by training an image captioning application (Karpathy & Li, 2015) with MSCOCO dataset (Lin et al., 2014) and a machine translation application known as Seq2Seq (Sutskever et al., 2014) with WMT15 dataset. As both the two applications are implemented based on graph model (Torch and TensorFlow, respectively), we plugged a custom operation in the automatically generated backward propagation subgraph to implement our proposed sparsified SGD. The experiment results show that the conclusion for the character-based language model still holds for the two applications.

4.3 DISCUSSION
So far all our experiment results show promising results and we believe our sparsified SGD is a general approach to induce sparsity for LSTM-based RNN training. From the computer hardware perspective, the sparsified SGD is similar to reduced precision implementation while the impact of sparsified SGD is much less since we still use full 32-bit floating point numbers. From the theory perspective, SGD itself is a gradient descent with noise and thresholding very small gradients to zero is nothing more than an additional noise source. Since training with SGD is robust to noise, the thresholding approach will likely not affect the overall training performance. Additionally, the weight gradients dW are aggregated through many time steps, which makes the LSTM more robust to the noise introduce by sparsifying the linear gate gradients.

5 CONCLUSION AND FUTURE WORK
In this paper, we conducted an application characterization to an LSTM-based RNN application and observe skewed distribution in the sigmoid-based gates and the tanh-based new cell state, which indicates potential sparsity in the linear gate gradients during backward propagation with SGD. The linear gate gradients are involved with 67% MAC operations in an entire LSTM training process so that we can improve the energy efficiency of hardware implementations if the linear gate gradients are sparse. We propose a simple yet effective rounding to zero technique, which can make the sparsity of the linear gate gradients higher than 80% without loss of performance. Therefore, more than 50% MAC operations are redundant in an entire sparsified LSTM training.
Obviously, the static-threshold approach is not optimal. In future, we will design a dynamicthreshold approach based on the learning rate, L2-norm of the gradients and the network topology. Hardware techniques will also be introduced to exploit the sparsity to improve the energy efficiency and training speed of LSTM-based RNNs for GPU and other hardware accelerators.
","Prior work has demonstrated that exploiting the sparsity can dramatically improve the energy efficiency and reduce the memory footprint of Convolutional Neural Networks (CNNs). However, these sparsity-centric optimization techniques might be less effective for Long Short-Term Memory (LSTM) based Recurrent Neural Networks (RNNs), especially for the training phase, because of the significant structural difference between the neurons. To investigate if there is possible sparsity-centric optimization for training LSTM-based RNNs, we studied several applications and observed that there is potential sparsity in the gradients generated in the backward propagation. In this paper, we investigate why the sparsity exists and propose a simple yet effective thresholding technique to induce further more sparsity during the LSTM-based RNN training. The experimental results show that the proposed technique can increase the sparsity of linear gate gradients to more than 80% without loss of performance, which makes more than 50% multiply-accumulate (MAC) operations redundant for the entire LSTM training process. These redundant MAC operations can be eliminated by hardware techniques to improve the energy efficiency and the training speed of LSTM-based RNNs.",ICLR 2017 conference submission,False,,"CONTRIBUTIONS
When training LSTMs, many of the intermediate gradients are close to zero due to the flat shape of the tanh and sigmoid nonlinearities far from the origin. This paper shows that rounding these small gradients to zero results in matrices with up to 80% sparsity during training, and that training character-level LSTM language models with this sparsification does not significantly change the final performance of the model. The authors argue that this sparsity could be exploited with specialized hardware to improve the energy efficiency and speed of recurrent network training.

NOVELTY
Thresholding gradients to induce sparsity and improve efficiency in RNN training is a novel result to my knowledge.

MISSING CITATIONS
Prior work has explored low-precision arithmetic for recurrent neural network language models:

Hubara et al, “Quantized Neural Networks: Training Neural Networks with
Low Precision Weights and Activations”,

---

The main point of the paper was that sparsifying gradients does not hurt performance; however, this in itself is not enough for a publication in this venue. As noted by R1 and R2, showing how this can help in more energy efficient training would make for a good paper; without that aspect the paper only presents an observation that is not too surprising to the practitioners in this area.
 
 Further, while the main point of the paper was relatively clear, the scientific presentation was not rigorous enough. All the reviewers point out that several details were missing (including test set performance, reporting of results on the different sets, etc). 
 
 Paper would be strengthened by a better exploration of the problem.

---

The findings of applying sparsity in the backward gradients for training LSTMs is interesting. 

But the paper seems incomplete without the proper experimental justification. Only the validation loss is reported which is definitely insufficient. Proper testing results and commonly reported evaluation criterion needs to be included to support the claim of no degradation when applying the proposed sparsity technique. 

Also actual justification of the gains in terms of speed and efficiency would make the paper much stronger.

---

This paper presents the observation that it is possible to utilize sparse operations in the training of LSTM networks without loss of accuracy. This observation is novel (although not too surprising) to my knowledge, but I must state that I am not very familiar with research on fast RNN implmentations.

Minor note:
The LSTM language model does not use a 'word2vec' layer. It is simply a linear embedding layer. Word2vec is the name of a specific model which is not directly to character level language models.

The paper presents the central observation clearly. However, it will be much more convincing if a well known dataset and experiment set up are used, such as Graves (2013) or Sutskever et al (2014), and actual training, validation and test performances are reported.

While the main observation is certainly interesting, I think it is not sufficient to be the subject of a full conference paper without implementation (or simulation) and benchmarking of the promised speedups on multiple tasks. For example, how would the gains be affected by various architecture choices?

At present this is an interesting technical report and I would like to see more detailed results in the future.

---

CONTRIBUTIONS
When training LSTMs, many of the intermediate gradients are close to zero due to the flat shape of the tanh and sigmoid nonlinearities far from the origin. This paper shows that rounding these small gradients to zero results in matrices with up to 80% sparsity during training, and that training character-level LSTM language models with this sparsification does not significantly change the final performance of the model. The authors argue that this sparsity could be exploited with specialized hardware to improve the energy efficiency and speed of recurrent network training.

NOVELTY
Thresholding gradients to induce sparsity and improve efficiency in RNN training is a novel result to my knowledge.

MISSING CITATIONS
Prior work has explored low-precision arithmetic for recurrent neural network language models:

Hubara et al, “Quantized Neural Networks: Training Neural Networks with
Low Precision Weights and Activations”,

---

CONTRIBUTIONS
When training LSTMs, many of the intermediate gradients are close to zero due to the flat shape of the tanh and sigmoid nonlinearities far from the origin. This paper shows that rounding these small gradients to zero results in matrices with up to 80% sparsity during training, and that training character-level LSTM language models with this sparsification does not significantly change the final performance of the model. The authors argue that this sparsity could be exploited with specialized hardware to improve the energy efficiency and speed of recurrent network training.

NOVELTY
Thresholding gradients to induce sparsity and improve efficiency in RNN training is a novel result to my knowledge.

MISSING CITATIONS
Prior work has explored low-precision arithmetic for recurrent neural network language models:

Hubara et al, “Quantized Neural Networks: Training Neural Networks with
Low Precision Weights and Activations”,

---

The main point of the paper was that sparsifying gradients does not hurt performance; however, this in itself is not enough for a publication in this venue. As noted by R1 and R2, showing how this can help in more energy efficient training would make for a good paper; without that aspect the paper only presents an observation that is not too surprising to the practitioners in this area.
 
 Further, while the main point of the paper was relatively clear, the scientific presentation was not rigorous enough. All the reviewers point out that several details were missing (including test set performance, reporting of results on the different sets, etc). 
 
 Paper would be strengthened by a better exploration of the problem.

---

The findings of applying sparsity in the backward gradients for training LSTMs is interesting. 

But the paper seems incomplete without the proper experimental justification. Only the validation loss is reported which is definitely insufficient. Proper testing results and commonly reported evaluation criterion needs to be included to support the claim of no degradation when applying the proposed sparsity technique. 

Also actual justification of the gains in terms of speed and efficiency would make the paper much stronger.

---

This paper presents the observation that it is possible to utilize sparse operations in the training of LSTM networks without loss of accuracy. This observation is novel (although not too surprising) to my knowledge, but I must state that I am not very familiar with research on fast RNN implmentations.

Minor note:
The LSTM language model does not use a 'word2vec' layer. It is simply a linear embedding layer. Word2vec is the name of a specific model which is not directly to character level language models.

The paper presents the central observation clearly. However, it will be much more convincing if a well known dataset and experiment set up are used, such as Graves (2013) or Sutskever et al (2014), and actual training, validation and test performances are reported.

While the main observation is certainly interesting, I think it is not sufficient to be the subject of a full conference paper without implementation (or simulation) and benchmarking of the promised speedups on multiple tasks. For example, how would the gains be affected by various architecture choices?

At present this is an interesting technical report and I would like to see more detailed results in the future.

---

CONTRIBUTIONS
When training LSTMs, many of the intermediate gradients are close to zero due to the flat shape of the tanh and sigmoid nonlinearities far from the origin. This paper shows that rounding these small gradients to zero results in matrices with up to 80% sparsity during training, and that training character-level LSTM language models with this sparsification does not significantly change the final performance of the model. The authors argue that this sparsity could be exploited with specialized hardware to improve the energy efficiency and speed of recurrent network training.

NOVELTY
Thresholding gradients to induce sparsity and improve efficiency in RNN training is a novel result to my knowledge.

MISSING CITATIONS
Prior work has explored low-precision arithmetic for recurrent neural network language models:

Hubara et al, “Quantized Neural Networks: Training Neural Networks with
Low Precision Weights and Activations”,",,,,,,4.333333333333333,,,3.6666666666666665,,
775,"LEARNING LOCOMOTION SKILLS USING DEEPRL: DOES
Authors: SPACE MATTER, Xue Bin Peng, Michiel van de Panne
Source file: 775.pdf

ABSTRACT
The use of deep reinforcement learning allows for high-dimensional state descriptors, but little is known about how the choice of action representation impacts the learning difficulty and the resulting performance. We compare the impact of four different action parameterizations (torques, muscle-activations, target joint angles, and target joint-angle velocities) in terms of learning time, policy robustness, motion quality, and policy query rates. Our results are evaluated on a gaitcycle imitation task for multiple planar articulated figures and multiple gaits. We demonstrate that the local feedback provided by higher-level action parameterizations can significantly impact the learning, robustness, and quality of the resulting policies.

The use of deep reinforcement learning allows for high-dimensional state descriptors, but little is known about how the choice of action representation impacts the learning difficulty and the resulting performance. We compare the impact of four different action parameterizations (torques, muscle-activations, target joint angles, and target joint-angle velocities) in terms of learning time, policy robustness, motion quality, and policy query rates. Our results are evaluated on a gaitcycle imitation task for multiple planar articulated figures and multiple gaits. We demonstrate that the local feedback provided by higher-level action parameterizations can significantly impact the learning, robustness, and quality of the resulting policies.

1 INTRODUCTION
The introduction of deep learning models to reinforcement learning (RL) has enabled policies to operate directly on high-dimensional, low-level state features. As a result, deep reinforcement learning (DeepRL) has demonstrated impressive capabilities, such as developing control policies that can map from input image pixels to output joint torques (Lillicrap et al., 2015). However, the quality and robustness often falls short of what has been achieved with hand-crafted action abstractions, e.g., Coros et al. (2011); Geijtenbeek et al. (2013). While much is known about the learning of state representations, the choice of action parameterization is a design decision whose impact is not yet well understood.
Joint torques can be thought of as the most basic and generic representation for driving the movement of articulated figures, given that muscles and other actuation models eventually result in joint torques. However this ignores the intrinsic embodied nature of biological systems, particularly the synergy between control and biomechanics. Passive-dynamics, such as elasticity and damping from muscles and tendons, play an integral role in shaping motions: they provide mechanisms for energy storage, and mechanical impedance which generates instantaneous feedback without requiring any explicit computation. Loeb coins the term preflexes (Loeb, 1995) to describe these effects, and their impact on motion control has been described as providing intelligence by mechanics (Blickhan et al., 2007). This can also be thought of as a kind of partitioning of the computations between the control and physical system.
In this paper we explore the impact of four different actuation models on learning to control dynamic articulated figure locomotion: (1) torques (Tor); (2) activations for musculotendon units (MTU); (3) target joint angles for proportional-derivative controllers (PD); and (4) target joint velocities (Vel). Because Deep RL methods are capable of learning control policies for all these models, it now becomes possible to directly assess how the choice of actuation model affects the learning difficulty. We also assess the learned policies with respect to robustness, motion quality, and policy query rates. We show that action spaces which incorporate local feedback can significantly improve learning speed and performance, while still preserving the generality afforded by torque-level control. Such parameterizations also allow for more complex body structures and subjective improvements in motion quality.
Our specific contributions are: (1) We introduce a DeepRL framework for motion imitation tasks; (2) We evaluate the impact of four different actuation models on learned control policies according to four criteria; and (3) We propose an optimization approach that combines policy learning and actuator optimization, allowing neural networks to effective control complex muscle models.

2 BACKGROUND
Our task will be structured as a standard reinforcement problem where an agent interacts with its environment according to a policy in order to maximize a reward signal. The policy π(s, a) = p(a|s) represents the conditional probability density function of selecting action a ∈ A in state s ∈ S. At each control step t, the agent observes a state st and samples an action at from π. The environment in turn responds with a scalar reward rt, and a new state s′t = st+1 sampled from its dynamics p(s′|s, a). For a parameterized policy πθ(s, a), the goal of the agent is learn the parameters θ which maximizes the expected cumulative reward
J(πθ) = E [ T∑ t=0 γtrt ∣∣∣∣∣πθ ]
with γ ∈ [0, 1] as the discount factor, and T as the horizon. The gradient of the expected reward OθJ(πθ) can be determined according to the policy gradient theorem (Sutton et al., 2001), which provides a direction of improvement to adjust the policy parameters θ.
OθJ(πθ) = ∫ S dθ(s) ∫ A Oθlog(πθ(s, a))A(s, a)da ds
where dθ(s) = ∫ S ∑T t=0 γ
tp0(s0)p(s0 → s|t, πθ)ds0 is the discounted state distribution, where p0(s) represents the initial state distribution, and p(s0 → s|t, πθ) models the likelihood of reaching state s by starting at s0 and following the policy πθ(s, a) for t steps (Silver et al., 2014). A(s, a) represents a generalized advantage function. The choice of advantage function gives rise to a family of policy gradient algorithms, but in this work, we will focus on the one-step temporal difference advantage function (Schulman et al., 2015)
A(st, at) = rt + γV (s ′ t)− V (st) where V (s) = E [∑T
t=0 γ trt ∣∣∣s0 = s, πθ] is the state-value function, and can be defined recursively via the Bellman equation
V (st) = E rt,s′t
[rt + γV (s ′ t)|st, πθ]
A parameterized value function Vφ(s), with parameters φ, can be learned iteratively in a manner similar to Q-Learning by minimizing the Bellman loss,
L(φ) = E st,rt,s′t
[ 1
2 (yt − Vφ(st))2
] , yt = rt + γVφ(s ′ t)
πθ and Vφ can be trained in tandem using an actor-critic framework (Konda & Tsitsiklis, 2000).
In this work, each policy will be represented as a gaussian distribution with a parameterized mean µθ(s) and fixed covariance matrix Σ = diag{σ2i }, where σi is manually specified for each action parameter. Actions can be sampled from the distribution by applying gaussian noise to the mean action at = µθ(st) +N (0,Σ) The corresponding policy gradient will assume the form
OθJ(πθ) = ∫ S dθ(s) ∫ A Oθµθ(s)Σ −1 (a− µθ(s))A(s, a)da ds
which can be interpreted as shifting the mean of the action distribution towards actions that lead to higher than expected rewards, while moving away from actions that lead to lower than expected rewards.

3 TASK REPRESENTATION

3.1 REFERENCE MOTION
In our task, the goal of a policy is to imitate a given reference motion {q∗t } which consists of a sequence of kinematic poses q∗t in reduced coordinates. The reference velocity q̇∗t at a given time t is approximated by finite-difference q̇∗t ≈ q∗t+4t−q ∗ t
4t . Reference motions are generated via either using a recorded simulation result from a preexisting controller (“Sim”), or via manually-authored keyframes. Since hand-crafted reference motions may not be physically realizable, the goal is to closely reproduce a motion while satisfying physical constraints.

3.2 STATES
To define the state of the agent, a feature transformation Φ(q, q̇) is used to extract a set of features from the reduced-coordinate pose q and velocity q̇. The features consist of the height of the root (pelvis) from the ground, the position of each link with respect to the root, and the center of mass velocity of each link. When training a policy to imitate a cyclic reference motion {q∗t }, knowledge of the motion phase can help simplify learning. Therefore, we augment the state features with a set of target features Φ(q∗t , q̇t ∗), resulting in a combined state represented by st = (Φ(qt, q̇t),Φ(q∗t , q̇t ∗)). Similar results can also be achieved by providing a single motion phase variable as a state feature, as we show in Figure 15 (supplemental material).

3.3 ACTIONS
We train separate policies for each of the four actuation models, as described below. Each actuation model also has related actuation parameters, such as feedback gains for PD-controllers and musculotendon properties for MTUs. These parameters can be manually specified, as we do for the PD and Vel models, or they can be optimized for the task at hand, as for the MTU models. Table 1 provides a list of actuator parameters for each actuation model.
Target Joint Angles (PD): Each action represents a set of target angles q̂, where q̂i specifies the target angles for joint i. q̂ is applied to PD-controllers which compute torques according to τ i = kip(q̂ i − qi) + kid(ˆ̇qi − q̇i), where ˆ̇qi = 0, and kip and kid are manually-specified gains.
Target Joint Velocities (Vel): Each action specifies a set of target velocities ˆ̇q which are used to compute torques according to τ i = kid(ˆ̇q
i − q̇i), where the gains kid are specified to be the same as those used for target angles.
Torques (Tor): Each action directly specifies torques for every joint, and constant torques are applied for the duration of a control step. Due to torque limits, actions are bounded by manually specified limits for each joint. Unlike the other actuation models, the torque model does not require additional actuator parameters, and can thus be regarded as requiring the least amount of domain knowledge. Torque limits are excluded from the actuator parameter set as they are common for all parameterizations.
Muscle Activations (MTU): Each action specifies activations for a set of musculotendon units (MTU). Detailed modeling and implementation information are available in Wang et al. (2012). Each MTU is modeled as a contractile element (CE) attached to a serial elastic element (SE) and parallel elastic element (PE). The force exerted by the MTU can be calculated according to FMTU = FSE = FCE + FPE . Both FSE and FPE are modeled as passive springs, while FCE is actively controlled according to FCE = aMTUF0fl(lCE)fv(vCE), with aMTU being the muscle activation, F0 the maximum isometric force, lCE and vCE being the length and velocity of the contractile element. The functions fl(lCE) and fv(vCE) represent the force-length and force-velocity relationships, modeling the variations in the maximum force that can be exerted by a muscle as a function of its length and contraction velocity. Analytic forms are available in Geyer et al. (2003). Activations are bounded between [0, 1]. The length of each contractile element lCE are included as state features. To simplify control and reduce the number of internal state parameters per MTU, the policies directly control muscle activations instead of indirectly through excitations (Wang et al., 2012).

3.4 REWARD
The reward function consists of a weighted sum of terms that encourage the policy to track a reference motion.
r = wposerpose + wvelrvel + wendrend + wrootrroot + wcomrcom
wpose = 0.5, wvel = 0.05, wend = 0.15, wroot = 0.1, wcom = 0.2
Details of each term are available in the supplemental material. rpose penalizes deviation of the character pose from the reference pose, and rvel penalizes deviation of the joint velocities. rend and rroot accounts for the position error of the end-effectors and root. rcom penalizes deviations in the center of mass velocity from that of the reference motion.

3.5 INITIAL STATE DISTRIBUTION
We design the initial state distribution, p0(s), to sample states uniformly along the reference trajectory. At the start of each episode, q∗ and q̇∗ are sampled from the reference trajectory, and used to initialize the pose and velocity of the agent. This helps guide the agent to explore states near the target trajectory.

4 ACTOR-CRITIC LEARNING ALGORITHM
Instead of directly using the temporal difference advantage function, we adapt a positive temporal difference (PTD) update as proposed by Van Hasselt (2012).
A(s, a) = I [δ > 0] =
{ 1, δ > 0
0, otherwise
δ = r + γV (s′)− V (s) Unlike more conventional policy gradient methods, PTD is less sensitive to the scale of the advantage function and avoids instabilities that can result from negative TD updates. For a Gaussian policy, a negative TD update moves the mean of the distribution away from an observed action, effectively shifting the mean towards an unknown action that may be no better than the current mean action (Van Hasselt, 2012). In expectation, these updates converges to the true policy gradient, but for stochastic estimates of the policy gradient, these updates can cause the agent to adopt undesirable behaviours which affect subsequent experiences collected by the agent. Furthermore, we incorporate experience replay, which has been demonstrated to improve stability when training neural network policies with Q-learning in discrete action spaces. Experience replay often requires off-policy methods, such as importance weighting, to account for differences between the policy being trained and the behavior policy used to generate experiences (WawrzyńSki & Tanwani, 2013). However, we have not found importance weighting to be beneficial for PTD.
Stochastic policies are used during training for exploration, while deterministic policy are deployed for evaluation at runtime. The choice between a stochastic and deterministic policy can be specified by the addition of a binary indicator variable λ ∈ [0, 1]
at = µθ(st) + λN (0,Σ)
where λ = 1 corresponds to a stochastic policy with exploration noise, and λ = 0 corresponds to a deterministic policy that always selects the mean of the distribution. Noise from a stochastic policy will result in a state distribution that differs from that of the deterministic policy at runtime. To imitate this discrepancy, we incorporate -greedy exploration in addition to the original Gaussian exploration. During training, λ is determined by a Bernoulli random variable λ ∼ Ber( ), where λ = 1 with probability ∈ [0, 1]. The exploration rate is annealed linearly from 1 to 0.2 over 500k iterations, which slowly adjusts the state distribution encountered during training to better resemble the distribution at runtime. Since the policy gradient is defined for stochastic policies, only tuples recorded with exploration noise (i.e. λ = 1) can be used to update the actor, while the critic can be updated using all tuples.
Training proceeds episodically, where the initial state of each episode is sampled from p0(s), and the episode duration is drawn from an exponential distribution with a mean of 2s. To discourage falling, an episode will also terminate if any part of the character’s trunk makes contact with the ground for an extended period of time, leaving the agent with zero reward for all subsequent steps. Algorithm 1 in the supplemental material summarizes the complete learning process.
MTU Actuator Optimization: Actuation models such as MTUs are defined by further parameters whose values impact performance (Geijtenbeek et al., 2013). Geyer et al. (2003) uses existing anatomical estimates for humans to determine MTU parameters, but such data is not be available for more arbitrary creatures. Alternatively, Geijtenbeek et al. (2013) uses covariance matrix adaptation (CMA), a derivative-free evolutionary search strategy, to simultaneously optimize MTU and policy parameters. This approach is limited to policies with reasonably low dimensional parameter spaces, and is thus ill-suited for neural network models with hundreds of thousands of parameters. To avoid manual-tuning of actuator parameters, we propose a heuristic approach that alternates between policy learning and actuator optimization, as detailed in the supplemental material.

5 RESULTS
The motions are best seen in the supplemental video https://youtu.be/L3vDo3nLI98. We evaluate the action parameterizations by training policies for a simulated 2D biped, dog, and raptor as shown in Figure 1. Depending on the agent and the actuation model, our systems have 58–214 state dimensions, 6–44 action dimensions, and 0–282 actuator parameters, as summarized in Table 3 (supplemental materials). The MTU models have at least double the number of action parameters because they come in antagonistic pairs. As well, additional MTUs are used for the legs to more accurately reflect bipedal biomechanics. This includes MTUs that span multiple joints.
Each policy is represented by a three layer neural network, as illustrated in Figure 8 (supplemental material) with 512 and 256 fully-connected units, followed by a linear output layer where the number of output units vary according to the number of action parameters for each character and actuation model. ReLU activation functions are used for both hidden layers. Each network has approximately 200k parameters. The value function is represented by a similar network, except having a single linear output unit. The policies are queried at 60Hz for a control step of about 0.0167s. Each network is randomly initialized and trained for about 1 million iterations, requiring 32 million tuples, the equivalent of approximately 6 days of simulated time. Each policy requires about 10 hours for the biped, and 20 hours for the raptor and dog on an 8-core Intel Xeon E5-2687W.
Only the actuator parameters for MTUs are optimized with Algorithm 2, since the parameters for the other actuation models are few and reasonably intuitive to determine. The initial actuator parameters ψ0 are manually specified, while the initial policy parameters θ0 are randomly initialized. Each pass optimizes ψ using CMA for 250 generations with 16 samples per generation, and θ is trained for 250k iterations. Parameters are initialized with values from the previous pass. The expected value of each CMA sample of ψ is estimated using the average cumulative reward over 16 rollouts with a duration of 10s each. Separate MTU parameters are optimized for each character and motion. Each set of parameters is optimized for 6 passes following Algorithm 2, requiring approximately 50 hours. Figure 5 illustrates the performance improvement per pass. Figure 6 compares the performance of MTUs before and after optimization. For most examples, the optimized actuator parameters significantly improve learning speed and final performance. For the sake of comparison, after a set of actuator parameters has been optimized, a new policy is retrained with the new actuator parameters and its performance compared to the other actuation models.
Policy Performance and Learning Speed: Figure 2 shows learning curves for the policies and the performance of the final policies are summarized in Table 4. Performance is evaluated using the normalized cumulative reward (NCR), calculated from the average cumulative reward over 32 episodes with lengths of 10s, and normalized by the maximum and minimum cumulative reward possible for each episode. No discounting is applied when calculating the NCR. The initial state of each episode is sampled from the reference motion according to p(s0). To compare learning speeds, we use the normalized area under each learning curve (AUC) as a proxy for the learning speed of a particular actuation model, where 0 represents the worst possible performance and no progress during training, and 1 represents the best possible performance without requiring training.
PD performs well across all examples, achieving comparable-to-the-best performance for all motions. PD also learns faster than the other parameterizations for 5 of the 7 motions. The final performance of Tor is among the poorest for all the motions. Differences in performance appear more pronounced as characters become more complex. For the simple 7-link biped, most parameterizations achieve similar performance. However, for the more complex dog and raptor, the performance of Tor policies deteriorate with respect to other policies such as PD and Vel. MTU policies often exhibited the slowest learning speed, which may be a consequence of the higher dimensional action spaces, i.e., requiring antagonistic muscle pairs, and complex muscle dynamics. Nonetheless, once optimized, the MTU policies produce more natural motions and responsive behaviors as compared to other parameterizations. We note that the naturalness of motions is not well captured by the reward, since it primarily gauges similarity to the reference motion, which may not be representative of natural responses when perturbed from the nominal trajectory. A sensitivity analysis of the policies’ performance to variations in network architecture and hyperparameters are available in the supplemental material.
Policy Robustness: To evaluate robustness, we recorded the NCR achieved by each policy when subjected to external perturbations. The perturbations assume the form of random forces applied
to the trunk of the characters. Figure 3 illustrates the performance of the policies when subjected to perturbations of different magnitudes. The magnitude of the forces are constant, but direction varies randomly. Each force is applied for 0.1 to 0.4s, with 1 to 4s between each perturbation. Performance is estimated using the average over 128 episodes of length 20s each. For the biped walk, the Tor policy is significantly less robust than those for the other types of actions, while the MTU policy is the least robust for the raptor run. Overall, the PD policies are among the most robust for all the motions. In addition to external forces, we also evaluate robustness over randomly generated terrain consisting of bumps with varying heights and slopes with varying steepness. We evaluate the performance on irregular terrain (Figure 12, supplemental material). There are few discernible patterns for this test. The Vel and MTU policies are significantly worse than the Tor and PD policies for the dog bound on the bumpy terrain. The unnatural jittery behavior of the dog Tor policy proves to be surprisingly robust for this scenario. We suspect that the behavior prevents the trunk from contacting the ground for extended periods for time, and thereby escaping our system’s fall detection.
Query Rate: Figure 4 compares the performance of different parameterizations for different policy query rates. Separate policies are trained with queries of 15Hz, 30Hz, 60Hz, and 120Hz. Actuation models that incorporate low-level feedback such as PD and Vel, appear to cope more effectively to lower query rates, while the Tor degrades more rapidly at lower query rates. It is not yet obvious to us why MTU policies appear to perform better at lower query rates and worse at higher rates. Lastly, Figure 14 shows the policy outputs as a function of time for the four actuation models, for a particular joint, as well as showing the resulting joint torque. Interestingly, the MTU action is visibly smoother than the other actions and results in joint torques profiles that are smoother than those seen for PD and Vel.

6 RELATED WORK
DeepRL has driven impressive recent advances in learning motion control, i.e., solving for continuous-action control problems using reinforcement learning. All four of the actions types that we explore have seen previous use in the machine learning literature. WawrzyńSki & Tanwani (2013) use an actor-critic approach with experience replay to learn skills for an octopus arm (actuated by a simple muscle model) and a planar half cheetah (actuated by joint-based PD-controllers).
Recent work on deterministic policy gradients (Lillicrap et al., 2015) and on RL benchmarks, e.g., OpenAI Gym, generally use joint torques as the action space, as do the test suites in recent work (Schulman et al., 2015) on using generalized advantage estimation. Other recent work uses: the PR2 effort control interface as a proxy for torque control (Levine et al., 2015); joint velocities (Gu et al., 2016); velocities under an implicit control policy (Mordatch et al., 2015); or provide abstract actions (Hausknecht & Stone, 2015). Our learning procedures are based on prior work using actorcritic approaches with positive temporal difference updates (Van Hasselt, 2012).
Work in biomechanics has long recognized the embodied nature of the control problem and the view that musculotendon systems provide “preflexes” (Loeb, 1995) that effectively provide a form intelligence by mechanics (Blickhan et al., 2007), as well as allowing for energy storage. The control strategies for physics-based character simulations in computer animation also use all the forms of actuation that we evaluate in this paper. Representative examples include quadratic programs that solve for joint torques (de Lasa et al., 2010), joint velocities for skilled bicycle stunts (Tan et al., 2014), muscle models for locomotion (Wang et al., 2012; Geijtenbeek et al., 2013), mixed use of feed-forward torques and joint target angles (Coros et al., 2011), and joint target angles computed by learned linear (time-indexed) feedback strategies (Liu et al., 2016). Lastly, control methods in robotics use a mix of actuation types, including direct-drive torques (or their virtualized equivalents), series elastic actuators, PD control, and velocity control. These methods often rely heavily on modelbased solutions and thus we do not describe these in further detail here.

7 CONCLUSIONS
Our experiments suggest that action parameterizations that include basic local feedback, such as PD target angles, MTU activations, or target velocities, can improve policy performance and learning speed across different motions and character morphologies. Such models more accurately reflect the embodied nature of control in biomechanical systems, and the role of mechanical components in shaping the overall dynamics of motions and their control. The difference between low-level and high-level action parameterizations grow with the complexity of the characters, with high-level parameterizations scaling more gracefully to complex characters. As a caveat, there may well be tasks, such as impedance control, where lower-level action parameterizations such as Tor may prove advantageous. We believe that no single action parameterization will be the best for all problems. However, since objectives for motion control problems are often naturally expressed in terms of kinematic properties, higher-level actions such as target joint angles and velocities may be effective for a wide variety of motion control problems. We hope that our work will help open discussions around the choice of action parameterizations.
Our results have only been demonstrated on planar articulated figure simulations; the extension to 3D currently remains as future work. Furthermore, our current torque limits are still large as compared to what might be physically realizable. Tuning actuator parameters for complex actuation models such as MTUs remains challenging. Though our actuator optimization technique is able to improve performance as compared to manual tuning, the resulting parameters may still not be optimal for the desired task. Therefore, our comparisons of MTUs to other action parameterizations may not be reflective of the full potential of MTUs with more optimal actuator parameters. Furthermore, our actuator optimization currently tunes parameters for a specific motion, rather than a larger suite of motions, as might be expected in nature.
Since the reward terms are mainly calculated according to joint positions and velocities, it may seem that it is inherently biased in favour of PD and Vel. However, the real challenges for the control policies lie elsewhere, such as learning to compensate for gravity and ground-reaction forces, and learning foot-placement strategies that are needed to maintain balance for the locomotion gaits. The reference pose terms provide little information on how to achieve these hidden aspects of motion control that will ultimately determine the success of the locomotion policy. While we have yet to provide a concrete answer for the generalization of our results to different reward functions, we believe that the choice of action parameterization is a design decision that deserves greater attention regardless of the choice of reward function.
Finally, it is reasonable to expect that evolutionary processes would result in the effective co-design of actuation mechanics and control capabilities. Developing optimization and learning algorithms to allow for this kind of co-design is a fascinating possibility for future work.
","The use of deep reinforcement learning allows for high-dimensional state descriptors, but little is known about how the choice of action representation impacts the learning difficulty and the resulting performance. We compare the impact of four different action parameterizations (torques, muscle-activations, target joint angles, and target joint-angle velocities) in terms of learning time, policy robustness, motion quality, and policy query rates. Our results are evaluated on a gaitcycle imitation task for multiple planar articulated figures and multiple gaits. We demonstrate that the local feedback provided by higher-level action parameterizations can significantly impact the learning, robustness, and quality of the resulting policies.",ICLR 2017 conference submission,False,,"This paper addresses a question that is often overlooked in reinforcement learning or locomotion experiment.

My biggest point of critique is that it's difficult to draw conclusions or reason beyond the results of the experiments. 
The authors only consider a single neural network architecture and a single reward function. For example, is the torque controller limited by the policy network?  
My suggestion is to vary the number of neurons or show that the same results hold for a different state representation (e.g. trained on pixel data). In the paper's current form, the term ""DeepRL"" seems arbitrary.

On the positive side, the paper is well-structured and easy to read. The experiments are sound, clear and easy to interpret. 
It's definitely an interesting line of work and beyond the extension to 3D, I would argue that considering more realistic physical constraints (e.g. actuator constraints, communication delays etc. on real robots) could greatly improve the impact of this work.

---

After reading the paper and the reviews, I believe that the paper presents a solid contribution and a detailed empirical exploration of the choice of action space representations for continuous control trajectory tracking tasks, but has limited relevance to the ICLR audience in its present form.
 
 The conclusion that PD controllers with learned gains provide improved learning speed and sometimes better final results than the ""default"" joint torque representation is intriguing and has some implications for future work on trajectory tracking for simulated articulated rigid body characters, but it's unclear from the current results what conclusion there is to take away for general algorithm or model design. Since the only evaluation is on trajectory tracking, it's also not even clear (as pointed out by other reviewers) to what degree these results will generalize to other tasks. In fact, a contrarian view might be that PD tracking is specifically a good fit for trajectory tracking with a known reference trajectory, but might be a poor fit for accomplishing a particular task, where more open-loop behaviors might be more optimal. The passive compliance of MTUs is also shown to often not be beneficial, but it's again not clear whether this might in fact be an artifact of the trajectory tracking task.
 
 But perhaps more problematically, the primary conclusions of the paper are of limited relevance when it comes to design of algorithms or models (or at least, the authors don't discuss this), but are more relevant for practitioners interested in applying deep RL for learning controllers for articulated rigid body systems such as robots or virtual characters. As such, it's not clear what fraction of the ICLR audience will find the paper relevant to their interests.
 
 I would strongly encourage the authors to continue their research: there is a lot to like about the present paper, including an elegant reinforcement learning methodology, rigorous empirical results for the specific case of trajectory following, and some very nice videos and examples.

---

We would like to thank the reviewers again for their feedback and suggestions. We have revised the paper to better address the points raised in the reviews. To help track the changes, we have highlighted them in blue. On page 15 of the supplemental material, you will find a new set experiments comparing performance across different initializations of the network, different network architectures, and sensitivity to the amount of exploration noise applied during training. We hope these new additions will help to address some of the previously raised concerns.

We look forward to receiving additional feedback from the reviewers.
Thank you,

---

Thank you for the feedback and comments in the reviews + questions.
Please find below a summary of our further reflections, which we will be incorporating into the paper.

Additional experiments

We are currently performing several additional experiments, which we will add to the next version of the paper:
- Analysis of variance of performance for multiple runs of training (Anon Rev 2)
- Sensitivity to exploration noise, i.e., covariance matrix, (Anon Rev 2)
- Impact of network architecture (Anon Rev 1)

Re:   Relevance of paper to ICLR audience

1) While much is known about learning input (state) representations, much less is known about the extent to which output (action) representations matter for continuous control policies, and whether useful “output representations” can be learned. This paper doesn’t provide all the answers, but it shows that output representations do matter (for locomotion): they significantly impact the learning rate, robustness, overall performance, and policy query rate.

2) Many methods for continuous action spaces use physics-based simulations of locomotion as one of their key examples. However, the use of torques as the defacto unit of action is rather arbitrary; it ignores the notion that the passive properties of  actuation mechanisms, e.g., muscles, can make a significant contribution to motion control, i.e., “intelligence by mechanics” (Blickhan et al., 2007).This can be thought of as a kind of partitioning of the computations between the control and the physical system. Our paper provides a first look at the impact of such “partitioning”.

3) A question that we would like to draw more attention to is: should musculo-tendon systems (as seen in nature) and other actuation details be considered as part of the control policy or part of the environment?  I.e., can they be safely ignored because the control policy would “learn the useful aspects” anyhow? 

Re: generalization of results

Here are our thoughts on this;  we will update the paper to reflect these.

1) Admittedly, we do not provide a concrete answer for the generalization to arbitrary objective functions, and we will clearly state this in the paper. However physics-based locomotion is commonly used as an example in continuous-action RL work. While we do optimize for only one reward function (imitation), we do explore results for different characters, different motions, different query rates, and overall policy robustness. We believe that these are all critical dimensions for testing capabilities and generalization for locomotion control. The imitation objective can also be used as a motion prior and so it is likely to be useful more generally.  It further allows us to produce more natural motions (subjectively speaking) than previously seen for similar work. We hope that the capability of learning more natural locomotion policies, even if this arrives in part via the objective function rather than a theoretical advancement, will be of broad interest to the ICLR community.

2) We believe that no single action parameterization will be the best for all problems. However, since objectives for motion control problems are often naturally expressed in terms of kinematic properties, higher-level actions such as target joint angles and velocities may be effective for a wide variety of motion control problems. We hope that our work will help open discussions around the choice of action parameterizations.

Re: specificity of results to the use of a reference pose cost

While the reward terms are mainly calculated according to joint positions and velocities, the real challenge for the control policy lies with: (a) learning to compensate for various state-dependent forces, such as gravity and ground-reaction forces, and (b) learning strategies such as foot-placement that are needed to maintain balance for all the locomotion gaits. The reference pose terms provides no information on how to achieve these “hidden” aspects of motion control that will ultimately determine the success of the locomotion policy. There is also only a weak correlation between the action space and the actual reference poses for any of the action spaces; the action space trajectories illustrated in Figure 11 are all quite different from the actual right hip angle trajectory (not shown, but it varies smoothly over time).

Re: considering more realistic physical constraints (e.g. actuator constraints, communication delays etc. on real robots) could greatly improve the impact of this work. (Anon Rev 1)

We enforce torque limits and joint limits, which are shared across all actuation models. We do not currently include activation time delays for any of the models.

---

The paper is straightforward, easy to read, and has clear results. 

Since all these parameterisations end up outputting torques, it seems like there shouldn't be much difference between them. There is a known function that convert from one representation to another (or at least to torques). Is it not possible that the only reason proportional control is a little better is that the tracking cost is a function of positions?

Would we get the same result if there was no reference-pose cost, only a locomotion cost?

Would we get the same result if the task was to spin a top? My guess is no. 

This work is interesting, but not likely to generalise to other scenarios, and in that sense is rather limited.

The video is nice.

---

Paper studies deep reinforcement learning paradigm for controlling high dimensional characters. Experiments compare the effect different control parameterizations (torques, muscle-activations, PD control with target joint positions and target joint velocities) have on the performance of reinforcement learning and optimized control policies. Evaluated are different planer gate cycle trajectories. It is illustrated that more abstract parameterizations are in fact better and result in more robust and higher quality policies. 

> Significance & Originality:

The explored parameterizations are relatively standard in humanoid control. The real novelty is systematic evaluation of the various parameterizations. I think this type of study is important and insightful. However, the findings are very specific to the problem and specific tested architecture. Its not clear that findings will transferable to other networks on other control problems/domains. As such for the ICLR community, this may have limited breadth and perhaps would have broader appeal in robotics / graphics community. 

> Clarity:

The paper is well written and is pretty easy to understand for someone who has some background with constrained multi-body simulation and control. 

> Experiments:

Experimental validation is lacking somewhat in my opinion. Given that this is a fundamentally experimental paper, I would have liked to see more analysis of sensitivity to various parameters and analysis of variance of performance when policy is optimized multiple times.

---

This paper addresses a question that is often overlooked in reinforcement learning or locomotion experiment.

My biggest point of critique is that it's difficult to draw conclusions or reason beyond the results of the experiments. 
The authors only consider a single neural network architecture and a single reward function. For example, is the torque controller limited by the policy network?  
My suggestion is to vary the number of neurons or show that the same results hold for a different state representation (e.g. trained on pixel data). In the paper's current form, the term ""DeepRL"" seems arbitrary.

On the positive side, the paper is well-structured and easy to read. The experiments are sound, clear and easy to interpret. 
It's definitely an interesting line of work and beyond the extension to 3D, I would argue that considering more realistic physical constraints (e.g. actuator constraints, communication delays etc. on real robots) could greatly improve the impact of this work.

---

This paper addresses a question that is often overlooked in reinforcement learning or locomotion experiment.

My biggest point of critique is that it's difficult to draw conclusions or reason beyond the results of the experiments. 
The authors only consider a single neural network architecture and a single reward function. For example, is the torque controller limited by the policy network?  
My suggestion is to vary the number of neurons or show that the same results hold for a different state representation (e.g. trained on pixel data). In the paper's current form, the term ""DeepRL"" seems arbitrary.

On the positive side, the paper is well-structured and easy to read. The experiments are sound, clear and easy to interpret. 
It's definitely an interesting line of work and beyond the extension to 3D, I would argue that considering more realistic physical constraints (e.g. actuator constraints, communication delays etc. on real robots) could greatly improve the impact of this work.

---

After reading the paper and the reviews, I believe that the paper presents a solid contribution and a detailed empirical exploration of the choice of action space representations for continuous control trajectory tracking tasks, but has limited relevance to the ICLR audience in its present form.
 
 The conclusion that PD controllers with learned gains provide improved learning speed and sometimes better final results than the ""default"" joint torque representation is intriguing and has some implications for future work on trajectory tracking for simulated articulated rigid body characters, but it's unclear from the current results what conclusion there is to take away for general algorithm or model design. Since the only evaluation is on trajectory tracking, it's also not even clear (as pointed out by other reviewers) to what degree these results will generalize to other tasks. In fact, a contrarian view might be that PD tracking is specifically a good fit for trajectory tracking with a known reference trajectory, but might be a poor fit for accomplishing a particular task, where more open-loop behaviors might be more optimal. The passive compliance of MTUs is also shown to often not be beneficial, but it's again not clear whether this might in fact be an artifact of the trajectory tracking task.
 
 But perhaps more problematically, the primary conclusions of the paper are of limited relevance when it comes to design of algorithms or models (or at least, the authors don't discuss this), but are more relevant for practitioners interested in applying deep RL for learning controllers for articulated rigid body systems such as robots or virtual characters. As such, it's not clear what fraction of the ICLR audience will find the paper relevant to their interests.
 
 I would strongly encourage the authors to continue their research: there is a lot to like about the present paper, including an elegant reinforcement learning methodology, rigorous empirical results for the specific case of trajectory following, and some very nice videos and examples.

---

We would like to thank the reviewers again for their feedback and suggestions. We have revised the paper to better address the points raised in the reviews. To help track the changes, we have highlighted them in blue. On page 15 of the supplemental material, you will find a new set experiments comparing performance across different initializations of the network, different network architectures, and sensitivity to the amount of exploration noise applied during training. We hope these new additions will help to address some of the previously raised concerns.

We look forward to receiving additional feedback from the reviewers.
Thank you,

---

Thank you for the feedback and comments in the reviews + questions.
Please find below a summary of our further reflections, which we will be incorporating into the paper.

Additional experiments

We are currently performing several additional experiments, which we will add to the next version of the paper:
- Analysis of variance of performance for multiple runs of training (Anon Rev 2)
- Sensitivity to exploration noise, i.e., covariance matrix, (Anon Rev 2)
- Impact of network architecture (Anon Rev 1)

Re:   Relevance of paper to ICLR audience

1) While much is known about learning input (state) representations, much less is known about the extent to which output (action) representations matter for continuous control policies, and whether useful “output representations” can be learned. This paper doesn’t provide all the answers, but it shows that output representations do matter (for locomotion): they significantly impact the learning rate, robustness, overall performance, and policy query rate.

2) Many methods for continuous action spaces use physics-based simulations of locomotion as one of their key examples. However, the use of torques as the defacto unit of action is rather arbitrary; it ignores the notion that the passive properties of  actuation mechanisms, e.g., muscles, can make a significant contribution to motion control, i.e., “intelligence by mechanics” (Blickhan et al., 2007).This can be thought of as a kind of partitioning of the computations between the control and the physical system. Our paper provides a first look at the impact of such “partitioning”.

3) A question that we would like to draw more attention to is: should musculo-tendon systems (as seen in nature) and other actuation details be considered as part of the control policy or part of the environment?  I.e., can they be safely ignored because the control policy would “learn the useful aspects” anyhow? 

Re: generalization of results

Here are our thoughts on this;  we will update the paper to reflect these.

1) Admittedly, we do not provide a concrete answer for the generalization to arbitrary objective functions, and we will clearly state this in the paper. However physics-based locomotion is commonly used as an example in continuous-action RL work. While we do optimize for only one reward function (imitation), we do explore results for different characters, different motions, different query rates, and overall policy robustness. We believe that these are all critical dimensions for testing capabilities and generalization for locomotion control. The imitation objective can also be used as a motion prior and so it is likely to be useful more generally.  It further allows us to produce more natural motions (subjectively speaking) than previously seen for similar work. We hope that the capability of learning more natural locomotion policies, even if this arrives in part via the objective function rather than a theoretical advancement, will be of broad interest to the ICLR community.

2) We believe that no single action parameterization will be the best for all problems. However, since objectives for motion control problems are often naturally expressed in terms of kinematic properties, higher-level actions such as target joint angles and velocities may be effective for a wide variety of motion control problems. We hope that our work will help open discussions around the choice of action parameterizations.

Re: specificity of results to the use of a reference pose cost

While the reward terms are mainly calculated according to joint positions and velocities, the real challenge for the control policy lies with: (a) learning to compensate for various state-dependent forces, such as gravity and ground-reaction forces, and (b) learning strategies such as foot-placement that are needed to maintain balance for all the locomotion gaits. The reference pose terms provides no information on how to achieve these “hidden” aspects of motion control that will ultimately determine the success of the locomotion policy. There is also only a weak correlation between the action space and the actual reference poses for any of the action spaces; the action space trajectories illustrated in Figure 11 are all quite different from the actual right hip angle trajectory (not shown, but it varies smoothly over time).

Re: considering more realistic physical constraints (e.g. actuator constraints, communication delays etc. on real robots) could greatly improve the impact of this work. (Anon Rev 1)

We enforce torque limits and joint limits, which are shared across all actuation models. We do not currently include activation time delays for any of the models.

---

The paper is straightforward, easy to read, and has clear results. 

Since all these parameterisations end up outputting torques, it seems like there shouldn't be much difference between them. There is a known function that convert from one representation to another (or at least to torques). Is it not possible that the only reason proportional control is a little better is that the tracking cost is a function of positions?

Would we get the same result if there was no reference-pose cost, only a locomotion cost?

Would we get the same result if the task was to spin a top? My guess is no. 

This work is interesting, but not likely to generalise to other scenarios, and in that sense is rather limited.

The video is nice.

---

Paper studies deep reinforcement learning paradigm for controlling high dimensional characters. Experiments compare the effect different control parameterizations (torques, muscle-activations, PD control with target joint positions and target joint velocities) have on the performance of reinforcement learning and optimized control policies. Evaluated are different planer gate cycle trajectories. It is illustrated that more abstract parameterizations are in fact better and result in more robust and higher quality policies. 

> Significance & Originality:

The explored parameterizations are relatively standard in humanoid control. The real novelty is systematic evaluation of the various parameterizations. I think this type of study is important and insightful. However, the findings are very specific to the problem and specific tested architecture. Its not clear that findings will transferable to other networks on other control problems/domains. As such for the ICLR community, this may have limited breadth and perhaps would have broader appeal in robotics / graphics community. 

> Clarity:

The paper is well written and is pretty easy to understand for someone who has some background with constrained multi-body simulation and control. 

> Experiments:

Experimental validation is lacking somewhat in my opinion. Given that this is a fundamentally experimental paper, I would have liked to see more analysis of sensitivity to various parameters and analysis of variance of performance when policy is optimized multiple times.

---

This paper addresses a question that is often overlooked in reinforcement learning or locomotion experiment.

My biggest point of critique is that it's difficult to draw conclusions or reason beyond the results of the experiments. 
The authors only consider a single neural network architecture and a single reward function. For example, is the torque controller limited by the policy network?  
My suggestion is to vary the number of neurons or show that the same results hold for a different state representation (e.g. trained on pixel data). In the paper's current form, the term ""DeepRL"" seems arbitrary.

On the positive side, the paper is well-structured and easy to read. The experiments are sound, clear and easy to interpret. 
It's definitely an interesting line of work and beyond the extension to 3D, I would argue that considering more realistic physical constraints (e.g. actuator constraints, communication delays etc. on real robots) could greatly improve the impact of this work.",,,,,,6.0,,,3.6666666666666665,,
791,"DEEP UNSUPERVISED LEARNING THROUGH SPATIAL CONTRASTING
Authors: Elad Hoffer, Itay Hubara
Source file: 791.pdf

ABSTRACT
Convolutional networks have marked their place over the last few years as the best performing model for various visual tasks. They are, however, most suited for supervised learning from large amounts of labeled data. Previous attempts have been made to use unlabeled data to improve model performance by applying unsupervised techniques. These attempts require different architectures and training methods. In this work we present a novel approach for unsupervised training of Convolutional networks that is based on contrasting between spatial regions within images. This criterion can be employed within conventional neural networks and optimized using standard techniques such as SGD and backpropagation, thus complementing supervised methods.

1 INTRODUCTION
For the past few years convolutional networks (ConvNets, CNNs) LeCun et al. (1998) have proven themselves as a successful model for vision related tasks Krizhevsky et al. (2012) Mnih et al. (2015) Pinheiro et al. (2015) Razavian et al. (2014). A convolutional network is composed of multiple convolutional and pooling layers, followed by a fully-connected affine transformations. As with other neural network models, each layer is typically followed by a non-linearity transformation such as a rectified-linear unit (ReLU). A convolutional layer is applied by cross correlating an image with a trainable weight filter. This stems from the assumption of stationarity in natural images, which means that parameters learned for one local region in an image can be shared for other regions and images.
Deep learning models, including convolutional networks, are usually trained in a supervised manner, requiring large amounts of labeled data (ranging between thousands to millions of examples per-class for classification tasks) in almost all modern applications. These models are optimized using a variant of stochastic-gradient-descent (SGD) over batches of images sampled from the whole training dataset and their ground truth-labels. Gradient estimation for each one of the optimized parameters is done by back propagating the objective error from the final layer towards the input. This is commonly known as ”backpropagation” Rumelhart et al..
In early works, unsupervised training was used as a part of pre-training procedure to obtain an effective initial state of the model. The network was later fine-tuned in a supervised manner as displayed by Hinton (2007). Such unsupervised pre-training procedures were later abandoned, since they provided no apparent benefit over other initialization heuristics in more careful fully supervised training regimes. This led to the de-facto almost exclusive usage of neural networks in supervised environments.
In this work we will present a novel unsupervised learning criterion for convolutional network based on comparison of features extracted from regions within images. Our experiments indicate that by
using this criterion to pre-train networks we can improve their performance and achieve state-ofthe-art results.

2 PREVIOUS WORKS
Using unsupervised methods to improve performance have been the holy grail of deep learning for the last couple of years and vast research efforts have been focused on that. We hereby give a short overview of the most popular and recent methods that tried to tackle this problem.
AutoEncoders and reconstruction loss These are probably the most popular models for unsupervised learning using neural networks, and ConvNets in particular. Autoencoders are NNs which aim to transform inputs into outputs with the least possible amount of distortion. An Autoencoder is constructed using an encoder G(x;w1) that maps an input to a hidden compressed representation, followed by a decoder F (y;w2), that maps the representation back into the input space. Mathematically, this can be written in the following general form:
x̂ = F (G(x;w1);w2)
The underlying encoder and decoder contain a set of trainable parameters that can be tied together and optimized for a predefined criterion. The encoder and decoder can have different architectures, including fully-connected neural networks, ConvNets and others. The criterion used for training is the reconstruction loss, usually the mean squared error (MSE) between the original input and its reconstruction Zeiler et al. (2010)
min‖x− x̂‖2
This allows an efficient training procedure using the aforementioned backpropagation and SGD techniques. Over the years autoencoders gained fundamental role in unsupervised learning and many modification to the classic architecture were made. Ng (2011) regularized the latent representation to be sparse, Vincent et al. (2008) substituted the input with a noisy version thereof, requiring the model to denoise while reconstructing. Kingma et al. (2014) obtained very promising results with variational autoencoders (VAE). A variational autoencoder model inherits typical autoencoder architecture, but makes strong assumptions concerning the distribution of latent variables. They use variational approach for latent representation learning, which results in an additional loss component which required a new training algorithm called Stochastic Gradient Variational Bayes (SGVB). VAE assumes that the data is generated by a directed graphical model p(x|z) and require the encoder to learn an approximation qw1(z|x) to the posterior distribution pw2(z|x) where w1 and w2 denote the parameters of the encoder and decoder. The objective of the variational autoencoder in that case has the following form:
L(w1, w2, x) = −DKL(qw1(z|x)||pw2(z)) + Eqw1 (z|x) ( log pw2(x|z) ) Recently, a stacked set of denoising autoencoders architectures showed promising results in both semi-supervised and unsupervised tasks. A stacked what-where autoencoder by Zhao et al. (2015) computes a set of complementary variables that enable reconstruction whenever a layer implements a many-to-one mapping. Ladder networks by Rasmus et al. (2015) - use lateral connections and layer-wise cost functions to allow the higher levels of an autoencoder to focus on invariant abstract features.
Exemplar Networks: The unsupervised method introduced byDosovitskiy et al. (2014) takes a different approach to this task and trains the network to discriminate between a set of pseudo-classes. Each pseudo-class is formed by applying multiple transformations to a randomly sampled image patch. The number of pseudo-classes can be as big as the size of the input samples. This criterion ensures that different input samples would be distinguished while providing robustness to the applied transformations. In this work we will explore an alternative method with a similar motivation.
Context prediction Another method for unsupervised learning by context was introduced by Doersch et al. (2015). This method uses an auxiliary criterion of predicting the location of an image patch given another from the same image. This is done by classification to 1 of 9 possible locations. Although the work of Doersch et al. (2015) and ours both use patches from an image to perform unsupervised learning, the methods are quite different. Whereas the former used a classification criterion over the spatial location of each patch within a single image, our work is concerned with comparing patches from several images to each other. We claim that this encourages discriminability between images (which we feel to be important aspect of feature learning), and was not an explicit goal in previous work.
Adversarial Generative Models: This a recently introduced model that can be used in an unsupervised fashion Goodfellow et al. (2014). Adversarial Generative Models uses a set of networks, one trained to discriminate between data sampled from the true underlying distribution (e.g., a set of images), and a separate generative network trained to be an adversary trying to confuse the first network. By propagating the gradient through the paired networks, the model learns to generate samples that are distributed similarly to the source data. As shown by Radford et al. (2015),this model can create useful latent representations for subsequent classification tasks.
Sampling Methods: Methods for training models to discriminate between a very large number of classes often use a noise contrasting criterion. In these methods, roughly speaking, the posterior probability P (t|yt) of the ground-truth target t given the model output on an input sampled from the true distribution yt = F (x) is maximized, while the probability P (t|yn) given a noise measurement y = F (n) is minimized. This was successfully used in a language domain to learn unsupervised representation of words. The most noteworthy case is the word2vec model introduced by Mikolov et al. (2013). When using this setting in language applications, a natural contrasting noise is a smooth approximation of the Unigram distribution. A suitable contrasting distribution is less obvious when data points are sampled from a high dimensional continuous space, such as the case of image patches.

2.1 PROBLEMS WITH CURRENT APPROACHES
Only recently the potential of ConvNets in an unsupervised environment began to bear fruit, still we believe it is not fully uncovered.
The majority of unsupervised optimization criteria currently used are based on variations of reconstruction losses. One limitation of this fact is that a pixel level reconstruction is non-compliant with the idea of a discriminative objective, which is expected to be agnostic to low level information in the input. In addition, it is evident that MSE is not best suited as a measurement to compare images, for example, viewing the possibly large square-error between an image and a single pixel shifted copy of it. Another problem with recent approaches such as Rasmus et al. (2015); Zeiler et al. (2010) is their need to extensively modify the original convolutional network model. This leads to a gap between unsupervised method and the state-of-the-art, supervised, models for classification - which can hurt future attempt to reconcile them in a unified framework, as well as efficiently leverage unlabeled data with otherwise supervised regimes.

3 LEARNING BY COMPARISONS
The most common way to train NN is by defining a loss function between the target values and the network output. Learning by comparison approaches the supervised task from a different angle. The main idea is to use distance comparisons between samples to learn useful representations. For example, we consider relative and qualitative examples of the form X1 is closer to X2 than X1 is to X3. Using a comparative measure with neural network to learn embedding space was introduced in the “Siamese network” framework by Bromley et al. (1993) and later used in the works of Chopra et al. (2005). One use for this methods is when the number of classes is too large or expected to vary over time, as in the case of face verification, where a face contained in an image has to compared against another image of a face. This problem was recently tackled by Schroff et al. (2015) for training a convolutional network model on triplets of examples. There, one image served as an anchor x, and an additional pair of images served as a positive example x+ (containing an instance
of the face of the same person) together with a negative example x−, containing a face of a different person. The training objective was on the embedded distance of the input faces, where the distance between the anchor and positive example is adjusted to be smaller by at least some constant α from the negative distance. More precisely, the loss function used in this case was defined as
L(x, x+, x−) = max {‖F (x)− F (x+)‖2 − ‖F (x)− F (x−)‖2 + α, 0} (1)
where F (x) is the embedding (the output of a convolutional neural network), and α is a predefined margin constant. Another similar model used by Hoffer & Ailon (2015) with triplets comparisons for classification, where examples from the same class were trained to have a lower embedded distance than that of two images from distinct classes. This work introduced a concept of a distance ratio loss, where the defined measure amounted to:
L(x, x+, x−) = e−‖F (x)−F (x+)‖2
e−‖F (x)−F (x+)‖2 + e−‖F (x)−F (x−)‖2 (2)
This loss has a flavor of a probability of a biased coin flip. By ‘pushing’ this probability to zero, we express the objective that pairs of samples coming from distinct classes should be less similar to each other, compared to pairs of samples coming from the same class. It was shown empirical by Balntas et al. (2016) to provide better feature embeddings than the margin based distance loss 1

4 OUR CONTRIBUTION: SPATIAL CONTRASTING
One implicit assumption in convolutional networks, is that features are gradually learned hierarchically, each level in the hierarchy corresponding to a layer in the network. Each spatial location within a layer corresponds to a region in the original image. It is empirically observed that deeper layers tend to contain more ‘abstract’ information from the image. Intuitively, features describing different regions within the same image are likely to be semantically similar (e.g. different parts of an animal), and indeed the corresponding deep representations tend to be similar. Conversely, regions from two probably unrelated images (say, two images chosen at random) tend to be far from each other in the deep representation. This logic is commonly used in modern deep networks such as Szegedy et al. (2015) Lin et al. (2013) He et al. (2015), where a global average pooling is used to aggregate spatial features in the final layer used for classification.
Our suggestion is that this property, often observed as a side effect of supervised applications, can be used as a desired objective when learning deep representations in an unsupervised task. Later, the resulting representation can be used, as typically done, as a starting point or a supervised learning task. We call this idea which we formalize below Spatial contrasting. The spatial contrasting criterion is similar to noise contrasting estimation Gutmann & Hyvärinen (2010) Mnih & Kavukcuoglu (2013), in trying to train a model by maximizing the expected probability on desired inputs, while minimizing it on contrasting sampled measurements.

4.1 FORMULATION
We will concern ourselves with samples of images patches x̃(m) taken from an image x. Our convolutional network model, denoted by F (x), extracts spatial features f so that f (m) = F (x̃(m)) for an image patch x̃(m). We will also define P (f1|f2) as the probability for two features f1, f2 to occur together in the same image. We wish to optimize our model such that for two features representing patches taken from the same image x̃(1)i , x̃ (2) i ∈ xi for which f (1) i = F (x̃ (1) i ) and f (2) i = F (x̃ (2) i ), P (f (1) i |f (2) i ) will be maximized. This means that features from a patch taken from a specific image can effectively predict, under our model, features extracted from other patches in the same image. Conversely, we want our model to minimize P (fi|fj) for i, j being two patches taken from distinct images. Following the logic presented before, we will need to sample contrasting patch x̃(1)j from a different image xj such that P (f (1) i |f (2) i ) > P (f (1) j |f (2) i ), where f (1) j = F (x̃ (1) j ). In order to obtain contrasting samples, we use regions from two random images in the training set. We will use a distance ratio, described earlier
in Eq. (2) for the supervised case, to represent the probability two feature vectors were taken from the same image. The resulting training loss for a pair of images will be defined as
LSC(x1, x2) = − log e−‖f (1) 1 −f (2) 1 ‖2
e−‖f (1) 1 −f (2) 1 ‖2 + e−‖f (1) 1 −f (1) 2 ‖2
(3)
Effectively minimizing a log-probability under the SoftMax measure. This formulation is portrayed in figure 4.1. Since we sample our contrasting sample from the same underlying distribution, we can evaluate this loss considering the image patch as both patch compared (anchor) and contrast symmetrically. The final loss will be the average between these estimations:
L̂SC(x1, x2) = 1
2 [LSC(x1, x2) + LSC(x2, x1)]

4.2 METHOD
Convolutional network are usually trained using SGD over mini-batch of samples, therefore we can extract patches and contrasting patches without changing the network architecture. Each image serves as both anchor and positive patches, for which the corresponding features should be closer, as well as contrasting samples for other images in that batch. For a batch of N images, two samples from each image are taken, and N2 different distance comparisons are made. The final loss is defined as the average distance ratio for all images in the batch:
LSC({x}Ni=1) = 1
N N∑ i=1 LSC(xi, {x}j 6=i) = − 1 N N∑ i=1 log e−‖f (1) i −f (2) i ‖2∑N j=1 e −‖f(1)i −f (2) j ‖2
(4)
Since the criterion is differentiable with respect to its inputs, it is fully compliant with standard methods for training convolutional network and specifically using backpropagation and gradient descent. Furthermore, SC can be applied to any layer in the network hierarchy. In fact, SC can be used at multiple layers within the same convolutional network. The spatial properties of the
features means that we can sample directly from feature space f̃ (m) ∈ f instead of from the original image. Therefore SC has a simple implementation which doesn’t require substation amount of computation. The complete algorithm for batch training is described in Algorithm (1). Similar to the batch normalization (BN) layer Ioffe & Szegedy (2015), a recent usage for batch statistics in neural networks, SC also uses the batch statistics. While BN normalize the input based on the batch statistics, SC sample from it. This can be viewed as a simple sampling from the space of possible features describing a patch of image.
Algorithm 1 Calculation the spatial contrasting loss Require: X = {x}Ni=1 # Training on batches of images
# Get the spatial features for the whole batch of images # Size: N ×Wf ×Hf × C {f}Ni=1 ← ConvNet(X)
# Sample spatial features and calculate embedded distance between all pairs of images for i = 1 to N do f̃ (1) i ← sample(fi)
for j = 1 to N do f̃ (2) j ← sample(fj) Dist(i, j)← ‖f̃ (1)i − f̃ (2) j ‖2
end for end for
# Calculate log SoftMax normalized distances di ← − log e
−Dist(i,i)∑N k=1 e −Dist(i,k)
# Spatial contrasting loss is the mean of distance ratios return 1N ∑N i=1 di

5 EXPERIMENTS
In this section we report empirical results showing that using SC loss as an unsupervised pretraining procedure can improve state-of-the-art performance on subsequent classification. We experimented with MNIST, CIFAR-10 and STL10 datasets. We used modified versions of well studied networks such as those of Lin et al. (2013) and Rasmus et al. (2015). A detailed description of our architecture can be found in 4.
In each one of the experiments, we used the spatial contrasting criterion to train the network on the unlabeled images. In each usage of SC criterion, patch features were sampled from the preceding layer in uniform. We note that spatial size of sampled patches ranged between datasets, where on STL10 and Cifar10 it covered about 30% of the image, MNIST required the use of larger patches covering almost the entire image.Training was done by using SGD with an initial learning rate of 0.1 that was decreased by a factor of 10 whenever the measured loss stopped decreasing. After convergence, we used the trained model as an initialization for a supervised training on the complete labeled dataset. The supervised training was done following the same regime, only starting with a lower initial learning rate of 0.01. We used mild data augmentations, such as small translations and horizontal mirroring. The datasets we used are:
• STL10 (Coates et al. (2011)). This dataset consists of 100, 000 96× 96 colored, unlabeled images, together with another set of 5, 000 labeled training images and 8, 000 test images . The label space consists of 10 object classes.
• Cifar10 (Krizhevsky & Hinton (2009)). The well known CIFAR-10 is an image classification benchmark dataset containing 50, 000 training images and 10, 000 test images. The
All experiments were conducted using the Torch7 framework by Collobert et al. (2011). Code reproducing these results will by available at https://github.com/eladhoffer/ SpatialContrasting.

5.1 RESULTS ON STL10
Since STL10 dataset is comprised of mostly unlabeled data, it is most suitable to highlight the benefits of the spatial contrasting criterion. The initial training was unsupervised, as described earlier, using the entire set of 105, 000 samples (union of the original unlabeled set and labeled training set). The representation outputted by the training, was used to initialize supervised training on the 5, 000 labeled images. Evaluation was done on a separate test set of 8, 000 samples. Comparing with state of the art results, we see an improvement of 7% in test accuracy over the best model by Zhao et al. (2015), setting the SC as best model at 81.3% test classification accuracy (see Table (1)). We note that the results of Dosovitskiy et al. (2014) are achieved with no fine-tuning over labeled examples, which may be unfair to this work. We also compare with the same network, but without SC initialization, which achieves a lower classification of 72.6%. This is an indication that indeed SC managed to leverage unlabeled examples to provide a better initialization point for the supervised model.

5.2 RESULTS ON CIFAR10
For Cifar10 dataset, we use the same setting as Coates & Ng (2012) and Hui (2013) to test a model’s ability to learn from unlabeled images. Here, only 4, 000 samples out of 50, 000 are used with their label annotation, and the rest of the samples can be used only in an unsupervised manner. The final test accuracy is measured on the entire 10, 000 test set. In our experiments, we trained our model using SC criterion on the entire dataset, and then used only 400 labeled samples per class (for a total of 4000) in a supervised regime over the initialized network. The results are compared with previous efforts in Table (2). Using the SC criterion allowed an improvement of 6.8% over a non-initialized model, and achieved a final test accuracy of 79.2%. This is a competitive result with current state-of-the-art models.

5.3 RESULTS ON MNIST
The MNIST dataset is very different in nature from the Cifar10 and STL10 datasets, we experimented earlier. The biggest difference, relevant to this work, is that spatial regions sampled from MNIST images usually provide very little, or no information. Thus, SC is much less suited for MNIST dataset, and was conjured to have little benefit. We still, however, experimented with initializing a model with SC criterion and continuing with a fully-supervised regime over all labeled
examples. We found again that this provided benefit over training the same network without preinitialization, improving results from 0.63% to 0.34% error on test set. As mentioned previously, the effective compared patches of MNIST covered almost the entire image area. This can be attributed to the fact that MNIST requires global features to differentiate between digits. The results, compared with previous attempts are included in Table (3).

6 CONCLUSIONS AND FUTURE WORK
In this work we presented spatial contrasting - a novel unsupervised criterion for training convolutional networks on unlabeled data. Its is based on comparison between spatial features sampled from a number of images. We’ve shown empirically that using spatial contrasting as a pretraining technique to initialize a ConvNet, can improve its performance on a subsequent supervised training. In cases where a lot of unlabeled data is available, such as the STL10 dataset, this translates to state-of-the-art classification accuracy in the final model.
Since the spatial contrasting loss is a differentiable estimation that can be computed within a network parallel to supervised losses, in future work we plan to embed it as a semi-supervised model. This usage will allow to create models that can leverage both labeled an unlabeled data, and can be compared to similar semi-supervised models such as the ladder network Rasmus et al. (2015). It is is also apparent that contrasting can occur in dimensions other than the spatial, the most straightforward is the temporal dimension. This suggests that similar training procedure can be applied on segments of sequences to learn useful representation without explicit supervision.

7 APPENDIX
","Convolutional networks have marked their place over the last few years as the best performing model for various visual tasks. They are, however, most suited for supervised learning from large amounts of labeled data. Previous attempts have been made to use unlabeled data to improve model performance by applying unsupervised techniques. These attempts require different architectures and training methods. In this work we present a novel approach for unsupervised training of Convolutional networks that is based on contrasting between spatial regions within images. This criterion can be employed within conventional neural networks and optimized using standard techniques such as SGD and backpropagation, thus complementing supervised methods.",ICLR 2017 conference submission,False,,"This paper presents a novel way to do unsupervised pretraining in a deep convolutional network setting (though likely applicable to fully-connected nets as well). The method is that of ‘spatial constrasting’, i.e. of building triplets from patches of input images and learning a presentation that assigns a high score for patches coming from the same image and a low score for patches from diferent images. The method is simple enough that I am surprised that no-one has tried this before (at least according to the previous work in the submission). Here are some comments:


The usage of P(f_i^1 | f_i^2) in Section 4.1 is a bit odd. May be worth defining mathematically what kind of probability the authors are talking about, or just taking that part out (“probability” can be replaced with another word).

I would like to know more about how the method is using the “batch statistics” (end of Section 4.2) by sampling from it, unless the authors simply mean that the just sample from all the possible triples in their batch.

Shouldn’t the number of patches sampled in Algorithm 1 be a hyper-parameter rather than just be 1? Have the authors tried any other value?

I think there are some missing details in the paper, like the patch size or whether the authors have played with it at all (I think this is an important hyper-parameter).

The STL results are quite impressive, but CIFAR-10 maybe not so much. For CIFAR I’d expect that one can try to pre-train on, say, Imagenet + CIFAR to build a better representation. Have the authors considered this?


All in all, this is an interesting piece of work with some obvious applications, and it seems relatively straightforward to implemenent and try. I think I would’ve liked more understanding of what the spatial contrasting actually learns, more empirical studies on the effects of various parameter choices (e.g., patch size) and more attempts at beating the state of the art (e.g. CIFAR).

---

The paper proposes a formulation for unsupervised learning of ConvNets based on the distance between patches sampled from the same and different images. The novelty of the method is rather limited as it's similar to [Doersch et al. 2015] and [Dosovitsky et al. 2015]. The evaluation is only performed on the small datasets, which limits the potential impact of the contribution.

---

This paper proposes an unsupervised training objective based on patch contrasting for visual representation learning using deep neural networks. In particular, the feature representations of the patches from the same image are encouraged to be closer than the those from different images. The distance ratios of positive training pairs are optimized. The proposed method are empirically shown to be effective as an initialization method for supervised training. 

Strengths:

- The training objective is reasonable. In particular, high-level features show translation invariance. 

- The proposed methods are effective for initializing neural networks for supervised training on several datasets. 


Weaknesses:

- The methods are technically similar to the “exemplar network” (Dosovitskiy 2015). Cropping patches from a single image can be taken as a type of data augmentation, which is comparable to the data augmentation of positive sample (the exemplar) in (Dosovitskiy 2015). 

- The paper is experimentally misleading.
The results reported in this paper are based on fine-tuning the whole network with supervision. However, in Table 2, the results of exemplar convnets (Dosovitskiy 2015) is from unsupervised feature learning (the network is not finetuned with labeled samples, and only a classifier is trained upon the features). Therefore, the comparison is not fair. I suspect that exemplar convnets (Dosovitskiy 2015) would achieve similar improvements from fine-tuning; so, without such comparisons (head-to-head comparison with and without fine-tuning based on the same architecture except for the loss), the experimental results are not fully convincing. 

Regarding the comparison to “What-where” autoencoder (Zhao et al, 2015), it will be interesting to compare against it in large-scale settings, as shown by Zhang et al, ICML 2016 (Augmenting Supervised Neural Networks with Unsupervised Objectives for Large-Scale Image Classification). Training an AlexNet is not very time-consuming with latest (e.g., TITAN-X level) GPUs. 

The proposed method seems useful only for natural images where different patches from the same image can be similar to each other.

---

The proposed self supervised loss is formulated using a Siamese architecture and encourages patches from the same image to lie closer in feature space than a contrasting patch taken from a different, random image. The loss is very similar in spirit to that of Doersch et al. ICCV 2015 and Isola et al. ICLR 2016 workshop. It seems that the proposed loss is actually a simplified version of Doersch et al. ICCV 2015 in that it does not make use of the spatial offset, a freely available self supervised signal in natural images. Intuitively, it seems that the self-supervised problem posed by this method is strictly simpler, and therefore less powerful, than that of the aforementioned work. I would like to see more discussion on the comparison of these two approaches. Nevertheless the proposed method seems to be effective in achieving good empirical results using this simple loss. Though more implementation details should be provided, such as the effect of patch size, overlap between sampled patches, and any other important measures taken to avoid trivial solutions.

---

This paper presents a novel way to do unsupervised pretraining in a deep convolutional network setting (though likely applicable to fully-connected nets as well). The method is that of ‘spatial constrasting’, i.e. of building triplets from patches of input images and learning a presentation that assigns a high score for patches coming from the same image and a low score for patches from diferent images. The method is simple enough that I am surprised that no-one has tried this before (at least according to the previous work in the submission). Here are some comments:


The usage of P(f_i^1 | f_i^2) in Section 4.1 is a bit odd. May be worth defining mathematically what kind of probability the authors are talking about, or just taking that part out (“probability” can be replaced with another word).

I would like to know more about how the method is using the “batch statistics” (end of Section 4.2) by sampling from it, unless the authors simply mean that the just sample from all the possible triples in their batch.

Shouldn’t the number of patches sampled in Algorithm 1 be a hyper-parameter rather than just be 1? Have the authors tried any other value?

I think there are some missing details in the paper, like the patch size or whether the authors have played with it at all (I think this is an important hyper-parameter).

The STL results are quite impressive, but CIFAR-10 maybe not so much. For CIFAR I’d expect that one can try to pre-train on, say, Imagenet + CIFAR to build a better representation. Have the authors considered this?


All in all, this is an interesting piece of work with some obvious applications, and it seems relatively straightforward to implemenent and try. I think I would’ve liked more understanding of what the spatial contrasting actually learns, more empirical studies on the effects of various parameter choices (e.g., patch size) and more attempts at beating the state of the art (e.g. CIFAR).

---

This paper presents a novel way to do unsupervised pretraining in a deep convolutional network setting (though likely applicable to fully-connected nets as well). The method is that of ‘spatial constrasting’, i.e. of building triplets from patches of input images and learning a presentation that assigns a high score for patches coming from the same image and a low score for patches from diferent images. The method is simple enough that I am surprised that no-one has tried this before (at least according to the previous work in the submission). Here are some comments:


The usage of P(f_i^1 | f_i^2) in Section 4.1 is a bit odd. May be worth defining mathematically what kind of probability the authors are talking about, or just taking that part out (“probability” can be replaced with another word).

I would like to know more about how the method is using the “batch statistics” (end of Section 4.2) by sampling from it, unless the authors simply mean that the just sample from all the possible triples in their batch.

Shouldn’t the number of patches sampled in Algorithm 1 be a hyper-parameter rather than just be 1? Have the authors tried any other value?

I think there are some missing details in the paper, like the patch size or whether the authors have played with it at all (I think this is an important hyper-parameter).

The STL results are quite impressive, but CIFAR-10 maybe not so much. For CIFAR I’d expect that one can try to pre-train on, say, Imagenet + CIFAR to build a better representation. Have the authors considered this?


All in all, this is an interesting piece of work with some obvious applications, and it seems relatively straightforward to implemenent and try. I think I would’ve liked more understanding of what the spatial contrasting actually learns, more empirical studies on the effects of various parameter choices (e.g., patch size) and more attempts at beating the state of the art (e.g. CIFAR).

---

The paper proposes a formulation for unsupervised learning of ConvNets based on the distance between patches sampled from the same and different images. The novelty of the method is rather limited as it's similar to [Doersch et al. 2015] and [Dosovitsky et al. 2015]. The evaluation is only performed on the small datasets, which limits the potential impact of the contribution.

---

This paper proposes an unsupervised training objective based on patch contrasting for visual representation learning using deep neural networks. In particular, the feature representations of the patches from the same image are encouraged to be closer than the those from different images. The distance ratios of positive training pairs are optimized. The proposed method are empirically shown to be effective as an initialization method for supervised training. 

Strengths:

- The training objective is reasonable. In particular, high-level features show translation invariance. 

- The proposed methods are effective for initializing neural networks for supervised training on several datasets. 


Weaknesses:

- The methods are technically similar to the “exemplar network” (Dosovitskiy 2015). Cropping patches from a single image can be taken as a type of data augmentation, which is comparable to the data augmentation of positive sample (the exemplar) in (Dosovitskiy 2015). 

- The paper is experimentally misleading.
The results reported in this paper are based on fine-tuning the whole network with supervision. However, in Table 2, the results of exemplar convnets (Dosovitskiy 2015) is from unsupervised feature learning (the network is not finetuned with labeled samples, and only a classifier is trained upon the features). Therefore, the comparison is not fair. I suspect that exemplar convnets (Dosovitskiy 2015) would achieve similar improvements from fine-tuning; so, without such comparisons (head-to-head comparison with and without fine-tuning based on the same architecture except for the loss), the experimental results are not fully convincing. 

Regarding the comparison to “What-where” autoencoder (Zhao et al, 2015), it will be interesting to compare against it in large-scale settings, as shown by Zhang et al, ICML 2016 (Augmenting Supervised Neural Networks with Unsupervised Objectives for Large-Scale Image Classification). Training an AlexNet is not very time-consuming with latest (e.g., TITAN-X level) GPUs. 

The proposed method seems useful only for natural images where different patches from the same image can be similar to each other.

---

The proposed self supervised loss is formulated using a Siamese architecture and encourages patches from the same image to lie closer in feature space than a contrasting patch taken from a different, random image. The loss is very similar in spirit to that of Doersch et al. ICCV 2015 and Isola et al. ICLR 2016 workshop. It seems that the proposed loss is actually a simplified version of Doersch et al. ICCV 2015 in that it does not make use of the spatial offset, a freely available self supervised signal in natural images. Intuitively, it seems that the self-supervised problem posed by this method is strictly simpler, and therefore less powerful, than that of the aforementioned work. I would like to see more discussion on the comparison of these two approaches. Nevertheless the proposed method seems to be effective in achieving good empirical results using this simple loss. Though more implementation details should be provided, such as the effect of patch size, overlap between sampled patches, and any other important measures taken to avoid trivial solutions.

---

This paper presents a novel way to do unsupervised pretraining in a deep convolutional network setting (though likely applicable to fully-connected nets as well). The method is that of ‘spatial constrasting’, i.e. of building triplets from patches of input images and learning a presentation that assigns a high score for patches coming from the same image and a low score for patches from diferent images. The method is simple enough that I am surprised that no-one has tried this before (at least according to the previous work in the submission). Here are some comments:


The usage of P(f_i^1 | f_i^2) in Section 4.1 is a bit odd. May be worth defining mathematically what kind of probability the authors are talking about, or just taking that part out (“probability” can be replaced with another word).

I would like to know more about how the method is using the “batch statistics” (end of Section 4.2) by sampling from it, unless the authors simply mean that the just sample from all the possible triples in their batch.

Shouldn’t the number of patches sampled in Algorithm 1 be a hyper-parameter rather than just be 1? Have the authors tried any other value?

I think there are some missing details in the paper, like the patch size or whether the authors have played with it at all (I think this is an important hyper-parameter).

The STL results are quite impressive, but CIFAR-10 maybe not so much. For CIFAR I’d expect that one can try to pre-train on, say, Imagenet + CIFAR to build a better representation. Have the authors considered this?


All in all, this is an interesting piece of work with some obvious applications, and it seems relatively straightforward to implemenent and try. I think I would’ve liked more understanding of what the spatial contrasting actually learns, more empirical studies on the effects of various parameter choices (e.g., patch size) and more attempts at beating the state of the art (e.g. CIFAR).",,,,,,6.0,,,4.0,,
