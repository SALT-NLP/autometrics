id,full_text,abstract,conference,accepted,format,review_comments,appropriateness,clarity,impact,meaningful_comparison,originality,recommendation,recommendation_unofficial,replicability,reviewer_confidence,soundness_correctness,substance
330,"MENTS THROUGH CORRUPTION
Authors: Minmin Chen
Source file: 330.pdf

ABSTRACT
We present an efficient document representation learning framework, Document Vector through Corruption (Doc2VecC). Doc2VecC represents each document as a simple average of word embeddings. It ensures a representation generated as such captures the semantic meanings of the document during learning. A corruption model is included, which introduces a data-dependent regularization that favors informative or rare words while forcing the embeddings of common and non-discriminative ones to be close to zero. Doc2VecC produces significantly better word embeddings than Word2Vec. We compare Doc2VecC with several state-of-the-art document representation learning algorithms. The simple model architecture introduced by Doc2VecC matches or out-performs the state-of-the-art in generating high-quality document representations for sentiment analysis, document classification as well as semantic relatedness tasks. The simplicity of the model enables training on billions of words per hour on a single machine. At the same time, the model is very efficient in generating representations of unseen documents at test time.

1 INTRODUCTION
Text understanding starts with the challenge of finding machine-understandable representation that captures the semantics of texts. Bag-of-words (BoW) and its N-gram extensions are arguably the most commonly used document representations. Despite its simplicity, BoW works surprisingly well for many tasks (Wang & Manning, 2012). However, by treating words and phrases as unique and discrete symbols, BoW often fails to capture the similarity between words or phrases and also suffers from sparsity and high dimensionality.
Recent works on using neural networks to learn distributed vector representations of words have gained great popularity. The well celebrated Word2Vec (Mikolov et al., 2013a), by learning to predict the target word using its neighboring words, maps words of similar meanings to nearby points in the continuous vector space. The surprisingly simple model has succeeded in generating high-quality word embeddings for tasks such as language modeling, text understanding and machine translation. Word2Vec naturally scales to large datasets thanks to its simple model architecture. It can be trained on billions of words per hour on a single machine.
Paragraph Vectors (Le & Mikolov, 2014) generalize the idea to learn vector representation for documents. A target word is predicted by the word embeddings of its neighbors in together with a unique document vector learned for each document. It outperforms established document representations, such as BoW and Latent Dirichlet Allocation (Blei et al., 2003), on various text understanding tasks (Dai et al., 2015). However, two caveats come with this approach: 1) the number of parameters grows with the size of the training corpus, which can easily go to billions; and 2) it is expensive to generate vector representations for unseen documents at test time.
We propose an efficient model architecture, referred to as Document Vector through Corruption (Doc2VecC), to learn vector representations for documents. It is motivated by the observation that linear operations on the word embeddings learned by Word2Vec can sustain substantial amount of syntactic and semantic meanings of a phrase or a sentence (Mikolov et al., 2013b). For example, vec(“Russia”) + vec(“river”) is close to vec(“Volga River”) (Mikolov & Dean, 2013), and
vec(“king”) - vec(“man”) + vec(“women”) is close to vec(“queen”) (Mikolov et al., 2013b). In Doc2VecC, we represent each document as a simple average of the word embeddings of all the words in the document. In contrast to existing approaches which post-process learned word embeddings to form document representation (Socher et al., 2013; Mesnil et al., 2014), Doc2VecC enforces a meaningful document representation can be formed by averaging the word embeddings during learning. Furthermore, we include a corruption model that randomly remove words from a document during learning, a mechanism that is critical to the performance and learning speed of our algorithm.
Doc2VecC has several desirable properties: 1. The model complexity of Doc2VecC is decoupled from the size of the training corpus, depending only on the size of the vocabulary; 2. The model architecture of Doc2VecC resembles that of Word2Vec, and can be trained very efficiently; 3. The new framework implicitly introduces a data-dependent regularization, which favors rare or informative words and suppresses words that are common but not discriminative; 4. Vector representation of a document can be generated by simply averaging the learned word embeddings of all the words in the document, which significantly boost test efficiency; 5. The vector representation generated by Doc2VecC matches or beats the state-of-the-art for sentiment analysis, document classification as well as semantic relatedness tasks.

2 RELATED WORKS AND NOTATIONS
Text representation learning has been extensively studied. Popular representations range from the simplest BoW and its term-frequency based variants (Salton & Buckley, 1988), language model based methods (Croft & Lafferty, 2013; Mikolov et al., 2010; Kim et al., 2015), topic models (Deerwester et al., 1990; Blei et al., 2003), Denoising Autoencoders and its variants (Vincent et al., 2008; Chen et al., 2012), and distributed vector representations (Mesnil et al., 2014; Le & Mikolov, 2014; Kiros et al., 2015). Another prominent line of work includes learning task-specific document representation with deep neural networks, such as CNN (Zhang & LeCun, 2015) or LSTM based approaches (Tai et al., 2015; Dai & Le, 2015).
In this section, we briefly introduce Word2Vec and Paragraph Vectors, the two approaches that are most similar to ours. There are two well-know model architectures used for both methods, referred to as Continuous Bag-of-Words (CBoW) and Skipgram models (Mikolov et al., 2013a). In this work, we focus on CBoW. Extending to Skipgram is straightforward. Here are the notations we are going to use throughout the paper:
D = {D1, · · · , Dn}: a training corpus of size n, in which each document Di contains a variablelength sequence of words w1i , · · · , w Ti i ;
V : the vocabulary used in the training corpus, of sizes v;
x ∈ Rv×1: BoW of a document, where xj = 1 iff word j does appear in the document. ct ∈ Rv×1: BoW of the local context wt−k, · · · , wt−1, wt+1, · · · , wt+k at the target position t. ctj = 1 iff word j appears within the sliding window of the target;
U ∈ Rh×v: the projection matrix from the input space to a hidden space of size h. We use uw to denote the column in U for word w, i.e., the “input“ vector of word w;
V> ∈ Rv×h: the projection matrix from the hidden space to output. Similarly, we use vw to denote the column in V for word w, i.e., the “output“ vector of word w.
Word2Vec. Word2Vec proposed a neural network architecture of an input layer, a projection layer parameterized by the matrix U and an output layer by V>. It defines the probability of observing the target word wt in a document D given its local context ct as
P (wt|ct) = exp(v>wtUc t)∑ w′∈V exp(v > w′Uc t)
The word vectors are then learned to maximize the log likelihood of observing the target word at each position of the document. Various techniques (Mitchell & Lapata, 2010; Zanzotto et al., 2010; Yessenalina & Cardie, 2011; Grefenstette et al., 2013; Socher et al., 2013; Kusner et al., 2015)
have been studied to generate vector representations of documents from word embeddings, among which the simplest approach is to use weighted average of word embeddings. Similarly, our method forms document representation by averaging word embeddings of all the words in the document. Differently, as our model encodes the compositionality of words in the learned word embeddings, heuristic weighting at test time is not required.
Paragraph Vectors. Paragraph Vectors, on the other hands, explicitly learns a document vector with the word embeddings. It introduces another projection matrix D ∈ Rh×n. Each column of D acts as a memory of the global topic of the corresponding document. It then defines the probability of observing the target word wt in a document D given its local context ct as
P (wt|ct,d) = exp(v>wt(Uc t + d))∑ w′∈V exp(v > w′(Uc t + d))
where d ∈ D is the vector representation of the document. As we can see from this formula, the complexity of Paragraph Vectors grows with not only the size of the vocabulary, but also the size of the training corpus. While we can reasonably limit the size of a vocabulary to be within a million for most datasets, the size of a training corpus can easily go to billions. What is more concerning is that, in order to come up with the vector representations of unseen documents, we need to perform an expensive inference by appending more columns to D and gradient descent on D while fixing other parameters of the learned model.

3 METHOD
Several works (Mikolov & Dean, 2013; Mikolov et al., 2013b) showcased that syntactic and semantic regularities of phrases and sentences are reasonably well preserved by adding or subtracting word embeddings learned through Word2Vec. It prompts us to explore the option of simply representing a document as an average of word embeddings. Figure 1 illustrates the new model architecture.
Similar to Word2Vec or Paragraph Vectors, Doc2VecC consists of an input layer, a projection layer as well as an output layer to predict the target word, “ceremony” in this example. The embeddings of neighboring words (“opening”, “for”, “the”) provide local context while the vector representation of the entire document (shown in grey) serves as the global context. In contrast to Paragraph Vectors, which directly learns a unique vector for each document, Doc2VecC represents each document as an average of the embeddings of words randomly sampled from the document (“performance” at position p, “praised” at position q, and “brazil” at position r).
Huang et al. (2012) also proposed the idea of using average of word embeddings to represent the global context of a document. Different from their work, we choose to corrupt the original document by randomly removing significant portion of words, and represent the document using only the embeddings of the words remained. This corruption mechanism offers us great speedup during training as it significantly reduces the number of parameters to update in back propagation. At the same time, as we are going to detail in the next section, it introduces a special form of regularization, which brings great performance improvement.
Here we describe the stochastic process we used to generate a global context at each update. The global context, which we denote as x̃, is generated through a unbiased mask-out/drop-out corruption, in which we randomly overwrites each dimension of the original document x with probability q. To make the corruption unbiased, we set the uncorrupted dimensions to 1/(1 − q) times its original value. Formally,
x̃d = { 0, with probability q xd 1−q , otherwise
(1)
Doc2VecC then defines the probability of observing a target word wt given its local context ct as well as the global context x̃ as
P (wt|ct, x̃) = exp(v>wt(
local context︷︸︸︷ Uct + global context︷ ︸︸ ︷ 1
T Ux̃ ))∑
w′∈V exp(v > w′ ( Uct + 1T Ux̃) ) (2) Here T is the length of the document. Exactly computing the probability is impractical, instead we approximate it with negative sampling (Mikolov et al., 2013a).
f(w, c, x̃) ≡ logP (wt|ct, x̃) ≈ log σ ( v>w(Uc + 1
T Ux̃)
) + ∑
w′∼Pv
log σ ( −v>w′(Uc + 1
T Ux̃)
) (3)
here Pv stands for a uniform distribution over the terms in the vocabulary. The two projection matrices U and V are then learned to minimize the loss:
` = − n∑
i=1 Ti∑ t=1 f(wti , c t i, x̃ t i) (4)
Given the learned projection matrix U, we then represent each document simply as an average of the embeddings of the words in the document,
d = 1
T ∑ w∈D uw. (5)
We are going to elaborate next why we choose to corrupt the original document with the corruption model in eq.(1) during learning, and how it enables us to simply use the average word embeddings as the vector representation for documents at test time.

3.1 CORRUPTION AS DATA-DEPENDENT REGULARIZATION
We approximate the log likelihood for each instance f(w, c, x̃) in eq.(4) with its Taylor expansion with respect to x̃ up to the second-order (Van Der Maaten et al., 2013; Wager et al., 2013; Chen et al., 2014). Concretely, we choose to expand at the mean of the corruption µx = Ep(x̃|x)[x̃]:
f(w, c, x̃) ≈ f(w, c, µx) + (x̃− µx)>∇x̃f + 1
2 (x̃− µx)>∇2x̃f(x̃− µx)
where ∇x̃f and ∇2x̃f are the first-order (i.e., gradient) and second-order (i.e., Hessian) of the log likelihood with respect to x̃. Expansion at the mean µx is crucial as shown in the following steps. Let us assume that for each instance, we are going to sample the global context x̃ infinitely many times, and thus compute the expected log likelihood with respect to the corrupted x̃.
Ep(x̃|x)[f(w, c, x̃)] ≈ f(w, c, µx) + 1 2 tr ( E[(x̃− x)(x̃− x)>]∇2x̃f ) The linear term disappears as Ep(x̃|x)[x̃ − µx] = 0. We substitute in x for the mean µx of the corrupting distribution (unbiased corruption) and the matrix Σx = E[(x̃ − µx)(x̃ − µx)>] for the variance, and obtain
Ep(x̃|x)[f(w, c, x̃)] ≈ f(w, c,x) + 1 2 tr ( Σx∇2x̃f ) (6)
As each word in a document is corrupted independently of others, the variance matrix Σx is simplified to a diagonal matrix with jth element equals q1−qx 2 j . As a result, we only need to compute the diagonal terms of the Hessian matrix∇2x̃f .
The jth dimension of the Hessian’s diagonal evaluated at the mean x is given by
∂2f ∂x2j = −σw,c,x(1− σw,c,x)( 1 T v>wuj)
2 − ∑
w′∼Pv
σw′,c,x(1− σw′,c,x)( 1
T v>w′uj) 2
Plug the Hessian matrix and the variance matrix back into eq.(6), and then back to the loss defined in eq.(4), we can see that Doc2VecC intrinsically minimizes
` = − n∑
i=1 Ti∑ t=1 f(wti , c t i,xi) + q 1− q v∑ j=1 R(uj) (7)
Each f(wti , c t i,xi) in the first term measures the log likelihood of observing the target word w t i given its local context cti and the document vector di = 1 T Uxi. As such, Doc2VecC enforces that a document vector generated by averaging word embeddings can capture the global semantics of the document, and fill in information missed in the local context.
The second term here is a data-dependent regularization. The regularization on the embedding uj of each word j takes the following form, R(uj) ∝ n∑
i=1 Ti∑ t=1 x2ij [ σwti ,cti,xi(1− σwti ,cti,xi)( 1 T v>wti uj) 2 + ∑ w′∼Pv σw′,cti,xi(1− σw′,cti,xi)( 1 T v>w′uj) 2 ] where σw,c,x = σ(v>w(Uc+ 1 T Ux)) prescribes the confidence of predicting the target wordw given its neighboring context c as well as the document vector d = 1T Ux.
Closely examining R(uj) leads to several interesting findings: 1. the regularizer penalizes more on the embeddings of common words. A word j that frequently appears across the training corpus, i.e, xij = 1 often, will have a bigger regularization than a rare word; 2. on the other hand, the regularization is modulated by σw,c,x(1 − σw,c,x), which is small if σw,c,x → 1 or 0. In other words, if uj is critical to a confident prediction σw,c,x when it is active, then the regularization is diminished. Similar effect was observed for dropout training for logistic regression model (Wager et al., 2013) and denoising autoencoders (Chen et al., 2014).

4 EXPERIMENTS
We evaluate Doc2VecC on a sentiment analysis task, a document classification task and a semantic relatedness task, along with several document representation learning algorithms. All experiments can be reproduced using the code available at https://github.com/mchen24/iclr2017

4.1 BASELINES
We compare against the following document representation baselines: bag-of-words (BoW); Denoising Autoencoders (DEA) (Vincent et al., 2008), a representation learned from reconstructing original document x using corrupted one x̃. SDAs have been shown to be the state-of-the-art for sentiment analysis tasks (Glorot et al., 2011). We used Kullback-Liebler divergence as the reconstruction error and an affine encoder. To scale up the algorithm to large vocabulary, we only take into account the non-zero elements of x in the reconstruction error and employed negative sampling for the remainings; Word2Vec (Mikolov et al., 2013a)+IDF, a representation generated through weighted average of word vectors learned using Word2Vec; Doc2Vec (Le & Mikolov, 2014); Skip-thought Vectors(Kiros et al., 2015), a generic, distributed sentence encoder that extends the Word2Vec skipgram model to sentence level. It has been shown to produce highly generic sentence representations that apply to various natural language processing tasks. We also include RNNLM (Mikolov et al., 2010), a recurrent neural network based language model in the comparison. In the semantic relatedness task, we further compare to LSTM-based methods (Tai et al., 2015) that have been reported on this dataset.

4.2 SENTIMENT ANALYSIS
For sentiment analysis, we use the IMDB movie review dataset. It contains 100,000 movies reviews categorized as either positive or negative. It comes with predefined train/test split (Maas et al., 2011): 25,000 reviews are used for training, 25,000 for testing, and the rest as unlabeled data. The two classes are balanced in the training and testing sets. We remove words that appear less than 10 times in the training set, resulting in a vocabulary of 43,375 distinct words and symbols.
Setup. We test the various representation learning algorithms under two settings: one follows the same protocol proposed in (Mesnil et al., 2014), where representation is learned using all the available data, including the test set; another one where the representation is learned using training and unlabeled set only. For both settings, a linear support vector machine (SVM) (Fan et al., 2008) is trained afterwards on the learned representation for classification. For Skip-thought Vectors, we used the generic model1 trained on a much bigger book corpus to encode the documents. A vector of 4800 dimensions, first 2400 from the uni-skip model, and the last 2400 from the bi-skip model, are generated for each document. In comparison, all the other algorithms produce a vector representation of size 100. The supervised RNN-LM is learned on the training set only. The hyper-parameters are tuned on a validation set subsampled from the training set.
Accuracy. Comparing the two columns in Table 1, we can see that all the representation learning algorithms benefits from including the testing data during the representation learning phrase. Doc2VecC achieved similar or even better performance than Paragraph Vectors. Both methods outperforms the other baselines, beating the BOW representation by 15%. In comparison with Word2Vec+IDF, which applies post-processing on learned word embeddings to form document representation, Doc2VecC naturally enforces document semantics to be captured by averaged word embeddings during training. This leads to better performance. Doc2VecC reduces to Denoising Autoencoders (DEA) if the local context words are removed from the paradigm shown in Figure 1. By including the context words, Doc2VecC allows the document vector to focus more on capturing the global context. Skip-thought vectors perform surprisingly poor on this dataset comparing to other methods. We hypothesized that it is due to the length of paragraphs in this dataset. The average length of paragraphs in the IMDB movie review dataset is 296.5, much longer than the ones used for training and testing in the original paper, which is in the order of 10. As noted in (Tai et al., 2015), the performance of LSTM based method (similarly, the gated RNN used in Skip-thought vectors) drops significantly with increasing paragraph length, as it is hard to preserve state over long sequences of words.
Time. Table 2 summarizes the time required by these algorithms to learn and generate the document representation. Word2Vec is the fastest one to train. Denoising Autoencoders and Doc2VecC second that. The number of parameters that needs to be back-propagated in each update was increased by the number of surviving words in x̃. We found that both models are not sensitive to the corruption rate q in the noise model. Since the learning time decreases with higher corruption rate, we used q = 0.9 throughout the experiments. Paragraph Vectors takes longer time to train as there are more parameters (linear to the number of document in the learning set) to learn. At test time, Word2Vec+IDF, DEA and Doc2VecC all use (weighted) averaging of word embeddings as document
1available at https://github.com/ryankiros/skip-thoughts
representation. Paragraph Vectors, on the other hand, requires another round of inference to produce the vector representation of unseen test documents. It takes Paragraph Vectors 4 minutes and 17 seconds to infer the vector representations for the 25,000 test documents, in comparison to 7 seconds for the other methods. As we did not re-train the Skip-thought vector models on this dataset, the training time2 reported in the table is the time it takes to generate the embeddings for the 25,000 training documents. Due to repeated high-dimensional matrix operations required for encoding long paragraphs, it takes fairly long time to generate the representations for these documents. Similarly for testing. The experiments were conducted on a desktop with Intel i7 2.2Ghz cpu.
Data dependent regularization. As explained in Section 3.1, the corruption introduced in Doc2VecC acts as a data-dependent regularization that suppresses the embeddings of frequent but uninformative words. Here we conduct an experiment to exam the effect. We used a cutoff of 100 in this experiment. Table 3 lists the words having the smallest l2 norm of embeddings found by different algorithms. The number inside the parenthesis after each word is the number of times this word appears in the learning set. In word2Vec or Paragraph Vectors, the least frequent words have embeddings that are close to zero, despite some of them being indicative of sentiment such as debacle, bliss and shabby. In contrast, Doc2VecC manages to clamp down the representation of words frequently appear in the training set, but are uninformative, such as symbols and stop words.
Subsampling frequent words. Note that for all the numbers reported, we applied the trick of subsampling of frequent words introduced in (Mikolov & Dean, 2013) to counter the imbalance between frequent and rare words. It is critical to the performance of simple Word2Vec+AVG as the sole remedy to diminish the contribution of common words in the final document representation. If we were to remove this step, the error rate of Word2Vec+AVG will increases from 12.1% to 13.2%. Doc2VecC on the other hand naturally exerts a stronger regularization toward embeddings of words that are frequent but uninformative, therefore does not rely on this trick.

4.3 WORD ANALOGY
In table 3, we demonstrated that the corruption model introduced in Doc2VecC dampens the embeddings of words which are common and non-discriminative (stop words). In this experiment, we are going to quantatively compare the word embeddings generated by Doc2VecC to the ones generated by Word2Vec, or Paragraph Vectors on the word analogy task introduced by Mikolov et al. (2013a). The dataset contains five types of semantic questions, and nine types of syntactic questions, with a total of 8,869 semantic and 10,675 syntactic questions. The questions are answered through simple linear algebraic operations on the word embeddings generated by different methods. Please refer to the original paper for more details on the evaluation protocol.
2As reported in the original paper, training of the skip-thought vector model on the book corpus dataset takes around 2 weeks on GPU.
We trained the word embeddings of different methods using the English news dataset released under the ACL workshop on statistical machine translation. The training set includes close to 15M paragraphs with 355M tokens. We compare the performance of word embeddings trained by different methods with increasing embedding dimensionality as well as increasing training data.
We observe similar trends as in Mikolov et al. (2013a). Increasing embedding dimensionality as well as training data size improves performance of the word embeddings on this task. However, the improvement is diminishing. Doc2VecC produces word embeddings which performs significantly better than the ones generated by Word2Vec. We observe close to 20% uplift when we train on the full training corpus. Paragraph vectors on the other hand performs surprisingly bad on this dataset. Our hypothesis is that due to the large capacity of the model architecture, Paragraph Vectors relies mostly on the unique document vectors to capture the information in a text document instead of learning the word semantic or syntactic similarities. This also explains why the PV-DBOW Le & Mikolov (2014) model architecture proposed in the original work, which completely removes word embedding layers, performs comparable to the distributed memory version.
In table 5, we list a detailed comparison of the performance of word embeddings generated by Word2Vec and Doc2VecC on the 14 subtasks, when trained on the full dataset with embedding of size 100. We can see that Doc2VecC significantly outperforms the word embeddings produced by Word2Vec across almost all the subtasks.

4.4 DOCUMENT CLASSIFICATION
For the document classification task, we use a subset of the wikipedia dump, which contains over 300,000 wikipedia pages in 100 categories. The 100 categories includes categories under sports,
Table 5: Classification error (%) of a linear classifier trained on various document representations on the Wikipedia dataset.
Model BOW DEA Word2Vec + AVG Word2Vec + IDF ParagraphVectors Doc2VecC h = 100 36.03 32.30 33.2 33.16 35.78 31.92 h = 200 36.03 31.36 32.46 32.48 34.92 30.84 h = 500 36.03 31.10 32.02 32.13 33.93 30.43 h = 1000 36.03 31.13 31.78 32.06 33.02 30.24
(a) Doc2Vec (b) Doc2VecC
Figure 3: Visualization of document vectors on Wikipedia dataset using t-SNE.
entertainment, literature, and politics etc. Examples of categories include American drama films, Directorial debut films, Major League Baseball pitchers and Sydney Swans players. Body texts (the second paragraph) were extracted for each page as a document. For each category, we select 1,000 documents with unique category label, and 100 documents were used for training and 900 documents for testing. The remaining documents are used as unlabeled data. The 100 classes are balanced in the training and testing sets. For this data set, we learn the word embedding and document representation for all the algorithms using all the available data. We apply a cutoff of 10, resulting in a vocabulary of size 107, 691.
Table 5 summarizes the classification error of a linear SVM trained on representations of different sizes. We can see that most of the algorithms are not sensitive to the size of the vector representation. Doc2Vec benefits most from increasing representation size. Across all sizes of representations, Doc2VecC outperform the existing algorithms by a significant margin. In fact, Doc2VecC can achieve same or better performance with a much smaller representation vector.
Figure 3 visualizes the document representations learned by Doc2Vec (left) and Doc2VecC (right) using t-SNE (Maaten & Hinton, 2008). We can see that documents from the same category are nicely clustered using the representation generated by Doc2VecC. Doc2Vec, on the other hand, does not produce a clear separation between different categories, which explains its worse performance reported in Table 5.
Figure 4 visualizes the vector representation generated by Doc2VecC w.r.t. coarser categorization. we manually grouped the 100 categories into 7 coarse categories, television, albums, writers, musicians, athletes, species and actors. Categories that do no belong to any of these 7 groups are not included in the figure.
We can see that documents belonging to a coarser category are grouped together. This subset includes is a wide range of sports descriptions, ranging from football, crickets, baseball, and cycling etc., which explains why the athletes category are less concentrated. In the projection, we can see documents belonging to the musician category are closer to those belonging to albums category than those of athletes or species.

4.5 SEMANTIC RELATEDNESS
We test Doc2VecC on the SemEval 2014 Task 1: semantic relatedness SICK dataset (Marelli et al., 2014). Given two sentences, the task is to determine how closely they are semantically related. The set contains 9,927 pairs of sentences with human annotated relatedness score, ranging from 1 to 5. A score of 1 indicates that the two sentences are not related, while 5 indicates high relatedness. The set is splitted into a training set of 4,500 instances, a validation set of 500, and a test set of 4,927.
We compare Doc2VecC with several winning solutions of the competition as well as several more recent techniques reported on this dataset, including bi-directional LSTM and Tree-LSTM3 trained from scratch on this dataset, Skip-thought vectors learned a large book corpus 4 (Zhu et al., 2015) and produced sentence embeddings of 4,800 dimensions on this dataset. We follow the same protocol as in skip-thought vectors, and train Doc2VecC on the larger book corpus dataset. Contrary to the vocabulary expansion technique used in (Kiros et al., 2015) to handle out-of-vocabulary words, we extend the vocabulary of the learned model directly on the target dataset in the following way: we use the pre-trained word embedding as an initialization, and fine-tune the word and sentence representation on the SICK dataset. Notice that the fine-tuning is done for sentence representation learning only, and we did not use the relatedness score in the learning. This step brings small improvement to the performance of our algorithm. Given the sentence embeddings, we used the exact same training and testing protocol as in (Kiros et al., 2015) to score each pair of sentences: with two sentence embedding u1 and u2, we concatenate their component-wise product, u1 ·u2 and their absolute difference, |u1 − u2| as the feature representation. Table 6 summarizes the performance of various algorithms on this dataset. Despite its simplicity, Doc2VecC significantly out-performs the winning solutions of the competition, which are heavily feature engineered toward this dataset and several baseline methods, noticeably the dependency-tree RNNs introduced in (Socher et al., 2014), which relies on expensive dependency parsers to compose sentence vectors from word embeddings. The performance of Doc2VecC is slightly worse than the LSTM based methods or skip-thought vectors on this dataset, while it significantly outperforms skip-thought vectors on the IMDB movie review dataset (11.70% error rate vs 17.42%). As we hypothesized in previous section, while Doc2VecC is better at handling longer paragraphs, LSTMbased methods are superior for relatively short sentences (of length in the order of 10s). We would like to point out that Doc2VecC is much faster to train and test comparing to skip-thought vectors. It takes less than 2 hours to learn the embeddings on the large book corpus for Doc2VecC on a desktop with Intel i7 2.2Ghz cpu, in comparison to the 2 weeks on GPU required by skip-thought vectors.

5 CONCLUSION
We introduce a new model architecture Doc2VecC for document representation learning. It is very efficient to train and test thanks to its simple model architecture. Doc2VecC intrinsically makes sure document representation generated by averaging word embeddings capture semantics of document during learning. It also introduces a data-dependent regularization which favors informative or rare words while dampening the embeddings of common and non-discriminative words. As such, each document can be efficiently represented as a simple average of the learned word embeddings. In comparison to several existing document representation learning algorithms, Doc2VecC outperforms not only in testing efficiency, but also in the expressiveness of the generated representations.
3The word representation was initialized using publicly available 300-dimensional Glove vectors trained on 840 billion tokens of Common Crawl data
4The dataset contains 11,038 books with over one billion words
","We present an efficient document representation learning framework, Document Vector through Corruption (Doc2VecC). Doc2VecC represents each document as a simple average of word embeddings. It ensures a representation generated as such captures the semantic meanings of the document during learning. A corruption model is included, which introduces a data-dependent regularization that favors informative or rare words while forcing the embeddings of common and non-discriminative ones to be close to zero. Doc2VecC produces significantly better word embeddings than Word2Vec. We compare Doc2VecC with several state-of-the-art document representation learning algorithms. The simple model architecture introduced by Doc2VecC matches or out-performs the state-of-the-art in generating high-quality document representations for sentiment analysis, document classification as well as semantic relatedness tasks. The simplicity of the model enables training on billions of words per hour on a single machine. At the same time, the model is very efficient in generating representations of unseen documents at test time.",ICLR 2017 conference submission,True,,"Extended the paper with experiments on the word relationship dataset, showing Doc2VecC generates better word embeddings in comparison to Word2Vec or Paragraph Vectors.

---

I'm using

---

The introduced method for producing document representations is simple, efficient and potentially quite useful. Though we could quibble a bit that the idea is just a combination of known techniques, the reviews generally agree that the idea is interesting.
 
 Pros:
 + interesting and simple algorithm
 + strong performance
 + efficient
 
 Cons:
 + individual ideas are not so novel
 
 This is a paper that will be well received at a poster presentation.

---

Dear reviewers, 

I added another dataset to the draft (appendix): SemEval 2014 Task 1 semantic relatedness SICK dataset, as several of the recent works the reviewers pointed out have been reported on this dataset. Despite its simplicity, Doc2VecC significantly out-performs the winning solutions of the competition and several baseline methods, noticeably the dependency-tree RNNs introduced in [1], which relies on additional dependency parsers to compose sentence vectors from word embeddings. The performance of Doc2VecC is comparable (slightly worse) than the LSTM based methods or skip-thought vectors on this dataset. On the other hand, it is much faster to train and test. As reported in the original paper, training of the skip-thought vector models on the book corpus dataset takes around 2 weeks on GPU. In contrast, it takes less than 2 hours to learn the embeddings for Doc2VecC on a desktop with Intel i7 2.2Ghz cpu.

I also provided more insight on why Skip-thought vectors did not perform well on the movie review dataset in Section 4.2 (Accuracy). 

I would greatly appreciate it if you could take another look at revisions and provide me with some feedbacks. 

[1] Socher, Richard, et al. ""Grounded compositional semantics for finding and describing images with sentences."" Transactions of the Association for Computational Linguistics 2 (2014): 207-218.

---

Dear reviewers, 

Thank you for your feedback. The updated manuscript included skip-thought as another baseline method. We will test this idea on more datasets, in particular the ones experimented in Skip-thought vectors in the submission.

---

This paper discusses a method for computing vector representations for documents by using a skip-gram style learning mechanism with an added regularizer in the form of a global context vector with various bits of drop out. While none of the individual components proposed in this paper are new, I believe that the combination in this fashion is. Further, I appreciated the detailed analysis of model behaviour in section 3.

The main downside to this submission is in its relative weakness on the empirical front. Arguably there are more interesting tasks than sentiment analysis and k-way classification! Likewise, why waste 2/3 of a page on t-sne projections rather than use that space for further analysis?

While I am a bit disappointed by this reduced evaluation and agree with the other reviewers concerning soft baselines, I think this paper should be accepted: it's an interesting algorithm, nicely composed and very efficient, so it's reasonable to assume that other readers might have use for some of the ideas presented here.

---

This paper presents a framework for creating document representations. 
The main idea is to represent a document as an average of its word embeddings with a data-dependent regularization that favors informative or rare words while forcing common words to be close to 0. 
Experiments on sentiment analysis and document classification show that the proposed method has the lowest error rates compared to baseline document embedding methods. 

While I like the motivation of finding the best way to encode a document into a vector, the paper does not offer significant technical contributions.
Most of the techniques are not new, and the main selling point is the simplicity and speed of the proposed method. 
For this reason, I would like to see good results for more than two tasks to be convinced that this is the best way to learn document representations.  
For RNN-LM, is the LM trained to minimize classification error, or is it trained  as a language model? Did you use the final hidden state as the representation, or the average of all hidden states?
One of the most widely used method to represent documents now is to have a bidirectional LSTM and concatenate the final hidden states as the document representation. 
I think it would be useful to know how the proposed method compares to this approach for tasks such as document classification or sentiment analysis.

---

Unsupervised document representations is an active area of research, so it would be useful to benchmark against something more recent than doc2vec, which was in ICML 2014. Skip-thought vectors, in particular, should really be included.

---

This paper proposes learning document embeddings as a sum of the constituent word embeddings, which are jointly learned and randomly dropped out ('corrupted') during training. While none of the pieces of this model are particularly novel, the result is an efficient learning algorithm for document representation with good empirical performance.

Joint training of word and document embeddings is not a new idea, nor is the idea of enforcing the document to be represented by the sum of its word embeddings (see, e.g. '“The Sum of Its Parts”: Joint Learning of Word and Phrase Representations with Autoencoders' by Lebret and Collobert). Furthermore, the corruption mechanism is nothing other than traditional dropout on the input layer. Coupled with the word2vec-style loss and training methods, this paper offers little on the novelty front.

On the other hand, it is very efficient at generation time, requiring only an average of the word embeddings rather than a complicated inference step as in Doc2Vec. Moreover, by construction, the embedding captures salient global information about the document -- it captures specifically that information that aids in local-context prediction. For such a simple model, the performance on sentiment analysis and document classification is quite encouraging.

Overall, despite the lack of novelty, the simplicity, efficiency, and performance of this model make it worthy of wider readership and study, and I recommend acceptance.

---

Extended the paper with experiments on the word relationship dataset, showing Doc2VecC generates better word embeddings in comparison to Word2Vec or Paragraph Vectors.

---

I'm using

---

The introduced method for producing document representations is simple, efficient and potentially quite useful. Though we could quibble a bit that the idea is just a combination of known techniques, the reviews generally agree that the idea is interesting.
 
 Pros:
 + interesting and simple algorithm
 + strong performance
 + efficient
 
 Cons:
 + individual ideas are not so novel
 
 This is a paper that will be well received at a poster presentation.

---

Dear reviewers, 

I added another dataset to the draft (appendix): SemEval 2014 Task 1 semantic relatedness SICK dataset, as several of the recent works the reviewers pointed out have been reported on this dataset. Despite its simplicity, Doc2VecC significantly out-performs the winning solutions of the competition and several baseline methods, noticeably the dependency-tree RNNs introduced in [1], which relies on additional dependency parsers to compose sentence vectors from word embeddings. The performance of Doc2VecC is comparable (slightly worse) than the LSTM based methods or skip-thought vectors on this dataset. On the other hand, it is much faster to train and test. As reported in the original paper, training of the skip-thought vector models on the book corpus dataset takes around 2 weeks on GPU. In contrast, it takes less than 2 hours to learn the embeddings for Doc2VecC on a desktop with Intel i7 2.2Ghz cpu.

I also provided more insight on why Skip-thought vectors did not perform well on the movie review dataset in Section 4.2 (Accuracy). 

I would greatly appreciate it if you could take another look at revisions and provide me with some feedbacks. 

[1] Socher, Richard, et al. ""Grounded compositional semantics for finding and describing images with sentences."" Transactions of the Association for Computational Linguistics 2 (2014): 207-218.

---

Dear reviewers, 

Thank you for your feedback. The updated manuscript included skip-thought as another baseline method. We will test this idea on more datasets, in particular the ones experimented in Skip-thought vectors in the submission.

---

This paper discusses a method for computing vector representations for documents by using a skip-gram style learning mechanism with an added regularizer in the form of a global context vector with various bits of drop out. While none of the individual components proposed in this paper are new, I believe that the combination in this fashion is. Further, I appreciated the detailed analysis of model behaviour in section 3.

The main downside to this submission is in its relative weakness on the empirical front. Arguably there are more interesting tasks than sentiment analysis and k-way classification! Likewise, why waste 2/3 of a page on t-sne projections rather than use that space for further analysis?

While I am a bit disappointed by this reduced evaluation and agree with the other reviewers concerning soft baselines, I think this paper should be accepted: it's an interesting algorithm, nicely composed and very efficient, so it's reasonable to assume that other readers might have use for some of the ideas presented here.

---

This paper presents a framework for creating document representations. 
The main idea is to represent a document as an average of its word embeddings with a data-dependent regularization that favors informative or rare words while forcing common words to be close to 0. 
Experiments on sentiment analysis and document classification show that the proposed method has the lowest error rates compared to baseline document embedding methods. 

While I like the motivation of finding the best way to encode a document into a vector, the paper does not offer significant technical contributions.
Most of the techniques are not new, and the main selling point is the simplicity and speed of the proposed method. 
For this reason, I would like to see good results for more than two tasks to be convinced that this is the best way to learn document representations.  
For RNN-LM, is the LM trained to minimize classification error, or is it trained  as a language model? Did you use the final hidden state as the representation, or the average of all hidden states?
One of the most widely used method to represent documents now is to have a bidirectional LSTM and concatenate the final hidden states as the document representation. 
I think it would be useful to know how the proposed method compares to this approach for tasks such as document classification or sentiment analysis.

---

Unsupervised document representations is an active area of research, so it would be useful to benchmark against something more recent than doc2vec, which was in ICML 2014. Skip-thought vectors, in particular, should really be included.

---

This paper proposes learning document embeddings as a sum of the constituent word embeddings, which are jointly learned and randomly dropped out ('corrupted') during training. While none of the pieces of this model are particularly novel, the result is an efficient learning algorithm for document representation with good empirical performance.

Joint training of word and document embeddings is not a new idea, nor is the idea of enforcing the document to be represented by the sum of its word embeddings (see, e.g. '“The Sum of Its Parts”: Joint Learning of Word and Phrase Representations with Autoencoders' by Lebret and Collobert). Furthermore, the corruption mechanism is nothing other than traditional dropout on the input layer. Coupled with the word2vec-style loss and training methods, this paper offers little on the novelty front.

On the other hand, it is very efficient at generation time, requiring only an average of the word embeddings rather than a complicated inference step as in Doc2Vec. Moreover, by construction, the embedding captures salient global information about the document -- it captures specifically that information that aids in local-context prediction. For such a simple model, the performance on sentiment analysis and document classification is quite encouraging.

Overall, despite the lack of novelty, the simplicity, efficiency, and performance of this model make it worthy of wider readership and study, and I recommend acceptance.",,4.6,,,,6.666666666666667,5.0,,3.6666666666666665,4.666666666666667,
333,"WHAT DOES IT TAKE TO GENERATE NATURAL TEXTURES?
Authors: Ivan Ustyuzhaninov, Wieland Brendel, Leon Gatys, Matthias Bethge
Source file: 333.pdf

ABSTRACT
Natural image generation is currently one of the most actively explored fields in Deep Learning. Many approaches, e.g. for state-of-the-art artistic style transfer or natural texture synthesis, rely on the statistics of hierarchical representations in supervisedly trained deep neural networks. It is, however, unclear what aspects of this feature representation are crucial for natural image generation: is it the depth, the pooling or the training of the features on natural images? We here address this question for the task of natural texture synthesis and show that none of the above aspects are indispensable. Instead, we demonstrate that natural textures of high perceptual quality can be generated from networks with only a single layer, no pooling and random filters.

1 INTRODUCTION
During the last two years several different approaches towards natural image generation have been suggested, among them generative adversarial networks (Goodfellow et al., 2014; Chen et al., 2016), probabilistic generative models like the conditional PixelCNN (van den Oord et al., 2016b;a) or maximum entropy models that rely on the representations of deep neural networks (e.g. Gatys et al., 2015b; Johnson et al., 2016; Ulyanov et al., 2016). The latter approach has been particularly groundbreaking for artistic style transfer and natural texture generation (e.g. Gatys et al., 2015a;b) and has the potential to uncover the regularities that supervisedly trained deep neural networks infer from natural images.
For the sake of clarity and concreteness, this paper will focus on natural texture synthesis. Parametric texture models aim to uniquely describe each texture by a set of statistical measurements that are taken over the spatial extent of the image. Each image with the same spatial summary statistics should be perceived as the same texture. Consequently, synthesizing a texture corresponds to finding a new image that reproduces the summary statistics inferred from the reference texture. Starting from Nth-order joint histograms of the pixels by Julesz (1962), many different statistical measures have been proposed (see e.g. Heeger & Bergen, 1995; Portilla & Simoncelli, 2000). The quality of the synthesized textures is usually determined by human inspection; the synthesis is successful if a human observer cannot tell the reference texture from the synthesized ones.
The current state of the art in parametric texture modeling (Gatys et al., 2015a) employs the hierarchical image representation in a deep 19-layer convolutional network (Simonyan & Zisserman (2014); in the following referred to as VGG network) that was trained on object recognition in natural images(Russakovsky et al. (2015)). In this model textures are described by the raw correlations between feature activations in response to the texture image from a collection of network layers (see section 5 for details). Since its initial reception several papers explored which additional elements or constraints can further increase the perceptual quality of the generated textures (Berger & Memisevic, 2016; Liu et al., 2016; Aittala et al., 2016). In this work we go the opposite way and ask which elements of the original texture synthesis algorithm (Gatys et al., 2015a) are absolutely indispensable.
In particular two aspects have been deemed critical for natural texture synthesis: the hierarchical multi-layer representation of the textures, and the supervised training of the feature spaces. Here we show that neither aspect is imperative for texture modeling and that in fact a single convolutional layer with random features can synthesize textures that often rival the perceptual quality of Gatys et al. (2015a). This is in contrast to earlier reports (Gatys et al., 2015a) that suggested that networks with random weights fail to generate perceptually interesting images. We suggest that this discrepancy originates from a more elaborate tuning of the optimization procedure (see section 4).
Our main contributions are:
• We present a strong minimal baseline for parametric texture synthesis that solely relies on a single-layer network and random, data-independent filters. • We show that textures synthesized from the baseline are of high quality and often rival
state-of-the-art approaches, suggesting that the depth and the pre-training of multi-layer image representations are not as indispensable for natural image generation as has previously been thought. • We test and compare a wide range of single-layer architectures with different filter-sizes and
different types of filters (random, hand-crafted and unsupervisedly learnt filters) against the state-of-the-art texture model by Gatys et al. (2015a). • We utilize a quantitative texture quality measure based on the synthesis loss in the VGG-
based model (Gatys et al., 2015a) to replace the common-place evaluation of texture models through qualitative human inspection. • We discuss a formal generalization of maximum entropy models to account for the natural
variability of textures with limited spatial extent.

2 CONVOLUTIONAL NEURAL NETWORK
If not mentioned otherwise, all our models employ single-layer CNNs with standard rectified linear units (ReLUs) and convolutions with stride one, no bias and padding (f − 1)/2 where f is the filter-size. This choice ensures that the spatial dimension of the output feature maps is the same as the input. All networks except the last one employ filters of size 11× 11× 3 (filter width × filter height × no. of input channels), but the number of feature maps as well as the selection of the filters differ:
• Fourier-363: Each color channel (R, G, B) is filtered separately by each element Bi ∈ R11×11 of the 2D Fourier basis (11×11 = 121 feature maps/channel), yielding 3·121 = 363 feature maps in total. More concretely, each filter can be described as the tensor product Bi ⊗ ek where the elements of the unit-norm ek ∈ R3 are all zero except one. • Fourier-3267: All color channels (R, G, B) are filtered simultaneously by each element Bi of the 2D Fourier basis but with different weighting terms wR, wG, wB ∈ [1, 0,−1], yielding 3 · 3 · 3 · 121 = 3267 feature maps in total. More concretely, each filter can be described by the tensor product Bi ⊗ [wR, wG, wB ]. • Kmeans-363: We randomly sample and whiten 1e7 patches of size 11 × 11 from the
Imagenet dataset (Russakovsky et al., 2015), partition the patches into 363 clusters using k-means (Rubinstein et al., 2009), and use the cluster means as convolutional filters. • Kmeans-3267: Same as Kmeans-363 but with 3267 clusters. • Kmeans-NonWhite-363/3267: Same as Kmeans-363/3267 but without whitening of the
patches. • Kmeans-Sample-363/3267: Same as Kmeans-363/3267, but patches are only sampled
from the target texture. • PCA-363: We randomly sample 1e7 patches of size 11 × 11 from the Imagenet dataset
(Russakovsky et al., 2015), vectorize each patch, perform PCA and use the set of principal axes as convolutional filters. • Random-363: Filters are drawn from a uniform distribution according to (Glorot & Bengio,
2010), 363 feature maps in total. • Random-3267: Same as Random-363 but with 3267 feature maps.
Original Fourier-363, 11x11 K-means-363, 11x11 K-samples-363, 11x11 K-NW-363, 11x11 Random-363, 11x11 PCA-363, 11x11
Gatys et al. Fourier-3267, 11x11 K-means-3267, 11x11 K-samples-3267, 11x11 K-NW-3267, 11x11 Random-3267, 11x11 Random multi-scale
The networks were implemented in Lasagne (Dieleman et al., 2015; Theano Development Team, 2016). We remove the DC component of the inputs by subtracting the mean intensity in each color channel (estimated over the Imagenet dataset (Russakovsky et al., 2015)).

3 TEXTURE MODEL
The texture model closely follows (Gatys et al., 2015a). In essence, to characterise a given vectorised texture x ∈ RM , we first pass x through the convolutional layer and compute the output activations. The output can be understood as a non-linear filter bank, and thus its activations form a set of filtered images (so-called feature maps). For N distinct feature maps, the rectified output activations can be
described by a matrix F ∈ RN×M . To capture the stationary structure of the textures, we compute the covariances (or, more precisely, the Gramian matrix) G ∈ RN×N between the feature activations F by averaging the outer product of the point-wise feature vectors,
Gij = 1
M M∑ m=1 FimFjm. (1)
We will denote G(x) as the Gram matrix of the feature activations for the input x. To determine the relative distance between two textures x and y we compute the euclidean distance of the normalized Gram matrices,
d(x,y) = 1√∑
m,n Gmn(x)2 √∑ m,n Gmn(y)2 N∑ i,j=1 (Gij(x)−Gij(y))2 . (2)
To compare with the distance in the raw pixel values, we compute
dp(x,y) = 1√∑ m x2m √∑ m y2m N∑ i=1 (xi − yi)2 . (3)

4 TEXTURE SYNTHESIS
To generate a new texture we start from a uniform noise image (in the range [0, 1]) and iteratively optimize it to match the Gram matrix of the reference texture. More precisely, let G(x) be the Gram matrix of the reference texture. The goal is to find a synthesised image x̃ such that the squared distance between G(x) and the Gram matrix G(x̃) of the synthesized image is minimized, i.e.
x̃ = argmin y∈RM E(y), (4)
E(y) = 1∑N
i,j=1Gij(x) 2 N∑ i,j=1 ( Gij(x)−Gij(y) )2 . (5)
The gradient ∂E(y)/∂y of the reconstruction error with respect to the image can readily be computed using standard backpropagation, which we then use in conjunction with the L-BFGS-B algorithm (Jones et al., 2001–) to solve (4). We leave all parameters of the optimization algorithm at their default value except for the maximum number of iterations (2000), and add a box constraints with range [0, 1]. In addition, we scale the loss and the gradients by a factor of 107 in order to avoid early stopping of the optimization algorithm.

5 TEXTURE EVALUATION
Evaluating the quality of the synthesized textures is traditionally performed by human inspection. Optimal texture synthesis should generate samples that humans perceive as being the same texture as the reference. The high quality of the synthesized textures by (Gatys et al., 2015a) suggests that the summary statistics from multiple layers of VGG can approximate the perceptual metric of humans. Even though the VGG texture representation is not perfect, this allows us to utilize these statistics as a more objective quantification of texture quality.
For all details of the VGG-based texture model see (Gatys et al., 2015a). Here we use the standard 19-layer VGG network (Simonyan & Zisserman, 2014) with pretrained weights and average- instead of max-pooling1. We compute a Gram matrix on the output of each convolutional layer that follows a pooling layer. Let G`(.) be the Gram matrix on the activations of the `-th layer and
E`(y) = 1∑N
i,j=1G ` ij(x) 2 N∑ i,j=1 ( G`ij(x)−G`ij(y) )2 . (6)
the corresponding relative reconstruction cost. The total reconstruction cost is then defined as the average distance between the reference Gram matrices and the synthesized ones, i.e.
E(y) = 1
5 5∑ `=1 E`(y). (7)
This cost is reported on top of each synthesised texture in Figures 4. To visually evaluate samples from our single- and multi-scale model against the VGG-based model (Gatys et al., 2015a), we additionally synthesize textures from VGG by minimizing (7) using L-BFGS-B as in section 4.

6 RESULTS
In Fig. 1 we show textures synthesised from two random single- and multi-scale models, as well as eight other non-random single-layer models for three different source images (top left). For
1https://github.com/Lasagne/Recipes/blob/master/modelzoo/vgg19.py as accessed on 12.05.2016.
comparison, we also plot samples generated from the VGG model by Gatys et al. (Gatys et al., 2015a) (bottom left). There are roughly two groups of models: those with a small number of feature maps (363, top row), and those with a large number of feature maps (3267, bottom row). Only the multi-scale model employs 1024 feature maps. Within each group, we can differentiate models for which the filters are unsupervisedly trained on natural images (e.g. sparse coding filters from k-means), principally devised filter banks (e.g. 2D Fourier basis) and completely random filters (see sec. 2 for all details). All single-layer networks, except for multi-scale, feature 11× 11 filters. Remarkably, despite the small spatial size of the filters, all models capture much of the small- and mid-scale structure of the textures, in particular if the number of feature maps is large. Notably, the scale of these structures extends far beyond the receptive fields of the single units (see e.g. the pebble texture). We further observe that a larger number of feature maps generally increases the perceptual quality of the generated textures. Surprisingly, however, completely random filters perform on par or better then filters that have been trained on the statistics of natural images. This is particularly true for the multi-scale model that clearly outperforms the single-scale models on all textures. The captured structures in the multi-scale model are generally much larger and often reach the full size of the texture (see e.g. the wall).
While the above results show that for natural texture synthesis one neither needs a hierarchical deep network architecture with spatial pooling nor filters that are adapted to the statistics of natural images, we now focus on the aspects that are crucial for high quality texture synthesis. First, we evaluate whether the success of the random multi-scale network arises from the combination of filters on multiple scales or whether it is simply the increased size of its largest receptive fields (55× 55 vs. 11× 11) that leads to the improvement compared to the single-scale model. Thus, to investigate the influence of the spatial extend of the filters and the importance of combining multiple filter sizes in one model, we generate textures from multiple single-scale models, where each model has the same number of random filters as the multi-scale model (1024) but only uses filters from a single scale of the multi-scale model (Fig. 2). We find that while 3× 3 filters mainly capture the marginal distribution of the color channels, larger filters like 11 × 11 model small- to mid-scale structures (like small stones) but miss more long-range structures (larger stones are not well separated). Very large filters like 55× 55, on the other hand, are capable of modeling long-range structures but then miss much of the small- to midscale statistics (like the texture of the stone). Therefore we conclude that the combination of different scales in the multi-scale network is important for good texture synthesis since it allows to simultaneously model small-, mid- and long-range correlations of the textures. Finally we note that a further indispensable component for good texture models are the non-linearities: textures synthesised the multi-scale model without ReLU (Fig. 2, right column) are unable to capture the statistical dependencies of the texture.
The perceptual quality of the textures generated from models with only a single layer and random filters is quite remarkable and surpasses parametric methods like Portilla & Simoncelli (2000) that have been state-of-the-art two years ago (before the use of DNNs). The multi-scale model often rivals the current state of the art (Gatys et al., 2015a) as we show in Fig. 4 where we compare samples synthesized from 20 different textures for the random single- and multi-scale model, as well as VGG. The multi-scale model generates very competitive samples in particular for textures with extremely regular structures across the whole image (e.g. for the brick wall, the grids or the scales). In part, this effect can be attributed to the more robust optimization of the single-layer model that is less prone to local minima then the optimization in deeper models. This can be seen by initializing the VGG-based synthesis with textures from the single-layer model, which consistently yields superior synthesis results (see Appendix A, Fig. 5). In addition, for a few textures such as the grid structures, the VGG-based loss is paradoxically lower for samples from the multi-scale model then for the VGG-based model (which directly optimized the VGG-based loss). This suggests that the naive synthesis performed here favors images that are perceptually similar to the reference texture and thus looses variability (see sec. 7 for further discussion). Nonetheless, samples from the single-layer model still exhibit large perceptual differences, see Fig. 3. The VGG-based loss (7) appears to generally be an acceptable approximation of the perceptual differences between the reference and the synthesized texture. Only for a few textures, especially those with very regular men-made structures (e.g. the wall or the grids), the VGG-based loss fails to capture the perceptual advantage of the multi-scale synthesis.

7 DISCUSSION
We proposed a generative model of natural textures based on a single-layer convolutional neural network with completely random filters and showed that the model is able to qualitatively capture the perceptual differences between natural textures. Samples from the model often rival the current state-of-the-art (Gatys et al., 2015a) (Fig. 4, third vs fourth row), even though the latter relies on a high-performance deep neural network with features that are tuned to the statistics of natural images. Seen more broadly, this finding suggests that natural image generation does not necessarily depend on deep hierarchical representations or on the training of the feature maps. Instead, for texture synthesis, both aspects rather seem to serve as fine-tuning of the image representation.
One concern about the proposed single-layer multi-scale model is its computational inefficiency since it involves convolutions with spatially large filters (up to 55× 55). A more efficient way to achieve receptive fields of similar size would be to use a hierarchical multi-layer net. We conducted extensive experiments with various hierarchical architectures and while the synthesis is indeed significantly faster, the quality of the synthesized textures does not improve compared to a single-layer model. Thus for a minimal model of natural textures, deep hierarchical representations are not necessary but they can improve the efficiency of the texture synthesis.
Our results clearly demonstrate that Gram matrices computed from the feature maps of convolutional neural networks generically lead to useful summary statistics for texture synthesis. The Gram matrix on the feature maps transforms the representations from the convolutional neural network into a stationary feature space that captures the pairwise correlations between different features. If the number of feature maps is large, then the local structures in the image are well preserved in the projected space and the overlaps of the convolutional filtering add additional constraints. At the same time, averaging out the spatial dimensions yields sufficient flexibility to generate entirely new textures that differ from the reference on a patch by patch level, but still share much of the small- and long-range statistics.
The success of shallow convolutional networks with random filters in reproducing the structure of the reference texture is remarkable and indicates that they can be useful for parametric texture synthesis. Besides reproducing the stationary correlation structure of the reference image (""perceptual similarity"") another desideratum of a texture synthesis is to exhibit a large variety between different samples generated from the same given image (""variability""). Hence, synthesis algorithms need to balance perceptual similarity and variability. This balance is determined by a complex interplay between the choice of summary statistics and the optimization algorithm used. For example the stopping criterion of the optimization algorithm can be adjusted to trade perceptual similarity for larger variability.
Finding the right balance between perceptual similarity and variability is challenging because we are currently lacking robust measures of these quantities. In this work we introduced VGG-loss as a measure of perceptual similarity, and, even though, it works much better than other common measures such as Structural Similarity Index (SSIM, Wang et al., 2004, see Appendix A, Figure 6) or Euclidean distance in the pixel space (not shown), it is still not perfect (Figure 4). Measuring variability is probably even more difficult: in principle it requires measuring the entropy of generated samples, which is intractable in a high-dimensional space. A different approach could be based on a psychophysical assessment of generated samples. For example, we could use an inpainting task (illustrated in Appendix A, Figure 7) to make human observers decide between actual texture patches and inpaited ones. Performance close to a chance-level would indicate that the texture model produces variable enough samples to capture the diversity of actual patches. The further exploration of variability measures lies, however, beyond the scope of this work.
In this paper we focused on maximizing perceptual similarity only, and it is worth pointing out that additional efforts will be necessary to find an optimal trade-off between perceptual similarity and variability. For the synthesis of textures from the random models considered here, the trade-off leans more towards perceptual similarity in comparison to Gatys et al. (2015a)(due to the simpler optimization) which also explains the superior performance on some samples. In fact, we found some anecdotal evidence (not shown) in deeper multi-layer random CNNs where the reference texture was exactly reconstructed during the synthesis. From a theoretical point of view this is likely a finite size effect which does not necessarily constitute a failure of the chosen summary statistics: for finite size images it is well possible that only the reference image can exactly reproduce all the summary
statistics. Therefore, in practice, the Gram matrices are not treated as hard constraints but as soft constraints only. More generally, we do not expect a perceptual distance metric to assign exactly zero to a random pair of patches from the same texture. Instead, we expect it to assign small values for pairs from the same texture, and large values for patches from different textures. Therefore, the selection of constraints is not sufficient to characterize a texture synthesis model but only determines the exact minima of the objective function (which are sought for by the synthesis). If we additionally consider images with small but non-zero distance to the reference statistics, then the set of equivalent textures increases substantially, and the precise composition of this set becomes critically dependent on the perceptual distance metric.
Mathematically, parametric texture synthesis models are described as ergodic random fields that have maximum entropy subject to certain constraints Zhu et al. (1997); Bruna & Mallat (2013); Zhu et al. (2000) (MaxEnt framework). Practical texture synthesis algorithms, however, always deal with finite size images. As discussed above, two finite-size patches from the same ergodic random field will almost never feature the exact same summary statistics. This additional uncertainty in estimating the constraints on finite length processes is not thoroughly accounted for by the MaxEnt framework (see discussion on its “ad hockeries” by Jaynes (Jaynes (1982))). Thus, a critical difference of practical implementations of texture synthesis algorithms from the conceptual MaxEnt texture modeling framework is that they genuinely allow a small mismatch in the constraints. Accordingly, specifying the summary statistics is not sufficient but a comprehensive definition of a texture synthesis model should specify:
1. A metric d(x,y) that determines the distance between any two arbitrary textures x,y.
2. A bipartition Px of the image space that determines which images are considered perceptually equivalent to a reference texture x. A simple example for such a partition is the -environment U (y) := {y : d(y,x) < } and its complement.
This definition is relevant for both under- as well as over-constrained models, but its importance becomes particularly obvious for the latter. According to the Minimax entropy principle for texture modeling suggested by Zhu et al Zhu et al. (1997), as many constraints as possible should be used to reduce the (Kullback-Leibler) divergence between the true texture model and its estimate. However, for finite spatial size, the synthetic samples become exactly equivalent to the reference texture (up to shifts) in the limit of sufficiently many independent constraints. In contrast, if we explicitly allow for a small mismatch between the summary statistics of the reference image and the synthesized textures, then the set of possible textures does not constitute a low-dimensional manifold but rather a small volume within the pixel space. Alternatively, instead of introducing an -environment it is also possible to extent the MaxEnt framework to allow for variability in the summary statistics (Joan Bruna, personal communication). It will be interesting to compare in the future to what extent the difference between the two approaches can lead to differences in the perceptual appearance of the textures.
Taken together we have shown that simple single-layer CNNs with random filters can serve as the basis for excellent texture synthesis models that outperform previous hand-crafted synthesis models and sometimes even rivals the current state-of-the-art. This finding repeals previous observations that suggested a critical role for the multi-layer representations in trained deep networks for natural texture generation. On the other hand, it is not enough to just use sufficiently many constraints as one would predict from the MaxEnt framework. Instead, for the design of good texture synthesis algorithms it will be crucial to find distance measures for which the -environment around the reference texture leads to perceptually satisfying results. In this way, building better texture synthesis models is inherently related to better quantitative models of human perception.

A APPENDIX
","Natural image generation is currently one of the most actively explored fields in Deep Learning. Many approaches, e.g. for state-of-the-art artistic style transfer or natural texture synthesis, rely on the statistics of hierarchical representations in supervisedly trained deep neural networks. It is, however, unclear what aspects of this feature representation are crucial for natural image generation: is it the depth, the pooling or the training of the features on natural images? We here address this question for the task of natural texture synthesis and show that none of the above aspects are indispensable. Instead, we demonstrate that natural textures of high perceptual quality can be generated from networks with only a single layer, no pooling and random filters.",ICLR 2017 conference submission,True,,"The framework of Gatys et al. demonstrated that correlation statistics (empirical Gram matrices) of deep feature responses provide an excellent characterisation of visual textures. This paper investigates in detail which kind of deep or shallow networks may work well in this framework. One of the main findings is that that very shallow nets, consisting of a single filter bank with random weights, work surprisingly well, and for simple and regular textures may produce results which are visually superior to complex data-adapted filters such as the ones in networks like VGG-19. More broadly, the paper contains an interesting and informative discussion on the strength and limitations on such methods for texture synthesis.

Figure 4 shows that the optimisation of images with respect to shallow filter banks may result in texture images that have a lower VGG-19 loss than optimising the VGG-19 objective directly. This is imputed to the difficulty of optimising the highly non-linear VGG-19 cost function, which is a reasonable explanation. In the new supplementary material, the authors show that better optimisation results can be obtained by initialising the VGG-19-based optimisation with the shallow network optimisation results, which is a useful complement to the original experiments.

The main limitation of the paper is that it does not systematically compare different methods against a quantifiable objective. It is trivial to define image statistics that would allow to simply generate an exact copy of any reference texture, hence with very good visual quality. Such trivial statistics would also be very shallow. The aim is instead to capture a texture distribution, and measuring how well a method meets this challenge remains an open problem. Hence, while the empirical results seem to confirm the intuition that simple statistics are good enough for texture synthesis both in terms of quality and diversity (when compared to more complex statistics), it is difficult to conclusively confirm that this is the case.

The authors indicate that diversity could be measured in terms of entropy. This is reasonable, but, as they acknowledge in their answers to questions, difficult to do in practice. Furthermore, this would still not account for the other aspect of the problem, namely visual quality. They also suggest to perform a psychophysical assessment, which may be the only practical way of addressing this problem, but deem that to be material for future work.

Overall, since evaluation of image generation is such an hard problem, I think the paper still has sufficient strengths to warrant publication in ICLR. Still, some form of psychophysical assessment would be useful to confirm the intuitions that, at present, can only be obtained by inspecting the figure sin the paper and in the supplementary material.

---

The authors agreed that the paper presented a solid contribution and interesting (and somewhat surprising findings). The experiments are thorough and convincing, and while some reviewers raised concerns about a lack of comparisons of methods on a clear quantifiable objective, this is unfortunately a common issue in this field. Overall, this paper is a solid contribution with findings that would be interesting to the ICLR audience.

---

This work proposed a simple but strong baseline for parametric texture synthesis. In empirical experiments, samples generated by the baseline composed by multi-scale and random filters sometime rival the VGG-based model which has multi-layer and pre-trained filters. The authors concluded that texture synthesis does not necessarily depend on deep hierarchical representations or the learned feature maps. 
This work is indeed interesting and insightful. However, the conclusions are needed to be further testified (especially for deep hierarchical representations). Firstly, all of generated samples by both VGG and single layer model are not perfect and much worse than the results from non-parametric methods.  Besides VGG-based model seems to do better in inpainting task in Figure 7. Last but not least, would a hierarchical model (instead of lots of filters with different size) handle multi-scale more efficiently?

---

This paper provides an interesting analysis of the conditions which enable generation of natural looking textures. The results is quite surprising, and analysis is quite thorough. 
I do think the evaluation methods require more work, but as other reviewers mentioned this could be an interesting line of work moving forwards and does not take too much from this current paper which, I think, should be accepted.

---

The framework of Gatys et al. demonstrated that correlation statistics (empirical Gram matrices) of deep feature responses provide an excellent characterisation of visual textures. This paper investigates in detail which kind of deep or shallow networks may work well in this framework. One of the main findings is that that very shallow nets, consisting of a single filter bank with random weights, work surprisingly well, and for simple and regular textures may produce results which are visually superior to complex data-adapted filters such as the ones in networks like VGG-19. More broadly, the paper contains an interesting and informative discussion on the strength and limitations on such methods for texture synthesis.

Figure 4 shows that the optimisation of images with respect to shallow filter banks may result in texture images that have a lower VGG-19 loss than optimising the VGG-19 objective directly. This is imputed to the difficulty of optimising the highly non-linear VGG-19 cost function, which is a reasonable explanation. In the new supplementary material, the authors show that better optimisation results can be obtained by initialising the VGG-19-based optimisation with the shallow network optimisation results, which is a useful complement to the original experiments.

The main limitation of the paper is that it does not systematically compare different methods against a quantifiable objective. It is trivial to define image statistics that would allow to simply generate an exact copy of any reference texture, hence with very good visual quality. Such trivial statistics would also be very shallow. The aim is instead to capture a texture distribution, and measuring how well a method meets this challenge remains an open problem. Hence, while the empirical results seem to confirm the intuition that simple statistics are good enough for texture synthesis both in terms of quality and diversity (when compared to more complex statistics), it is difficult to conclusively confirm that this is the case.

The authors indicate that diversity could be measured in terms of entropy. This is reasonable, but, as they acknowledge in their answers to questions, difficult to do in practice. Furthermore, this would still not account for the other aspect of the problem, namely visual quality. They also suggest to perform a psychophysical assessment, which may be the only practical way of addressing this problem, but deem that to be material for future work.

Overall, since evaluation of image generation is such an hard problem, I think the paper still has sufficient strengths to warrant publication in ICLR. Still, some form of psychophysical assessment would be useful to confirm the intuitions that, at present, can only be obtained by inspecting the figure sin the paper and in the supplementary material.

---

The framework of Gatys et al. demonstrated that correlation statistics (empirical Gram matrices) of deep feature responses provide an excellent characterisation of visual textures. This paper investigates in detail which kind of deep or shallow networks may work well in this framework. One of the main findings is that that very shallow nets, consisting of a single filter bank with random weights, work surprisingly well, and for simple and regular textures may produce results which are visually superior to complex data-adapted filters such as the ones in networks like VGG-19. More broadly, the paper contains an interesting and informative discussion on the strength and limitations on such methods for texture synthesis.

Figure 4 shows that the optimisation of images with respect to shallow filter banks may result in texture images that have a lower VGG-19 loss than optimising the VGG-19 objective directly. This is imputed to the difficulty of optimising the highly non-linear VGG-19 cost function, which is a reasonable explanation. In the new supplementary material, the authors show that better optimisation results can be obtained by initialising the VGG-19-based optimisation with the shallow network optimisation results, which is a useful complement to the original experiments.

The main limitation of the paper is that it does not systematically compare different methods against a quantifiable objective. It is trivial to define image statistics that would allow to simply generate an exact copy of any reference texture, hence with very good visual quality. Such trivial statistics would also be very shallow. The aim is instead to capture a texture distribution, and measuring how well a method meets this challenge remains an open problem. Hence, while the empirical results seem to confirm the intuition that simple statistics are good enough for texture synthesis both in terms of quality and diversity (when compared to more complex statistics), it is difficult to conclusively confirm that this is the case.

The authors indicate that diversity could be measured in terms of entropy. This is reasonable, but, as they acknowledge in their answers to questions, difficult to do in practice. Furthermore, this would still not account for the other aspect of the problem, namely visual quality. They also suggest to perform a psychophysical assessment, which may be the only practical way of addressing this problem, but deem that to be material for future work.

Overall, since evaluation of image generation is such an hard problem, I think the paper still has sufficient strengths to warrant publication in ICLR. Still, some form of psychophysical assessment would be useful to confirm the intuitions that, at present, can only be obtained by inspecting the figure sin the paper and in the supplementary material.

---

The authors agreed that the paper presented a solid contribution and interesting (and somewhat surprising findings). The experiments are thorough and convincing, and while some reviewers raised concerns about a lack of comparisons of methods on a clear quantifiable objective, this is unfortunately a common issue in this field. Overall, this paper is a solid contribution with findings that would be interesting to the ICLR audience.

---

This work proposed a simple but strong baseline for parametric texture synthesis. In empirical experiments, samples generated by the baseline composed by multi-scale and random filters sometime rival the VGG-based model which has multi-layer and pre-trained filters. The authors concluded that texture synthesis does not necessarily depend on deep hierarchical representations or the learned feature maps. 
This work is indeed interesting and insightful. However, the conclusions are needed to be further testified (especially for deep hierarchical representations). Firstly, all of generated samples by both VGG and single layer model are not perfect and much worse than the results from non-parametric methods.  Besides VGG-based model seems to do better in inpainting task in Figure 7. Last but not least, would a hierarchical model (instead of lots of filters with different size) handle multi-scale more efficiently?

---

This paper provides an interesting analysis of the conditions which enable generation of natural looking textures. The results is quite surprising, and analysis is quite thorough. 
I do think the evaluation methods require more work, but as other reviewers mentioned this could be an interesting line of work moving forwards and does not take too much from this current paper which, I think, should be accepted.

---

The framework of Gatys et al. demonstrated that correlation statistics (empirical Gram matrices) of deep feature responses provide an excellent characterisation of visual textures. This paper investigates in detail which kind of deep or shallow networks may work well in this framework. One of the main findings is that that very shallow nets, consisting of a single filter bank with random weights, work surprisingly well, and for simple and regular textures may produce results which are visually superior to complex data-adapted filters such as the ones in networks like VGG-19. More broadly, the paper contains an interesting and informative discussion on the strength and limitations on such methods for texture synthesis.

Figure 4 shows that the optimisation of images with respect to shallow filter banks may result in texture images that have a lower VGG-19 loss than optimising the VGG-19 objective directly. This is imputed to the difficulty of optimising the highly non-linear VGG-19 cost function, which is a reasonable explanation. In the new supplementary material, the authors show that better optimisation results can be obtained by initialising the VGG-19-based optimisation with the shallow network optimisation results, which is a useful complement to the original experiments.

The main limitation of the paper is that it does not systematically compare different methods against a quantifiable objective. It is trivial to define image statistics that would allow to simply generate an exact copy of any reference texture, hence with very good visual quality. Such trivial statistics would also be very shallow. The aim is instead to capture a texture distribution, and measuring how well a method meets this challenge remains an open problem. Hence, while the empirical results seem to confirm the intuition that simple statistics are good enough for texture synthesis both in terms of quality and diversity (when compared to more complex statistics), it is difficult to conclusively confirm that this is the case.

The authors indicate that diversity could be measured in terms of entropy. This is reasonable, but, as they acknowledge in their answers to questions, difficult to do in practice. Furthermore, this would still not account for the other aspect of the problem, namely visual quality. They also suggest to perform a psychophysical assessment, which may be the only practical way of addressing this problem, but deem that to be material for future work.

Overall, since evaluation of image generation is such an hard problem, I think the paper still has sufficient strengths to warrant publication in ICLR. Still, some form of psychophysical assessment would be useful to confirm the intuitions that, at present, can only be obtained by inspecting the figure sin the paper and in the supplementary material.",,4.0,,3.0,4.0,7.666666666666667,,,4.0,4.0,
358,"Authors: NEURAL STATISTICIAN, Harrison Edwards, Amos Storkey
Source file: 358.pdf

ABSTRACT
An efficient learner is one who reuses what they already know to tackle a new problem. For a machine learner, this means understanding the similarities amongst datasets. In order to do this, one must take seriously the idea of working with datasets, rather than datapoints, as the key objects to model. Towards this goal, we demonstrate an extension of a variational autoencoder that can learn a method for computing representations, or statistics, of datasets in an unsupervised fashion. The network is trained to produce statistics that encapsulate a generative model for each dataset. Hence the network enables efficient learning from new datasets for both unsupervised and supervised tasks. We show that we are able to learn statistics that can be used for: clustering datasets, transferring generative models to new datasets, selecting representative samples of datasets and classifying previously unseen classes. We refer to our model as a neural statistician, and by this we mean a neural network that can learn to compute summary statistics of datasets without supervision.

1 INTRODUCTION
The machine learning community is well-practised at learning representations of data-points and sequences. A middle-ground between these two is representing, or summarizing, datasets - unordered collections of vectors, such as photos of a particular person, recordings of a given speaker or a document as a bag-of-words. Where these sets take the form of i.i.d samples from some distribution, such summaries are called statistics. We explore the idea of using neural networks to learn statistics and we refer to our approach as a neural statistician.
The key result of our approach is a statistic network that takes as input a set of vectors and outputs a vector of summary statistics specifying a generative model of that set - a mean and variance specifying a Gaussian distribution in a latent space we term the context. The advantages of our approach are that it is:
• Unsupervised: It provides principled and unsupervised way to learn summary statistics as the output of a variational encoder of a generative model.
• Data efficient: If one has a large number of small but related datasets, modelling the datasets jointly enables us to gain statistical strength.
• Parameter Efficient: By using summary statistics instead of say categorical labellings of each dataset, we decouple the number of parameters of the model from the number of datasets.
• Capable of few-shot learning: If the datasets correspond to examples from different classes, class embeddings (summary statistics associated with examples from a class), allow us to handle new classes at test time.

2 PROBLEM STATEMENT
We are given datasets Di for i ∈ I. Each dataset Di = {x1, . . . , xki} consists of a number of i.i.d samples from an associated distribution pi over Rn. The task can be split into learning and inference components. The learning component is to produce a generative model p̂i for each dataset Di. We assume there is a common underlying generative process p such that pi = p(·|ci) for ci ∈ Rl drawn
from p(c). We refer to c as the context. The inference component is to give an approximate posterior over the context q(c|D) for a given dataset produced by a statistic network.

3 NEURAL STATISTICIAN
In order to exploit the assumption of a hierarchical generative process over datasets we will use a ‘parameter-transfer approach’ (see Pan & Yang, 2010) to extend the variational autoencoder model of Kingma & Welling (2013).

3.1 VARIATIONAL AUTOENCODER
The variational autoencoder is a latent variable model p(x|z; θ) (often called the decoder) with parameters θ. For each observed x, a corresponding latent variable z is drawn from p(z) so that
p(x) = ∫ p(x|z; θ)p(z) dz. (1)
The generative parameters θ are learned by introducing a recognition network (also called an encoder) q(z|x;φ) with parameters φ. The recognition network gives an approximate posterior over the latent variables that can then be used to give the standard variational lower bound (Saul & Jordan, 1996) on the single-datum log-likelihood. I.e. logP (x|θ) ≥ Lx, where
Lx = Eq(z|x,φ) [log p(x|z; θ)]−DKL (q(z|x;φ)‖p(z)) . (2)
Likewise the full-data log likelihood is lower bounded by the sum of the Lx terms over the whole dataset. We can then optimize this lower bound with respect to φ and θ using the reparameterization trick introduced by Kingma & Welling (2013) and Rezende et al. (2014) to get a Monte-Carlo estimate of the gradient.

3.2 BASIC MODEL
We extend the variational autoencoder to the model depicted on the left in Figure 1. This includes a latent variable c, the context, that varies between different datasets but is constant, a priori, for items within the same dataset. Now, the likelihood of the parameters θ for one single particular dataset D is given by
p(D) = ∫ p(c) [∏ x∈D ∫ p(x|z; θ)p(z|c; θ) dz ] dc. (3)
The prior p(c) is chosen to be a spherical Gaussian with zero mean and unit variance. The conditional p(z|c; θ) is Gaussian with diagonal covariance, where all the mean and variance parameters depend on c through a neural network. Similarly the observation model p(x|z; θ) will be a simple likelihood function appropriate to the data modality with dependence on z parameterized by a neural network. For example, with real valued data, a diagonal Gaussian likelihood could be used where the mean and log variance of x are created from z via a neural network.
We use approximate inference networks q(z|x, c;φ), q(c|D;φ), with parameters collected into φ, to once again enable the calculation and optimization of a variational lower bound on the loglikelihood. The single dataset log likelihood lower bound is given by
LD = Eq(c|D;φ) [∑ x∈d Eq(z|c,x;φ) [log p(x|z; θ)]−DKL (q(z|c, x;φ)‖p(z|c; θ)) ] −DKL (q(c|D;φ)‖p(c)) . (4)
As with the generative distributions, the likelihood forms for q(z|x, c;φ) and q(c|D;φ) are diagonal Gaussian distributions, where all the mean and log variance parameters in each distribution are produced by a neural network taking the conditioning variables as inputs. Note that q(c|D;φ) accepts as input a dataset D and we refer to this as the statistic network. We describe this in Subsection 3.4.
The full-data variational bound is given by summing the variational bound for each dataset in our collection of datasets. It is by learning the difference of the within-dataset and between-dataset distributions that we are able to discover an appropriate statistic network.

3.3 FULL MODEL
The basic model works well for modelling simple datasets, but struggles when the datasets have complex internal structure. To increase the sophistication of the model we use multiple stochastic layers z1, . . . , zk and introduce skip-connections for both the inference and generative networks. The generative model is shown graphically in Figure 1 in the center. The probability of a dataset D is then given by
p(D) = ∫ p(c) ∏ x∈D ∫ p(x|c, z1:L; θ)p(zL|c; θ) L−1∏ i=1 p(zi|zi+1, c; θ) dz1:L dc (5)
where the p(zi|zi+1, c, θ) are again Gaussian distributions where the mean and log variance are given as the output of neural networks. The generative process for the full model is described in Algorithm 1.
The full approximate posterior factorizes analogously as
q(c, z1:L|D;φ) = q(c|D;φ) ∏ x∈D q(zL|x, c;φ) L−1∏ i=1 q(zi|zi+1, x, c;φ). (6)
For convenience we give the variational lower bound as sum of a three parts, a reconstruction term RD, a context divergence CD and a latent divergence LD:
LD = RD + CD + LD with (7) RD = Eq(c|D;φ) ∑ x∈D Eq(z1:L|c,x;φ) log p(x|z1:L, c; θ) (8)
CD = DKL (q(c|D;φ)‖p(c)) (9)
LD = Eq(c,z1:L|D;φ) [∑ x∈D DKL (q(zL|c, x;φ)‖p(zL|c; θ))
+ L−1∑ i=1 DKL (q(zi|zi+1, c, x;φ)‖p(zi|zi+1, c; θ))
] . (10)
The skip-connections p(zi|zi+1, c; θ) and q(zi|zi+1, x;φ) allow the context to specify a more precise distribution for each latent variable by explaining-away more generic aspects of the dataset at each stochastic layer. This architecture was inspired by recent work on probabilistic ladder networks in Kaae Sønderby et al. (2016). Complementing these are the skip-connections from each latent variable to the observation p(x|z1:L, c; θ), the intuition here is that each stochastic layer can focus on representing a certain level of abstraction, since its information does not need to be copied into the next layer, a similar approach was used in Maaløe et al. (2016).
Once again, note that we are maximizing the lower bound to the log likelihood over many datasets D: we want to maximize the expectation of LD over all datasets. We do this optimization using stochastic gradient descent. In contrast to a variational autoencoder where a minibatch would consist of a subsample of datapoints from the dataset, we use minibatches consisting of a subsample of datasets - tensors of shape (batch size, sample size, number of features).

3.4 STATISTIC NETWORK
In addition to the standard inference networks we require a statistic network q(c|D;φ) to give an approximate posterior over the context c given a datasetD = {x1, . . . , xk} . This inference network must capture the exchangeability of the data in D.
We use a feedforward neural network consisting of three main elements:
• An instance encoder E that takes each individual datapoint xi to a vector ei = E(xi). • An exchangeable instance pooling layer that collapses the matrix (e1, . . . , ek) to a single
pre-statistic vector v. Examples include elementwise means, sums, products, geometric means and maximum. We use the sample mean for all experiments. • A final post-pooling network that takes v to a parameterization of a diagonal Gaussian.
The graphical model for this is given at the right of Figure 1.
We note that the humble sample mean already gives the statistic network a great deal of representational power due to the fact that the instance encoder can learn a representation where averaging makes sense. For example since the instance encoder can approximate a polynomial on a compact domain, and so can the post-pooling network, a statistic network can approximate any moment of a distribution.

4 RELATED WORK
Due to the general nature of the problem considered, our work touches on many different topics which we now attempt to summarize.
Topic models and graphical models The form of the graphical model in Figure 1 on the left is equivalent to that of a standard topic model. In contrast to traditional topic models we do not use discrete latent variables, or restrict to discrete data. Work such as that by Ranganath et al. (2014) has extended topic models in various directions, but importantly we use flexible conditional distributions and dependency structures parameterized by deep neural networks. Recent work has explored neural networks for document models (see e.g. Miao et al., 2015) but has been limited to modelling datapoints with little internal structure. Along related lines are ‘structured variational autoencoders’ (see Johnson et al., 2016), where they treat the general problem of integrating graphical models with variational autoencoders.
Transfer learning There is a considerable literature on transfer learning, for a survey see Pan & Yang (2010). There they discuss ‘parameter-transfer’ approaches whereby parameters or priors are shared across datasets, and our work fits into that paradigm. For examples see Lawrence & Platt (2004) where share they priors between Gaussian processes, and Evgeniou & Pontil (2004) where they take an SVM-like approach to share kernels.
One-shot Learning Learning quickly from small amounts of data is a topic of great interest. Lake et al. (2015) use Bayesian program induction for one-shot generation and classification, and Koch (2015) train a Siamese (Chopra et al. (2005)) convolutional network for one-shot image classification. We note the relation to the recent work (Rezende et al., 2016) in which the authors use a conditional recurrent variational autoencoder capable of one-shot generalization by taking as extra input a conditioning data point. The important differences here are that we jointly model datasets and datapoints and consider datasets of any size. Recent approaches to one-shot classification are matching networks (Vinyals et al., 2016b) (which was concurrent with the initial preprint of this work), and related previous work (Santoro et al., 2016). The former can be considered a kind of differentiable nearest neighbour classifier, and the latter augments their network with memory to store information about the classification problem. Both are trained end-to-end for the classification problem, whereas the present work is a general approach to learning representations of datasets. Probably the closest previous work is by Salakhutdinov et al. (2012) where the authors learn a topic
model over the activations of a DBM for one-shot learning. Compared with their work we use modern architectures and easier to train VAEs, in particular we have fast and amortized feedforward inference for test (and training) datasets, avoiding the need for MCMC.
Multiple-Instance Learning There is previous work on classifying sets in multiple-instance learning, for a useful survey see Cheplygina et al. (2015). Typical approaches involve adapting kernel based methods such as support measure machines (Muandet et al., 2012), support distribution machines (Póczos et al., 2012) and multiple-instance-kernels (Gartner et al., 2002). We do not consider applications to multiple-instance learning type problems here, but it may be fruitful to do so in the future.
Set2Seq In very related work, Vinyals et al. (2016a) explore architectures for mapping sets to sequences. There they use an LSTM to repeatedly compute weighted-averages of the datapoints and use this to tackle problems such as sorting a list of numbers. The main difference between their work and ours is that they primarily consider supervised problems, whereas we present a general unsupervised method for learning representations of sets of i.i.d instances. In future work we may also explore recurrently computing statistics.
ABC There has also been work on learning summary statistics for Approximate Bayesian Computation by either learning to predict the parameters generating a sample as a supervised problem, or by using kernel embeddings as infinite dimensional summary statistics. See the work by Fukumizu et al. (2013) for an example of kernel-based approaches. More recently Jiang et al. (2015) used deep neural networks to predict the parameters generating the data. The crucial differences are that their problem is supervised, they do not leverage any exchangeability properties the data may have, nor can it deal with varying sample sizes.

5 EXPERIMENTAL RESULTS
Given an input set x1, . . . xk we can use the statistic network to calculate an approximate posterior over contexts q(c|x1, . . . , xk;φ). Under the generative model, each context c specifies a conditional model p(x|c; θ). To get samples from the model corresponding to the most likely posterior value of c, we set c to the mean of the approximate posterior and then sample directly from the conditional distributions. This is described in Algorithm 2. We use this process in our experiments to show samples. In all experiments, we use the Adam optimization algorithm (Kingma & Ba, 2014) to optimize the parameters of the generative models and variational approximations. Batch normalization (Ioffe & Szegedy, 2015) is implemented for convolutional layers and we always use a batch size of 16. We primarily use the Theano (Theano Development Team, 2016) framework with the Lasagne (Dieleman et al., 2015) library, but the final experiments with face data were done using Tensorflow (Abadi et al., 2015). In all cases experiments were terminated after a given number of epochs when training appeared to have sufficiently converged (300 epochs for omniglot, youtube and spatial MNIST examples, and 50 epochs for the synthetic experiment).

5.1 SIMPLE 1-D DISTRIBUTIONS
In our first experiment we wanted to know if the neural statistician will learn to cluster synthetic 1-D datasets by distribution family. We generated a collection of synthetic 1-D datasets each containing 200 samples. Datasets consist of samples from either an Exponential, Gaussian, Uniform or Laplacian distribution with equal probability. Means and variances are sampled from U [−1, 1] and U [0.5, 2] respectively. The training data contains 10K sets.
The architecture for this experiment contains a single stochastic layer with 32 units for z and 3 units for c, . The model p(x|z, c; θ) and variational approximation q(z|x, c;φ) are each a diagonal Gaussian distribution with all mean and log variance parameters given by a network composed of three dense layers with ReLU activations and 128 units. The statistic network determining the mean and log variance parameters of posterior over context variables is composed of three dense layers before and after pooling, each with 128 units with Rectified Linear Unit (ReLU) activations.
Figure 2 shows 3-D scatter plots of the summary statistics learned. Notice that the different families of distribution cluster. It is interesting to observe that the Exponential cluster is differently orientated to the others, perhaps reflecting the fact that it is the only non-symmetric distribution. We also see that between the Gaussian and Laplacian clusters there is an area of ambiguity which is as one
might expect. We also see that within each cluster the mean and variance are mapped to orthogonal directions.
5.2 SPATIAL MNIST
Building on the previous experiments we investigate 2-D datasets that have complex structure, but the datapoints contain little information by themselves, making it a good test of the statistic network. We created a dataset called spatial MNIST. In spatial MNIST each image from MNIST (LeCun et al., 1998) is turned into a dataset by interpreting the normalized pixel intensities as a probability density and sampling coordinate values. An example is shown in Figure 3. This creates two-dimensional spatial datasets. We used a sample size of 50. Note that since the pixel coordinates are discrete, it is necessary to dequantize them by adding uniform noise u ∼ U [0, 1] to the coordinates if one models them as real numbers, else you can get arbitrarily high densities (see Theis et al. (2016) for a discussion of this point).
The generative architecture for this experiment contains 3 stochastic z layers, each with 2 units, and a single c layer with 64 units. The means and log variances of the Gaussian likelihood for p(x|z1:3, c; θ), and each subnetwork for z in both the encoder and decoder contained 3 dense layers with 256 ReLU units each. The statistic network also contained 3 dense layers pre-pooling and 3 dense layers post pooling with 256 ReLU units.
In addition to being able to sample from the model conditioned on a set of inputs, we can also summarize a dataset by choosing a subset S ⊆ D to minimise the KL divergence of q(C|D;φ) from q(C|S;φ). We do this greedily by iteratively discarding points from the full sample. Pseudocode for this process is given in Algorithm 3. The results are shown in Figure 4. We see that the model is capable of handling complex arrangements of datapoints. We also see that it can select sensible subsets of a dataset as a summary.

5.3 OMNIGLOT
Next we work with the OMNIGLOT data (Lake et al., 2015). This contains 1628 classes of handwritten characters but with just 20 examples per class. This makes it an excellent test-bed for transfer / few-shot learning. We constructed datasets by splitting each class into datasets of size 5. We train
on datasets drawn from 1200 classes and reserve the remaining classes to test few-shot sampling and classification. We created new classes by rotating and reflecting characters. We resized the images to 28 × 28. We sampled a binarization of each image for each epoch. We also randomly applied the dilation operator from computer vision as further data augmentation since we observed that the stroke widths are quite uniform in the OMNIGLOT data, whereas there is substantial variation in MNIST, this augmentation improved the visual quality of the few-shot MNIST samples considerably and increased the few-shot classification accuracy by about 3 percent. Finally we used ‘sample dropout’ whereby a random subset of each dataset was removed from the pooling in the statistic network, and then included the number of samples remaining as an extra feature. This was beneficial since it reduced overfitting and also allowed the statistic network to learn to adjust the approximate posterior over c based on the number of samples.
We used a single stochastic layer with 16 units for z, and 512 units for c. We used a shared convolutional encoder between the inference and statistic networks and a deconvolutional decoder network. Full details of the networks are given in Appendix B.1. The decoder used a Bernoulli likelihood.
In Figure 5 we show two examples of few-shot learning by conditioning on samples of unseen characters from OMNIGLOT, and conditioning on samples of digits from MNIST. The samples are mostly of a high-quality, and this shows that the neural statistician can generalize even to new datasets.
As a further test we considered few-shot classification of both unseen OMNIGLOT characters and MNIST digits. Given a sets of labelled examples of each class D0, . . . , D9 (for MNIST say), we computed the approximate posteriors q(C|Di;φ) using the statistic network. Then for each test image x we also computed the posterior q(C|x;φ) and classified it according to the training dataset Di minimizing the KL divergence from the test context to the training context. This process is described in Algorithm 4. We tried this with either 1 or 5 labelled examples per class and either 5 or 20 classes. For each trial we randomly select K classes, randomly select training examples for each class, and test on the remaining examples. This process is repeated 100 times and the results averaged. The results are shown in Table 1. We compare to a number of results reported in Vinyals et al. (2016b) including Santoro et al. (2016) and Koch (2015). Overall we see that
the neural statistician model can be used as a strong classifier, particularly for the 5-way tasks, but performs worse than matching networks for the 20-way tasks. One important advantage that matching networks have is that, whilst each class is processed independently in our model, the representation in matching networks is conditioned on all of the classes in the few-shot problem. This means that it can exaggerate differences between similar classes, which are more likely to appear in a 20-way problem than a 5-way problem.

5.4 YOUTUBE FACES
Finally, we provide a proof of concept for generating faces of a particular person. We use the Youtube Faces Database from Wolf et al. (2011). It contains 3, 245 videos of 1, 595 different people. We use the aligned and cropped to face version, resized to 64 × 64. The validation and test sets contain 100 unique people each, and there is no overlap of persons between data splits. The sets were created by sampling frames randomly without replacement from each video, we use a set size of 5 frames. We resample the sets for the training data each epoch.
Our architecture for this problem is based on one presented in Lamb et al. (2016). We used a single stochastic layer with 500 dimensional latent c and 16 dimensional z variable. The statistic network and the inference network q(z|x, c;φ) share a common convolutional encoder, and the deocder uses deconvolutional layers. For full details see Appendix B.2. The likelihood function is a Gaussian, but where the variance parameters are shared across all datapoints, this was found to make training faster and more stable.
The results are shown in Figure 6. Whilst there is room for improvement, we see that it is possible to specify a complex distribution on-the-fly with a set of photos of a previously unseen person. The samples conditioned on an input set have a reasonable likeness of the input faces. We also show the ability of the model to generate new datasets and see that the samples have a consistent identity and varied poses.

6 CONCLUSION
We have demonstrated a highly flexible model on a variety of tasks. Going forward our approach will naturally benefit from advances in generative models as we can simply upgrade our base generative model, and so future work will pursue this. Compared with some other approaches in the literature for few-shot learning, our requirement for supervision is weaker: we only ask at training time that we are given datasets, but we do not need labels for the datasets, nor even information on whether two datasets represent the same or different classes. It would be interesting then to explore application areas where only this weaker form of supervision is available. There are two important limitations to this work, firstly that the method is dataset hungry: it will likely not learn useful representations of datasets given only a small number of them. Secondly at test time the few-shot fit of the generative model will not be greatly improved by using larger datasets unless the model was also trained on similarly large datasets. The latter limitation seems like a promising future research direction - bridging the gap between fast adaptation and slow training.

ACKNOWLEDGMENTS
This work was supported in part by the EPSRC Centre for Doctoral Training in Data Science, funded by the UK Engineering and Physical Sciences Research Council (grant EP/L016427/1) and the University of Edinburgh.

A APPENDIX A: PSEUDOCODE
Algorithm 1 Sampling a dataset of size k sample c ∼ p(c) for i = 1 to k do
sample zi,L ∼ p(zL|c; θ) for j = L− 1 to 1 do
sample zi,j ∼ p(zj |zi,j+1, c; θ) end for sample xi ∼ p(x|zi,1, . . . , zi,L, c; θ)
end for
Algorithm 2 Sampling a dataset of size k conditioned on a dataset of size m µc, σ 2 c ← q(c|x1, . . . , xm;φ) {Calculate approximate posterior over c using statistic network.}
c← µc {Set c to be the mean of the approximate posterior.} for i = 1 to k do
sample zi,L ∼ p(zL|c; θ) for j = L− 1 to 1 do
sample zi,j ∼ p(zj |zi,j+1, c; θ) end for sample xi ∼ p(x|zi,1, . . . , zi,L, c; θ)
end for
Algorithm 3 Selecting a representative sample of size k S ← {x1, . . . , xm} I ← {1, . . . ,m} SI = {xi ∈ S : i ∈ I} NSI ← q(c|SI ;φ) {Calculate approximate posterior over c using statistic network.} for i = 1 to k do t← argminj∈IDKL ( NS‖NSI−j
) I ← I − t
end for
Algorithm 4 K-way few-shot classification D0, . . . , DK ← sets of labelled examples for each class x← datapoint to be classified Nx ← q(c|x;φ) {approximate posterior over c given query point} for i = 1 to K do Ni ← q(c|Di;φ)
end for ŷ ← argminiDKL (Ni‖Nx)

B APPENDIX B: FURTHER EXPERIMENTAL DETAILS
B.1 OMNIGLOT
Shared encoder x→ h 2× { conv2d 64 feature maps with 3× 3 kernels and ELU activations } conv2d 64 feature maps with 3× 3 kernels, stride 2 and ELU activations 2× {conv2d 128 feature maps with 3× 3 kernels and ELU activations } conv2d 128 feature maps with 3× 3 kernels, stride 2 and ELU activations 2× { conv2d 256 feature maps with 3× 3 kernels and ELU activations } conv2d 256 feature maps with 3× 3 kernels, stride 2 and ELU activations
Statistic network q(c|D;φ) : h1, . . . , hk → µc, σ2c fully-connected layer with 256 units and ELU activations sample-dropout and concatenation with number of samples average pooling within each dataset 2× {fully-connected layer with 256 units and ELU activations } fully-connected linear layers to µc and log σ2c
Inference network q(z|x, c;φ) : h, c→ µz, σ2z concatenate c and h 3× {fully-connected layer with 256 units and ELU activations } fully-connected linear layers to µz and log σ2z
Latent decoder network p(z|c; θ) : c→ µz, σ2z 3× {fully-connected layer with 256 units and ELU activations } fully-connected linear layers to µz and log σ2z
Observation decoder network p(x|c, z; θ) : c, z → µx concatenate z and c fully-connected linear layers with 4 · 4 · 256 units 2× { conv2d 256 feature maps with 3× 3 kernels and ELU activations } deconv2d 256 feature maps with 2× 2 kernels, stride 2, ELU activations 2× { conv2d 128 feature maps with 3× 3 kernels and ELU activations } deconv2d 128 feature maps with 2× 2 kernels, stride 2, ELU activations 2× { conv2d 64 feature maps with 3× 3 kernels and ELU activations } deconv2d 64 feature maps with 2× 2 kernels, stride 2, ELU activations conv2d 1 feature map with 1× 1 kernels, sigmoid activations
B.2 YOUTUBE FACES
Shared encoder x→ h 2× { conv2d 32 feature maps with 3× 3 kernels and ELU activations } conv2d 32 feature maps with 3× 3 kernels, stride 2 and ELU activations 2× {conv2d 64 feature maps with 3× 3 kernels and ELU activations } conv2d 64 feature maps with 3× 3 kernels, stride 2 and ELU activations 2× { conv2d 128 feature maps with 3× 3 kernels and ELU activations } conv2d 128 feature maps with 3× 3 kernels, stride 2 and ELU activations 2× { conv2d 256 feature maps with 3× 3 kernels and ELU activations } conv2d 256 feature maps with 3× 3 kernels, stride 2 and ELU activations
Statistic network q(c|D,φ) : h1, . . . , hk → µc, σ2c fully-connected layer with 1000 units and ELU activations average pooling within each dataset fully-connected linear layers to µc and log σ2c
Inference network q(z|x, c, φ) : h, c→ µz, σ2z concatenate c and h fully-connected layer with 1000 units and ELU activations fully-connected linear layers to µz and log σ2z
Latent decoder network p(z|c, ; θ) : c→ µz, σ2z fully-connected layer with 1000 units and ELU activations fully-connected linear layers to µz and log σ2z
Observation decoder network p(x|c, z; θ) : c, z → µx concatenate z and c fully-connected layer with 1000 units and ELU activations fully-connected linear layer with 8 · 8 · 256 units 2× { conv2d 256 feature maps with 3× 3 kernels and ELU activations } deconv2d 256 feature maps with 2× 2 kernels, stride 2, ELU activations 2× { conv2d 128 feature maps with 3× 3 kernels and ELU activations } deconv2d 128 feature maps with 2× 2 kernels, stride 2, ELU activations 2× { conv2d 64 feature maps with 3× 3 kernels and ELU activations } deconv2d 64 feature maps with 2× 2 kernels, stride 2, ELU activations 2× { conv2d 32 feature maps with 3× 3 kernels and ELU activations } deconv2d 32 feature maps with 2× 2 kernels, stride 2, ELU activations conv2d 3 feature maps with 1× 1 kernels, sigmoid activations
","An efficient learner is one who reuses what they already know to tackle a new problem. For a machine learner, this means understanding the similarities amongst datasets. In order to do this, one must take seriously the idea of working with datasets, rather than datapoints, as the key objects to model. Towards this goal, we demonstrate an extension of a variational autoencoder that can learn a method for computing representations, or statistics, of datasets in an unsupervised fashion. The network is trained to produce statistics that encapsulate a generative model for each dataset. Hence the network enables efficient learning from new datasets for both unsupervised and supervised tasks. We show that we are able to learn statistics that can be used for: clustering datasets, transferring generative models to new datasets, selecting representative samples of datasets and classifying previously unseen classes. We refer to our model as a neural statistician, and by this we mean a neural network that can learn to compute summary statistics of datasets without supervision.",ICLR 2017 conference submission,True,,"The authors introduce a variant of the variational autoencoder (VAE) that models dataset-level latent variables. The idea is clearly motivated and well described. In my mind the greatest contribution of this paper is the movement beyond the relatively simple graphical model structure of the traditional VAEs and the introduction of more interesting structures to the deep learning community. 

Comments:

- It's not clear to me why this should be called a ""statistician"". Learning an approximate posterior over summary statistics is not the only imaginable way to summarize a dataset with a neural network. One could consider a maximum likelihood approach, etc. In general it felt like the paper could be more clear, if it avoided coining new terms like ""statistic network"" and stuck to the more accurate ""approximate posterior"".

- The experiments are nice, and I appreciate the response to my question regarding ""one shot generation"". I still think that language needs to be clarified, specifically at the end of page 6. My understanding of Figure 5 is the following: Take an input set, compute the approximate posterior over the context vector, then generate from the forward model given samples from the approximate posterior. I would like clarification on the following: 

(a) Are the data point dependent vectors z generated from the forward model or taken from the approximate posterior? 

(b) I agree that the samples are of high-quality, but that is not a quantified statement. The advantage of VAEs over GANs is that we have natural ways of computing log-probabilities. To that end, one ""proper"" way of computing the ""one shot generation"" performance is to report log p(x | c) (where c is sampled from the approximate posterior) or log p(x) for held-out datasets. I suspect that log probability performance of these networks relative to a vanilla VAE without the context latent variable will be impressive. I still don't see a reason not to include that.

---

This is an interesting paper that adds nicely to the literature on VAEs and one-shot generalisation. This will be of interest to the community and will contribute positively to the conference.

---

This paper proposes a hierarchical generative model where the lower level consists of points within datasets and the higher level models unordered sets of datasets.  The basic idea is to use a ""double"" variational bound where a higher level latent variable describes datasets and a lower level latent variable describes individual examples.  

Hierarchical modeling is an important and high impact problem, and I think that it's under-explored in the Deep Learning literature.  

Pros:
  -The few-shot learning results look good, but I'm not an expert in this area.  
  -The idea of using a ""double"" variational bound in a hierarchical generative model is well presented and seems widely applicable.  

Questions: 
  -When training the statistic network, are minibatches (i.e. subsets of the examples) used?  
  -If not, does using minibatches actually give you an unbiased estimator of the full gradient (if you had used all examples)?  For example, what if the statistic network wants to pull out if *any* example from the dataset has a certain feature and treat that as the characterization.  This seems to fit the graphical model on the right side of figure 1.  If your statistic network is trained on minibatches, it won't be able to learn this characterization, because a given minibatch will be missing some of the examples from the dataset.  Using minibatches (as opposed to using all examples in the dataset) to train the statistic network seems like it would limit the expressive power of the model.  

Suggestions: 
  -Hierarchical forecasting (electricity / sales) could be an interesting and practical use case for this type of model.

---

Sorry for the late review -- I've been having technical problems with OpenReview which prevented me from posting.

This paper presents a method for learning to predict things from sets of data points. The method is a hierarchical version of the VAE, where the top layer consists of an abstract context unit that summarizes a dataset. Experiments show that the method is able to ""learn to learn"" by acquiring the ability to learn distributions from small numbers of examples.

Overall, this paper is a nice addition to the literature on one- or few-shot learning. The method is conceptually simple and elegant, and seems to perform well. Compared to other recent papers on one-shot learning, the proposed method is simpler, and is based on unsupervised representation learning. The paper is clearly written and a pleasure to read.

The name of the paper is overly grandiose relative to what was done; the proposed method doesn’t seem to have much in common with a statistician, unless one means by that ""someone who thinks up statistics"". 

The experiments are well chosen, and the few-shot learning results seem pretty solid given the simplicity of the method.

The spatial MNIST dataset is interesting and might make a good toy benchmark. The inputs in Figure 4 seem pretty dense, though; shouldn’t the method be able to recognize the distribution with fewer samples?  (Nitpick: the red points in Figure 4 don’t seem to correspond to meaningful points as was claimed in the text.) 

Will the authors release the code?

---

The authors introduce a variant of the variational autoencoder (VAE) that models dataset-level latent variables. The idea is clearly motivated and well described. In my mind the greatest contribution of this paper is the movement beyond the relatively simple graphical model structure of the traditional VAEs and the introduction of more interesting structures to the deep learning community. 

Comments:

- It's not clear to me why this should be called a ""statistician"". Learning an approximate posterior over summary statistics is not the only imaginable way to summarize a dataset with a neural network. One could consider a maximum likelihood approach, etc. In general it felt like the paper could be more clear, if it avoided coining new terms like ""statistic network"" and stuck to the more accurate ""approximate posterior"".

- The experiments are nice, and I appreciate the response to my question regarding ""one shot generation"". I still think that language needs to be clarified, specifically at the end of page 6. My understanding of Figure 5 is the following: Take an input set, compute the approximate posterior over the context vector, then generate from the forward model given samples from the approximate posterior. I would like clarification on the following: 

(a) Are the data point dependent vectors z generated from the forward model or taken from the approximate posterior? 

(b) I agree that the samples are of high-quality, but that is not a quantified statement. The advantage of VAEs over GANs is that we have natural ways of computing log-probabilities. To that end, one ""proper"" way of computing the ""one shot generation"" performance is to report log p(x | c) (where c is sampled from the approximate posterior) or log p(x) for held-out datasets. I suspect that log probability performance of these networks relative to a vanilla VAE without the context latent variable will be impressive. I still don't see a reason not to include that.

---

The authors introduce a variant of the variational autoencoder (VAE) that models dataset-level latent variables. The idea is clearly motivated and well described. In my mind the greatest contribution of this paper is the movement beyond the relatively simple graphical model structure of the traditional VAEs and the introduction of more interesting structures to the deep learning community. 

Comments:

- It's not clear to me why this should be called a ""statistician"". Learning an approximate posterior over summary statistics is not the only imaginable way to summarize a dataset with a neural network. One could consider a maximum likelihood approach, etc. In general it felt like the paper could be more clear, if it avoided coining new terms like ""statistic network"" and stuck to the more accurate ""approximate posterior"".

- The experiments are nice, and I appreciate the response to my question regarding ""one shot generation"". I still think that language needs to be clarified, specifically at the end of page 6. My understanding of Figure 5 is the following: Take an input set, compute the approximate posterior over the context vector, then generate from the forward model given samples from the approximate posterior. I would like clarification on the following: 

(a) Are the data point dependent vectors z generated from the forward model or taken from the approximate posterior? 

(b) I agree that the samples are of high-quality, but that is not a quantified statement. The advantage of VAEs over GANs is that we have natural ways of computing log-probabilities. To that end, one ""proper"" way of computing the ""one shot generation"" performance is to report log p(x | c) (where c is sampled from the approximate posterior) or log p(x) for held-out datasets. I suspect that log probability performance of these networks relative to a vanilla VAE without the context latent variable will be impressive. I still don't see a reason not to include that.

---

This is an interesting paper that adds nicely to the literature on VAEs and one-shot generalisation. This will be of interest to the community and will contribute positively to the conference.

---

This paper proposes a hierarchical generative model where the lower level consists of points within datasets and the higher level models unordered sets of datasets.  The basic idea is to use a ""double"" variational bound where a higher level latent variable describes datasets and a lower level latent variable describes individual examples.  

Hierarchical modeling is an important and high impact problem, and I think that it's under-explored in the Deep Learning literature.  

Pros:
  -The few-shot learning results look good, but I'm not an expert in this area.  
  -The idea of using a ""double"" variational bound in a hierarchical generative model is well presented and seems widely applicable.  

Questions: 
  -When training the statistic network, are minibatches (i.e. subsets of the examples) used?  
  -If not, does using minibatches actually give you an unbiased estimator of the full gradient (if you had used all examples)?  For example, what if the statistic network wants to pull out if *any* example from the dataset has a certain feature and treat that as the characterization.  This seems to fit the graphical model on the right side of figure 1.  If your statistic network is trained on minibatches, it won't be able to learn this characterization, because a given minibatch will be missing some of the examples from the dataset.  Using minibatches (as opposed to using all examples in the dataset) to train the statistic network seems like it would limit the expressive power of the model.  

Suggestions: 
  -Hierarchical forecasting (electricity / sales) could be an interesting and practical use case for this type of model.

---

Sorry for the late review -- I've been having technical problems with OpenReview which prevented me from posting.

This paper presents a method for learning to predict things from sets of data points. The method is a hierarchical version of the VAE, where the top layer consists of an abstract context unit that summarizes a dataset. Experiments show that the method is able to ""learn to learn"" by acquiring the ability to learn distributions from small numbers of examples.

Overall, this paper is a nice addition to the literature on one- or few-shot learning. The method is conceptually simple and elegant, and seems to perform well. Compared to other recent papers on one-shot learning, the proposed method is simpler, and is based on unsupervised representation learning. The paper is clearly written and a pleasure to read.

The name of the paper is overly grandiose relative to what was done; the proposed method doesn’t seem to have much in common with a statistician, unless one means by that ""someone who thinks up statistics"". 

The experiments are well chosen, and the few-shot learning results seem pretty solid given the simplicity of the method.

The spatial MNIST dataset is interesting and might make a good toy benchmark. The inputs in Figure 4 seem pretty dense, though; shouldn’t the method be able to recognize the distribution with fewer samples?  (Nitpick: the red points in Figure 4 don’t seem to correspond to meaningful points as was claimed in the text.) 

Will the authors release the code?

---

The authors introduce a variant of the variational autoencoder (VAE) that models dataset-level latent variables. The idea is clearly motivated and well described. In my mind the greatest contribution of this paper is the movement beyond the relatively simple graphical model structure of the traditional VAEs and the introduction of more interesting structures to the deep learning community. 

Comments:

- It's not clear to me why this should be called a ""statistician"". Learning an approximate posterior over summary statistics is not the only imaginable way to summarize a dataset with a neural network. One could consider a maximum likelihood approach, etc. In general it felt like the paper could be more clear, if it avoided coining new terms like ""statistic network"" and stuck to the more accurate ""approximate posterior"".

- The experiments are nice, and I appreciate the response to my question regarding ""one shot generation"". I still think that language needs to be clarified, specifically at the end of page 6. My understanding of Figure 5 is the following: Take an input set, compute the approximate posterior over the context vector, then generate from the forward model given samples from the approximate posterior. I would like clarification on the following: 

(a) Are the data point dependent vectors z generated from the forward model or taken from the approximate posterior? 

(b) I agree that the samples are of high-quality, but that is not a quantified statement. The advantage of VAEs over GANs is that we have natural ways of computing log-probabilities. To that end, one ""proper"" way of computing the ""one shot generation"" performance is to report log p(x | c) (where c is sampled from the approximate posterior) or log p(x) for held-out datasets. I suspect that log probability performance of these networks relative to a vanilla VAE without the context latent variable will be impressive. I still don't see a reason not to include that.",,3.0,3.0,,2.0,7.333333333333333,3.0,,3.3333333333333335,,3.0
363,"A COMPARE-AGGREGATE MODEL FOR MATCHING TEXT SEQUENCES
Authors: Shuohang Wang, Jing Jiang
Source file: 363.pdf

ABSTRACT
Many NLP tasks including machine comprehension, answer selection and text entailment require the comparison between sequences. Matching the important units between sequences is a key to solve these problems. In this paper, we present a general “compare-aggregate” framework that performs word-level matching followed by aggregation using Convolutional Neural Networks. We particularly focus on the different comparison functions we can use to match two vectors. We use four different datasets to evaluate the model. We find that some simple comparison functions based on element-wise operations can work better than standard neural network and neural tensor network.

1 INTRODUCTION
Many natural language processing problems involve matching two or more sequences to make a decision. For example, in textual entailment, one needs to determine whether a hypothesis sentence can be inferred from a premise sentence (Bowman et al., 2015). In machine comprehension, given a passage, a question needs to be matched against it in order to find the correct answer (Richardson et al., 2013; Tapaswi et al., 2016). Table 1 gives two example sequence matching problems. In the first example, a passage, a question and four candidate answers are given. We can see that to get the correct answer, we need to match the question against the passage and identify the last sentence to be the answer-bearing sentence. In the second example, given a question and a set of candidate answers, we need to find the answer that best matches the question. Because of the fundamental importance of comparing two sequences of text to judge their semantic similarity or relatedness, sequence matching has been well studied in natural language processing.
With recent advances of neural network models in natural language processing, a standard practice for sequence modeling now is to encode a sequence of text as an embedding vector using models such as RNN and CNN. To match two sequences, a straightforward approach is to encode each sequence as a vector and then to combine the two vectors to make a decision (Bowman et al., 2015; Feng et al., 2015). However, it has been found that using a single vector to encode an entire sequence is not sufficient to capture all the important information from the sequence, and therefore advanced techniques such as attention mechanisms and memory networks have been applied to sequence matching problems (Hermann et al., 2015; Hill et al., 2016; Rocktäschel et al., 2015).
A common trait of a number of these recent studies on sequence matching problems is the use of a “compare-aggregate” framework (Wang & Jiang, 2016b; He & Lin, 2016; Parikh et al., 2016). In such a framework, comparison of two sequences is not done by comparing two vectors each representing an entire sequence. Instead, these models first compare vector representations of smaller units such as words from these sequences and then aggregate these comparison results to make the final decision. For example, the match-LSTM model proposed by Wang & Jiang (2016b) for textual entailment first compares each word in the hypothesis with an attention-weighted version of the premise. The comparison results are then aggregated through an LSTM. He & Lin (2016) proposed a pairwise word interaction model that first takes each pair of words from two sequences and applies a comparison unit on the two words. It then combines the results of these word interactions using a similarity focus layer followed by a multi-layer CNN. Parikh et al. (2016) proposed a decomposable attention model for textual entailment, in which words from each sequence are compared with an
attention-weighted version of the other sequence to produce a series of comparison vectors. The comparison vectors are then aggregated and fed into a feed forward network for final classification.
Although these studies have shown the effectiveness of such a “compare-aggregate” framework for sequence matching, there are at least two limitations with these previous studies: (1) Each of the models proposed in these studies is tested on one or two tasks only, but we hypothesize that this general framework is effective on many sequence matching problems. There has not been any study that empirically verifies this. (2) More importantly, these studies did not pay much attention to the comparison function that is used to compare two small textual units. Usually a standard feedforward network is used (Hu et al., 2014; Wang & Jiang, 2016b) to combine two vectors representing two units that need to be compared, e.g., two words. However, based on the nature of these sequence matching problems, we essentially need to measure how semantically similar the two sequences are. Presumably, this property of these sequence matching problems should guide us in choosing more appropriate comparison functions. Indeed He & Lin (2016) used cosine similarity, Euclidean distance and dot product to define the comparison function, which seem to be better justifiable. But they did not systematically evaluate these similarity or distance functions or compare them with a standard feedforward network.
In this paper, we argue that the general “compare-aggregate” framework is effective for a wide range of sequence matching problems. We present a model that follows this general framework and test it on four different datasets, namely, MovieQA, InsuranceQA, WikiQA and SNLI. The first three datasets are for Question Answering, but the setups of the tasks are quite different. The last dataset is for textual entailment. More importantly, we systematically present and test six different comparison functions. We find that overall a comparison function based on element-wise subtraction and multiplication works the best on the four datasets.
The contributions of this work are twofold: (1) Using four different datasets, we show that our model following the “compare-aggregate” framework is very effective when compared with the state-ofthe-art performance on these datasets. (2) We conduct systematic evaluation of different comparison functions and show that a comparison function based on element-wise operations, which is not widely used for word-level matching, works the best across the different datasets. We believe that these findings will be useful for future research on sequence matching problems. We have also made our code available online.1

2 METHOD
In this section, we propose a general model following the “compare-aggregate” framework for matching two sequences. This general model can be applied to different tasks. We focus our discussion on six different comparison functions that can be plugged into this general “compare-aggregate” model. In particular, we hypothesize that two comparison functions based on element-wise operations, SUB and MULT, are good middle ground between highly flexible functions using standard neural network models and highly restrictive functions based on cosine similarity and/or Euclidean
1https://github.com/shuohangwang/SeqMatchSeq
distance. As we will show in the experiment section, these comparison functions based on elementwise operations can indeed perform very well on a number of sequence matching problems.

2.1 PROBLEM DEFINITION AND MODEL OVERVIEW
The general setup of the sequence matching problem we consider is the following. We assume there are two sequences to be matched. We use two matrices Q ∈ Rd×Q and A ∈ Rd×A to represent the word embeddings of the two sequences, where Q and A are the lengths of the two sequences, respectively, and d is the dimensionality of the word embeddings. In other words, each column vector of Q or A is an embedding vector representing a single word. Given a pair of Q and A, the goal is to predict a label y. For example, in textual entailment, Q may represent a premise and A a hypothesis, and y indicates whether Q entails A or contradicts A. In question answering, Q may be a question and A a candidate answer, and y indicates whether A is the correct answer to Q.
We treat the problem as a supervised learning task. We assume that a set of training examples in the form of (Q,A, y) is given and we aim to learn a model that maps any pair of (Q,A) to a y.
An overview of our model is shown in Figure 1. The model can be divided into the following four layers:
1. Preprocessing: We use a preprocessing layer (not shown in the figure) to process Q and A to obtain two new matrices Q ∈ Rl×Q and A ∈ Rl×A. The purpose here is to use some gate values to control the importance of different words in making the predictions on the sequence pair. For example, qi ∈ Rl, which is the ith column vector of Q, encodes the ith word in Q.
2. Attention: We apply a standard attention mechanism on Q and A to obtain attention weights over the column vectors in Q for each column vector in A. With these attention weights, for each column vector aj in A, we obtain a corresponding vector hj , which is an attention-weighted sum of the column vectors of Q.
3. Comparison: We use a comparison function f to combine each pair of aj and hj into a vector tj .
4. Aggregation: We use a CNN layer to aggregate the sequence of vectors tj for the final classification.
Although this model follows more or less the same framework as the model proposed by Parikh et al. (2016), our work has some notable differences. First, we will pay much attention to the comparison function f and compare a number of options, including some uncommon ones based on elementwise operations. Second, we apply our model to four different datasets representing four different tasks to evaluate its general effectiveness for sequence matching problems. There are also some other differences from the work by Parikh et al. (2016). For example, we use a CNN layer instead of summation and concatenation for aggregation. Our attention mechanism is one-directional instead of two-directional.
In the rest of this section we will present the model in detail. We will focus mostly on the comparison functions we consider.

2.2 PREPROCESSING AND ATTENTION
Inspired by the use of gates in LSTM and GRU, we preprocess Q and A with the following formulas:
Q = σ(WiQ+ bi ⊗ eQ) tanh(WuQ+ bu ⊗ eQ), A = σ(WiA+ bi ⊗ eA) tanh(WuA+ bu ⊗ eA), (1)
where is element-wise multiplication, and Wi,Wu ∈ Rl×d and bi,bu ∈ Rl are parameters to be learned. The outer product (· ⊗ eX) produces a matrix or row vector by repeating the vector or scalar on the left for X times. Here σ(WiQ + bi ⊗ eQ) and σ(WiA + bi ⊗ eA) act as gate values to control the degree to which the original values of Q and A are preserved in Q and A. For example, for stop words, their gate values would likely be low for tasks where stop words make little difference to the final predictions.
In this preprocessing step, the word order does not matter. Although a better way would be to use RNN such as LSTM and GRU to chain up the words such that we can capture some contextual information, this could be computationally expensive for long sequences. In our experiments, we only incorporated LSTM into the formulas above for the SNLI task.
The general attention (Luong et al., 2015) layer is built on top of the resulting Q and A as follows: G = softmax ( (WgQ+ bg ⊗ eQ)TA ) ,
H = QG, (2)
where Wg ∈ Rl×l and bg ∈ Rl are parameters to be learned, G ∈ RQ×A is the attention weight matrix, and H ∈ Rl×A are the attention-weighted vectors. Specifically, hj , which is the jth column vector of H, is a weighted sum of the column vectors of Q and represents the part of Q that best matches the jth word in A. Next we will combine hj and aj using a comparison function.

2.3 COMPARISON
The goal of the comparison layer is to match each aj , which represents the jth word and its context in A, with hj , which represents a weighted version of Q that best matches aj . Let f denote a comparison function that transforms aj and hj into a vector tj to represent the comparison result.
A natural choice of f is a standard neural network layer that consists of a linear transformation followed by a non-linear activation function. For example, we can consider the following choice:
NEURALNET (NN): tj = f(aj ,hj) = ReLU(W [ aj hj ] + b), (3)
where matrix W ∈ Rl×2l and vector b ∈ Rl are parameters to be learned. Alternatively, another natural choice is a neural tensor network (Socher et al., 2013) as follows:
NEURALTENSORNET (NTN): tj = f(aj ,hj) = ReLU(aTjT [1...l]hj + b), (4)
where tensor T[1...l] ∈ Rl×l×l and vector b ∈ Rl are parameters to be learned.
However, we note that for many sequence matching problems, we intend to measure the semantic similarity or relatedness of the two sequences. So at the word level, we also intend to check how similar or related aj is to hj . For this reason, a more natural choice used in some previous work is Euclidean distance or cosine similarity between aj and hj . We therefore consider the following definition of f :
EUCLIDEAN+COSINE (EUCCOS): tj = f(aj ,hj) = [ ‖aj − hj‖2 cos(aj ,hj) ] . (5)
Note that with EUCCOS, the resulting vector tj is only a 2-dimensional vector. Although EUCCOS is a well-justified comparison function, we suspect that it may lose some useful information from the original vectors aj and hj . On the other hand, NN and NTN are too general and thus do not capture the intuition that we care mostly about the similarity between aj and hj .
To use something that is a good compromise between the two extreme cases, we consider the following two new comparison functions, which operate on the two vectors in an element-wise manner. These functions have been used previously by Mou et al. (2016).
SUBTRACTION (SUB): tj = f(aj ,hj) = (aj − hj) (aj − hj), (6) MULTIPLICATION (MULT): tj = f(aj ,hj) = aj hj . (7)
Note that the operator is element-wise multiplication. For both comparison functions, the resulting vector tj has the same dimensionality as aj and hj .
We can see that SUB is closely related to Euclidean distance in that Euclidean distance is the sum of all the entries of the vector tj produced by SUB. But by not summing up these entries, SUB preserves some information about the different dimensions of the original two vectors. Similarly, MULT is closely related to cosine similarity but preserves some information about the original two vectors.
Finally, we consider combining SUB and MULT followed by an NN layer as follows: SUBMULT+NN: tj = f(aj ,hj) = ReLU(W [ (aj − hj) (aj − hj)
aj hj
] + b). (8)
In summary, we consider six different comparison functions: NN, NTN, EUCCOS, SUB, MULT and SUBMULT+NN. Among these functions, the last three (SUB, MULT and SUBMULT+NN) have not been widely used in previous work for word-level matching.

2.4 AGGREGATION
After we apply the comparison function to each pair of aj and hj to obtain a series of vectors tj , finally we aggregate these vectors using a one-layer CNN (Kim, 2014):
r = CNN([t1, . . . , tA]). (9)
r ∈ Rnl is then used for the final classification, where n is the number of windows in CNN.

3 EXPERIMENTS
In this section, we evaluate our model on four different datasets representing different tasks. The first three datasets are question answering tasks while the last one is on textual entailment. The statistics of the four datasets are shown in Table 2. We will fist introduce the task settings and the way we customize the “compare-aggregate” structure to each task. Then we will show the baselines for the different datasets. Finally, we discuss the experiment results shown in Table 3 and the ablation study shown in Table 4.

3.1 TASK-SPECIFIC MODEL STRUCTURES
In all these tasks, we use matrix Q ∈ Rd×Q to represent the question or premise and matrix Ak ∈ Rd×Ak (k ∈ [1,K]) to represent the kth answer or the hypothesis. For the machine comprehension task MovieQA (Tapaswi et al., 2016), there is also a matrix P ∈ Rd×P that represents the plot of a movie. Here Q is the length of the question or premise, Ak the length of the kth answer, and P the length of the plot.
For the SNLI (Bowman et al., 2015) dataset, the task is text entailment, which identifies the relationship (entailment, contradiction or neutral) between a premise sentence and a hypothesis sentence. Here K = 1, and there are exactly two sequences to match. The actual model structure is what we have described before.
For the InsuranceQA (Feng et al., 2015) dataset, the task is an answer selection task which needs to select the correct answer for a question from a candidate pool. For the WikiQA (Yang et al., 2015) datasets, we need to rank the candidate answers according to a question. For both tasks,
there are K candidate answers for each question. Let us use rk to represent the resulting vector produced by Eqn. 9 for the kth answer. In order to select one of the K answers, we first define R = [r1, r2, . . . , rK ]. We then compute the probability of the kth answer to be the correct one as follows:
p(k|R) = softmax(wT tanh(WsR+ bs ⊗ eK) + b⊗ eK), (10) where Ws ∈ Rl×nl, w ∈ Rl, bs ∈ Rl, b ∈ R are parameters to be learned. For the machine comprehension task MovieQA, each question is related to Plot Synopses written by fans after watching the movie and each question has five candidate answers. So for each candidate answer there are three sequences to be matched: the plot P, the question Q and the answer Ak. For each k, we first match Q and P and refer to the matching result at position j as tqj , as generated by one of the comparison functions f . Similarly, we also match Ak with P and refer to the matching result at position j as tak,j . We then define
tk,j = [ tqj tak,j ] ,
and
rk = CNN([tk,1, . . . , tk,P ]).
To select an answer from the K candidate answers, again we use Eqn. 10 to compute the probabilities.
The implementation details of the modes are as follows. The word embeddings are initialized from GloVe (Pennington et al., 2014). During training, they are not updated. The word embeddings not found in GloVe are initialized with zero.
The dimensionality l of the hidden layers is set to be 150. We use ADAMAX (Kingma & Ba, 2015) with the coefficients β1 = 0.9 and β2 = 0.999 to optimize the model. We do not use L2regularization. The main parameter we tuned is the dropout on the embedding layer. For WikiQA, which is a relatively small dataset, we also tune the learning rate and the batch size. For the others, we set the batch size to be 30 and the learning rate 0.002.

3.2 BASELINES
Here, we will introduce the baselines for each dataset. We did not re-implement these models but simply took the reported performance for the purpose of comparison.
SNLI: •W-by-W Attention: The model by Rocktäschel et al. (2015), who first introduced attention mechanism into text entailment. • match-LSTM: The model by Wang & Jiang (2016b), which concatenates the matched words as the inputs of an LSTM. • LSTMN: Long short-term memorynetworks proposed by Cheng et al. (2016). • Decomp Attention: Another “compare-aggregate” model proposed by Parikh et al. (2016). • EBIM+TreeLSTM: The state-of-the-art model proposed by Chen et al. (2016) on the SNLI dataset.
InsuranceQA: • IR model: This model by Bendersky et al. (2010) learns the concept information to help rank the candidates. • CNN with GESD: This model by Feng et al. (2015) uses Euclidean distance and dot product between sequence representations built through convolutional neural networks to select the answer. • Attentive LSTM: Tan et al. (2016) used soft-attention mechanism to select the most important information from the candidates according to the representation of the questions. • IARNN-Occam: This model by Wang et al. (2016) adds regularization on the attention weights. • IARNN-Gate: This model by Wang et al. (2016) uses the representation of the question to build the GRU gates for each candidate answer.
WikiQA: • IARNN-Occam and IARNN-Gate as introduced before. • CNN-Cnt: This model by Yang et al. (2015) combines sentence representations built by a convolutional neural network with logistic regression. • ABCNN: This model is Attention-Based Convolutional Neural Network proposed by Yin et al. (2015). • CubeCNN proposed by He & Lin (2016) builds a CNN on all pairs of word similarity.
MovieQA: All the baselines we consider come from Tapaswi et al. (2016)’s work: • Cosine Word2Vec: A sliding window is used to select the answer according to the similarities computed
through Word2Vec between the sentences in plot and the question/answer. • Cosine TFIDF: This model is similar to the previous method but uses bag-of-word with tf-idf scores to compute similarity. • SSCB TFIDF: Instead of using the sliding window method, a convolutional neural network is built on the sentence level similarities.

3.3 ANALYSIS OF RESULTS
We use accuracy as the evaluation metric for the datasets MovieQA, InsuranceQA and SNLI, as there is only one correct answer or one label for each instance. For WikiQA, there may be multiple correct answers, so evaluation metrics we use are Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR).
We observe the following from the results. (1) Overall, we can find that our general “compareaggregate” structure achieves the best performance on MovieQA, InsuranceQA, WikiQA datasets and very competitive performance on the SNLI dataset. Especially for the InsuranceQA dataset, with any comparison function we use, our model can outperform all the previous models. (2) The comparison method SUBMULT+NN is the best in general. (3) Some simple comparison functions can achieve better performance than the neural networks or neural tensor network comparison functions. For example, the simplest comparison function EUCCOS achieves nearly the best performance in the MovieQA dataset, and the element-wise comparison functions, which do not need parameters can achieve the best performance on the WikiQA dataset. (4) We find the preprocessing layer and the attention layer for word selection to be important in the “compare-aggregate” structure through the experiments of removing these two layers separately. We also see that for sequence matching with big difference in length, such as the MovieQA and InsuranceQA tasks, the attention layer plays a more important role. For sequence matching with smaller difference in length, such as the WikiQA and SNLI tasks, the pre-processing layer plays a more important role. (5) For the MovieQA, InsuranceQA and WikiQA tasks, our preprocessing layer is order-insensitive so that it will not take the context information into consideration during the comparison, but our model can still outperform the previous work with order-sensitive preprocessing layer. With this finding, we believe the word-by-word comparison part plays a very important role in these tasks. We will further explore the preprocessing layer in the future.

3.4 FURTHER ANALYSES
To further explain how our model works, we visualize the max values in each dimension of the convolutional layer. We use two examples shown in Table 1 from MovieQA and InsuranceQA datasets respectively. In the top of Figure 2, we can see that the plot words that also appear in either the question or the answer will draw more attention by the CNN. We hypothesize that if the nearby words in the plot can match both the words in question and the words in one answer, then this answer is more likely to be the correct one. Similarly, the bottom one of Figure 2 also shows that the CNN will focus more on the matched word representations. If the words in one answer continuously match the words in the question, this answer is more likely to be the correct one.

4 RELATED WORK
We review related work in three types of general structures for matching sequences.
Siamense network: These kinds of models use the same structure, such as RNN or CNN, to build the representations for the sequences separately and then use them for classification. Then cosine similarity (Feng et al., 2015; Yang et al., 2015), element-wise operation (Tai et al., 2015; Mou et al., 2016) or neural network-based combination Bowman et al. (2015) are used for sequence matching.
Attentive network: Soft-attention mechanism (Bahdanau et al., 2014; Luong et al., 2015) has been widely used for sequence matching in machine comprehension (Hermann et al., 2015), text entailment (Rocktäschel et al., 2015) and question answering (Tan et al., 2016). Instead of using the final state of RNN to represent a sequence, these studies use weighted sum of all the states for the sequence representation.
Compare-Aggregate network: This kind of framework is to perform the word level matching (Wang & Jiang, 2016a; Parikh et al., 2016; He & Lin, 2016; Trischler et al., 2016; Wan et al.,
2016). Our work is under this framework. But our structure is different from previous models and our model can be applied on different tasks. Besides, we analyzed different word-level comparison functions separately.

5 CONCLUSIONS
In this paper, we systematically analyzed the effectiveness of a “compare-aggregate” model on four different datasets representing different tasks. Moreover, we compared and tested different kinds of word-level comparison functions and found that some element-wise comparison functions can outperform the others. According to our experiment results, many different tasks can share the same “compare-aggregate” structure. In the future work, we would like to test its effectiveness on multi-task learning.

6 ACKNOWLEDGMENTS
This research is supported by the National Research Foundation, Prime Ministers Office, Singapore under its International Research Centres in Singapore Funding Initiative.
","Many NLP tasks including machine comprehension, answer selection and text entailment require the comparison between sequences. Matching the important units between sequences is a key to solve these problems. In this paper, we present a general “compare-aggregate” framework that performs word-level matching followed by aggregation using Convolutional Neural Networks. We particularly focus on the different comparison functions we can use to match two vectors. We use four different datasets to evaluate the model. We find that some simple comparison functions based on element-wise operations can work better than standard neural network and neural tensor network.",ICLR 2017 conference submission,True,,"The paper presents a general approach to modeling for natural language understanding problems with two distinct textual inputs (such as a question and a source text) that can be aligned in some way. In the approach, soft attention is first used to derive alignments between the tokens of the two texts, then a comparison function uses the resulting alignments (represented as pairs of attention queries and attention results) to derive a representations that are aggregated by CNN into a single vector from which an output can be computed. The paper both presents this as an overall modeling strategy that can be made to work quite well, and offers a detailed empirical analysis of the comparison component of the model.

This work is timely. Language understanding problems of this kind are a major open issue in NLP, and are just at the threshold of being addressable with representation learning methods. The work presents a general approach which is straightforward and reasonable, and shows that it can yield good results. The work borders on incremental (relative to their earlier work or that of Parikh et al.), but it contributes in enough substantial ways that I'd strongly recommend acceptance.

Detail: 
- The model, at least as implemented for the problems with longer sequences (everything but SNLI), is not sensitive to word order. It is empirically competitive, but this insensitivity places a strong upper bound on its performance. The paper does make this clear, but it seems salient enough to warrant a brief mention in the introduction or discussion sections.
- If I understand correctly, your attention strategy is based more closely on the general/bilinear strategy of Luong et al. '15 than it is on the earlier Bahdanau work. You should probably cite the former (or some other more directly relevant reference for that strategy).
- Since the NTN risks overfitting because of its large number of parameters, did you try using a version with input dimension l and a smaller output dimension m (so an l*l*m tensor)?
- You should probably note that SubMultNN looks a lot like the strategy for *sentence*-level matching in the Lili Mou paper you cite.
- Is there a reason you use the same parameters for preprocessing the question and answer in (1)? These could require different things to be weighted highly.

---

This paper proposes a framework whereby, to an attention mechanism relating one text segment to another piecewise, an aggregation mechanism is added to yield an architecture matching words of one segment to another. Different vector comparison operations are explored in this framework. The reviewers were satisfied that this work is relevant, timely, clearly presented, and that the empirical validation was sound.

---

This paper proposed a compare-aggregate model for the NLP tasks that require semantically comparing the text sequences, such as question answering and textual entailment.
The basic framework of this model is to apply a convolutional neural network (aggregation) after a element-wise operation (comparison) over the attentive outputs of the LSTMs. 
The highlighted part is the comparison, where this paper compares several different methods for matching text sequences, and the element-wise subtraction/multiplication operations are demonstrated to achieve generally better performance on four different datasets.
While the weak point is that this is an incremental work and a bit lack of innovation. A qualitative evaluation about how subtraction, multiplication and other comparison functions perform on varied kinds of sentences would be more interesting.

---

This paper proposes a compare-aggregate framework that performs word-level matching followed by aggregation with convolutional neural networks. It compares six different comparison functions and evaluates them on four datasets. Extensive experimental results have been reported and compared against various published baselines.

The paper is well written overall.

A few detailed comments:
* page 4, line5: including a some -> including some
* What's the benefit of the preprocessing and attention step? Can you provide the results without it?
* Figure 2 is hard to read, esp. when on printed hard copy. Please enhance the quality.

---

The paper presents a general approach to modeling for natural language understanding problems with two distinct textual inputs (such as a question and a source text) that can be aligned in some way. In the approach, soft attention is first used to derive alignments between the tokens of the two texts, then a comparison function uses the resulting alignments (represented as pairs of attention queries and attention results) to derive a representations that are aggregated by CNN into a single vector from which an output can be computed. The paper both presents this as an overall modeling strategy that can be made to work quite well, and offers a detailed empirical analysis of the comparison component of the model.

This work is timely. Language understanding problems of this kind are a major open issue in NLP, and are just at the threshold of being addressable with representation learning methods. The work presents a general approach which is straightforward and reasonable, and shows that it can yield good results. The work borders on incremental (relative to their earlier work or that of Parikh et al.), but it contributes in enough substantial ways that I'd strongly recommend acceptance.

Detail: 
- The model, at least as implemented for the problems with longer sequences (everything but SNLI), is not sensitive to word order. It is empirically competitive, but this insensitivity places a strong upper bound on its performance. The paper does make this clear, but it seems salient enough to warrant a brief mention in the introduction or discussion sections.
- If I understand correctly, your attention strategy is based more closely on the general/bilinear strategy of Luong et al. '15 than it is on the earlier Bahdanau work. You should probably cite the former (or some other more directly relevant reference for that strategy).
- Since the NTN risks overfitting because of its large number of parameters, did you try using a version with input dimension l and a smaller output dimension m (so an l*l*m tensor)?
- You should probably note that SubMultNN looks a lot like the strategy for *sentence*-level matching in the Lili Mou paper you cite.
- Is there a reason you use the same parameters for preprocessing the question and answer in (1)? These could require different things to be weighted highly.

---

Text matching models based on Attention mechanism make sense. 
There are also some matching models based on Matching Matrix.
Attention mechanism also computes a matching matrix implicitly and the attention weights before softmax are the values of the matching matrix. 
I wonder which way is better, Attention or Matching Matrix, and Why?
How do you think？
I will appreciate it if you could compare these models in your future works.

Reference of Matching Matrix Models:
1. A Deep Architecture for Semantic Matching with Multiple Positional Sentence Representations. AAAI 2016.
2. Match-SRNN: Modeling the Recursive Matching Structure with Spatial RNN. IJCAI 2016.
3. Text Matching as Image Recognition. AAAI 2016.

---

The paper presents a general approach to modeling for natural language understanding problems with two distinct textual inputs (such as a question and a source text) that can be aligned in some way. In the approach, soft attention is first used to derive alignments between the tokens of the two texts, then a comparison function uses the resulting alignments (represented as pairs of attention queries and attention results) to derive a representations that are aggregated by CNN into a single vector from which an output can be computed. The paper both presents this as an overall modeling strategy that can be made to work quite well, and offers a detailed empirical analysis of the comparison component of the model.

This work is timely. Language understanding problems of this kind are a major open issue in NLP, and are just at the threshold of being addressable with representation learning methods. The work presents a general approach which is straightforward and reasonable, and shows that it can yield good results. The work borders on incremental (relative to their earlier work or that of Parikh et al.), but it contributes in enough substantial ways that I'd strongly recommend acceptance.

Detail: 
- The model, at least as implemented for the problems with longer sequences (everything but SNLI), is not sensitive to word order. It is empirically competitive, but this insensitivity places a strong upper bound on its performance. The paper does make this clear, but it seems salient enough to warrant a brief mention in the introduction or discussion sections.
- If I understand correctly, your attention strategy is based more closely on the general/bilinear strategy of Luong et al. '15 than it is on the earlier Bahdanau work. You should probably cite the former (or some other more directly relevant reference for that strategy).
- Since the NTN risks overfitting because of its large number of parameters, did you try using a version with input dimension l and a smaller output dimension m (so an l*l*m tensor)?
- You should probably note that SubMultNN looks a lot like the strategy for *sentence*-level matching in the Lili Mou paper you cite.
- Is there a reason you use the same parameters for preprocessing the question and answer in (1)? These could require different things to be weighted highly.

---

This paper proposes a framework whereby, to an attention mechanism relating one text segment to another piecewise, an aggregation mechanism is added to yield an architecture matching words of one segment to another. Different vector comparison operations are explored in this framework. The reviewers were satisfied that this work is relevant, timely, clearly presented, and that the empirical validation was sound.

---

This paper proposed a compare-aggregate model for the NLP tasks that require semantically comparing the text sequences, such as question answering and textual entailment.
The basic framework of this model is to apply a convolutional neural network (aggregation) after a element-wise operation (comparison) over the attentive outputs of the LSTMs. 
The highlighted part is the comparison, where this paper compares several different methods for matching text sequences, and the element-wise subtraction/multiplication operations are demonstrated to achieve generally better performance on four different datasets.
While the weak point is that this is an incremental work and a bit lack of innovation. A qualitative evaluation about how subtraction, multiplication and other comparison functions perform on varied kinds of sentences would be more interesting.

---

This paper proposes a compare-aggregate framework that performs word-level matching followed by aggregation with convolutional neural networks. It compares six different comparison functions and evaluates them on four datasets. Extensive experimental results have been reported and compared against various published baselines.

The paper is well written overall.

A few detailed comments:
* page 4, line5: including a some -> including some
* What's the benefit of the preprocessing and attention step? Can you provide the results without it?
* Figure 2 is hard to read, esp. when on printed hard copy. Please enhance the quality.

---

The paper presents a general approach to modeling for natural language understanding problems with two distinct textual inputs (such as a question and a source text) that can be aligned in some way. In the approach, soft attention is first used to derive alignments between the tokens of the two texts, then a comparison function uses the resulting alignments (represented as pairs of attention queries and attention results) to derive a representations that are aggregated by CNN into a single vector from which an output can be computed. The paper both presents this as an overall modeling strategy that can be made to work quite well, and offers a detailed empirical analysis of the comparison component of the model.

This work is timely. Language understanding problems of this kind are a major open issue in NLP, and are just at the threshold of being addressable with representation learning methods. The work presents a general approach which is straightforward and reasonable, and shows that it can yield good results. The work borders on incremental (relative to their earlier work or that of Parikh et al.), but it contributes in enough substantial ways that I'd strongly recommend acceptance.

Detail: 
- The model, at least as implemented for the problems with longer sequences (everything but SNLI), is not sensitive to word order. It is empirically competitive, but this insensitivity places a strong upper bound on its performance. The paper does make this clear, but it seems salient enough to warrant a brief mention in the introduction or discussion sections.
- If I understand correctly, your attention strategy is based more closely on the general/bilinear strategy of Luong et al. '15 than it is on the earlier Bahdanau work. You should probably cite the former (or some other more directly relevant reference for that strategy).
- Since the NTN risks overfitting because of its large number of parameters, did you try using a version with input dimension l and a smaller output dimension m (so an l*l*m tensor)?
- You should probably note that SubMultNN looks a lot like the strategy for *sentence*-level matching in the Lili Mou paper you cite.
- Is there a reason you use the same parameters for preprocessing the question and answer in (1)? These could require different things to be weighted highly.

---

Text matching models based on Attention mechanism make sense. 
There are also some matching models based on Matching Matrix.
Attention mechanism also computes a matching matrix implicitly and the attention weights before softmax are the values of the matching matrix. 
I wonder which way is better, Attention or Matching Matrix, and Why?
How do you think？
I will appreciate it if you could compare these models in your future works.

Reference of Matching Matrix Models:
1. A Deep Architecture for Semantic Matching with Multiple Positional Sentence Representations. AAAI 2016.
2. Match-SRNN: Modeling the Recursive Matching Structure with Spatial RNN. IJCAI 2016.
3. Text Matching as Image Recognition. AAAI 2016.",,5.0,3.0,,3.0,7.0,5.0,,4.666666666666667,5.0,4.0
398,"A RECURRENT NEURAL NETWORK WITHOUT CHAOS
Authors: Thomas Laurent
Source file: 398.pdf

ABSTRACT
We introduce an exceptionally simple gated recurrent neural network (RNN) that achieves performance comparable to well-known gated architectures, such as LSTMs and GRUs, on the word-level language modeling task. We prove that our model has simple, predicable and non-chaotic dynamics. This stands in stark contrast to more standard gated architectures, whose underlying dynamical systems exhibit chaotic behavior.

1 INTRODUCTION
Gated recurrent neural networks, such as the Long Short Term Memory network (LSTM) introduced by Hochreiter & Schmidhuber (1997) and the Gated Recurrent Unit (GRU) proposed by Cho et al. (2014), prove highly effective for machine learning tasks that involve sequential data. We propose an exceptionally simple variant of these gated architectures. The basic model takes the form
ht = θt tanh(ht−1) + ηt tanh(Wxt), (1)
where stands for the Hadamard product. The horizontal/forget gate (i.e. θt) and the vertical/input gate (i.e. ηt) take the usual form used in most gated RNN architectures. Specifically
θt := σ (Uθht−1 + Vθxt + bθ) and ηt := σ (Uηht−1 + Vηxt + bη) (2)
where σ(x) := (1 + e−x)−1 denotes the logistic sigmoid function. The network (1)–(2) has quite intuitive dynamics. Suppose the data xt present the model with a sequence
(Wxt)(i) = { 10 if t = T 0 otherwise,
(3)
where (Wxt)(i) stands for the ith component of the vector Wxt. In other words we consider an input sequence xt for which the learned ith feature (Wxt)(i) remains off except at time T . When initialized from h0 = 0, the corresponding response of the network to this “impulse” in the ith feature is
ht(i) ≈  0 if t < T ηT if t = T αt if t > T
(4)
with αt a sequence that relaxes toward zero. The forget gate θt control the rate of this relaxation. Thus ht(i) activates when presented with a strong ith feature, and then relaxes toward zero until the data present the network once again with strong ith feature. Overall this leads to a dynamically simple model, in which the activation patterns in the hidden states of the network have a clear cause and predictable subsequent behavior.
Dynamics of this sort do not occur in other RNN models. Instead, the three most popular recurrent neural network architectures, namely the vanilla RNN, the LSTM and the GRU, have complex, irregular, and unpredictable dynamics. Even in the absence of input data, these networks can give rise to chaotic dynamical systems. In other words, when presented with null input data the activation patterns in their hidden states do not necessarily follow a predictable path. The proposed network (1)–(2) has rather dull and minimalist dynamics in comparison; its only attractor is the zero state,
and so it stands at the polar-opposite end of the spectrum from chaotic systems. Perhaps surprisingly, at least in the light of this comparison, the proposed network (1) performs as well as LSTMs and GRUs on the word level language modeling task. We therefore conclude that the ability of an RNN to form chaotic temporal dynamics, in the sense we describe in Section 2, cannot explain its success on word-level language modeling tasks.
In the next section, we review the phenomenon of chaos in RNNs via both synthetic examples and trained models. We also prove a precise, quantified description of the dynamical picture (3)–(4) for the proposed network. In particular, we show that the dynamical system induced by the proposed network is never chaotic, and for this reason we refer to it as a Chaos-Free Network (CFN). The final section provides a series of experiments that demonstrate that CFN achieve results comparable to LSTM on the word-level language modeling task. All together, these observations show that an architecture as simple as (1)–(2) can achieve performance comparable to the more dynamically complex LSTM.

2 CHAOS IN RECURRENT NEURAL NETWORKS
The study of RNNs from a dynamical systems point-of-view has brought fruitful insights into generic features of RNNs (Sussillo & Barak, 2013; Pascanu et al., 2013). We shall pursue a brief investigation of CFN, LSTM and GRU networks using this formalism, as it allows us to identify key distinctions between them. Recall that for a given mapping Φ : Rd 7→ Rd, a given initial time t0 ∈ N and a given initial state u0 ∈ Rd, a simple repeated iteration of the mapping Φ
ut+1 = Φ(ut) t > t0,
ut0 = u0 t = t0,
defines a discrete-time dynamical system. The index t ∈ N represents the current time, while the point ut ∈ Rd represents the current state of the system. The set of all visited states O+(u0) := {ut0 , ut0+1, . . . , ut0+n, . . .} defines the forward trajectory or forward orbit through u0. An attractor for the dynamical system is a set that is invariant (any trajectory that starts in the set remains in the set) and that attracts all trajectories that start sufficiently close to it. The attractors of chaotic dynamical systems are often fractal sets, and for this reason they are referred to as strange attractors.
Most RNNs generically take the functional form
ut = Ψ(ut−1,W1xt,W2xt, . . . ,Wkxt), (5)
where xt denotes the tth input data point. For example, in the case of the CFN (1)–(2), we have W1 = W , W2 = Vθ and W3 = Vη . To gain insight into the underlying design of the architecture of an RNN, it proves usefull to consider how trajectories behave when they are not influenced by any external input. This lead us to consider the dynamical system
ut = Φ(ut−1) Φ(u) := Ψ(u, 0, 0, . . . , 0), (6)
which we refer to as the dynamical system induced by the recurrent neural network. The timeinvariant system (6) is much more tractable than (5), and it offers a mean to investigate the inner working of a given architecture; it separates the influence of input data xt, which can produce essentially any possible response, from the model itself. Studying trajectories that are not influenced by external data will give us an indication on the ability of a given RNN to generate complex and sophisticated trajectories by its own. As we shall see shortly, the dynamical system induced by a CFN has excessively simple and predictable trajectories: all of them converge to the zero state. In other words, its only attractor is the zero state. This is in sharp contrast with the dynamical systems induced by LSTM or GRU, who can exhibit chaotic behaviors and have strange attractors.
The learned parameters Wj in (5) describe how data influence the evolution of hidden states at each time step. From a modeling perspective, (6) would occur in the scenario where a trained RNN has learned a weak coupling between a specific data point xt0 and the hidden state at that time, in the sense that the data influence is small and so all Wjxt0 ≈ 0 nearly vanish. The hidden state then transitions according to ut0 ≈ Ψ(ut0−1, 0, 0, . . . , 0) = Φ(ut0−1). We refer to Bertschinger & Natschläger (2004) for a study of the chaotic behavior of a simplified vanilla RNN with a specific statistical model, namely an i.i.d. Bernoulli process, for the input data as well as a specific statistical model, namely i.i.d. Gaussian, for the weights of the recurrence matrix.

2.1 CHAOTIC BEHAVIOR OF LSTM AND GRU IN THE ABSENCE OF INPUT DATA
In this subsection we briefly show that LSTM and GRU, in the absence of input data, can lead to dynamical systems ut = Φ(ut−1) that are chaotic in the classical sense of the term (Strogatz, 2014). Figure 1 depicts the strange attractor of the dynamical system:
ut = [ ht ct ] u 7→ Φ(u) = [ o tanh (f c+ i g) f c+ i g ] (7)
i := σ(Wih+ bi) f := σ(Wfh+ bf ) o := σ(Woh+ bo) g := tanh(Wgh+ bg) (8)
induced by a two-unit LSTM with weight matrices
Wi = [ −1 −4 −3 −2 ] Wo = [ 4 1 −9 −7 ] Wf = [ −2 6 0 −6 ] Wg = [ −1 −6 6 −9 ] (9)
and zero bias for the model parameters. These weights were randomly generated from a normal distribution with standard deviation 5 and then rounded to the nearest integer. Figure 1(a) was obtained by choosing an initial state u0 = (h0, c0) uniformly at random in [0, 1]2 × [0, 1]2 and plotting the h-component of the iterates ut = (ht, ct) for t between 103 and 105 (so this figure should be regarded as a two dimensional projection of a four dimensional attractor, which explain its tangled appearance). Most trajectories starting in [0, 1]2 × [0, 1]2 converge toward the depicted attractor. The resemblance between this attractor and classical strange attractors such as the Hénon attractor is striking (see Figure 5 in the appendix for a depiction of the Hénon attractor). Successive zooms on the branch of the LSTM attractor from Figure 1(a) reveal its fractal nature. Figure 1(b) is an enlargement of the red box in Figure 1(a), and Figure 1(c) is an enlargement of the magenta box in Figure 1(b). We see that the structure repeats itself as we zoom in.
The most practical consequence of chaos is that the long-term behavior of their forward orbits can exhibit a high degree of sensitivity to the initial states u0. Figure 2 provides an example of such behavior for the dynamical system (7)–(9). An initial condition u0 was drawn uniformly at random in [0, 1]2 × [0, 1]2. We then computed 100, 000 small amplitude perturbations û0 of u0 by adding a small random number drawn uniformly from [−10−7, 10−7] to each component. We then iterated (7)–(9) for 200 steps and plotted the h-component of the final state û200 for each of the 100, 000 trials on Figure 2(a). The collection of these 100, 000 final states essentially fills out the entire attractor, despite the fact that their initial conditions are highly localized (i.e. at distance of no more than 10−7) around a fixed point. In other words, the time t = 200 map of the dynamical system will map a small neighborhood around a fixed initial condition u0 to the entire attractor. Figure 2(b) additionally illustrates this sensitivity to initial conditions for points on the attractor itself. We take an initial condition u0 on the attractor and perturb it by 10−7 to a nearby initial condition û0. We then plot the distance ‖ût − ut‖ between the two corresponding trajectories for the first 200 time steps. After an initial phase of agreement, the trajectories strongly diverge.
The synthetic example (7)–(9) illustrates the potentially chaotic nature of the LSTM architecture. We now show that chaotic behavior occurs for trained models as well, and not just for synthetically generated instances. We take the parameter values of an LSTM with 228 hidden units trained on the
Penn Treebank corpus without dropout (c.f. the experimental section for the precise procedure). We then set all data inputs xt to zero and run the corresponding induced dynamical system. Two trajectories starting from nearby initial conditions u0 and û0 were computed (as before û0 was obtained by adding to each components of u0 a small random number drawn uniformly from [−10−7, 10−7]). Figure 3(a) plots the first component h(1) of the hidden state for both trajectories over the first 1600 time steps. After an initial phase of agreement, the forward trajectories O+(u0) and O+(û0) strongly diverge. We also see that both trajectories exhibit the typical aperiodic behavior that characterizes chaotic systems. If the inputs xt do not vanish, but come from actual word-level data, then the behavior is very different. The LSTM is now no longer an autonomous system whose dynamics are driven by its hidden states, but a time dependent system whose dynamics are mostly driven by the external inputs. Figure 3(b) shows the first component h(1) of the hidden states of two trajectories that start with initial conditions u0 and û0 that are far apart. The sensitivity to initial condition disappears, and instead the trajectories converge toward each other after about 70 steps. The memory of this initial difference is lost. Overall these experiments indicate that a trained LSTM, when it is not driven by external inputs, can be chaotic. In the presence of input data, the LSTM becomes a forced system whose dynamics are dominated by external forcing.
Like LSTM networks, GRU can also lead to dynamical systems that are chaotic and they can also have strange attractors. The depiction of such an attractor, in the case of a two-unit GRU, is provided in Figure 6 of the appendix.

2.2 CHAOS-FREE BEHAVIOR OF THE CFN
The dynamical behavior of the CFN is dramatically different from that of the LSTM. In this subsection we start by showing that the hidden states of the CFN activate and relax toward zero in a predictable fashion in response to input data. On one hand, this shows that the CFN cannot produce non-trivial dynamics without some influence from data. On the other, this leads to an interpretable model; any non-trivial activations in the hidden states of a CFN have a clear cause emanating from
data-driven activation. This follows from a precise, quantified description of the intuitive picture (3)–(4) sketched in the introduction.
We begin with the following simple estimate that sheds light on how the hidden states of the CFN activate and then relax toward the origin. Lemma 1. For any T, k > 0 we have
|hT+k(i)| ≤ Θk |hT (i)|+ H
1−Θ
( max
T≤t≤T+k |(Wxt)(i)| ) where Θ and H are the maximum values of the ith components of the θ and η gate in the time interval [T, T + k], that is:
Θ = max T≤t≤T+k θt(i) and H = max T≤t≤T+k ηt(i).
This estimate shows that if during a time interval [T1, T2] one of
(i) the embedded inputs Wxt have weak ith feature (i.e. maxT≤t≤T+k |(Wxt)(i)| is small), (ii) or the input gates ηt have their ith component close to zero (i.e. H is small),
occurs then the ith component of the hidden state ht will relaxes toward zero at a rate that depends on the value of the ith component the the forget gate. Overall this leads to the following simple picture: ht(i) activates when presented with an embedded input Wxt with strong ith feature, and then relaxes toward zero until the data present the network once again with strong ith feature. The strength of the activation and the decay rate are controlled by the ith component of the input and forget gates. The proof of Lemma 1 is elementary —
Proof of Lemma 1. Using the non-expansivity of the hyperbolic tangent, i.e. | tanh(x)| ≤ |x|, and the triangle inequality, we obtain from (1)
|ht(i)| ≤ Θ |ht−1(i)|+H max T≤t≤T+k |(Wxt)(i)|
whenever t is in the interval [T, T + k]. Iterating this inequality and summing the geometric series then gives |hT+k(i)| ≤ Θk|hT (i)|+ ( 1−Θk
1−Θ
) H max
T≤t≤T+k |(Wxt)(i)|
from which we easily conclude.
We now turn toward the analysis of the long-term behavior of the the dynamical system
ut = ht, u 7→ Φ(u) := σ (Uθu + bθ) tanh(u). (10) induced by a CFN. The following lemma shows that the only attractor of this dynamical system is the zero state. Lemma 2. Starting from any initial state u0, the trajectory O+(u0) will eventually converge to the zero state. That is, limt→+∞ ut = 0 regardless of the the initial state u0.
Proof. From the definition of Φ we clearly have that the sequence defined by ut+1 = Φ(ut) satisfies −1 < ut(i) < 1 for all t and all i. Since the sequence ut is bounded, so is the sequence vt := Uθut + bθ. That is there exists a finite C > 0 such that (Uθut)(i) + bθ(i) < C for all t and i. Using the non-expansivity of the hyperbolic tangent, we then obtain that |ut(i)| ≤ σ(C)|ut−1(i)|, for all t and all i. We conclude by noting that 0 < σ(C) < 1.
Lemma 2 remains true for a multi-layer CFN, that is, a CFN in which the first layer is defined by (1) and the subsequent layers 2 ≤ ` ≤ L are defined by:
h (`) t = θ (`) t tanh(h (`) t−1) + η (`) t tanh(W (`)h (`−1) t ).
Assume that Wxt = 0 for all t > T , then an extension of the arguments contained in the proof of the two previous lemmas shows that
|h(`)T+k| ≤ C(1 + k) (`−1)Θk (11)
where 0 < Θ < 1 is the maximal values for the input gates involved in layer 1 to ` of the network, and C > 0 is some constant depending only on the norms ‖W (j)‖∞ of the matrices and the sizes |h(j)T | of the initial conditions at all previous 1 ≤ j ≤ ` levels. Estimate (11) shows that Lemma 2 remains true for multi-layer architectures.
Inequality (11) shows that higher levels (i.e. larger `) decay more slowly, and remain non-trivial, while earlier levels (i.e. smaller `) decay more quickly. We illustrate this behavior computationally with a simple experiment. We take a 2-layer, 224-unit CFN network trained on Penn Treebank and feed it the following input data: The first 1000 inputs xt are the first 1000 words of the test set of Penn Treebank; All subsequent inputs are zero. In other words, xt = 0 if t > 1000. For each of the two layers we then select the 10 units that decay the slowest after t > 1000 and plot them on Figure 4. The figure illustrates that the second layer retains information for much longer than the first layer. To quantify this observation we define the relaxation time (or half-life) of the ith unit as the smallest T such that |h1000+T (i)| < 0.5|h1000(i)|. Using this definition yields average relaxation times of 2.2 time steps for the first layer and 23.2 time steps for the second layer. The first layer has a standard deviations of approximately 5 steps while the second layer has a standard deviation of approximately 75 time steps. A more fine-grained analysis reveals that some units in the second layer have relaxation times of several hundred steps. For instance, if instead of averaging the relaxation times over the whole layer we average them over the top quartile (i.e. the 25% units that decay the most slowly) we get 4.8 time steps and 85.6 time steps for the first and second layers, respectively. In other words, by restricting attention to long-term units the difference between the first and second layers becomes much more striking.
Overall, this experiment conforms with the analysis (11), and indicates that adding a third or fourth layer would potentially allow a multi-layer CFN architecture to retain information for even longer.

3 EXPERIMENTS
In this section we show that despite its simplicity, the CFN network achieves performance comparable to the much more complex LSTM network on the word level language modeling task. We use two datasets for these experiments, namely the Penn Treebank corpus (Marcus et al., 1993) and the Text8 corpus (Mikolov et al., 2014). We consider both one-layer and two-layer CFNs and LSTMs for our experiments. We train both CFN and LSTM networks in a similar fashion and always compare models that use the same number of parameters. We compare their performance with and without dropout, and show that in both cases they obtain similar results. We also provide results published in Mikolov et al. (2014), Jozefowicz et al. (2015) and Sukhbaatar et al. (2015) for the sake of comparison.
For concreteness, the exact implementation for the two-layer architecture of our model is
h (0) t = W (0)xt
ĥ (0) t = Drop(h (0) t , p)
h (1) t = θ (1) t tanh(h (1) t−1) + η (1) t tanh(W (1)ĥ (0) t )
ĥ (1) t = Drop(h (1) t , p)
h (2) t = θ (2) t tanh(h (2) t−1) + η (2) t tanh(W (2)ĥ (1) t )
ĥ (2) t = Drop(h (2) t , p)
yt = LogSoftmax(W (3)ĥ (2) t + b)
where Drop(z, p) denotes the dropout operator with a probability p of setting components in z to zero. We compute the gates according to
θ (`) t := σ
( U
(`) θ h̃ (`) t−1 + V (`) θ h̃ (`−1) t + bθ ) and η(`)t := σ ( U (`)η h̃ (`) t−1 + V (`) η h̃ (`−1) t + bη ) where h̃(`)t−1 = Drop(h (`) t−1, q) and h̃ (`−1) t = Drop(h (`−1) t , q),
and thus the model has two dropout hyperparameters. The parameter p controls the amount of dropout between layers; the parameter q controls the amount of dropout inside each gate. We use a similar dropout strategy for the LSTM, in that all sigmoid gates f, o and i receive the same amount q of dropout.
To train the CFN and LSTM networks, we use a simple online steepest descent algorithm. We update the weights w via w(k+1) = w(k) − lr · ~p where ~p = ∇wL‖∇wL‖2 , (12) where lr is the learning rate and ∇wL denotes the approximate gradient of the loss with respect to the weights as estimated from a certain number of presented examples. We use the usual backpropagation through time approximation when estimating the gradient: we unroll the net T steps in the past and neglect longer dependencies. In all experiments, the CFN and LSTM networks are unrolled for T = 35 steps and we take minibatches of size 20. As all search directions ~p have Euclidean norm ‖~p‖2 = 1, we perform no gradient clipping during training. We initialize all the weights in the CFN, except for the bias of the gates, uniformly at random in [−0.07, 0.07]. We initialize the bias bθ and bη of the gates to 1 and −1, respectively, so that at the beginning of the training θt ≈ σ(1) ≈ 0.73 and ηt ≈ σ(−1) ≈ 0.23. We initialize the weights of the LSTM in exactly the same way; the bias for the forget and input gate are initialized to 1 and −1, and all the other weights are initialized uniformly in [−0.07, 0.07]. This initialization scheme favors
the flow of information in the horizontal direction. The importance of a careful initialization of the forget gate was pointed out in Gers et al. (2000) and Jozefowicz et al. (2015). Finally, we initialize all hidden states to zero for both models.
Dataset Construction. The Penn Treebank Corpus has 1 million words and a vocabulary size of 10,000. We used the code from Zaremba et al. (2014) to construct and split the dataset into a training set (929K words), a validation set (73K words) and a test set (82K words). The Text8 corpus has 100 million characters and a vocabulary size of 44,000. We used the script from Mikolov et al. (2014) to construct and split the dataset into a training set (first 99M characters) and a development set (last 1M characters).
Experiments without Dropout. Tables 1 and 2 provide a comparison of various recurrent network architectures without dropout evaluated on the Penn Treebank corpus and the Text8 corpus. The last two rows of each table provide results for LSTM and CFN networks trained and initialized in the manner described above. We have tried both one and two layer architectures, and reported only the best result. The learning rate schedules used for each network are described in the appendix.
We also report results published in Jozefowicz et al. (2015) were a vanilla RNN, a GRU and an LSTM network were trained on Penn Treebank, each of them having 5 million parameters (only the test perplexity was reported). Finally we report results published in Mikolov et al. (2014) and Sukhbaatar et al. (2015) where various networks are trained on Text8. Of these four networks, only the LSTM network from Mikolov et al. (2014) has the same number of parameters than the CFN and LSTM networks we trained (46.4M parameters). The vanilla RNN, Structurally Constrained Recurrent Network (SCRN) and End-To-End Memory Network (MemN2N) all have 500 units, but less than 46.4M parameters. We nonetheless indicate their performance in Table 2 to provide some context.
Experiments with Dropout. Table 3 provides a comparison of various recurrent network architectures with dropout evaluated on the Penn Treebank corpus. The first three rows report results published in (Jozefowicz et al., 2015) and the last four rows provide results for LSTM and CFN networks trained and initialized with the strategy previously described. The dropout rate p and q are chosen as follows: For the experiments with 20M parameters, we set p = 55% and q = 45% for the CFN and p = 60% and q = 40% for the LSTM; For the experiments with 50M parameters, we set p = 65% and q = 55% for the CFN and p = 70% and q = 50% for the LSTM.

4 CONCLUSION
Despite its simple dynamics, the CFN obtains results that compare well against LSTM networks and GRUs on word-level language modeling. This indicates that it might be possible, in general, to build RNNs that perform well while avoiding the intricate, uninterpretable and potentially chaotic dynamics that can occur in LSTMs and GRUs. Of course, it remains to be seen if dynamically simple RNNs such as the proposed CFN can perform well on a wide variety of tasks, potentially requiring longer term dependencies than the one needed for word level language modeling. The experiments presented in Section 2 indicate a plausible path forward — activations in the higher layers of a multi-layer CFN decay at a slower rate than the activations in the lower layers. In theory, complexity and long-term dependencies can therefore be captured using a more “feed-forward” approach (i.e. stacking layers) rather than relying on the intricate and hard to interpret dynamics of an LSTM or a GRU.
Overall, the CFN is a simple model and it therefore has the potential of being mathematically wellunderstood. In particular, Section 2 reveals that the dynamics of its hidden states are inherently more interpretable than those of an LSTM. The mathematical analysis here provides a few key insights into the network, in both the presence and absence of input data, but obviously more work is needed before a complete picture can emerge. We hope that this investigation opens up new avenues of inquiry, and that such an understanding will drive subsequent improvements.
","We introduce an exceptionally simple gated recurrent neural network (RNN) that achieves performance comparable to well-known gated architectures, such as LSTMs and GRUs, on the word-level language modeling task. We prove that our model has simple, predicable and non-chaotic dynamics. This stands in stark contrast to more standard gated architectures, whose underlying dynamical systems exhibit chaotic behavior.",ICLR 2017 conference submission,True,,"This paper poses an interesting idea: removing chaotic behavior or RNNs.
While many other papers on new RNN architecture usually focus too much on the performance improvement and leave the analysis part on their success as a black-box, this paper does a good job on presenting why its method may work well.

Although, the paper shows lots of comparison between the chaotic systems (GRUs & LSTMs) and the stable system (proposed CFN model), the reviewer is not fully convinced by the main claim of this paper, the nuance that chaotic behaviour makes dynamic system to have rich representation power but makes the system too unstable. In the paper, the LSTM shows a very sensitive behaviour, even when a very small amount of noise is added to the input. However, it still performs surprisingly well with this chaotic behaviour. 

Measuring the model complexity is a very difficult task, therefore, many papers manage to use either same number of hidden units or choose approximately close model sizes. In this paper, the experiments were carried by using the same amount of parameters for both the LSTM and CFN. However, I think the CFN may have much more simpler computational graph. Taking the idea of this work, can we develop a stable dynamic system, but which does not only have one attractor?

It is also interesting to see that the layers of CFNs are updated in different timescales in a sense that the decaying speed decreases when the layer gets higher. Could you provide more statistics on this? For example, what is the average relaxation time of the whole hidden units at each layer?

Batch normalization and layer normalization can be helpful to make the training of RNNs become more stable. How would the behaviour of batch normalized or perhaps layer normalized LSTM look like? Also, it is often not trivial to make batch normalization or layer normalization to work on a new architecture. I think it may be useful to compare batch normalized or layer normalized versions of the LSTM and CFN.

The quality of the work is good, explanation is clear enough along with nice analyses and proofs. Overall, the performance is not any better than LSTMs, but it is still interesting when thinking of simplicity of this model. I am a bit concerned if this model might not work that well in more harder task, e.g., translation. Figure 4 of this paper is very interesting, where the proposed architecture shows that the hidden units at the second layer tends to keep its information longer than the first layer ones.

---

The reviewers all enjoyed this paper and the analysis.
 
 pros:
 - novel new model
 - interesting insights into the design of model, through analysis of trajectories of hidden states of RNNs.
 
 cons:
 - results are worse than LSTMs.

---

Thanks for a very interesting read.

What happens if instead of driving the LSTMs with x_t = 0, you drive it with a fixed input, like the word ""What""? Would that behave the same as in fig 3?

If you drive the LSTMs with some input and then fix x_t = 0 for t > T (as in fig 4), do you still see chaos? If there is gradual decay in the hidden units' activations, do you also see that the second layer forgets more slowly than the first?

Have you tried training on the copy task as in the algorithmic learning literature (like NTM), to see whether there is a actual difference in how long memory is retained in CFN vs LSTM?

---

The authors of the paper set out to answer the question whether chaotic behaviour is a necessary ingredient for RNNs to perform well on some tasks. For that question's sake, they propose an architecture which is designed to not have chaos. The subsequent experiments validate the claim that chaos is not necessary.

This paper is refreshing. Instead of proposing another incremental improvement, the authors start out with a clear hypothesis and test it. This might set the base for future design principles of RNNs.

The only downside is that the experiments are only conducted on tasks which are known to be not that demanding from a dynamical systems perspective; it would have been nice if the authors had traversed the set of data sets more to find data where chaos is actually necessary.

---

I think the authors provide an interesting direction for understanding and maybe constructing recurrent models that are easier to interpret. Is not clear where such direction will lead but I think it could be an interesting starting point for future work, one that worth exploring.

---

This paper poses an interesting idea: removing chaotic behavior or RNNs.
While many other papers on new RNN architecture usually focus too much on the performance improvement and leave the analysis part on their success as a black-box, this paper does a good job on presenting why its method may work well.

Although, the paper shows lots of comparison between the chaotic systems (GRUs & LSTMs) and the stable system (proposed CFN model), the reviewer is not fully convinced by the main claim of this paper, the nuance that chaotic behaviour makes dynamic system to have rich representation power but makes the system too unstable. In the paper, the LSTM shows a very sensitive behaviour, even when a very small amount of noise is added to the input. However, it still performs surprisingly well with this chaotic behaviour. 

Measuring the model complexity is a very difficult task, therefore, many papers manage to use either same number of hidden units or choose approximately close model sizes. In this paper, the experiments were carried by using the same amount of parameters for both the LSTM and CFN. However, I think the CFN may have much more simpler computational graph. Taking the idea of this work, can we develop a stable dynamic system, but which does not only have one attractor?

It is also interesting to see that the layers of CFNs are updated in different timescales in a sense that the decaying speed decreases when the layer gets higher. Could you provide more statistics on this? For example, what is the average relaxation time of the whole hidden units at each layer?

Batch normalization and layer normalization can be helpful to make the training of RNNs become more stable. How would the behaviour of batch normalized or perhaps layer normalized LSTM look like? Also, it is often not trivial to make batch normalization or layer normalization to work on a new architecture. I think it may be useful to compare batch normalized or layer normalized versions of the LSTM and CFN.

The quality of the work is good, explanation is clear enough along with nice analyses and proofs. Overall, the performance is not any better than LSTMs, but it is still interesting when thinking of simplicity of this model. I am a bit concerned if this model might not work that well in more harder task, e.g., translation. Figure 4 of this paper is very interesting, where the proposed architecture shows that the hidden units at the second layer tends to keep its information longer than the first layer ones.

---

We added a short conclusion reflecting some of the discussions with the reviewers.

---

Several reviewers have posted comments asking about the capability of the proposed model to capture long-term dependencies. This is a natural question since the model was designed so that units get activated when presented the correct feature, then relax to zero at a rate controlled by the forget gate. At a first glance it is unclear that such a simple mechanism could capture long term dependencies (the relaxing rates might be too fast).

We added a simple experiment in the paper showing that long term dependencies can be obtained by stacking multiple layers of the basic architecture (see Figure 4). We took a 2-layer, 224-unit CFN network trained on Penn Treebank and ran it with the following input data: The first 1000 inputs x_t are the first 1000 words of the test set of PTB; All subsequent inputs are set to zero, so that x_t=0 if t>1000. For each layer we then select the 10 units that decay the slowest after t>1000 and plotted them on Figure 4. The first layer retains information for about 10 time steps, whereas the second layer retains information for about 100 steps. Adding a third or fourth layer would then allow the architecture to retain information for even longer periods. We have not yet implemented a multi-layer network to handle tasks (other than language modeling) where such longer-term dependencies are needed, but we believe the main obstacle here is one of proper initialization and training rather than a shortcoming of the architecture itself.

Importantly, this behavior (i.e. higher layers decay more slowly) can be explained analytically, see equation (11).

Overall, we find it interesting that complexity and long-term dependencies can plausibly be obtained in a classical way (i.e. stacking layers) rather than relying on the intricate and hard to interpret dynamics of an LSTM.

---

I think it would be useful to discuss the concept of *edge* of chaos here (see e.g. Bertschinger, Nachschlager - Real-Time Computation at the Edge of Chaos in Recurrent Neural Networks), i.e. the hypothesis that RNNs are optimal (in some sense) at the boundary between chaotic and deterministic regimes. Specifically, it would be nice to see if your network gets closer to this edge during training (I think it will).

It wasn't clear to me if you studied the chaoticity in the case *with* input... the ""epsilon-activation"" thing seems very nonstandard. Why didn't you just compute the mean Lyapunov exponent? You can do that with or without input. I think you might find that the RNN with input will approach the edge of chaos during training (Lyapunov exp gets closer to zero, probably starting from negative values in your case).

The LSTM phase space diagram in Fig. 2 looks pretty bad... I think that particular unit is not behaving well at all. What you should get in properly trained models is something like in Fig. 1 (a), but more noisy because there's effects from the input.

Anyway, overall a very interesting paper! I'm glad to see RNNs studied from a chaotic dynamics perspective.

---

This paper poses an interesting idea: removing chaotic behavior or RNNs.
While many other papers on new RNN architecture usually focus too much on the performance improvement and leave the analysis part on their success as a black-box, this paper does a good job on presenting why its method may work well.

Although, the paper shows lots of comparison between the chaotic systems (GRUs & LSTMs) and the stable system (proposed CFN model), the reviewer is not fully convinced by the main claim of this paper, the nuance that chaotic behaviour makes dynamic system to have rich representation power but makes the system too unstable. In the paper, the LSTM shows a very sensitive behaviour, even when a very small amount of noise is added to the input. However, it still performs surprisingly well with this chaotic behaviour. 

Measuring the model complexity is a very difficult task, therefore, many papers manage to use either same number of hidden units or choose approximately close model sizes. In this paper, the experiments were carried by using the same amount of parameters for both the LSTM and CFN. However, I think the CFN may have much more simpler computational graph. Taking the idea of this work, can we develop a stable dynamic system, but which does not only have one attractor?

It is also interesting to see that the layers of CFNs are updated in different timescales in a sense that the decaying speed decreases when the layer gets higher. Could you provide more statistics on this? For example, what is the average relaxation time of the whole hidden units at each layer?

Batch normalization and layer normalization can be helpful to make the training of RNNs become more stable. How would the behaviour of batch normalized or perhaps layer normalized LSTM look like? Also, it is often not trivial to make batch normalization or layer normalization to work on a new architecture. I think it may be useful to compare batch normalized or layer normalized versions of the LSTM and CFN.

The quality of the work is good, explanation is clear enough along with nice analyses and proofs. Overall, the performance is not any better than LSTMs, but it is still interesting when thinking of simplicity of this model. I am a bit concerned if this model might not work that well in more harder task, e.g., translation. Figure 4 of this paper is very interesting, where the proposed architecture shows that the hidden units at the second layer tends to keep its information longer than the first layer ones.

---

The reviewers all enjoyed this paper and the analysis.
 
 pros:
 - novel new model
 - interesting insights into the design of model, through analysis of trajectories of hidden states of RNNs.
 
 cons:
 - results are worse than LSTMs.

---

Thanks for a very interesting read.

What happens if instead of driving the LSTMs with x_t = 0, you drive it with a fixed input, like the word ""What""? Would that behave the same as in fig 3?

If you drive the LSTMs with some input and then fix x_t = 0 for t > T (as in fig 4), do you still see chaos? If there is gradual decay in the hidden units' activations, do you also see that the second layer forgets more slowly than the first?

Have you tried training on the copy task as in the algorithmic learning literature (like NTM), to see whether there is a actual difference in how long memory is retained in CFN vs LSTM?

---

The authors of the paper set out to answer the question whether chaotic behaviour is a necessary ingredient for RNNs to perform well on some tasks. For that question's sake, they propose an architecture which is designed to not have chaos. The subsequent experiments validate the claim that chaos is not necessary.

This paper is refreshing. Instead of proposing another incremental improvement, the authors start out with a clear hypothesis and test it. This might set the base for future design principles of RNNs.

The only downside is that the experiments are only conducted on tasks which are known to be not that demanding from a dynamical systems perspective; it would have been nice if the authors had traversed the set of data sets more to find data where chaos is actually necessary.

---

I think the authors provide an interesting direction for understanding and maybe constructing recurrent models that are easier to interpret. Is not clear where such direction will lead but I think it could be an interesting starting point for future work, one that worth exploring.

---

This paper poses an interesting idea: removing chaotic behavior or RNNs.
While many other papers on new RNN architecture usually focus too much on the performance improvement and leave the analysis part on their success as a black-box, this paper does a good job on presenting why its method may work well.

Although, the paper shows lots of comparison between the chaotic systems (GRUs & LSTMs) and the stable system (proposed CFN model), the reviewer is not fully convinced by the main claim of this paper, the nuance that chaotic behaviour makes dynamic system to have rich representation power but makes the system too unstable. In the paper, the LSTM shows a very sensitive behaviour, even when a very small amount of noise is added to the input. However, it still performs surprisingly well with this chaotic behaviour. 

Measuring the model complexity is a very difficult task, therefore, many papers manage to use either same number of hidden units or choose approximately close model sizes. In this paper, the experiments were carried by using the same amount of parameters for both the LSTM and CFN. However, I think the CFN may have much more simpler computational graph. Taking the idea of this work, can we develop a stable dynamic system, but which does not only have one attractor?

It is also interesting to see that the layers of CFNs are updated in different timescales in a sense that the decaying speed decreases when the layer gets higher. Could you provide more statistics on this? For example, what is the average relaxation time of the whole hidden units at each layer?

Batch normalization and layer normalization can be helpful to make the training of RNNs become more stable. How would the behaviour of batch normalized or perhaps layer normalized LSTM look like? Also, it is often not trivial to make batch normalization or layer normalization to work on a new architecture. I think it may be useful to compare batch normalized or layer normalized versions of the LSTM and CFN.

The quality of the work is good, explanation is clear enough along with nice analyses and proofs. Overall, the performance is not any better than LSTMs, but it is still interesting when thinking of simplicity of this model. I am a bit concerned if this model might not work that well in more harder task, e.g., translation. Figure 4 of this paper is very interesting, where the proposed architecture shows that the hidden units at the second layer tends to keep its information longer than the first layer ones.

---

We added a short conclusion reflecting some of the discussions with the reviewers.

---

Several reviewers have posted comments asking about the capability of the proposed model to capture long-term dependencies. This is a natural question since the model was designed so that units get activated when presented the correct feature, then relax to zero at a rate controlled by the forget gate. At a first glance it is unclear that such a simple mechanism could capture long term dependencies (the relaxing rates might be too fast).

We added a simple experiment in the paper showing that long term dependencies can be obtained by stacking multiple layers of the basic architecture (see Figure 4). We took a 2-layer, 224-unit CFN network trained on Penn Treebank and ran it with the following input data: The first 1000 inputs x_t are the first 1000 words of the test set of PTB; All subsequent inputs are set to zero, so that x_t=0 if t>1000. For each layer we then select the 10 units that decay the slowest after t>1000 and plotted them on Figure 4. The first layer retains information for about 10 time steps, whereas the second layer retains information for about 100 steps. Adding a third or fourth layer would then allow the architecture to retain information for even longer periods. We have not yet implemented a multi-layer network to handle tasks (other than language modeling) where such longer-term dependencies are needed, but we believe the main obstacle here is one of proper initialization and training rather than a shortcoming of the architecture itself.

Importantly, this behavior (i.e. higher layers decay more slowly) can be explained analytically, see equation (11).

Overall, we find it interesting that complexity and long-term dependencies can plausibly be obtained in a classical way (i.e. stacking layers) rather than relying on the intricate and hard to interpret dynamics of an LSTM.

---

I think it would be useful to discuss the concept of *edge* of chaos here (see e.g. Bertschinger, Nachschlager - Real-Time Computation at the Edge of Chaos in Recurrent Neural Networks), i.e. the hypothesis that RNNs are optimal (in some sense) at the boundary between chaotic and deterministic regimes. Specifically, it would be nice to see if your network gets closer to this edge during training (I think it will).

It wasn't clear to me if you studied the chaoticity in the case *with* input... the ""epsilon-activation"" thing seems very nonstandard. Why didn't you just compute the mean Lyapunov exponent? You can do that with or without input. I think you might find that the RNN with input will approach the edge of chaos during training (Lyapunov exp gets closer to zero, probably starting from negative values in your case).

The LSTM phase space diagram in Fig. 2 looks pretty bad... I think that particular unit is not behaving well at all. What you should get in properly trained models is something like in Fig. 1 (a), but more noisy because there's effects from the input.

Anyway, overall a very interesting paper! I'm glad to see RNNs studied from a chaotic dynamics perspective.",,,4.0,,4.0,7.333333333333333,,,3.6666666666666665,4.5,
400,"TREE-STRUCTURED DECODING WITH DOUBLY- RECURRENT NEURAL NETWORKS
Authors: David Alvarez-Melis, Tommi S. Jaakkola
Source file: 400.pdf

ABSTRACT
We propose a neural network architecture for generating tree-structured objects from encoded representations. The core of the method is a doubly recurrent neural network model comprised of separate width and depth recurrences that are combined inside each cell (node) to generate an output. The topology of the tree is modeled explicitly together with the content. That is, in response to an encoded vector representation, co-evolving recurrences are used to realize the associated tree and the labels for the nodes in the tree. We test this architecture in an encoderdecoder framework, where we train a network to encode a sentence as a vector, and then generate a tree structure from it. The experimental results show the effectiveness of this architecture at recovering latent tree structure in sequences and at mapping sentences to simple functional programs.

We propose a neural network architecture for generating tree-structured objects from encoded representations. The core of the method is a doubly recurrent neural network model comprised of separate width and depth recurrences that are combined inside each cell (node) to generate an output. The topology of the tree is modeled explicitly together with the content. That is, in response to an encoded vector representation, co-evolving recurrences are used to realize the associated tree and the labels for the nodes in the tree. We test this architecture in an encoderdecoder framework, where we train a network to encode a sentence as a vector, and then generate a tree structure from it. The experimental results show the effectiveness of this architecture at recovering latent tree structure in sequences and at mapping sentences to simple functional programs.

1 INTRODUCTION
Recurrent neural networks have become extremely popular for modeling structured data. Key to their success is their ability to learn long-range temporal dependencies, their flexibility, and ease of customization. These architectures are naturally suited for modeling sequences since the underlying state evolution resulting from successive operations follows an inherently linear order (Williams & Zipser, 1995; Hochreiter & Schmidhuber, 1997). Indeed, they have been successfully adapted to language modeling (Zaremba et al., 2015), machine translation (Sutskever et al., 2014) and conversational agents (Vinyals & Le, 2015), among other applications.
Although sequences arise frequently in practice, other structures such as trees or graphs do not naturally conform to a linear ordering. For example, natural language sentences or associated parse trees, programs, hierarchical structures in biology, or molecules are not inherently linear structures. While sentences in natural language can be modeled as if they were linear sequences, the underlying process is compositional (Frege, 1892). Models that construct sentences compositionally should derive an advantage from adopting a more appropriate inductive bias.
The flexibility and success of recurrent neural networks in modeling and generating sequential data has prompted efforts to adapt them to non-sequential data too. Recent work has focused on the application of neural architectures to hierarchical structures, albeit in limited ways. Much of this work has assumed that either the full tree structure is given (Socher et al., 2012; Tai et al., 2015) or at least the nodes are (Socher & Lin, 2011; Chen & Manning, 2014; Kiperwasser & Goldberg, 2016). In the former scenario, the network aggregates the node information in a manner that is coherent with a given tree structure while, in the latter, generation is reduced to an attachment problem, i.e., sequentially deciding which pairs of nodes to join with an edge until a tree is formed.
The full problem of decoding with structure, i.e., generating a tree-structured object with node labels from a given vector representation, has remained largely unexplored until recently. Recent efforts to adapt RNNs to this context have so far remained relatively close to their sequential counterparts. For example, in order to capture depth and branching in the tree, one can introduce special tokens (Dong & Lapata, 2016) or use alternating RNNs coupled with external classifiers to predict branching (Zhang et al., 2016).
In this work, we propose a novel architecture tailored specifically to tree-structured decoding. At the heart of our approach is a doubly-recurrent (breadth and depth-wise recurrent) neural network which separately models the flow of information between parent and children nodes, and between siblings. Each of these relationships is modeled with a recurrent module whose hidden states are updated upon observing node labels. Every node in the tree receives two hidden states, which are then combined and used to predict a label for that node. Besides maintaining separate but simultaneous fraternal and paternal recurrences, the proposed architecture departs from previous methods in that it explicitly models tree topology. Each node in the network has modules that predict, based on the cell state, whether the node is terminal, both in terms of depth and width. Decoupling these decisions from the label prediction allows for a more concise formulation, which does not require artificial tokens to be added to the tree to simulate branching.
We test this novel architecture in various encoder-decoder frameworks, coupling it with sequential encoders to predict tree structure from encoded vector representations of sequences. The experimental results show the effectiveness of this approach at recovering latent structure in flattened string representations of trees (Section 4.1) and at mapping from natural language descriptions of simple programs to abstract syntax trees (Section 4.2). In addition, we show that even for sequence-tosequence tasks such as machine translation, the proposed architecture exhibits desirable properties, such as invariance to structural changes and coarse-to-fine generation (Section 4.3).
To summarize, the main contributions of this paper are as follows:
• We propose a novel neural network architecture specifically tailored to tree-structured decoding, which maintains separate depth and width recurrent states and combines them to obtain hidden states for every node in the tree.
• We equip this novel architecture with a mechanism to predict tree topology explicitly (as opposed to implicitly by adding nodes with special tokens).
• We show experimentally that the proposed method is capable of recovering trees from encoded representations and that it outperforms state-of-the-art methods in a task consisting of mapping sentences to simple functional programs.

2 RELATED WORK
Recursive Neural Networks. Recursive neural networks (Socher & Lin, 2011; Socher et al., 2012) were proposed to model data with hierarchical structures, such as parsed scenes and natural language sentences. Though they have been most successfully applied to encoding objects when their treestructured representation is given (Socher et al., 2013), the original formulation by Socher & Lin (2011) also considered using them to predict the structure (edges), albeit for the case where nodes are given. Thus, besides their limited applicability due to their assumption of binary trees, recursive neural networks are not useful for fully generating trees from scratch.
Tree-structured encoders. The Tree-LSTM of Tai et al. (2015) is a generalization of long shortterm memory networks (Hochreiter & Schmidhuber, 1997) to tree-structured inputs. Their model constructs a sentence representation bottom-up, obtaining at every step the representation of a node in the tree from those of its children. In this sense, this model can be seen as a generalization of recursive neural networks to trees with degree potentially greater than two, with the additional longrange dependency modeling provided by LSTMs. They propose two methods for aggregating the states of the children, depending on the type of underlying tree: N-ary trees or trees with unknown and potentially unbounded branching factor. TreeLSTMs have shown promising results for compositional encoding of structured data, though by construction they cannot be used for decoding, since they operate on a given tree structure.
Tree-structured decoders. Proposed only very recently, most tree-structured decoders rely on stacked on intertwined RNNs, and use heuristic methods for topological decisions during generation. Closest to our method is the Top-down Tree LSTM of Zhang et al. (2016), which generates a tree from an encoded representation. Their method relies on 4 independent LSTMs, which act in alternation—as opposed to simultaneously in our approach—yielding essentially a standard LSTM that changes the weights it uses based on the position of the current node. In addition, their method
provides children with asymmetric parent input: “younger” children receive information from the parent state only through the previous sibling’s state. Though most of their experiments focus on the case where the nodes are given, they mention how to use their method for full prediction by introducing additional binary classifiers which predict which of the four LSTMs is to be used. These classifiers are trained in isolation after the main architecture has been trained. Contrary to this approach, our method can be trained end-to-end in only one pass, has a simpler formulation and explicitly incorporates topological prediction as part of the functioning of each neuron.
A similar approach is proposed by Dong & Lapata (2016). They propose SEQ2TREE, an encoderdecoder architecture that maps sentences to tree structures. For the decoder, they rely on hierarchical use of an LSTM, similar to Tai et al. (2015), but in the opposite direction: working top-down from the root of the tree. To decide when to change levels in the hierarchy, they augment the training trees with nonterminal nodes labeled with a special token <n>, which when generated during decoding trigger the branching out into a lower level in the tree. Similar to our method, they feed nodes with hidden representations of their parent and sibling, but they do so by concatenating both states and running them through a single recurrent unit, as opposed to our method, where these two sources of information are handled separately. A further difference is that our approach does not require artificial nodes with special tokens to be added to the tree, resulting in smaller trees.
Hierarchical Neural Networks for Parsing. Neural networks have also been recently introduced to the problem of natural language parsing (Chen & Manning, 2014; Kiperwasser & Goldberg, 2016). In this problem, the task is to predict a parse tree over a given sentence. For this, Kiperwasser & Goldberg (2016) use recurrent neural networks as a building block, and compose them recursively to obtain a tree-structured encoder. Starting from the leaves (words) they predict a parse tree with a projective bottom-up strategy, which sequentially updates the encoded vector representation of the tree and uses it to guide edge-attaching decisions. Though conceptually similar to our approach, their method relies on having access to the nodes of the tree (words) and only predicts its topology, so—similar to recursive neural networks—it cannot be used for a fully generative decoding.

3 DOUBLY RECURRENT NEURAL NETWORKS
Generating a tree-structured object from scratch using only an encoded representation poses several design challenges. First, one must decide in which order to generate the tree. If the nodes on the decoder side were given (such as in parsing), it would be possible to generate a tree bottom-up from these nodes (e.g. as Kiperwasser & Goldberg 2016 do). In the setting we are interested in, however, not even the nodes are known when decoding, so the natural choice is a top-down decoder, which starting from an encoded representation generates the root of the tree and then recursively generates the children (if any) of every node.
The second challenge arises from the asymmetric hierarchical nature of trees. Unlike the sequenceto-sequence setting where encoding and decoding can be achieved with analogous procedures, when dealing with tree-structured data these two involve significantly different operations. For example, an encoder that processes a tree bottom-up using information of a node’s children to obtain its representation cannot be simply reversed and used as a decoder, since when generating the tree top-down, nodes have to be generated before their children are.
An additional design constraint comes from deciding what information to feed to each node. For sequences, the choice is obvious: a node should receive information from the node preceding or succeeding it (or both), i.e. there is a one-dimensional flow of information. In trees, there is an evident flow of information from parent to children (or vice-versa), but when generating nodes in a top-down order it seems unnatural to generate children in isolation: the label of one of them will likely influence what the states of the other children might be. For example, in the case of parse trees, generating a verb will reduce the chances of other verbs occurring in that branch.
With these considerations in mind, we propose an architecture tailored to tree decoding from scratch: top-down, recursive and doubly-recurrent, i.e. where both the ancestral (parent-to-children) and fraternal (sibling-to-sibling) flows of information are modeled with recurrent modules. Thus, the building block of a doubly recurrent neural network (DRNN) is a cell with two types of input states, one coming from its parent, updated and passed on to its descendants, and another one received from
its previous sibling,1 updated and passed on to the next one. We model the flow of information in the two directions with separate recurrent modules.
Formally, let T = {V, E ,X} be a connected labeled tree, where V is the set of nodes, E the set of edges and X are node labels.2 Let ga and gf be functions which apply one step of the two separate RNNs. For a node i ∈ V with parent p(i) and previous sibling s(i), the ancestral and fraternal hidden states are updated via
hai = g a(hap(i),xp(i)) (1)
hfi = g f (hfs(i),xs(i)) (2)
where xs(j),xp(i) are the vectors representing the previous sibling’s and parent’s values, respectively. Once the hidden depth and width states have been updated with these observed labels, they are combined to obtain a predictive hidden state:
h (pred) i = tanh ( Ufhfi +U ahai ) (3)
where Uf ∈ Rn×Df and Ua ∈ Rn×Da are learnable parameters. This state contains combined information of the node’s neighborhood in the tree, and is used to predict a label for it. In its simplest form, the network could compute the output of node i by sampling from distribution
oi = softmax(Wh (pred) i ) (4)
In the next section, we propose a slight modification to (4) whereby topological information is included in the computation of cell outputs. After the node’s output symbol xi has been obtained by sampling from oi, the cell passes hai to all its children and h f i to the next sibling (if any), enabling them to apply Eqs (1) and (2) to realize their states. This procedure continues recursively, until termination conditions (explained in the next section) cause it to halt.

3.1 TOPOLOGICAL PREDICTION
As mentioned before, the central issue with free-form tree construction is to predict the topology of the tree. When constructing the tree top-down, for each node we need to decide: (i) whether it is a leaf node (and thus it should not produce offspring) and (ii) whether there should be additional siblings produced after it. Answering these two questions for every node allows us to construct a tree from scratch and eventual stop growing it.
Sequence decoders typically rely on special tokens to terminate generation (Sutskever et al., 2014). The token is added to the vocabulary and treated as a regular word. During training, the examples are padded with this token at the end of the sequence, and during testing, generation of this token signals termination. These ideas has been adopted by most tree decoders (Dong & Lapata, 2016). There are two important downsides of using a padding strategy for topology prediction in trees. First, the size of the tree can grow considerably. While in the sequence framework only one stopping token is needed, a tree with n nodes might need up to O(n) padding nodes to be added. This can have important effects in training speed. The second reason is that a single stopping token selected competitively with other tokens requires one to continually update the associated parameters in response to any changes in the distribution over ordinary tokens so as to maintain topological control.
Based on these observations, we propose an alternative approach to stopping, in which topological decisions are made explicitly (as opposed to implicitly, with stopping tokens). For this, we use the predictive hidden state of the node h(pred) with a projection and sigmoid activation:
pai = σ(u a · h(pred)i ) (5)
The value pai ∈ [0, 1] is interpreted as the probability that node i has children. Analogously, we can obtain a probability of stopping fraternal branch growth after the current node as follows:
pfi = σ(u f · h(pred)i ) (6)
1Unlike the “ancestral” line, the order within sibling nodes is ambiguous. While in abstract trees it is assumed that the there is no such ordering, we assume that for the structures were are interested in learning there is always one: either chronological (the temporal order in which the nodes were generated) or latent (e.g. the grammatical order of the words in a parse tree with respect to their sentence representation).
2We assume throughout that these values are given as class indicators xi ∈ {1, . . . , N}.
Note that these stopping strategies depart from the usual padding methods in a fundamental property: the decision to stop is made before instead of in conjunction with the label prediction. The rationale behind this is that the label of a node will likely be influenced not only by its context, but also by the type of node (terminal or non-terminal) where it is to be assigned. This is the case in language, for example, where syntactic constraints restrict the type of words that can be found in terminal nodes. For this purpose, we include the topological information as inputs to the label prediction layer. Thus, (4) takes the form
oi = softmax(Wh (pred) i + αiv a + ϕiv f ) (7)
where αi, ϕi ∈ {0, 1} are binary variables indicating the topological decisions and va,vf are learnable offset parameters. During training, we use gold-truth values in (7), i.e. αi = 1 if node i has children and ϕi = 1 if it has a succeeding sibling. During testing, these values are obtained from pa, pf by sampling or beam-search. A schematic representation of the internal structure of a DRNN cell and the flow of information in a tree are shown in Figure 1.

3.2 TRAINING DRNNS
We train DRNNs with (reverse) back-propagation through structure (BPTS) (Goller & Kuechler, 1996). In the forward pass, node outputs are computed in a top-down fashion on the structureunrolled version of the network, following the natural3 dependencies of the tree. We obtain error signal at the node level from the two types of prediction: label and topology. For the former, we compute cross-entropy loss of oi with respect to the true label of the node xi. For the topological values pai and p f i we compute binary cross entropy loss with respect to gold topological indicators αi, ϕi ∈ {0, 1}. In the backward pass, we proceed in the reverse (bottom-up) direction, feeding into every node the gradients received from child and sibling nodes and computing internally gradients with respect to both topology and label prediction. Further details on the backpropagation flow are provided in the Appendix.
Note that the way BPTS is computed implies and underlying decoupled loss function L(x̂) = ∑ i∈V Llabel(xi, x̂i) + Ltopo(pi, p̂i) (8)
The decoupled nature of this loss allows us to weigh these two objectives differently, to emphasize either topology or label prediction accuracy. Investigating the effect of this is left for future work.
3The traversal is always breadth-first starting from the root, but the order in which sibling nodes are visited might depend on the specific problem. If the nodes of the tree have an underlying order (such as in dependency parse trees), it is usually desirable to preserve this order.
As is common with sequence generation, during training we perform teacher forcing: after predicting the label of a node and its corresponding loss, we replace it with its gold value, so that children and siblings receive the correct label for that node. Analogously, we obtain the probabilities pa and pf , compute their loss, and replace them for ground truth variables αi, ϕi for all downstream computations. Addressing this exposure bias by mixing ground truth labels with model predictions during training (Venkatraman et al., 2015) or by incremental hybrid losses (Ranzato et al., 2016) is left as an avenue for future work.

4 EXPERIMENTS

4.1 SYNTHETIC TREE RECOVERY
In our first set of experiments we evaluate the effectiveness of the proposed architecture to recover trees from flattened string representations. For this, we first generate a toy dataset consisting of simple labeled trees. To isolate the effect of label content from topological prediction, we take a small vocabulary consisting of the 26 letters of the English alphabet. We generate trees in a top-down fashion, conditioning the label and topology of every node on the state of its ancestors and siblings. For simplicity, we use a Markovian assumption on these dependencies, modeling the probability of a node’s label as depending only on the label of its parent and the last sibling generated before it (if any). Conditioned on these two inputs, we model the label of the node as coming from a multinomial distribution over the alphabet with a dirichlet prior. To generate the topology of the tree, we model the probability of a node having children and a next-sibling as depending only on its label and the depth of the tree. For each tree we generate a string representation by traversing it in breadth-first preorder, starting from the root. The labels of the nodes are concatenated into a string in the order in which they were visited, resulting in a string of |T | symbols. We create a dataset of 5,000 trees with this procedure, and split it randomly into train, validation and test sets (with a 80%,10%,10% split). Further details on the construction of this dataset are provided in the Appendix.
The task consists of learning a mapping from strings to trees, and using this learned mapping to recover the tree structure of the test set examples, given only their flattened representation. To do so, we use an encoder-decoder framework, where the strings are mapped to a fixed-size vector representation using a recurrent neural network. For the decoder, we use a DRNN with LSTM modules, which given the encoded representation generates a tree. We choose hyper-parameters with cross-validation. Full training details are provided in the Appendix.
Measuring performance only in terms of exact recovery would likely yield near-zero accuracies for most trees. Instead, we opt for a finer-grained metric of tree similarity that gives partial credit for correctly predicted subtrees. Treating tree generation as a retrieval problem, we evaluate the quality of the predicted tree in terms of the precision and recall of recovering nodes and edges present in the gold tree. Thus, we penalize both missing and superfluous components. As baseline, we induce a probabilistic context-free grammar (PCFG) on the full training data and use it to parse the test sentences. Note that unlike the DRNN, this parser has direct access to the sentence representation and thus its task is only to infer the tree structure on top of it, so this is indeed a strong baseline.
Figure 3 shows the results on the test set. Training on the full data yields node and edge retrieval F1-Scores of 75% and 71%, respectively, the latter considerably above the baseline.4 This 4% gap can be explained by correct nodes being generated in the wrong part of the tree, as in the example in
4Since the PCFG parser has access to the nodes by construction, node accuracy for the baseline method is irrelevant and thus omitted from the analysis.
Figure 2. The second plot in Figure 3 shows that although small trees are recovered more accurately, precision decays slowly with tree size, with depth accounting for the largest effect (Figure 4).

4.2 MAPPING SENTENCES TO FUNCTIONAL PROGRAMS
Tree structures arise naturally in the context of programs. A typical compiler takes human-readable source code (expressed as sequences of characters) and transforms it into an executable abstract syntax tree (AST). Source code, however, is already semi-structured. Mapping natural language sentences directly into executable programs is an open problem, which has received considerable interest in the natural language processing community (Kate et al., 2005; Branavan et al., 2009).
The IFTTT dataset (Quirk et al., 2015) is a simple testbed for language-to-program mapping. It consists of if-this-then-that programs (called recipes) crawled from the IFTTT website5, paired with natural language descriptions of their purpose. The recipes consist of a trigger and an action, each defined in terms of a channel (e.g. “Facebook”), a function (e.g. “Post a status update”) and potentially arguments and parameters. An example of a recipe and its description are shown in Figure 5. The data is user-generated and extremely noisy, which makes the task significantly challenging.
5www.ifttt.com
We approach this task using an encoder-decoder framework. We use a standard RNN encoder, either an LSTM or a GRU (Cho et al., 2014), to map the sentence to a vector representation, and we use a DRNN decoder to generate the AST representation of the recipe. We use the original data split, which consists of 77,495 training, 5,171 development and 4,294 test examples. For evaluation, we use the same metrics as Quirk et al. (2015), who note that computing exact accuracy on such a noisy dataset is problematic, and instead propose to evaluate the generated AST in terms of F1-score on the set of recovered productions. In addition, they compute accuracy at the channel level (i.e. when both channels are predicted correctly) and at the function level (both channels and both functions predicted correctly).
We compare our methods against the various extraction and phrased-based machine translation baselines of Quirk et al. (2015) and the the methods of Dong & Lapata (2016): SEQ2SEQ, a sequenceto-sequence model trained on flattened representations of the AST, and SEQ2TREE, a token-driven hierarchical RNN. Following these two works, we report results on two noise-filtered subsets of the data: one with all non-English and unintelligible recipes removed and the other one with recipes for which at least three humans agreed with the gold AST. The results are shown in Table 1. In both subsets, DRNNs perform on par or above previous approaches, with LSTM-DRNN achieving significantly better results. The improvement is particularly evident in terms of F1-score, which is the only metric used by previous approaches that measures global tree reconstruction accuracy. To better understand the quality of the predicted trees beyond the function level (i.e. (b) in Figure 5), we computed node accuracy on the arguments level. Our best performing model, LSTM-DRNN, achieves a Macro F1 score of 51% (0.71 precision, 0.40 recall) over argument nodes, which shows that the model is reasonably successful at predicting structure even beyond depth three. The best performing alternative model, SEQ2TREE, achieves a corresponding F1 score of 46%.

4.3 MACHINE TRANSLATION
In our last set of experiments, we offer a qualitative evaluation DRNNs in the context of machine translation. Obtaining state-of-the-art results in machine translation requires highly-optimized architectures and large parallel corpora. This is not our goal. Instead, we investigate whether decoding with structure can bring benefits to a task traditionally approached as a sequence-to-sequence problem. For this reason, we consider a setting with limited data: a subset of the WMT14 dataset consisting of about 50K English↔ French sentence pairs (see the Appendix for details) along with dependency parses of the target (English) side.
We train a sequence-to-tree model using an LSTM encoder and a DRNN decoder as in the previous experiments. A slight modification here is that we distinguish left and right children in the tree, using two symmetric width-modules gfL, g f R that produce children from the parent outwards. With this, children are lexically ordered, and therefore trees can be easily and un-ambiguously projected back into sentences. We compare our model against a sequence-to-sequence architecture of similar complexity (in terms of number of parameters) trained on the same data using the optimized OpenNMT library (Klein et al., 2017). For decoding, we use a simple best-of-k sampling scheme for our model, and beam search for the SEQ2SEQ models.
Source “ produit différentes réponses qui changent avec le temps selon nos expériences et nos relations ”
“je ne sais jamais quoi dire dans ces cas là”
SEQ2SEQ: l = 1 a I l = 4 with the different actions I do l = 8 with the different actions who change with I do not know what to say
DRNN: d = 1 answers know d = 2 different answers change but i do not know d = 3 product the different answers change . but i do not know to say
Table 2: Translations at different resolutions (size constraints imposed during decoding) for two example sentences.
First, we analyze the quality of translations as a function of the maximum allowed target sentence “size”. The notion of size for a sequence decoder is simply the length while for DRNN we use depth instead so as to tap into the inherent granularity at which sentences can be generated from this architecture. Two such examples are shown in Table 2. Since DRNN topology has been trained to mimic dependency parses top-down, the decoder tends to first generate the fundamental aspects of the sentence (verb, nouns), leaving less important refinements for deeper structures down in the tree. The sequence decoder, in contrast, is trained for left-to-right sequential generation, and thus produces less informative translations under max-length restrictions.
In our second experiment we investigate the decoders’ ability to entertain natural paraphrases of sentences. If we keep the semantic content of a sentence fixed and only change its grammatical structure, it is desirable that the decoder would assign nearly the same likelihood to the new sentence. One way to assess this invariance is to compare the relative likelihood that the model assigns to the gold sentence in comparison to its paraphrase. To test this, we take 50 examples from the WMT test split and manually generate paraphrases with various types of structural alterations (see details in the Appendix). For each type of decoder, we measure the relative change (in absolute value) of the log-likelihood resulting from the perturbation. All the models we compare have similar standard deviation (40 ± 20) of log-likelihood scores over these examples, so the relative changes in the log-likelihood remain directly comparable. For each architecture we train two versions of different sizes, where the sizes are balanced in terms of the number of parameters across the architectures. The results in Figure 6 show that DRNN’s exhibit significantly lower log-likelihood change, suggesting that, as language models, they are more robust to natural structural variation than their SEQ2SEQ counterparts.

5 DISCUSSION AND FUTURE WORK
We have presented doubly recurrent neural networks, a natural extension of (sequential) recurrent architectures to tree-structured objects. This architecture models the information flow in a tree with two separate recurrent modules: one carrying ancestral information (received from parent and passed on to offspring) and the other carrying fraternal information (passed from sibling to sibling). The topology of the tree is modeled explicitly and separately from the label prediction, with modules that given the state of a node predict whether it has children and siblings.
The experimental results show that the proposed method is able to predict reasonable tree structures from encoded vector representations. Despite the simple structure of the IFTTT trees, the results on that task suggest a promising direction of using DRNNs for generating programs or executable queries from natural language. On the other hand, the results on the toy machine translation task show that even when used to generate sequences, DRNN’s exhibit desirable properties, such as invariance over structural modifications and the ability to perform coarse-to-fine decoding. In order to truly use this architecture for machine translation, the approach must be scaled by resorting to batch processing in GPU. This is possible since forward and backward propagation are computed sequentially along tree traversal paths so that inputs and hidden states of parents and siblings can be grouped into tensors and operated in batch. We leave this as an avenue for future work.

ACKNOWLEDGEMENTS
DA-M acknowledges support from a CONACYT fellowship. The authors would like to thank the anonymous reviewers for their constructive comments.

B TRAINING DETAILS
B.1 BACKPROPAGATION WITH DRNN’S
During training, we do the forward pass over the trees in breadth-first preorder, feeding into every node an ancestral and a fraternal state. For computational efficiency, before passing on the ancestral state to the offspring, we update it through the RNN using the current node’s label, so as to avoid repeating this step for every child node. After the forward pass is complete, we compute label (cross-entropy) and topological (binary cross-entropy) loss for every node. In the backward pass, we compute in this order:
1. Gradient of the current node’s label prediction loss with respect to softmax layer parameters W,va,vf : ∇θL(xi, x̂i).
2. Gradients of topological prediction variable loss with respect to sigmoid layer parameters: ∇θL(pai , tai ) and ∇θL(pfi , tfi ).
3. Gradient of predictive state layer parameters with respect to h(pred). 4. Gradient of predicted ancestral and fraternal hidden states with respect to gf and ga’s pa-
rameters.
The gradients of the input ancestral and fraternal hidden states are then passed on to the previous sibling and parent. When nodes have more than one child, we combine gradients from multiple children by averaging them. This procedure is repeated until the root note is reached, after which a single (ancestral state) gradient is passed to the encoder.
B.2 MODEL SPECIFICATION AND TRAINING PARAMETERS
The best parameters for all tasks are chosen by performance on the validation sets. We perform early stopping based on the validation loss. For the IFTTT task, we initialize word embeddings with pretrained GloVe vectors (Pennington et al., 2014). For both tasks we clip gradients when the absolute value of any element exceeds 5. We regularize with a small penalty ρ on the l2 norm of the parameters. We train all methods with ADAM (Kingma & Ba, 2014), with initial learning rate chosen by cross-validation. The parameter configurations that yielded the best results and were used for the final models are shown in Table 3. Details about the four models used for the machine translation task are shown in Table 4.

C DATASET DETAILS
C.1 SYNTHETIC TREE DATASET GENERATION
We generate trees in a top-down fashion, conditioning the label and topology of every node on the state of its ancestors and siblings. For simplicity, we use a Markovian assumption on these dependencies, modeling the probability of a node’s label as depending only on the label of its parent p(i) and the last sibling s(i) generated before it (if any). Conditioned on these two inputs, we model the label of the node as coming from a multinomial distribution over the alphabet:
P (wi | T ) = P (w | wp(i), ws(i)) ∼ Multi(θwp(i),ws(i)) (9) where θwp(i),ws(i) are class probabilities drawn from a Dirichlet prior with parameter αv . On the other hand, we denote by bai the binary variable indicating whether node i has descendants, and by bfi that indicating whether it has an ensuing sibling. We model these variables as depending only on the label of the current node and its position in the tree:
P (bai | T ) = P (bai | wi, Di) = Bernoulli(pawi · ga(Di)) P (bfi | T ) = P (bfi | wi,Wi) = Bernoulli(pfwi · gf (Wi))
whereDi is the depth of node i andWi its width, defined as its position among the children of its parent p(i). Intuitively, we want to make P (bai = 1 | T ) decrease as we go deeper and further along the branches of the tree, so as to control its growth. Thus, we model ga and gf as decreasing functions with geometric decay, namely ga(D) = (γa)D and gf (W ) = (γf )W , with γa, γf ∈ (0, 1). For the label-conditioned branching probabilities P (bai | wi) and P (bfi | wi), we use Bernoulli distributions with probabilities drawn from beta priors with parameters (αa, βa) and (αf , βf ), respectively.
In summary, we use the following generative procedure to grow the trees:
1. For each wi ∈ V , draw pawi ∼ Beta(αa, βa) and pfwi ∼ Beta(αf , βf ) 2. For each pair (wi, wj) draw θwi,wj ∼ Dir(αV ) 3. While there is an unlabeled non-terminal node i do:
• Sample a label for i from w∗ ∼ P (w|wp(i), ws(i)) = Multi(θwp(i),ws(i)). • Draw ba ∼ P (ba|w∗, D) = Bernoulli(γDa · paw(i)), where D is the current depth. If ba = 1, generate an node k, set p(k) = i, and add it to the queue.
• Draw ba ∼ P (bf |w∗, D) = Bernoulli(γWf · pfw(i)), where W is the current width. If bf = 1, generate an node k, set s(k) = i, and add it to the queue.
Note that this generative process does create a dependence between the topology and content of the trees (since the variables ba and bf depend on the content of the tree via their dependence on the label of their corresponding node). However, the actual process by which labels and topological decision is generated relies on separate mechanisms. This is natural assumption which is reasonable to expect in practice.
The choice of prior parameters is done drawing inspiration from natural language parse trees. We want nodes to have low but diverse probabilities of generating children, so we seek a slow-decaying distribution with most mass allocated in values close to 0. For this, we use (αa, βa) = (0.25, 1). For sibling generation, we use (αf , βf ) = (7, 2), which yields a distribution concentrated in values close to 1, so that nodes have on average a high and similar probability of producing siblings. Since we seek trees that are wider than they are deep, we use decay parameters γa = 0.6, γf = 0.9. Finally, we use a αv = 10 · 1 for the parent-sibling probability prior, favoring non-uniform interactions. Using this configuration, we generate 5000 sentence-tree pairs, which we split into training (4000 examples), validation (500) and test (500) sets. The characteristics of the trees in the dataset are summarized in Table 5.
The IFTTT dataset comes with a script to generate the data by crawling and parsing the recipes. Unfortunately, by the time we ran the script many recipes had been removed or changed. We therefore resorted to the original dataset used by Quirk et al. (2015). We converted these recipes into our tree format, assigning a node to each element in the first three levels (channels, functions and arguments, see figure 5). For the parameters level, many recipes have sentences instead of single tokens, so we broke these up creating one node per word. The last two layers are therefore the most topologically diverse, whereas the structure of the first two layers is constant (all trees have channels and functions). A very small fraction (< 1%) of trees that could not by parsed into our format was excluded from the dataset.
Table 6 shows various statistics about the topological characteristics of the recipes in the IFTTT dataset. The middle columns show percentage of trees that contain nonempty arguments and parameters in trigger (IF) and action (THEN) branches. Almost all recipes have none empty arguments and parameters (and thus depth 4, excluding the root), and a lower percentage—but still a majority—has arguments and parameters on the trigger side too. The last two columns show tree statistics pertaining to the complexity of trees after conversion to our format. The distribution of tree sizes is mostly concentrated between 4 and 30 nodes, with a slow-decaying tail of examples above this range (see Figure 8).
Regarding the content of the trees, the labels of the nodes in the first two levels (channels and functions) come from somewhat reduced vocabularies: 111 and 434 unique symbols for the trigger branch, respectively, and 157 and 85 for the action branch. The lower layers of the tree have a much more diverse vocabulary, with about 60K unique tokens in total. On the source side, the vocabulary over the sentence descriptions is large too, with about 30K unique tokens. The average sentence size is 6.07 tokens, with 80% of the sentences having at most 12 tokens.
C.3 MACHINE TRANSLATION
Starting from a preprocessed6 2% sub-selection of the English-French section of the WMT14 dataset, we further prune down the data by keeping only sentences of length between 5 and 20 words, and for which every word is within the 20K most frequent. The reason for this is to simplify the task by keeping only common words and avoiding out-of-vocabulary tokens. After this filtering, we are left with 53,607, 918 and 371 sentences for train, validation and test sets. After tokenizing, we obtain dependency parses for the target (English) sentences using the Stanford CoreNLP toolkit (Manning et al., 2014).
For the perturbation experiments, we randomly selected 50 sentences from among those in the test that could be easily restructured without significantly altering their meaning. The type of alterations we perform are: subordinate clause swapping, alternative construction substitution, passive/active voice change. In doing this, we try to keep the number of added/deleted words to a minimum, to minimize vocabulary-induced likelihood variations. When inserting new words, be verify that they are contained in the original vocabulary of 20K words. In Table 7 we show a few examples of the source, original target and perturbed target sentences.
6http://www-lium.univ-lemans.fr/ schwenk/cslm joint paper/

D ADDITIONAL EXAMPLE GENERATED TREES
","We propose a neural network architecture for generating tree-structured objects from encoded representations. The core of the method is a doubly recurrent neural network model comprised of separate width and depth recurrences that are combined inside each cell (node) to generate an output. The topology of the tree is modeled explicitly together with the content. That is, in response to an encoded vector representation, co-evolving recurrences are used to realize the associated tree and the labels for the nodes in the tree. We test this architecture in an encoderdecoder framework, where we train a network to encode a sentence as a vector, and then generate a tree structure from it. The experimental results show the effectiveness of this architecture at recovering latent tree structure in sequences and at mapping sentences to simple functional programs.",ICLR 2017 conference submission,True,,"Authors' response well answered my questions. Thanks. 
Evaluation not changed.

###

This paper proposes a neural model for generating tree structure output from scratch. The model does 1) separate the recurrence between depths and siblings; 2) separate the topology and label generation, and outperforms previous methods on a benchmark IFTTT dataset. Compared to previous tree-decoding methods, the model avoids manually annotating subtrees with special tokens, and thus is a very good alternative to such problems. The paper does solid experiments on one synthetic dataset, and outperforms alternative methods on one real-world IFTTT dataset. 

There are couple of interesting results in the paper that I believe is worth further investigation. Firstly, on the synthetic dataset, the precision drops rapidly with the number of nodes. Is it because that the vector representation of the sequential encoder fails to provide sufficient information of long sequences, such that the tree decoder can not do a good job? Or is it because that such tree decoder is not tolerant to the long sequence input, i.e., large tree structure? I believe that it is important to understand this before a better model can be developed. For example, if it is the fault of encoder, maybe an attention layer can be added, as in a seq-to-seq model, to preserve more information of the input sequence. 

Moreover, besides only showing how the precision changes with the number of nodes in the tree, it might be interesting to investigate how it goes with 1) number of depths; 2) number of widths; 3) symmetricity; etc. Moreover, as greedy search is used in decoding, it might be interesting to see how it helps, if it does, to use beam-search in tree decoding. 

On the IFTTT dataset, listing more statistics about this dataset might be helpful for better understanding the difficulty of this task. How deep are the trees? How large are the vocabularies on both language and program sides?

The paper is well written, except for minor typo as mentioned in my pre-review questions. 

In general, I believe this is a solid paper, and more can be explored in this direction. So I tend to accept it.

---

The paper introduces a new model for generating trees decorated with node embeddings. Interestingly the authors do not assume that even leaf nodes in the tree are known a-priori. There has been very little work on this setting, and, the problem is quite important and general. Though the experiments are somewhat limited, reviewers generally believe that they are sufficient to show that the approach holds a promise.
 
 + an important and under-explored setting
 + novel model
 + well written
 
 - experimentation could be stronger (but seems sufficient -- both on real and artificial data)

---

It is really a nice work and paper is written quite well. The related work section is comprehensive and the problem is well motivated. And in my view, the experiments are good enough especially the paper contribution is introducing a new model which can be very useful in generating structured outputs using recurrent structure.

Questions: 
q1) How long did it take to train each of the networks in the paper?
q2) Wondering any plan to release the code?


Thanks.

---

This paper proposes a variant of a recurrent neural network that has two orthogonal temporal dimensions that can be used as a decoder to generate tree structures (including the topology) in an encoder-decoder setting. The architecture is well motivated and I can see several applications (in addition to what's presented in the paper) that need to generate tree structures given an unstructured data.

One weakness of the paper is the limitation of experiments. IFTTT dataset seems to be an interesting appropriate application, and there is also a synthetic dataset, however it would be more interesting to see more natural language applications with syntactic tree structures. Still, I consider the experiments sufficient as a first step to showcase a novel architecture.

A strength is that the authors experiment with different design decisions when building the topology predictor components of the architecture, about when / how to decide to terminate, as opposed to making a single arbitrary choice.

I see future applications of this architecture and it seems to have interesting directions for future work so I suggest its acceptance as a conference contribution.

---

The paper propose DRNN as a neural decoder for tree structures. I like the model architecture since it has two clear improvements over traditional approaches — (1) the information flows in two directions, both from the parent and from siblings, which is desirable in tree structures (2) the model use a probability distribution to model the tree boundary (i.e. the last sibling or the leaf). This avoids the use of special ending symbols which is larger in size and putting more things to learn for the parameters (shared with other symbols).

The authors test the DRNN using the tasks of recovering the synthetic trees and recovering functional programs. The model did better than traditional methods like seq2seq models.

I think the recovering synthetic tree task is not very satisfying for two reasons — (1) the surface form itself already containing some of the topological information which makes the task easier than it should be (2) as we can see from figure 3, when the number of nodes grows (even to a number not very large), the performance of the model drops dramatically, I am not sure if a simple baseline only captures the topological information in the surface string would be much worse than this. And DRNN in this case, seems can’t show its full potentials since the length of the information flow in the model won’t be very long.

I think the experiments are interesting. But I think there are some other tasks which are more difficult and the tree structure information are more important in such tasks. For example, we have the seq2seq parsing model (Vinyals et al, 2014), is it possible to use the DRNN proposed here on the decoder side? I think tasks like this can show more potentials of the DRNN and can be very convincing that model architectures like this are better than traditional alternatives.

---

Authors' response well answered my questions. Thanks. 
Evaluation not changed.

###

This paper proposes a neural model for generating tree structure output from scratch. The model does 1) separate the recurrence between depths and siblings; 2) separate the topology and label generation, and outperforms previous methods on a benchmark IFTTT dataset. Compared to previous tree-decoding methods, the model avoids manually annotating subtrees with special tokens, and thus is a very good alternative to such problems. The paper does solid experiments on one synthetic dataset, and outperforms alternative methods on one real-world IFTTT dataset. 

There are couple of interesting results in the paper that I believe is worth further investigation. Firstly, on the synthetic dataset, the precision drops rapidly with the number of nodes. Is it because that the vector representation of the sequential encoder fails to provide sufficient information of long sequences, such that the tree decoder can not do a good job? Or is it because that such tree decoder is not tolerant to the long sequence input, i.e., large tree structure? I believe that it is important to understand this before a better model can be developed. For example, if it is the fault of encoder, maybe an attention layer can be added, as in a seq-to-seq model, to preserve more information of the input sequence. 

Moreover, besides only showing how the precision changes with the number of nodes in the tree, it might be interesting to investigate how it goes with 1) number of depths; 2) number of widths; 3) symmetricity; etc. Moreover, as greedy search is used in decoding, it might be interesting to see how it helps, if it does, to use beam-search in tree decoding. 

On the IFTTT dataset, listing more statistics about this dataset might be helpful for better understanding the difficulty of this task. How deep are the trees? How large are the vocabularies on both language and program sides?

The paper is well written, except for minor typo as mentioned in my pre-review questions. 

In general, I believe this is a solid paper, and more can be explored in this direction. So I tend to accept it.

---

Authors' response well answered my questions. Thanks. 
Evaluation not changed.

###

This paper proposes a neural model for generating tree structure output from scratch. The model does 1) separate the recurrence between depths and siblings; 2) separate the topology and label generation, and outperforms previous methods on a benchmark IFTTT dataset. Compared to previous tree-decoding methods, the model avoids manually annotating subtrees with special tokens, and thus is a very good alternative to such problems. The paper does solid experiments on one synthetic dataset, and outperforms alternative methods on one real-world IFTTT dataset. 

There are couple of interesting results in the paper that I believe is worth further investigation. Firstly, on the synthetic dataset, the precision drops rapidly with the number of nodes. Is it because that the vector representation of the sequential encoder fails to provide sufficient information of long sequences, such that the tree decoder can not do a good job? Or is it because that such tree decoder is not tolerant to the long sequence input, i.e., large tree structure? I believe that it is important to understand this before a better model can be developed. For example, if it is the fault of encoder, maybe an attention layer can be added, as in a seq-to-seq model, to preserve more information of the input sequence. 

Moreover, besides only showing how the precision changes with the number of nodes in the tree, it might be interesting to investigate how it goes with 1) number of depths; 2) number of widths; 3) symmetricity; etc. Moreover, as greedy search is used in decoding, it might be interesting to see how it helps, if it does, to use beam-search in tree decoding. 

On the IFTTT dataset, listing more statistics about this dataset might be helpful for better understanding the difficulty of this task. How deep are the trees? How large are the vocabularies on both language and program sides?

The paper is well written, except for minor typo as mentioned in my pre-review questions. 

In general, I believe this is a solid paper, and more can be explored in this direction. So I tend to accept it.

---

The paper introduces a new model for generating trees decorated with node embeddings. Interestingly the authors do not assume that even leaf nodes in the tree are known a-priori. There has been very little work on this setting, and, the problem is quite important and general. Though the experiments are somewhat limited, reviewers generally believe that they are sufficient to show that the approach holds a promise.
 
 + an important and under-explored setting
 + novel model
 + well written
 
 - experimentation could be stronger (but seems sufficient -- both on real and artificial data)

---

It is really a nice work and paper is written quite well. The related work section is comprehensive and the problem is well motivated. And in my view, the experiments are good enough especially the paper contribution is introducing a new model which can be very useful in generating structured outputs using recurrent structure.

Questions: 
q1) How long did it take to train each of the networks in the paper?
q2) Wondering any plan to release the code?


Thanks.

---

This paper proposes a variant of a recurrent neural network that has two orthogonal temporal dimensions that can be used as a decoder to generate tree structures (including the topology) in an encoder-decoder setting. The architecture is well motivated and I can see several applications (in addition to what's presented in the paper) that need to generate tree structures given an unstructured data.

One weakness of the paper is the limitation of experiments. IFTTT dataset seems to be an interesting appropriate application, and there is also a synthetic dataset, however it would be more interesting to see more natural language applications with syntactic tree structures. Still, I consider the experiments sufficient as a first step to showcase a novel architecture.

A strength is that the authors experiment with different design decisions when building the topology predictor components of the architecture, about when / how to decide to terminate, as opposed to making a single arbitrary choice.

I see future applications of this architecture and it seems to have interesting directions for future work so I suggest its acceptance as a conference contribution.

---

The paper propose DRNN as a neural decoder for tree structures. I like the model architecture since it has two clear improvements over traditional approaches — (1) the information flows in two directions, both from the parent and from siblings, which is desirable in tree structures (2) the model use a probability distribution to model the tree boundary (i.e. the last sibling or the leaf). This avoids the use of special ending symbols which is larger in size and putting more things to learn for the parameters (shared with other symbols).

The authors test the DRNN using the tasks of recovering the synthetic trees and recovering functional programs. The model did better than traditional methods like seq2seq models.

I think the recovering synthetic tree task is not very satisfying for two reasons — (1) the surface form itself already containing some of the topological information which makes the task easier than it should be (2) as we can see from figure 3, when the number of nodes grows (even to a number not very large), the performance of the model drops dramatically, I am not sure if a simple baseline only captures the topological information in the surface string would be much worse than this. And DRNN in this case, seems can’t show its full potentials since the length of the information flow in the model won’t be very long.

I think the experiments are interesting. But I think there are some other tasks which are more difficult and the tree structure information are more important in such tasks. For example, we have the seq2seq parsing model (Vinyals et al, 2014), is it possible to use the DRNN proposed here on the decoder side? I think tasks like this can show more potentials of the DRNN and can be very convincing that model architectures like this are better than traditional alternatives.

---

Authors' response well answered my questions. Thanks. 
Evaluation not changed.

###

This paper proposes a neural model for generating tree structure output from scratch. The model does 1) separate the recurrence between depths and siblings; 2) separate the topology and label generation, and outperforms previous methods on a benchmark IFTTT dataset. Compared to previous tree-decoding methods, the model avoids manually annotating subtrees with special tokens, and thus is a very good alternative to such problems. The paper does solid experiments on one synthetic dataset, and outperforms alternative methods on one real-world IFTTT dataset. 

There are couple of interesting results in the paper that I believe is worth further investigation. Firstly, on the synthetic dataset, the precision drops rapidly with the number of nodes. Is it because that the vector representation of the sequential encoder fails to provide sufficient information of long sequences, such that the tree decoder can not do a good job? Or is it because that such tree decoder is not tolerant to the long sequence input, i.e., large tree structure? I believe that it is important to understand this before a better model can be developed. For example, if it is the fault of encoder, maybe an attention layer can be added, as in a seq-to-seq model, to preserve more information of the input sequence. 

Moreover, besides only showing how the precision changes with the number of nodes in the tree, it might be interesting to investigate how it goes with 1) number of depths; 2) number of widths; 3) symmetricity; etc. Moreover, as greedy search is used in decoding, it might be interesting to see how it helps, if it does, to use beam-search in tree decoding. 

On the IFTTT dataset, listing more statistics about this dataset might be helpful for better understanding the difficulty of this task. How deep are the trees? How large are the vocabularies on both language and program sides?

The paper is well written, except for minor typo as mentioned in my pre-review questions. 

In general, I believe this is a solid paper, and more can be explored in this direction. So I tend to accept it.",,3.5,1.0,1.5,3.0,6.333333333333333,2.0,,4.0,,
412,"Trusting SVM for Piecewise Linear CNNs
Authors: Leonard Berrada, Andrew Zisserman, Pawan Kumar
Source file: 412.pdf

ABSTRACT
We present a novel layerwise optimization algorithm for the learning objective of Piecewise-Linear Convolutional Neural Networks (PL-CNNs), a large class of convolutional neural networks. Specifically, PL-CNNs employ piecewise linear non-linearities such as the commonly used ReLU and max-pool, and an SVM classifier as the final layer. The key observation of our approach is that the problem corresponding to the parameter estimation of a layer can be formulated as a difference-of-convex (DC) program, which happens to be a latent structured SVM. We optimize the DC program using the concave-convex procedure, which requires us to iteratively solve a structured SVM problem. This allows to design an optimization algorithm with an optimal learning rate that does not require any tuning. Using the MNIST, CIFAR and ImageNet data sets, we show that our approach always improves over the state of the art variants of backpropagation and scales to large data and large network settings.

1 Introduction
The backpropagation algorithm is commonly employed to estimate the parameters of a convolutional neural network (CNN) using a supervised training data set (Rumelhart et al., 1986). Part of the appeal of backpropagation comes from the fact that it is applicable to a wide variety of networks, namely those that have (sub-)differentiable non-linearities and employ a (sub-)differentiable learning objective. However, the generality of backpropagation comes at the cost of a high sensitivity to its hyperparameters such as the learning rate and momentum. Standard line-search algorithms cannot be used on the primal objective function in this setting, as (i) there may not exist a step-size guaranteeing a monotonic decrease because of the use of sub-gradients, and (ii) even in the smooth case, each function evaluation requires a forward pass over the entire data set without any update, making the approach computationally unfeasible. Choosing the learning rate thus remains an open issue, with the state-of-the-art algorithms suggesting adaptive learning rates (Duchi et al., 2011; Zeiler, 2012; Kingma & Ba, 2015). In addition, techniques such as batch normalization (Ioffe & Szegedy, 2015) and dropout (Srivastava et al., 2014) have been introduced to respectively reduce the sensitivity to the learning rate and to prevent from overfitting.
With this work, we open a different line of inquiry, namely, is it possible to design more robust optimization algorithms for special but useful classes of CNNs? To this end, we focus on the networks that are commonly used in computer vision. Specifically, we consider CNNs with convolutional and dense layers that apply a set of piecewise linear (PL) non-linear operations to obtain a discriminative representation of an input image. While this assumption may sound restrictive at first, we show that commonly used non-linear operations such as ReLU and max-pool fall under the category of PL functions. The representation obtained in this way is used to classify the image via a multi-class SVM, which forms the final layer of the network. We refer to this class of networks as PL-CNN.
We design a novel, principled algorithm to optimize the learning objective of a PL-CNN. Our algorithm is a layerwise method, that is, it iteratively updates the parameters of one layer while keeping the other layers fixed. For this work, we use a simple schedule over the
layers, namely, repeated passes from the output layer to the input one. However, it may be possible to further improve the accuracy and efficiency of our algorithm by designing more sophisticated scheduling strategies. The key observation of our approach is that the parameter estimation of one layer of PL-CNN can be formulated as a difference-of-convex (DC) program that can be viewed as a latent structured SVM problem (Yu & Joachims, 2009). This allows us to solve the DC program using the concave-convex procedure (CCCP) (Yuille & Rangarajan, 2002). Each iteration of CCCP requires us to solve a convex structured SVM problem. To this end, we use the powerful block-coordinate Frank-Wolfe (BCFW) algorithm (Lacoste-Julien et al., 2013), which solves the dual of the convex program iteratively by computing the conditional gradients corresponding to a subset of training samples. In order to further improve BCFW for PL-CNNs, we extend it in three important ways. First, we introduce a trust-region term that allows us to initialize the BCFW algorithm using the current estimate of the layer parameters. Second, we reduce the memory requirement of BCFW by an order of magnitude, via an efficient representation of the feature vectors corresponding to the dense layers. Third, we show that, empirically, the number of constraints of the structural SVM problem can be reduced substantially without any loss in accuracy, which allows us to significantly reduce its time complexity.
Compared to backpropagation (Rumelhart et al., 1986) or its variants (Duchi et al., 2011; Zeiler, 2012; Kingma & Ba, 2015), our algorithm offers three advantages. First, the CCCP algorithm provides a monotonic decrease in the learning objective at each layer. Since layerwise optimization itself can be viewed as a block-coordinate method, our algorithm guarantees a monotonic decrease of the overall objective function after each layer’s parameters have been updated. Second, since the dual of the SVM problem is a smooth convex quadratic program, each step of the BCFW algorithm (in the inner iteration of the CCCP) provides a monotonic increase in its dual objective. Third, since the only step-size required in our approach comes while solving the SVM dual, we can use the optimal step-size that is computed analytically during each iteration of BCFW (Lacoste-Julien et al., 2013). In other words, our algorithm has no learning rate, initial or not, that requires tuning.
Using standard network architectures and publicly available data sets, we show that our algorithm provides a boost over the state of the art variants of backpropagation for learning PL-CNNs and we demonstrate scalability of the method.

2 Related Work
While some of the early successful approaches for the optimization of deep neural networks relied on greedy layer-wise training (Hinton et al., 2006; Bengio et al., 2007), most currently used methods are variants of backpropagation (Rumelhart et al., 1986) with adaptive learning rates, as discussed in the introduction.
At every iteration, backpropagation performs a forward pass and a backward pass on the network, and updates the parameters of each layer by stochastic or mini-batch gradient descent. This makes the choice of the learning rate critical for efficient optimization. Duchi et al. (2011) have proposed the Adagrad convex solver, which adapts the learning rate for every direction and takes into account past updates. Adagrad changes the learning rate to favor steps in gradient directions that have not been observed frequently in past updates. When applied to the non-convex CNN optimization problem, Adagrad may converge prematurely due to a rapid decrease in the learning rate (Goodfellow et al., 2016). In order to prevent this behavior, the Adadelta algorithm (Zeiler, 2012) makes the decay of the learning rate slower. It is worth noting that this fix is empirical, and to the best of our knowledge, provides no theoretical guarantees. Kingma & Ba (2015) propose a different scheme for the learning rate, called Adam, which uses an online estimation of the first and second moments of the gradients to provide centered and normalized updates. However all these methods still require the tuning of the initial learning rate to perform well.
Second-order and natural gradient optimization methods have also been a subject of attention. The focus in this line of work has been to come up with appropriate approximations to make the updates cheaper. Martens & Sutskever (2012) suggested a Hessian-free second order optimization using finite differences to approximate the Hessian and conjugate gradient to
compute the update. Martens & Grosse (2015) derive an approximation of the Fisher matrix inverse, which provides a more efficient method for natural gradient descent. Ollivier (2013) explore a set of Riemannian methods based on natural gradient descent and quasi-Newton methods to guarantee reparametrization invariance of the problem. Desjardins et al. (2015) demonstrate a scaled up natural gradient descent method by training on the ImageNet data set (Russakovsky et al., 2015). Though providing more informative updates and solid theoretical support than SGD-based approaches, these methods do not take into account the structure of the problem offered by the commonly used non-linear operations.
Our work is also related to some of the recent developments in optimization for deep learning. For example, Taylor et al. (2016) use ADMM for massive distribution of computation in a layer-wise fashion, and in particular their method will yield closed-form updates for any PLCNN. Lee et al. (2015) propose to use targets instead of gradients to propagate information through the network, which could help to extend our algorithm. Zhang et al. (2016) derive a convex relaxation for the learning objective for a restricted class of CNNs, which also relies on solving an approximate convex problem. In (Amos et al., 2016), the authors identify convex problems for the inference task, when the neural network is a convex function of some of its inputs.
With a more theoretical approach, Goel et al. (2016) propose an algorithm to learn shallow ReLU nets with guarantees of time convergence and generalization error. Heinemann et al. (2016) show that a subclass of neural networks can be modeled as an improper kernel, which then reduces the learning problem to a simple SVM with the constructed kernel.
More generally, we believe that our hitherto unknown observation regarding the relationship between PL-CNNs and latent SVMs can (i) allow the progress made in one field to be transferred to the other and (ii) help design a new generation of principled algorithms for deep learning optimization.

3 Piecewise Linear Convolutional Neural Networks
A piecewise linear convolutional neural network (PL-CNN) consists of a series of convolutional layers, followed by a series of dense layers, which provides a concise representation of an input image. Each layer of the network performs two operations: a linear transformation (that is, a convolution or a matrix multiplication), followed by a piecewise linear non-linear operation such as ReLU or max-pool. The resulting representation of the image is used for classification via an SVM. In the remainder of this section, we provide a formal description of PL-CNN.
Piecewise Linear Functions. A piecewise linear (PL) function f(u) is a function of the following form (Melzer, 1986):
f(u) = max i∈[m] {a>i u} −max j∈[n] {b>j u}, (1)
where [m] = {1, · · · ,m}, and [n] = {1, · · · , n}. Each of the two maxima above is a convex function, therefore such a function f is not generally convex, but it is rather a difference of two convex functions. Importantly, many commonly used non-linear operations such as ReLU or max-pool are PL functions of their input. For example, ReLU corresponds to the function R(v) = max{v, 0} where v is a scalar. Similarly, max-pool for a D-dimensional vector u corresponds to M(u) = maxi∈[D]{e>i u}, where ei is a vector whose i-th element is 1 and all other elements are 0. Given a value of u, we say that (i∗, j∗) is the activation of the PL function at u if i∗ = argmaxi∈[m]{a>i u} and j∗ = argmaxj∈[n]{b>j u}.
PL-CNN Parameters. We denote the parameters of an L layer PL-CNN by W = {W l; l ∈ [L]}. In other words, the parameters of the l-th layer is defined as W l. The CNN defines a composite function, that is, the output zl−1 of layer l− 1 is the input to the layer l. Given the input zl−1 to layer l, the output is computed as zl = σl(W l · zl−1), where “·” is either a convolution or a matrix multiplication, and σl is a PL non-linear function, such as ReLU or max-pool. The input to the first layer is an image x, that is, z0 = x. We denote
the input to the final layer by zL = Φ(x;W) ∈ RD. In other words, given an image x, the convolutional and dense layers of a PL-CNN provide a D-dimensional representation of x to the final classification layer. The final layer of a PL-CNN is a C class SVM W svm, which specifies one parameter W svmy ∈ RD for each class y ∈ Y.
Prediction. Given an image x, a PL-CNN predicts its class using the following rule:
y∗ = argmax y∈Y W svmy Φ(x;W). (2)
In other words, the dot product of the D-dimensional representation of x with the SVM parameter for a class y provides the score for the class. The desired prediction is obtained by maximizing the score over all possible classes.
Learning Objective. Given a training data set D = {(xi, yi), i ∈ [N ]}, where xi is the input image and yi is its ground-truth class, we wish to estimate the parameters W ∪W svm of the PL-CNN. To this end, we minimize a regularized upper bound on the empirical risk. The risk of a prediction y∗i given the ground-truth yi is measured with a user-specified loss function ∆(y∗i , yi). For example, the standard 0− 1 loss has a value of 0 for a correct prediction and 1 for an incorrect prediction. Formally, the parameters of a PL-CNN are estimated using the following learning objective:
min W,W svm
λ
2 ∑ l∈[L]∪{svm} ‖W l‖2F + 1 N N∑ i=1 max ȳi∈Y ( ∆(ȳi, yi) + ( W svmȳi −W svm yi )T Φ(xi;W) ) . (3)
The hyperparameter λ denotes the relative weight of the regularization compared to the upper bound of the empirical risk. Note that, due to the presence of piecewise linear non-linearities, the representation Φ(·;W) (and hence, the above objective) is highly non-convex in the PL-CNN parameters.

4 Parameter Estimation for PL-CNN
In order to enable layerwise optimization of PL-CNNs, we show that parameter estimation of a layer can be formulated as a difference-of-convex (DC) program (subsection 4.1). This allows us to use the concave-convex procedure, which solves a series of convex optimization problems (subsection 4.2). We show that each convex problem closely resembles a structured SVM objective, which can be addressed by the powerful block-coordinate Frank-Wolfe (BCFW) algorithm. We extend BCFW to improve its initialization, time complexity and memory requirements, thereby enabling its use in learning PL-CNNs (subsection 4.3). For the sake of clarity, we only provide sketches of the proofs for those propositions that are necessary for understanding the paper. The detailed proofs of the remaining propositions are provided in the Appendix.

4.1 Layerwise Optimization as a DC Program
Given the values of the parameters for the convolutional and the dense layers (that is, W), the learning objective (3) is the standard SVM problem in parameters W svm. In other words, it is a convex optimization problem with several efficient solvers (Tsochantaridis et al., 2004; Joachims et al., 2009; Shalev-Shwartz et al., 2009), including the BCFW algorithm (LacosteJulien et al., 2013). Hence, the optimization of the final layer is a computationally easy problem. In contrast, the optimization of the parameters of a convolutional or a dense layer l does not result in a convex program. In general, this problem can be arbitrarily hard to solve. However, in the case of PL-CNN, we show that the problem can be formulated as a specific type of DC program, which enables efficient optimization via the iterative use of BCFW. The key property that enables our approach is the following proposition that shows that the composition of PL functions is also a PL function.
Proposition 1. Consider PL functions g : Rm → R and gi : Rn → R, for all i ∈ [m]. Define a function f : Rn → R as f(u) = g([g1(u), g2(u), · · · , gm(u)]>). Then f is also a PL function (proof in Appendix A).
Using the above proposition, we can reformulate the problem of optimizing the parameters of one layer of the network as a DC program. Specifically, the following proposition shows that the problem can be formulated as a latent structured SVM objective (Yu & Joachims, 2009).
Proposition 2. The learning objective of a PL-CNN with respect to the parameters of the l-th layer can be specified as follows:
min W l
λ 2 ‖W l‖2F + 1 N N∑ i=1
max hi∈H ȳi∈Y
( ∆(ȳi, yi) + (W l)>Ψ(xi, ȳi,hi) ) − max
hi∈H
( (W l)>Ψ(xi, yi,hi) ) ,
(4)
for an appropriate choice of the latent space H and joint feature vectors Ψ(x, y,h) of the input x, the output y and the latent variables h. In other words, parameter estimation for the l-th layer corresponds to minimizing the sum of its Frobenius norm plus a PL function for each training sample.
Sketch of the Proof. For a given image x with the ground-truth class y, consider the input to the layer l, which we denote by zl−1. Since all the layers except the l-th one are fixed, the input zl−1 is a constant vector, which only depends on the image x (that is, its value does not depend on the variables W l). In other words, we can write zl−1 = ϕ(x).
Given the input zl−1, all the elements of the output of the l-th layer, denoted by zl, are a PL function of W l since the layer performs a linear transformation of zl−1 according to the parameters W l, followed by an application of PL operations such as ReLU or max-pool. The vector zl is then fed to the (l + 1)-th layer. The output zl+1 of the (l + 1)-th layer is a vector whose elements are PL functions of zl. Therefore, by proposition (1), the elements of zl+1 are a PL function of W l. By applying the same argument until we reach the layer L, we can conclude that the representation Φ(x;W) is a PL function of W l. Next, consider the upper bound of the empirical risk, which is specified as follows:
max ȳ∈Y
( ∆(ȳ, y) + ( W svmȳ −W svmy )T Φ(x;W) ) . (5)
Once again, since W svm is fixed, the above upper bound can be interpreted as a PL function of Φ(x;W), and thus, by proposition (1), the upper bound is a PL function of W l. It only remains to observe that the learning objective (3) also contains the Frobenius norm of W l. Thus, it follows that the estimation of the parameters of layer l can be reformulated as minimizing the sum of its Frobenius norm and the PL upper bound of the empirical risk over all training samples, as shown in problem (4). Note that we have ignored the constants corresponding to the Frobenius norm of the parameters of all the fixed layers. This constitutes an existential proof of Proposition 2. In the next paragraph, we give an intuition about the feature vectors Ψ(xi, ȳi,hi) and the latent space H.
Feature Vectors & Latent Space. The exact form of the joint feature vectors depends on the explicit DC decomposition of the objective function. In Appendix B, we detail the practical computations and give an example: we construct two interleaved neural networks whose outputs define the convex and concave parts of the DC objective function. Given the explicit DC objective function, the feature vectors are given by a subgradient and can therefore be obtained by automatic differentiation.
We now give an intuition of what the latent space H represents. Consider an input image x and a corresponding latent variable h ∈ H. The latent variable can be viewed as a set of variables hk, k ∈ {l + 1, · · · , L}. In other words, each subset hk of the latent variable corresponds to one of the layers of the network that follow the layer l. Intuitively, hk represents the choice of activation at layer k when going through the PL activation: for each neuron j of layer k, hkj takes value i if and only if the i-th piece of the piecewise linear activation is selected. For instance, i is the index of the selected input in the case of a max-pooling unit.
Note that the latent space only depends on the layers that follow the current layer being optimized. This is due to the fact that the input zl−1 to the l-th layer is a constant vector
that does not depend on the value of W l. However, the activations of all subsequent layers following the l-th one depend on the value of the parameters W l. As a consequence, the greater the number of following layers, the greater the size of the latent space, and this growth happens to be exponential. However, as will be seen shortly, it is still possible to efficiently optimize problem (4) for all the layers of the network despite this exponential increase.

4.2 Concave-Convex Procedure
The optimization problem (4) is a DC program in the parameters W l. This follows from the fact that the upper bound of the empirical risk is a PL function, and can therefore be expressed as the difference of two convex PL functions (Melzer, 1986). Furthermore, the Frobenius norm of W l is also a convex function of W l. This observation allows us to obtain an approximate solution of problem (4) using the iterative concave-convex procedure (CCCP) (Yuille & Rangarajan, 2002).
Algorithm 1 describes the main steps of CCCP. In step 3, we impute the best value of the latent variable corresponding to the ground-truth class yi for each training sample. This imputation corresponds to the linearization step of the CCCP. The selected latent variable corresponds to a choice of activations at each non-linear layer of the network, and therefore defines a path of activations to the ground truth. Next, in step 4, we update the parameters by solving a convex optimization problem. This convex problem amounts to finding the path of activations which minimizes the maximum margin violations given the path to the ground truth defined in step 3.
The CCCP algorithm has the desirable property of providing a monotonic decrease in the objective function at each iteration. In other words, the objective function value of problem (4) at W lt is greater than or equal to its value at W l t+1. Since layerwise optimization itself can be viewed as a block-coordinate algorithm for minimizing the learning objective (3), our overall algorithm provides guarantees of monotonic decrease until convergence. This is one of the main advantages of our approach compared to backpropagation and its variants, which fail to provide similar guarantees on the value of the objective function from one iteration to the next.
Algorithm 1 CCCP for parameter estimation of the l-th layer of the PL-CNN.
Require: Data set D = {(xi, yi), i ∈ [N ]}, fixed parameters {W ∪ W svm}\W l, initial estimate W l0.
1: t = 0 2: repeat 3: For each sample (xi, yi), find the best latent variable value by solving the following
problem: h∗i = argmax
h∈H (W lt ) >Ψ(xi, yi,h). (6)
4: Update the parameters by solving the following convex optimization problem:
W lt+1 = argmin W l
λ 2 ‖W l‖2F + 1 N N∑ i=1
max ȳi∈Y hi∈H
( ∆(ȳi, yi) + (W l)>Ψ(xi, ȳi,hi) ) −
( (W l)>Ψ(xi, yi,h ∗ i ) ) . (7)
5: t = t+1 6: until Objective function of problem (4) cannot be improved beyond a specified tolerance.
In order to solve the convex program (7), which corresponds to a structured SVM problem, we make use of the powerful BCFW algorithm (Lacoste-Julien et al., 2013) that solves its dual via conditional gradients. This has two main advantages: (i) as the dual is a smooth quadratic program, each iteration of BCFW provides a monotonic increase in its
objective; and (ii) the optimal step-size at each iteration can be computed analytically. This is once again in stark contrast to backpropagation, where the estimation of the step-size is still an active area of research (Duchi et al., 2011; Zeiler, 2012; Kingma & Ba, 2015). As shown by Lacoste-Julien et al. (2013), given the current estimate of the parameters W l, the conditional gradient of the dual of program (7) with respect to a training sample (xi, yi) can be obtained by solving the following problem:
(ŷi, ĥi) = argmax ȳ∈Y,h∈H
(W l)>Ψ(xi, ȳ,h) + ∆(ȳ, yi). (8)
We refer the interested reader to (Lacoste-Julien et al., 2013) for further details.
The overall efficiency of the CCCP algorithm relies on our ability to solve problems (6) and (8). At first glance, these problems may appear to be computationally intractable as the latent space H can be very large, especially for layers close to the input (of the order of millions of dimensions for a typical network). However, the following proposition shows that both the problems can be solved efficiently using the forward and backward passes that are employed in backpropagation.
Proposition 3. Given the current estimate W l of the parameters for the l-th layer, as well as the parameter values of all the other fixed layers, problems (6) and (8) can be solved
using a forward pass on the network. Furthermore, the joint feature vectors Ψ(xi, ŷi, ĥi) and Ψ(xi, yi,h ∗ i ) can be computed using a backward pass on the network.
Sketch of the Proof. Recall that the latent space consists of the putative activations for each PL operation in the layers following the current one. Thus, intuitively, the maximization over the latent variables corresponds to finding the exact activations of all such PL operations. In other words, we need to identify the indices of the linear pieces that are used to compute the value of the PL function in the current state of the network. For a ReLU operation, this corresponds to estimating max{0, v}, where the input to the ReLU is a scalar v. Similarly, for a max-pool operation, this corresponds to estimating maxi{e>i u}, where u is the input vector to the max-pool. This is precisely the computation that the forward pass of backpropagation performs. Given the activations, the joint feature vector is the subgradient of the sample with respect to the current layer. Once again, this is precisely what is computed during the backward pass of the backpropagation algorithm.
An example is constructed in Appendix B to illustrate how to compute the feature vectors in practice.

4.3 Improving the BCFW Algorithm
As the BCFW algorithm was originally designed to solve a structured SVM problem, it requires further extensions to be suitable for training a PL-CNN. In what follows, we present three such extensions that improve the initialization, memory requirements and time complexity of the BCFW algorithm respectively.
Trust-Region for Initialization. The original BCFW algorithm starts with an initial parameter W l = 0 (that is, all the parameters are set to 0). The reason for this initialization is that it is possible to compute the dual variables that correspond to the 0 primal variable. However, since our algorithm visits each layer of the network several times, it would be desirable to initialize its parameters using its current value W tl . To this end, we introduce a trust-region in the constraints of problem (7), or equivalently, an `2 norm based proximal term in its objective function (Parikh & Boyd, 2014). The following proposition shows that this has the desired effect of initializing the BCFW algorithm close to the current parameter values.
Proposition 4. By adding a proximal term µ2 ‖W l −W lt‖2F to the objective function in (7), we can compute a feasible dual solution whose corresponding primal solution is equal to µ λ+µW l t . Furthermore, the addition of the proximal term still allows us to efficiently compute the conditional gradient using a forward-backward pass (proof in Appendix D).
In practice, we always choose a value of µ = 10λ: this yields an initialization of ' 0.9W lt which does not significantly change the value of the objective function.
Efficient Representation of Joint Feature Vectors. The BCFW algorithm requires us to store a linear combination of the feature vectors for each mini-batch. While this requirement is not too stringent for convolutional and multi-class SVM layers, where the dimensionality of the feature vectors is small, it becomes prohibitively expensive for dense layers. The following proposition prevents a blow-up in the memory requirements of BCFW.
Proposition 5. When optimizing dense layer l, if W l ∈ Rp×q, we can store a representation of the joint feature vectors Ψ(x, y,h) with vectors of size p in problems (6) and (7). This is in contrast to the näıve approach that requires them to be of size p× q.
Sketch of the Proof. By Proposition (3), the feature vectors are subgradients of the hinge loss function, which we loosely denote by η for this proof. Then by the chain rule: ∂η ∂W l
= ∂η ∂zl ∂zl ∂W l = ∂η ∂zl · ( zl−1 )T . Noting that zl−1 ∈ Rq is a forward pass up until layer l (independent of W l), we can store only ∂η ∂zl ∈ Rp and still reconstruct the full feature vector ∂η ∂W l by a forward pass and an outer product.
Reducing the Number of Constraints. In order to reduce the amount of time required for the BCFW algorithm to converge, we use the structure of H to simplify problem (7) to a much simpler problem. Specifically, since H represents the activations of the network for a given sample, it has a natural decomposition over the layers: H = H1 × ...×HL. We use this structure in the following observation.
Observation 1. Problem (7) can be approximately solved by optimizing the dual problem on increasingly large search spaces. In other words, we start with constraints of Y, followed by Y ×HL, then Y ×HL ×HL−1 and so on. The algorithm converges when the primal-dual gap is below tolerance.
The latent variables which are not optimized over are set to be the same as the ones selected for the ground truth. Experimentally, we observe that for convolutional layers (architectures in section 5), restricting the search space to Y yields a dual gap low enough to consider the problem has converged. This means that in practice for these layers, problem (7) can be solved by searching directions over the search space Y instead of the much larger Y × H. The intuition is that the norm of the difference-of-convex decomposition grows with the number of activations selected differently in the convex and concave parts (see Appendix A for the decomposition of piecewise linear functions). This compels the path of activations to be the same in the convex and the concave part to avoid large margin violations, especially for convolutional layers which are followed by numerous non-linearities at the max-pooling layers.

5 Experiments
Our experiments are designed to assess the ability of LW-SVM (Layer-Wise SVM, our method) and the SGD baselines to optimize problem (3). To compare LW-SVM with the state-of-the-art variants of backpropagation, we look at the training and testing accuracies as well as the training objective value. Unlike dropout, which effectively learns an ensemble model, we learn a single model using each baseline optimization algorithm. All experiments are conducted on a GPU (Nvidia Titan X) and use Theano (Bergstra et al., 2010; Bastien et al., 2012). We compare LW-SVM with Adagrad, Adadelta and Adam. For all data sets, we start at a good solution provided by these solvers and fine-tune it with LW-SVM. We then check whether a longer run of the SGD solver reaches the same level of performance.
The practical use of the LW-SVM algorithm needs choices at the three following levels: how to select the layer to optimize (i), when to stop the CCCP on each layer (ii) and when to stop the convex optimization at each inner iteration of the CCCP (iii). These choices are detailed in the next paragraph.
The layer-wise schedule of LW-SVM is as follows: as long as the validation accuracy increases, we perform passes from the end of the network (SVM) to the first layer (i). At each pass, each layer is optimized with one outer iteration of the CCCP (ii). The inner iterations are stopped when the dual objective function does not increase by more than 1% over an epoch (iii). We point out that the dual objective function is cheap to compute since we are maintaining its value at all time. By contrast, to compute the exact primal objective function requires a forward pass over the data set without any update.

5.1 MNIST Data Set
Data set & Architecture The training data set consists in 60,000 gray scale images of size 28× 28 with 10 classes, which we split into 50,000 samples for training and 10,000 for validating. The images are normalized, and we do not use any data augmentation. The architecture used for this experiment is shown in Figure 1.
Method The number of epochs is set to 200, 100 and 100 for Adagrad, Adadelta and Adam - Adagrad is given more epochs as we observed it took a longer time to converge. We then use LW-SVM and compare the results on training objective, training accuracy and testing accuracy. We also let the solvers run to up to 500 epochs to verify that we have not stopped the optimization prematurely. The regularization hyperparameter λ and the initial learning rate are chosen by cross-validation. λ is set to 0.001 for all solvers, and the initial learning rates can be found in Appendix C. For LW-SVM, λ is set to the same value as the baseline, and the proximal term µ to µ = 10λ = 0.01.
Results As Table 1 shows, LW-SVM systematically improves on all training objective, training accuracy and testing accuracy. In particular, it obtains the best testing accuracy when combined with Adadelta. Because each convex sub-problem is run up to sufficient convergence, the objective function of LW-SVM features of monotonic decrease at each iteration of the CCCP (blue curves in first row of Figure 2).

5.2 CIFAR Data Sets
Data sets & Architectures The CIFAR-10/100 data sets are comprised of 60,000 RGB natural images of size 32× 32 with 10/100 classes (Krizhevsky, 2009)). We split the training set into 45,000 training samples and 5,000 validation samples in both cases. The images are centered and normalized, and we do not use any data augmentation. To obtain a strong enough baseline, we employ (i) a pre-training with a softmax and cross-entropy loss and (ii) Batch-Normalization (BN) layers before each non-linearity.
We have experimentally found out that pre-training with a softmax layer followed by a cross-entropy loss led to better behavior and results than using an SVM loss alone. The baselines are trained with batch normalization. Once they have converged, the estimated mean and standard deviation are fixed like they would be at test time. Then batch normalization becomes a linear transformation, which can be handled by the LW-SVM algorithm. This allows us to compare LW-SVM with a baseline benefiting from batch normalization. Specifically, we use the architecture shown in Figure 3:
Method Again, the initial learning rates and regularization weight λ are obtained by cross-validation, and a value of 0.001 is obtained for λ for all solvers on both datasets. As before, µ is set to 10λ. The initial learning rates are reported in Appendix C. The layer schedule and convergence criteria are as described at the beginning of the section. For each SGD optimizer, we train the network for 10 epochs with a cross-entropy loss (preceded by a softmax layer). Then it is trained with an SVM loss (without softmax) for respectively 1000, 100 and 100 epochs for Adagrad, Adadelta and Adam. This amount is doubled to verify that the baselines are not harmed by a premature stopping. Results are presented in Tables 2 and 3.
Results It can be seen from this set of results that LW-SVM always improves over the solution of the SGD algorithm, for example on CIFAR-100, decreasing the objective value of Adam from 0.22 to 0.06, or improving the test accuracy of Adadelta from 84.4% to 86.6% on
CIFAR-10. The automatic step-size allows for a precise fine-tuning to optimize the training objective, while the regularization of the proximal term helps for better generalization.

5.3 ImageNet Data Set
Network Top-1 Accuracy Top-5 Accuracy VGG-16 (PT) 73.30% 91.33% VGG-16 (PT + LW-SVM) 73.81% 91.61%
Since the objective function penalizes the top-1 error, it is logical to observe that the improvement is most important on the top-1 accuracy. Importantly, having an efficient representation of feature vectors proves to be essential for such large networks: for instance, in the optimization of the first fully connected layer with a batch-size of 100, the use of our representation lowers the memory requirements of the BCFW algorithm from 7,600GB to 20GB, which can then fit in the memory of a powerful computer.

6 Discussion
We presented a novel layerwise optimization algorithm for a large and useful class of convolutional neural networks, which we term PL-CNNs. Our key observation is that the optimization of the parameters of one layer of a PL-CNN is equivalent to solving a latent structured SVM problem. As the problem is a DC program, it naturally lends itself to the iterative CCCP approach, which optimizes a convex structured SVM objective at each iteration. This allows us to leverage the advancements made in structured SVM optimization over the past decade to design a computationally feasible approach for learning PL-CNNs. Specifically, we use the BCFW algorithm and extend it to improve its initialization, memory requirements and time complexity. In particular, this allows our method to not require the tuning of any learning rate. Using the publicly available MNIST, CIFAR-10 and CIFAR-100 data sets, we show that our approach provides a boost for learning PL-CNNs over the state of the art backpropagation algorithms. Furthermore, we demonstrate scalability of the method with results on the ImageNet data set with a large network.
When the mean and standard deviation estimations of batch normalization are not fixed (unlike in our experiments with LW-SVM), batch normalization is not a piecewise linear transformation, and therefore cannot be used in conjunction with the BCFW algorithm for SVMs. However, it is difference-of-convex as it is a C2 function (Horst & Thoai, 1999). Incorporating a normalization scheme into our framework will be the object of future work. With our current methodology, LW-SVM algorithm can already be used on most standard architectures like VGG, Inception and ResNet-type architectures.
It is worth noting that other approaches for solving structured SVM problems, such as cutting-plane algorithms (Tsochantaridis et al., 2004; Joachims et al., 2009) and stochastic subgradient descent (Shalev-Shwartz et al., 2009), also rely on the efficiency of estimating the conditional gradient of the dual. Hence, all these methods are equally applicable to our setting. Indeed, the main strength of our approach is the establishment of a hitherto unknown connection between CNNs and latent structured SVMs. We believe that our observation will allow researchers to transfer the substantial existing knowledge of DC programs in general, and latent SVMs specifically, to produce the next generation of principled optimization algorithms for deep learning. In fact, there are already several such improvements that can be readily applied in our setting, which were not explored only due to a lack of time. This includes multi-plane variants of BCFW (Shah et al., 2015; Osokin et al., 2016), as well as generalizations of Frank-Wolfe such as partial linearization (Mohapatra et al., 2016).

Acknowledgments
This work was supported by the EPSRC AIMS CDT grant EP/L015987/1, the EPSRC Programme Grant Seebibyte EP/M013774/1 and Yougov. Many thanks to A. Desmaison, R. Bunel and D. Bouchacourt for the helpful discussions.

A Piecewise Linear Functions
Proof of Proposition (1) By the definition from (Melzer, 1986), we can write each function as the difference of two point-wise maxima of linear functions:
g(v) = max j∈[m+] {a>i v} − max k∈[m−] {b>j v} And ∀i ∈ [n], gi(u) = g+i (u)− g − i (u)
Where all the g+i , g − i are linear point-wise maxima of linear functions. Then:
f(u) = g([g1(u), · · · , gn(u)]>) = max j∈[m+] {a>j [g1(u), · · · , gn(u)]>} − max k∈[m−] {b>k [g1(u), · · · , gn(u)]>}
= max j∈[m+] { n∑ i=1 aj,igi(u) } − max k∈[m−] { n∑ i=1 bk,igi(u) }
= max j∈[m+] { n∑ i=1 aj,ig + i (u)− n∑ i=1 aj,ig − i (u) } − max k∈[m−] { n∑ i=1 bk,ig + i (u)− n∑ i=1 bk,ig − i (u) }
= max j∈[m+]  n∑ i=1 aj,ig + i (u) + ∑ j′∈[m+]\{j} n∑ i=1 aj,ig − i (u) − ∑ j′∈[m+] n∑ i=1 aj,ig − i (u)
− max k∈[m−]  n∑ i=1 bk,ig + i (u) + ∑ k′∈[m−]\{k} n∑ i=1 bk,ig − i (u) + ∑ k′∈[m−] n∑ i=1 bk,ig − i (u)
= max j∈[m+]  n∑ i=1 aj,ig + i (u) + ∑ j′∈[m+]\{j} n∑ i=1 aj,ig − i (u) + ∑ k′∈[m−] n∑ i=1 bk,ig − i (u)
−  max k∈[m−]  n∑ i=1 bk,ig + i (u) + ∑ k′∈[m−]\{k} n∑ i=1 bk,ig − i (u) + ∑ j′∈[m+] n∑ i=1 aj,ig − i (u)  = max j∈[m+]  n∑ i=1 aj,ig + i (u) + ∑ j′∈[m+]\{j} n∑ i=1 aj,ig − i (u) + ∑ k′∈[m−] n∑ i=1 bk,ig − i (u)
 − max k∈[m−]  n∑ i=1 bk,ig + i (u) + ∑ k′∈[m−]\{k} n∑ i=1 bk,ig − i (u) + ∑ j′∈[m+] n∑ i=1 aj,ig − i (u)
 In each line of the last equality, we recognize a pointwise maximum of a linear combination of pointwise maxima of linear functions. This constitutes a pointwise maximum of linear functions.
This derivation also extends equation (10) to the multi-dimensional case by showing an explicit DC decomposition of the output.

B Computing the Feature Vectors
We describe here how to compute the feature vectors in practice. To this end, we show how to construct two (intertwined) neural networks that decompose the objective function into a convex and a concave part. We call these Difference of Convex (DC) networks. Once the DC networks are defined, a standard forward and backward pass in the two networks yields the feature vectors for the convex and concave contribution to the objective function. First, we derive how to perform a DC decomposition in linear and non-linear layers, and then we construct an example of DC networks.
DC Decomposition in a Linear Layer Let W be the weights of a fixed linear layer. We introduce W+ = 12 (|W |+W ) and W − = 12 (|W | −W ). We can note that W + and W− have exclusively non-negative weights, and that W = W+ −W−. Say we have an input u with the DC decomposition (ucvx, uccv), that is: u = ucvx − uccv, where both ucvx and uccv are convex. Then we can decompose the output of the layer as:
W · u = (W+ · ucvx +W− · uccv)︸ ︷︷ ︸ convex − (W− · ucvx +W+ · uccv)︸ ︷︷ ︸ convex
(9)
DC Decomposition in a Piecewise Linear Activation Layer For simplicity purposes, we consider that the non-linear layer is a point-wise maximum across [K] scalar inputs, that is, for an input (uk)k∈[K] ∈ RK , the output is maxk∈[K] uk (the general multi-dimensional case can be found in Appendix A). We suppose that we have a DC decomposition (ucvxk , u ccv k ) for each input k. Then we can write the following decomposition for the output of the layer:
max k∈[K] uk = max k∈[K]
(ucvxk − uccvk )
= max k∈[K] ucvxk + ∑ i∈[K],i6=k uccvi  ︸ ︷︷ ︸
convex
− ∑ k∈[K]
uccvk︸ ︷︷ ︸ convex
(10)
In particular, for a ReLU, we can write:
max(ucvx − uccv, 0) = max(ucvx, uccv)︸ ︷︷ ︸ convex − uccv︸︷︷︸ convex
(11)
And for a Max-Pooling layer, one can easily verify that equation (10) is equivalent to:
MaxPool(ucvx − uccv) = MaxPool(ucvx − uccv) + SumPool(uccv)︸ ︷︷ ︸ convex −SumPool(uccv)︸ ︷︷ ︸ convex (12)
An Example of DC Networks We use the previous observations to obtain a DC decomposition in any layer. We now take the example of the neural network used for the experiments on the MNIST data set, and we show how to construct the two neural networks when optimizing W 1, the weights of the first convolutional layer. First let us recall the architecture without decomposition:
We want to optimize the first convolutional layer, therefore we fix all other parameters. Then we apply all operations as described in the previous paragraphs, which yields the DC networks in Figure 7.
The network graph in Figure 7 illustrates Proposition 3 for the optimization of W 1: suppose we are interested in f cvx(x,W 1), the convex part of the objective function for a given sample x, and we wish to obtain the feature vector needed to perform an update of BCFW. With a
forward pass, the oracle for the latent and label variables (ĥ, ŷ) is efficiently computed; and
with a backward pass, we obtain the corresponding feature vector Ψ(x, ŷ, ĥ). Indeed, we recall from problem (8) that (ĥ, ŷ) are the latent and label variables maximizing f cvx(x,W 1). Then given x, the forward pass in the DC networks sequentially solves the nested maximization: it maximizes the activation of the ReLU and MaxPooling units at each layer, thereby selecting
the best latent variable ĥ at each non-linear layer, and maximizes the output of the SVM
Concave Network Convex Network
Non-Decomposed Corresponding
Network
layer, thereby selecting the best label ŷ. At the end of the forward pass, f cvx(x,W 1) is
therefore available as the output of the convex network, and the feature vector Ψ(x, ŷ, ĥ) can be computed as a subgradient of f cvx(x,W 1) with respect to W 1.
Linearizing the concave part is equivalent to fixing the activations of the DC networks, which can be done by using a fixed copy of W 1 at the linearization point (all other weights being fixed anyway). Then one can re-use the above reasoning to obtain the feature vectors for the linearized concave part. Altogether, this methodology allows our algorithm to be implemented in any standard deep learning library (our implementation is available at http://github.com/oval-group/pl-cnn).

C Experimental Details
Hyper-parameters The hyper-parameters are obtained by cross-validation with a search on powers of 10. In this section, η will denote the initial learning rate. We denote the Softmax + Cross-Entropy loss by SCE, while SVM stands for the usual Support Vector Machines loss.
One may note that the hyper-parameters are the same for both CIFAR-10 and CIFAR-100 for each combination of solver and loss. This makes sense since the initial learning rate mainly depends on the architecture of the network (and not so much on which particular images are fed to this network), which is very similar for the experiments on the CIFAR-10 and CIFAR-100 data sets.

D SVM Formulation & Dual Derivation
Multi-Class SVM Suppose we are given a data set of N samples, for which every sample i has a feature vector φi ∈ Rd and a ground truth label yi ∈ Y. For every possible label ȳi ∈ Y, we introduce the augmented feature vector ψi(ȳi) ∈ R|Y|×d containing φi at index ȳi, −φi at index yi, and zeros everywhere else (then ψi(yi) is just a vector of zeros). We also define ∆(ȳi, yi) as the loss by choosing the output ȳi instead of the ground truth yi in our task. For classification, this is the zero-one loss for example.
The SVM optimization problem is formulated as:
min w,ξi
λ 2 ‖w‖2 + 1 N N∑ i=1 ξi
subject to: ∀i ∈ [N ], ∀ȳi ∈ Y, ξi ≥ wTψi(ȳi) + ∆(yi, ȳi)
Where λ is the regularization hyperparameter. We now add a proximal term to a given starting point w0:
min w,ξi
λ 2 ‖w‖2 + µ 2 ‖w − w0‖2 + 1 N N∑ i=1 ξi
subject to: ∀i ∈ [N ], ∀ȳi ∈ Y, ξi ≥ wTψi(ȳi) + ∆(yi, ȳi)
Factorizing the second-order polynomial in w, we obtain the equivalent problem (changed by a constant):
min w,ξi
λ+ µ 2 ‖w − µ λ+ µ w0‖2 + 1 N N∑ i=1 ξi
subject to: ∀i ∈ [N ], ∀ȳi ∈ Y, ξi ≥ wTψi(ȳi) + ∆(yi, ȳi)
For simplicity, we introduce the ratio ρ = µ
λ+ µ .
Dual Objective function The primal problem is:
min w,ξi
λ+ µ
2 ‖w − ρw0‖2 +
1
N N∑ i=1 ξi
subject to: ∀i ∈ [N ], ∀ȳi ∈ Y, ξi ≥ wTψi(ȳi) + ∆(yi, ȳi)
The dual problem can be written as:
max α≥0 min w,ξi
λ+ µ
2 ‖w − ρw0‖2 +
1
N N∑ i=1 ξi + 1 N N∑ i=1 ∑ ȳi∈Y αi(ȳi) ( ∆(yi, ȳi) + w Tψi(ȳi)− ξi )
Then we obtain the following KKT conditions:
∀i ∈ [N ], ∂· ∂ξi = 0 −→ ∑ ȳi∈Y αi(ȳi) = 1
∂· ∂w = 0 −→ w = ρw0 − 1 N 1 λ+ µ N∑ i=1 ∑ ȳi∈Y
αi(ȳi)ψi(ȳi)︸ ︷︷ ︸ Aα
We also introduce b = 1N (∆(yi, ȳi))i,ȳi . We define Pn(Y) as the sample-wise probability simplex:
u ∈ Pn(Y) if: ∀i ∈ [N ], ∀ȳi ∈ Y, ui(ȳi) ≥ 0 ∀i ∈ [N ], ∑ ȳi∈Y ui(ȳi) = 1
We inject back and simplify to:
max α∈Pn(Y) −(λ+ µ) 2 ‖Aα‖2 + µwT0 (Aα) + αT b
Finally:
min α∈Pn(Y) f(α)
Where:
f(α) , λ+ µ
2 ‖Aα‖2 − µwT0 (Aα)− αT b
BCFW derivation We write ∇(i)f the gradient of f w.r.t. the block (i) of variables in α, padded with zeros on blocks (j) for j 6= i. Similarly, A(i) and b(i) contain the rows of A and the elements of b for the block of coordinates (i) and zeros elsewhere. We can write:
∇(i)f(α) = (λ+ µ)AT(i)Aα− µA(i)w0 − b(i)
Then the search corner for the block of coordinates (i) is given by:
si = argmin s′i
( < s′i,∇(i)f(α) > ) = argmin
s′i
( (λ+ µ)αTATA(i)s ′ i − µwT0 A(i)s′i − bT(i)s ′ i ) We replace:
Aα = ρw0 − w
A(i)s ′ i =
1
N
1
λ+ µ ∑ ȳi∈Y s′i(ȳi)ψi(ȳi)
bT(i)s ′ i =
1
N ∑ ȳi∈Y s′i(ȳi)∆(ȳi, yi)
We then obtain:
si = argmin s′i −(w − ρw0)T ∑ ȳi∈Y s′i(ȳi)ψi(ȳi)− wT0 ρ ∑ ȳi∈Y s′i(ȳi)ψi(ȳi)− ∑ ȳi∈Y s′i(ȳi)∆(ȳi, yi)  = argmax
s′i wT ∑ ȳi∈Y s′i(ȳi)ψi(ȳi) + ∑ ȳi∈Y s′i(ȳi)∆(ȳi, yi)  As expected, this maximum is obtained by setting si to one at y ∗ i = argmax ȳi∈Y ( wTψi(ȳi) + ∆(ȳi, yi) ) and zeros elsewhere. We introduce the notation:
wi = −A(i)α(i) li = b T (i)α(i)
ws = −A(i)si ls = b T (i)si
Then we have:
ws = − 1
N
1
λ+ µ ψ(y∗i ) = −
1
N
1
λ+ µ
∂Hi(y ∗ i )
∂w
ls = 1
N ∆(yi, y
∗ i )
The optimal step size in the direction of the block of coordinates (i) is given by :
γ∗ = argmin γ f(α+ γ(si − αi))
The optimal step-size is given by:
γ∗ = < ∇(i)f(α), si − αi > (λ+ µ)‖A(si − αi)‖2
We introduce wd = −Aα = w − ρw0. Then we obtain:
γ∗ = (wi − ws)T (w − ρw0) + ρwT0 (wi − ws)− 1λ+µ (li − ls)
‖wi − ws‖2
= (wi − ws)Tw − 1λ+µ (li − ls)
‖wi − ws‖2
And the updates are the same as in standard BCFW:
Algorithm 2 BCFW with warm start
1: Let w(0) = w0, ∀i ∈ [N ], w(0)i = 0 2: Let l(0) = 0, ∀i ∈ [N ], l(0)i = 0 3: for k=0...K do 4: Pick i randomly in {1, .., n}
5: Get y∗i = argmax ȳi∈Y Hi(ȳi, w (k)) and ws = −
1
N
1
λ+ µ
∂Hi(y ∗ i , w (k))
∂w(k)
6: ls = 1 N∆(y ∗ i , yi) 7: γ = (wi − ws)Tw − 1λ+µ (li − ls)
‖wi − ws‖2 clipped to [0, 1]
8: w (k+1) i = (1− γ)w (k) i + γws 9: l (k+1) i = (1− γ)l (k) i + γls 10: w(k+1) = w(k) + w (k+1) i − w (k) i = w (k) + γ(w (k) s − w(k)i ) 11: l(k+1) = l(k) + l (k+1) i − l (k) i 12: end for
In particular, we have proved Proposition (4) in this section: w is initialized to ρw0 (KKT conditions), and the direction of the conditional gradient, ws, is given by ∂Hi(y ∗ i )
∂w , which is
independent of w0.
Note that the derivation of the Lagrangian dual has introduced a dual variable αi(ȳi) for each linear constraint of the SVM problem (this can be replaced by αi(hi, (ȳi)) if we consider latent variables). These dual variables indicate the complementary slackness not only for the output class ȳi, but also for each of the activation which defines a piece of the piecewise linear hinge loss. Therefore a choice of α defines a path of activations.

E Sensitivity of SGD Algorithms
Here we discuss some weaknesses of the SGD-based algorithms that we have encountered in practice for our learning objective function. These behaviors have been observed in the case of PL-CNNs, and generally may not appear in different architectures (in particular the failure to learn with high regularization goes away with the use of batch normalization layers).
E.1 Initial Learning Rate
As mentioned in the experiments section, the choice of the initial learning rate is critical for good performance of all Adagrad, Adadelta and Adam. When the learning rate is too high, the network does not learn anything and the training and validating accuracies are stuck at random level. When it is too low, the network may take a considerably greater number of epochs to converge.
E.2 Failures to Learn
Regularization When the regularization hyper-parameter λ is set to a value of 0.01 or higher on CIFAR-10, SGD solvers get trapped in a local minimum and fail to learn. The SGD solvers indeed fall in the local minimum of shutting down all activations on ReLUs, which provide zero-valued feature vector to the SVM loss layer (and a hinge loss of one). As a consequence, no information can be back-propagated. We plot this behavior below:
In this situation, the network is at a bad saddle point (note that the training and validation accuracies are stuck at random levels). Our algorithm does not fall into such bad situations, however it is not able to get out of it either: each layer is at a pathological critical point of its own objective function, which makes our algorithm unable to escape from it.
With a lower initial learning rate, the evolution is slower, but eventually the solver goes back to the bad situation presented above.
Biases The same failing behavior as above has been observed when not using the biases in the network. Again our algorithm is robust to this change.
","We present a novel layerwise optimization algorithm for the learning objective of Piecewise-Linear Convolutional Neural Networks (PL-CNNs), a large class of convolutional neural networks. Specifically, PL-CNNs employ piecewise linear non-linearities such as the commonly used ReLU and max-pool, and an SVM classifier as the final layer. The key observation of our approach is that the problem corresponding to the parameter estimation of a layer can be formulated as a difference-of-convex (DC) program, which happens to be a latent structured SVM. We optimize the DC program using the concave-convex procedure, which requires us to iteratively solve a structured SVM problem. This allows to design an optimization algorithm with an optimal learning rate that does not require any tuning. Using the MNIST, CIFAR and ImageNet data sets, we show that our approach always improves over the state of the art variants of backpropagation and scales to large data and large network settings.",ICLR 2017 conference submission,True,,"A layer wise optimization for CNNs with ReLU activations and max-pooling is proposed and shown to correspond to a series of latent structured SVM problems. Using CCCP style optimization a monotonic decrease of the overall objective function can be guaranteed.

Summary:
———
I think the discussed insights are very interesting but not presented convincingly. Firstly, claims are emphasized which are often violated in practice (e.g., no convergence guarantees due to mini-batches), statements could be validated more convincingly (e.g., is monotone convergence a curse or a blessing), the experimental evaluation should be extended. In summary, I think the paper requires some more attention to form a compelling story.

Quality: I think some of the techniques could be described more carefully to better convey the intuition. At times apples are compared to oranges, e.g., back propagation is contrasted with CCCP.
Clarity: Some of the derivations and intuitions could be explained in more detail.
Originality: The suggested idea is reasonable albeit heuristics are required.
Significance: Since the experimental setup is somewhat limited according to my opinion, significance is hard to judge at this point in time.

Details:
————
1. I think the provided guarantees for the optimization procedure are certainly convenient theoretically but their practical relevance still needs to be demonstrated more convincingly, e.g., mini-batch optimization alleviates any form of monotonic decrease. Hence the emphasize in the paper is somewhat misguided according to my opinion and given he current experimental evaluation.

2. In spirit similar is work by B. Amos and J. Kolter, Input-Convex Deep Networks (

---

The authors present a novel layer-wise optimization approach for learning convolutional neural networks with piecewise linear nonlinearities. The proposed approach trains piecewise linear ConvNets layer by layer, reduces the sub-problem into latent structured SVM. 
 
 Reviewers mainly expressed concerns about the experimental results, which the authors have diligently addressed in their revised versions. While the reviewers haven't updated explicitly their reviews, I believe the changes made should have been sufficient for them to do so.
 
 Thus, I recommend this paper be accepted.

---

tldr: New results on ImageNet, CIFAR-100, improved results on CIFAR-10.

We thank the reviewers for their helpful feedbacks. We list here the changes made in the revisions of the paper (version 1 being the original submission read by the reviewers).

List of changes in version 2:

1) New results with batch-normalization on CIFAR-10 (new subsection 5.2)
2) Clarification of the objective of the paper and experiments (Methods paragraph in subsection 5.1)

List of changes in version 3:

1) New results on CIFAR-10: deeper architecture for a stronger baseline (subsection 5.2)
2) New results on CIFAR-100 (subsection 5.2)
3) Re-wording of the experiments section and removal of previous experiments on CIFAR-10 (with and without batch normalization) (section 5)
4) New Appendix about the computation of the feature vectors and detailed example (Appendix B).
5) Infeasibility of standard line-search in Introduction
6) New references, including suggestions from the reviewers (section 2)
7) More compact abstract
8) Inclusion of batch normalization in Discussion (section 6)
9) Minor rewording and typo fixes throughout the paper.

List of changes in version 4:
1) Added ImageNet results (subsection 5.3)

---

This paper presents a novel layer-wise optimization approach for learning CNN with piecewise linear nonlinearities.  The proposed approach trains piecewise linear CNNs layer by layer and reduces the sub-problem into latent structured SVM, which has been well-studied in the literature. In addition, the paper presents improvements of the BCFW algorithm used in the inner procedure. Overall, this paper is interesting. However, unfortunately, the experiment is not convincing. 

Pros:

- To my best knowledge, the proposed approach is novel, and the authors provide nice theoretical analysis.
- The paper is well-written and easy to follow. 

Cons:

- Although the proposed approach can be applied in general structured prediction problem, the experiments only conduct on a simple multi-class classification task. This makes this work less compelling. 
	
- The test accuracy performance on CIFAR-10 reported in the paper doesn't look right. The accuracy of the best model reported in this paper is 70.2% while existing work often reports 90+%. For example,

---

A layer wise optimization for CNNs with ReLU activations and max-pooling is proposed and shown to correspond to a series of latent structured SVM problems. Using CCCP style optimization a monotonic decrease of the overall objective function can be guaranteed.

Summary:
———
I think the discussed insights are very interesting but not presented convincingly. Firstly, claims are emphasized which are often violated in practice (e.g., no convergence guarantees due to mini-batches), statements could be validated more convincingly (e.g., is monotone convergence a curse or a blessing), the experimental evaluation should be extended. In summary, I think the paper requires some more attention to form a compelling story.

Quality: I think some of the techniques could be described more carefully to better convey the intuition. At times apples are compared to oranges, e.g., back propagation is contrasted with CCCP.
Clarity: Some of the derivations and intuitions could be explained in more detail.
Originality: The suggested idea is reasonable albeit heuristics are required.
Significance: Since the experimental setup is somewhat limited according to my opinion, significance is hard to judge at this point in time.

Details:
————
1. I think the provided guarantees for the optimization procedure are certainly convenient theoretically but their practical relevance still needs to be demonstrated more convincingly, e.g., mini-batch optimization alleviates any form of monotonic decrease. Hence the emphasize in the paper is somewhat misguided according to my opinion and given he current experimental evaluation.

2. In spirit similar is work by B. Amos and J. Kolter, Input-Convex Deep Networks (

---

This paper proposes a new approaches for optimizing the objective of CNNs. The proposed method uses a lay-wise optimization, i.e. at each step, it optimizes the parameters in one layer of CNN while fixing the parameters in other layers. The key insight of this paper is that, for a large class of CNNs, the optimization problem at a particular can be formulated as optimizing a piecewise linear (PL) function. This PL function optimization happens to be the optimization problem commonly encountered in latent structural SVM. This connection allows this paper to borrows ideas from the latent structural SVM literature, in particular concave-convex procedure, to learn the parameters of CNNs.

Overall, the paper is well-written. Traditional, CNNs and structural SVMs are almost two separate research communties. The connection of CNNs to latent structural SVM is interesting, and might bridge the gap and facilitate the transferring of ideas between these two camps.

Of course, the proposed method also has some limitations. 1) It is limited to layer-wise optimization. Nowadays layer-wise optimization is essentially a coordinate descent algorithm and is not really a competitive strategy in learning CNNs. When you choose layer-wise optimization, you already lose something in terms of optimizing the objective (since you are using coordinate descent, instead of gradient descent). Of course, you also gain something since now you can guarantee that each coordinate descent step always improve the objective. It is not clear to me how the loss/gain balances each other. 2) This paper focues on improving the optimization of CNN objective. However, we all know that a better objective does not necessarily correspond to a good model (e.g. due to overfitting). Although the SGD with backprop in standard CNN learning does not always improve the solution of the objective (unlike the proposed method in this paper), but to me, this might be a good thing since it can prevent overfitting (the goal of learning is not to get better solution for the optimization problem in the first place -- the optimization problem is merely a proxy to learn a model with good generalization ability).

The experiment is a bit weak.
1) Only CIFAR10 is used. This is a very small dataset by today's standard, while CNNs are typically used in large-scale datasets, such as ImageNet. It is not clear whether the conclusions of this paper still hold when applied on ImageNet.

2) This paper only compares with a crippled variant of SGD (without batch normalization, dropout, etc). Although this paper mentions that the reason is that it wants to focus on optimization. But I mentioned earlier, SGD is not designed to purely obtain the best solution that optimizes the objective, the goal of SGD is to reasonably optimize the objective, while preventing overfitting. So the comparison to SGD purely in terms of the optimization is that meaningful in the first place.

---

A layer wise optimization for CNNs with ReLU activations and max-pooling is proposed and shown to correspond to a series of latent structured SVM problems. Using CCCP style optimization a monotonic decrease of the overall objective function can be guaranteed.

Summary:
———
I think the discussed insights are very interesting but not presented convincingly. Firstly, claims are emphasized which are often violated in practice (e.g., no convergence guarantees due to mini-batches), statements could be validated more convincingly (e.g., is monotone convergence a curse or a blessing), the experimental evaluation should be extended. In summary, I think the paper requires some more attention to form a compelling story.

Quality: I think some of the techniques could be described more carefully to better convey the intuition. At times apples are compared to oranges, e.g., back propagation is contrasted with CCCP.
Clarity: Some of the derivations and intuitions could be explained in more detail.
Originality: The suggested idea is reasonable albeit heuristics are required.
Significance: Since the experimental setup is somewhat limited according to my opinion, significance is hard to judge at this point in time.

Details:
————
1. I think the provided guarantees for the optimization procedure are certainly convenient theoretically but their practical relevance still needs to be demonstrated more convincingly, e.g., mini-batch optimization alleviates any form of monotonic decrease. Hence the emphasize in the paper is somewhat misguided according to my opinion and given he current experimental evaluation.

2. In spirit similar is work by B. Amos and J. Kolter, Input-Convex Deep Networks (

---

The authors present a novel layer-wise optimization approach for learning convolutional neural networks with piecewise linear nonlinearities. The proposed approach trains piecewise linear ConvNets layer by layer, reduces the sub-problem into latent structured SVM. 
 
 Reviewers mainly expressed concerns about the experimental results, which the authors have diligently addressed in their revised versions. While the reviewers haven't updated explicitly their reviews, I believe the changes made should have been sufficient for them to do so.
 
 Thus, I recommend this paper be accepted.

---

tldr: New results on ImageNet, CIFAR-100, improved results on CIFAR-10.

We thank the reviewers for their helpful feedbacks. We list here the changes made in the revisions of the paper (version 1 being the original submission read by the reviewers).

List of changes in version 2:

1) New results with batch-normalization on CIFAR-10 (new subsection 5.2)
2) Clarification of the objective of the paper and experiments (Methods paragraph in subsection 5.1)

List of changes in version 3:

1) New results on CIFAR-10: deeper architecture for a stronger baseline (subsection 5.2)
2) New results on CIFAR-100 (subsection 5.2)
3) Re-wording of the experiments section and removal of previous experiments on CIFAR-10 (with and without batch normalization) (section 5)
4) New Appendix about the computation of the feature vectors and detailed example (Appendix B).
5) Infeasibility of standard line-search in Introduction
6) New references, including suggestions from the reviewers (section 2)
7) More compact abstract
8) Inclusion of batch normalization in Discussion (section 6)
9) Minor rewording and typo fixes throughout the paper.

List of changes in version 4:
1) Added ImageNet results (subsection 5.3)

---

This paper presents a novel layer-wise optimization approach for learning CNN with piecewise linear nonlinearities.  The proposed approach trains piecewise linear CNNs layer by layer and reduces the sub-problem into latent structured SVM, which has been well-studied in the literature. In addition, the paper presents improvements of the BCFW algorithm used in the inner procedure. Overall, this paper is interesting. However, unfortunately, the experiment is not convincing. 

Pros:

- To my best knowledge, the proposed approach is novel, and the authors provide nice theoretical analysis.
- The paper is well-written and easy to follow. 

Cons:

- Although the proposed approach can be applied in general structured prediction problem, the experiments only conduct on a simple multi-class classification task. This makes this work less compelling. 
	
- The test accuracy performance on CIFAR-10 reported in the paper doesn't look right. The accuracy of the best model reported in this paper is 70.2% while existing work often reports 90+%. For example,

---

A layer wise optimization for CNNs with ReLU activations and max-pooling is proposed and shown to correspond to a series of latent structured SVM problems. Using CCCP style optimization a monotonic decrease of the overall objective function can be guaranteed.

Summary:
———
I think the discussed insights are very interesting but not presented convincingly. Firstly, claims are emphasized which are often violated in practice (e.g., no convergence guarantees due to mini-batches), statements could be validated more convincingly (e.g., is monotone convergence a curse or a blessing), the experimental evaluation should be extended. In summary, I think the paper requires some more attention to form a compelling story.

Quality: I think some of the techniques could be described more carefully to better convey the intuition. At times apples are compared to oranges, e.g., back propagation is contrasted with CCCP.
Clarity: Some of the derivations and intuitions could be explained in more detail.
Originality: The suggested idea is reasonable albeit heuristics are required.
Significance: Since the experimental setup is somewhat limited according to my opinion, significance is hard to judge at this point in time.

Details:
————
1. I think the provided guarantees for the optimization procedure are certainly convenient theoretically but their practical relevance still needs to be demonstrated more convincingly, e.g., mini-batch optimization alleviates any form of monotonic decrease. Hence the emphasize in the paper is somewhat misguided according to my opinion and given he current experimental evaluation.

2. In spirit similar is work by B. Amos and J. Kolter, Input-Convex Deep Networks (

---

This paper proposes a new approaches for optimizing the objective of CNNs. The proposed method uses a lay-wise optimization, i.e. at each step, it optimizes the parameters in one layer of CNN while fixing the parameters in other layers. The key insight of this paper is that, for a large class of CNNs, the optimization problem at a particular can be formulated as optimizing a piecewise linear (PL) function. This PL function optimization happens to be the optimization problem commonly encountered in latent structural SVM. This connection allows this paper to borrows ideas from the latent structural SVM literature, in particular concave-convex procedure, to learn the parameters of CNNs.

Overall, the paper is well-written. Traditional, CNNs and structural SVMs are almost two separate research communties. The connection of CNNs to latent structural SVM is interesting, and might bridge the gap and facilitate the transferring of ideas between these two camps.

Of course, the proposed method also has some limitations. 1) It is limited to layer-wise optimization. Nowadays layer-wise optimization is essentially a coordinate descent algorithm and is not really a competitive strategy in learning CNNs. When you choose layer-wise optimization, you already lose something in terms of optimizing the objective (since you are using coordinate descent, instead of gradient descent). Of course, you also gain something since now you can guarantee that each coordinate descent step always improve the objective. It is not clear to me how the loss/gain balances each other. 2) This paper focues on improving the optimization of CNN objective. However, we all know that a better objective does not necessarily correspond to a good model (e.g. due to overfitting). Although the SGD with backprop in standard CNN learning does not always improve the solution of the objective (unlike the proposed method in this paper), but to me, this might be a good thing since it can prevent overfitting (the goal of learning is not to get better solution for the optimization problem in the first place -- the optimization problem is merely a proxy to learn a model with good generalization ability).

The experiment is a bit weak.
1) Only CIFAR10 is used. This is a very small dataset by today's standard, while CNNs are typically used in large-scale datasets, such as ImageNet. It is not clear whether the conclusions of this paper still hold when applied on ImageNet.

2) This paper only compares with a crippled variant of SGD (without batch normalization, dropout, etc). Although this paper mentions that the reason is that it wants to focus on optimization. But I mentioned earlier, SGD is not designed to purely obtain the best solution that optimizes the objective, the goal of SGD is to reasonably optimize the objective, while preventing overfitting. So the comparison to SGD purely in terms of the optimization is that meaningful in the first place.",,3.0,,,,5.0,,,4.0,,
438,"Authors: Piotr Mirowski, Razvan Pascanu, Fabio Viola, Hubert Soyer, Andrew J. Ballard, Andrea Banino, Misha Denil, Ross Goroshin, Laurent Sifre, Koray Kavukcuoglu, Dharshan Kumaran, Raia Hadsell
Source file: 438.pdf

ABSTRACT
Learning to navigate in complex environments with dynamic elements is an important milestone in developing AI agents. In this work we formulate the navigation question as a reinforcement learning problem and show that data efficiency and task performance can be dramatically improved by relying on additional auxiliary tasks leveraging multimodal sensory inputs. In particular we consider jointly learning the goal-driven reinforcement learning problem with auxiliary depth prediction and loop closure classification tasks. This approach can learn to navigate from raw sensory input in complicated 3D mazes, approaching human-level performance even under conditions where the goal location changes frequently. We provide detailed analysis of the agent behaviour1, its ability to localise, and its network activity dynamics, showing that the agent implicitly learns key navigation abilities.

1 INTRODUCTION
The ability to navigate efficiently within an environment is fundamental to intelligent behavior. Whilst conventional robotics methods, such as Simultaneous Localisation and Mapping (SLAM), tackle navigation through an explicit focus on position inference and mapping (Dissanayake et al., 2001), here we follow recent work in deep reinforcement learning (Mnih et al., 2015; 2016) and propose that navigational abilities could emerge as the by-product of an agent learning a policy that maximizes reward. One advantage of an intrinsic, end-to-end approach is that actions are not divorced from representation, but rather learnt together, thus ensuring that task-relevant features are present in the representation. Learning to navigate from reinforcement learning in partially observable environments, however, poses several challenges.
First, rewards are often sparsely distributed in the environment, where there may be only one goal location. Second, environments often comprise dynamic elements, requiring the agent to use memory at different timescales: rapid one-shot memory for the goal location, together with short term memory subserving temporal integration of velocity signals and visual observations, and longer term memory for constant aspects of the environment (e.g. boundaries, cues).
To improve statistical efficiency we bootstrap the reinforcement learning procedure by augmenting our loss with auxiliary tasks that provide denser training signals that support navigation-relevant representation learning. We consider two additional losses: the first one involves reconstruction of a low-dimensional depth map at each time step by predicting one input modality (the depth channel) from others (the colour channels). This auxiliary task concerns the 3D geometry of the environment, and is aimed to encourage the learning of representations that aid obstacle avoidance and short-term trajectory planning. The second task directly invokes loop closure from SLAM: the agent is trained to predict if the current location has been previously visited within a local trajectory. ∗Denotes equal contribution 1A video illustrating the navigation agents is available at: https://youtu.be/lNoaTyMZsWI
To address the memory requirements of the task we rely on a stacked LSTM architecture (Graves et al., 2013; Pascanu et al., 2013). We evaluate our approach using five 3D maze environments and demonstrate the accelerated learning and increased performance of the proposed agent architecture. These environments feature complex geometry, random start position and orientation, dynamic goal locations, and long episodes that require thousands of agent steps (see Figure 1). We also provide detailed analysis of the trained agent to show that critical navigation skills are acquired. This is important as neither position inference nor mapping are directly part of the loss; therefore, raw performance on the goal finding task is not necessarily a good indication that these skills are acquired. In particular, we show that the proposed agent resolves ambiguous observations and quickly localizes itself in a complex maze, and that this localization capability is correlated with higher task reward.

2 APPROACH
We rely on a end-to-end learning framework that incorporates multiple objectives. Firstly it tries to maximize cumulative reward using an actor-critic approach. Secondly it minimizes an auxiliary loss of inferring the depth map from the RGB observation. Finally, the agent is trained to detect loop closures as an additional auxiliary task that encourages implicit velocity integration.
The reinforcement learning problem is addressed with the Asynchronous Advantage Actor-Critic (A3C) algorithm (Mnih et al., 2016) that relies on learning both a policy π(at|st; θ) and value function V (st; θV ) given a state observation st. Both the policy and value function share all intermediate representations, both being computed using a separate linear layer from the topmost layer of the model. The agent setup closely follows the work of (Mnih et al., 2016) and we refer to this work for the details (e.g. the use of a convolutional encoder followed by either an MLP or an LSTM, the use of action repetition, entropy regularization to prevent the policy saturation, etc.). These details can also be found in the Appendix B.
The baseline that we consider in this work is an A3C agent (Mnih et al., 2016) that receives only RGB input from the environment, using either a recurrent or a purely feed-forward model (see Figure 2a,b). The encoder for the RGB input (used in all other considered architectures) is a 3 layer convolutional network. To support the navigation capability of our approach, we also rely on the Nav A3C agent (Figure 2c) which employs a two-layer stacked LSTM after the convolutional encoder. We expand the observations of the agents to include agent-relative velocity, the action sampled from the stochastic policy and the immediate reward, from the previous time step. We opt to feed the velocity and previously selected action directly to the second recurrent layer, with the first layer only receiving the reward. We postulate that the first layer might be able to make associations between reward and visual observations that are provided as context to the second layer from which the policy is computed. Thus, the observation st may include an image xt ∈ R3×W×H (where W and H are the width and
height of the image), the agent-relative lateral and rotational velocity vt ∈ R6, the previous action at−1 ∈ RNA , and the previous reward rt−1 ∈ R. Figure 2d shows the augmentation of the Nav A3C with the different possible auxiliary losses. In particular we consider predicting depth from the convolutional layer (we will refer to this choice as D1), or from the top LSTM layer (D2) or predicting loop closure (L). The auxiliary losses are computed on the current frame via a single layer MLP. The agent is trained by applying a weighted sum of the gradients coming from A3C, the gradients from depth prediction (multiplied with βd1 , βd2 ) and the gradients from the loop closure (scaled by βl). More details of the online learning algorithm are given in Appendix B.

2.1 DEPTH PREDICTION
The primary input to the agent is in the form of RGB images. However, depth information, covering the central field of view of the agent, might supply valuable information about the 3D structure of the environment. While depth could be directly used as an input, we argue that if presented as an additional loss it is actually more valuable to the learning process. In particular if the prediction loss shares representation with the policy, it could help build useful features for RL much faster, bootstrapping learning. Since we know from (Eigen et al., 2014) that a single frame can be enough to predict depth, we know this auxiliary task can be learnt. A comparison between having depth as input versus as an additional loss is given in Appendix C, which shows significant gain for depth as a loss.
Since the role of the auxiliary loss is just to build up the representation of the model, we do not necessarily care about the specific performance obtained or nature of the prediction. We do care about the data efficiency aspect of the problem and also computational complexity. If the loss is to be useful for the main task, we should converge faster on it compared to solving the RL problem (using less data samples), and the additional computational cost should be minimal. To achieve this we use a low resolution variant of the depth map, reducing the screen resolution to 4x16 pixels2.
We explore two different variants for the loss. The first choice is to phrase it as a regression task, the most natural choice. While this formulation, combined with a higher depth resolution, extracts the most information, mean square error imposes a unimodal distribution (van den Oord et al., 2016). To address this possible issue, we also consider a classification loss, where depth at each position is discretised into 8 different bands. The bands are non-uniformally distributed such that we pay more attention to far-away objects (details in Appendix B). The motivation for the classification formulation is that while it greatly reduces the resolution of depth, it is more flexible from a learning perspective and can result in faster convergence (hence faster bootstrapping).
2The image is cropped before being subsampled to lessen the floor and ceiling which have little relevant depth information.

2.2 LOOP CLOSURE PREDICTION
Loop closure, like depth, is valuable for a navigating agent, since can be used for efficient exploration and spatial reasoning. To produce the training targets, we detect loop closures based on the similarity of local position information during an episode, which is obtained by integrating 2D velocity over time. Specifically, in a trajectory noted {p0, p1, . . . , pT }, where pt is the position of the agent at time t, we define a loop closure label lt that is equal to 1 if the position pt of the agent is close to the position pt′ at an earlier time t′. In order to avoid trivial loop closures on consecutive points of the trajectory, we add an extra condition on an intermediary position pt′′ being far from pt. Thresholds η1 and η2 provide these two limits. Learning to predict the binary loop label is done by minimizing the Bernoulli loss Ll between lt and the output of a single-layer output from the hidden representation ht of the last hidden layer of the model, followed by a sigmoid activation.

3 RELATED WORK
There is a rich literature on navigation, primarily in the robotics literature. However, here we focus on related work in deep RL. Deep Q-networks (DQN) have had breakthroughs in extremely challenging domains such as Atari (Mnih et al., 2015). Recent work has developed on-policy RL methods such as advantage actor-critic that use asynchronous training of multiple agents in parallel (Mnih et al., 2016). Recurrent networks have also been successfully incorporated to enable state disambiguation in partially observable environments (Koutnik et al., 2013; Hausknecht & Stone, 2015; Mnih et al., 2016; Narasimhan et al., 2015).
Deep RL has recently been used in the navigation domain. Kulkarni et al. (2016) used a feedforward architecture to learn deep successor representations that enabled behavioral flexibility to reward changes in the MazeBase gridworld, and provided a means to detect bottlenecks in 3D VizDoom. Zhu et al. (2016) used a feedforward siamese actor-critic architecture incorporating a pretrained ResNet to support navigation to a target in a discretised 3D environment. Oh et al. (2016) investigated the performance of a variety of networks with external memory (Weston et al., 2014) on simple navigation tasks in the Minecraft 3D block world environment. Tessler et al. (2016) also used the Minecraft domain to show the benefit of combining feedforward deep-Q networks with the learning of resuable skill modules (cf options: (Sutton et al., 1999)) to transfer between navigation tasks. Tai & Liu (2016) trained a convnet DQN-based agent using depth channel inputs for obstacle avoidance in 3D environments. Barron et al. (2016) investigated how well a convnet can predict the depth channel from RGB in the Minecraft environment, but did not use depth for training the agent.
Auxiliary tasks have often been used to facilitate representation learning (Suddarth & Kergosien, 1990). Recently, the incorporation of additional objectives, designed to augment representation learning through auxiliary reconstructive decoding pathways (Zhang et al., 2016; Rasmus et al., 2015; Zhao et al., 2015; Mirowski et al., 2010), has yielded benefits in large scale classification tasks. In deep RL settings, however, only two previous papers have examined the benefit of auxiliary tasks. Specifically, Li et al. (2016) consider a supervised loss for fitting a recurrent model on the hidden representations to predict the next observed state, in the context of imitation learning of sequences provided by experts, and Lample & Chaplot (2016) show that the performance of a DQN agent in a first-person shooter game in the VizDoom environment can be substantially enhanced by the addition of a supervised auxiliary task, whereby the convolutional network was trained on an enemy-detection task, with information about the presence of enemies, weapons, etc., provided by the game engine.
In contrast, our contribution addresses fundamental questions of how to learn an intrinsic representation of space, geometry, and movement while simultaneously maximising rewards through reinforcement learning. Our method is validated in challenging maze domains with random start and goal locations.

4 EXPERIMENTS
We consider a set of first-person 3D mazes from the DeepMind Lab environment (Beattie et al., 2016) (see Fig. 1) that are visually rich, with additional observations available to the agent such as inertial
information and local depth information.3 The action space is discrete, yet allows finegrained control, comprising 8 actions: the agent can rotate in small increments, accelerate forward or backward or sideways, or induce rotational acceleration while moving. Reward is achieved in these environments by reaching a goal from a random start location and orientation. If the goal is reached, the agent is respawned to a new start location and must return to the goal. The episode terminates when a fixed amount of time expires, affording the agent enough time to find the goal several times. There are sparse ‘fruit’ rewards which serve to encourage exploration. Apples are worth 1 point, strawberries 2 points and goals are 10 points. Videos of the agent solving the maze are linked in Appendix A.
In the static variant of the maze, the goal and fruit locations are fixed and only the agent’s start location changes. In the dynamic (Random Goal) variant, the goal and fruits are randomly placed on every episode. Within an episode, the goal and apple locations stay fixed until the episode ends. This encourages an explore-exploit strategy, where the agent should initially explore the maze, then retain the goal location and quickly refind it after each respawn. For both variants (static and random goal) we consider a small and large map. The small mazes are 5× 10 and episodes last for 3600 timesteps, and the large mazes are 9× 15 with 10800 steps (see Figure 1). The RGB observation is 84× 84. The I-Maze environment (see Figure 1, right) is inspired by the classic T-maze used to investigate navigation in rodents (Olton et al., 1979): the layout remains fixed throughout, the agent spawns in the central corridor where there are apple rewards and has to locate the goal which is placed in the alcove of one of the four arms. Because the goal is hidden in the alcove, the optimal agent behaviour must rely on memory of the goal location in order to return to the goal using the most direct route. Goal location is constant within an episode but varies randomly across episodes.
The different agent architectures described in Section 2 are evaluated by training on the five mazes. Figure 3 shows learning curves (averaged over the 5 top performing agents). The agents are a feedforward model (FF A3C), a recurrent model (LSTM A3C), the stacked LSTM version with velocity, previous action and reward as input (Nav A3C), and Nav A3C with depth prediction from the convolution layer (Nav A3C+D1), Nav A3C with depth prediction from the last LSTM layer (Nav A3C+D2), Nav A3C with loop closure prediction (Nav A3C+L) as well as the Nav A3C with
3The environments used in this paper are publicly available at https://github.com/deepmind/lab.
all auxiliary losses considered together (Nav A3C+D1D2L). In each case we ran 64 experiments with randomly sampled hyper-parameters (for ranges and details please see the appendix). The mean over the top 5 runs as well as the top 5 curves are plotted. Expert human scores, established by a professional game player, are compared to these results. The Nav A3C+D2 agents reach human-level performance on Static 1 and 2, and attain about 91% and 59% of human scores on Random Goal 1 and 2.
In Mnih et al. (2015) reward clipping is used to stabilize learning, technique which we employed in this work as well. Unfortunately, for these particular tasks, this yields slightly suboptimal policies because the agent does not distinguish apples (1 point) from goals (10 points). Removing the reward clipping results in unstable behaviour for the base A3C agent (see Appendix C). However it seems that the auxiliary signal from depth prediction mediates this problem to some extent, resulting in stable learning dynamics (e.g. Figure 3f, Nav A3C+D1 vs Nav A3C*+D1). We clearly indicate whether reward clipping is used by adding an asterisk to the agent name.
Figure 3f also explores the difference between the two formulations of depth prediction, as a regression task or a classification task. We can see that the regression agent (Nav A3C*+D1[MSE]) performs worse than one that does classification (Nav A3C*+D1). This result extends to other maps, and we therefore only use the classification formulation in all our other results4. Also we see that predicting depth from the last LSTM layer (hence providing structure to the recurrent layer, not just the convolutional ones) performs better.
We note some particular results from these learning curves. In Figure 3 (a and b), consider the feedforward A3C model (red curve) versus the LSTM version (pink curve). Even though navigation seems to intrinsically require memory, as single observations could often be ambiguous, the feedforward model achieves competitive performance on static mazes. This suggest that there might be good strategies that do not involve temporal memory and give good results, namely a reactive policy held by the weights of the encoder, or learning a wall-following strategy. This motivates the dynamic environments that encourage the use of memory and more general navigation strategies.
Figure 3 also shows the advantage of adding velocity, reward and action as an input, as well as the impact of using a two layer LSTM (orange curve vs red and pink). Though this agent (Nav A3C) is better than the simple architectures, it is still relatively slow to train on all of the mazes. We believe that this is mainly due to the slower, data inefficient learning that is generally seen in pure RL approaches. Supporting this we see that adding the auxiliary prediction targets of depth and loop closure (Nav A3C+D1D2L, black curve) speeds up learning dramatically on most of the mazes (see Table 1: AUC metric). It has the strongest effect on the static mazes because of the accelerated learning, but also gives a substantial and lasting performance increase on the random goal mazes.
Although we place more value on the task performance than on the auxiliary losses, we report the results from the loop closure prediction task. Over 100 test episodes of 2250 steps each, within a large maze (random goal 2), the Nav A3C*+D1L agent demonstrated very successful loop detection, reaching an F-1 score of 0.83. A sample trajectory can be seen in Figure 4 (right).
4An exception is the Nav A3C*+D1L agent on the I-maze (Figure 3c), which uses depth regression and reward clipping. While it does worse, we include it because some analysis is based on this agent.

5 ANALYSIS

5.1 POSITION DECODING
In order to evaluate the internal representation of location within the agent (either in the hidden units ht of the last LSTM, or, in the case of the FF A3C agent, in the features ft on the last layer of the conv-net), we train a position decoder that takes that representation as input, consisting of a linear classifier with multinomial probability distribution over the discretized maze locations. Small mazes (5× 10) have 50 locations, large mazes (9× 15) have 135 locations, and the I-maze has 77 locations. Note that we do not backpropagate the gradients from the position decoder through the rest of the network. The position decoder can only see the representation exposed by the model, not change it.
An example of position decoding by the Nav A3C+D2 agent is shown in Figure 6, where the initial uncertainty in position is improved to near perfect position prediction as more observations are acquired by the agent. We observe that position entropy spikes after a respawn, then decreases once the agent acquires certainty about its location. Additionally, videos of the agent’s position decoding are linked in Appendix A. In these complex mazes, where localization is important for the purpose of reaching the goal, it seems that position accuracy and final score are correlated, as shown in Table 1. A pure feed-forward architecture still achieves 64.3% accuracy in a static maze with static goal, suggesting that the encoder memorizes the position in the weights and that this small maze is solvable by all the agents, with sufficient training time. In Random Goal 1, it is Nav A3C+D2 that achieves the best position decoding performance (85.5% accuracy), whereas the FF A3C and the LSTM A3C architectures are at approximately 50%.
In the I-maze, the opposite branches of the maze are nearly identical, with the exception of very sparse visual cues. We observe that once the goal is first found, the Nav A3C*+D1L agent is capable of directly returning to the correct branch in order to achieve the maximal score. However, the linear position decoder for this agent is only 68.5% accurate, whereas it is 87.8% in the plain LSTM A3C agent. We hypothesize that the symmetry of the I-maze will induce a symmetric policy that need not be sensitive to the exact position of the agent (see analysis below).
A desired property of navigation agents in our Random Goal tasks is to be able to first find the goal, and reliably return to the goal via an efficient route after subsequent re-spawns. The latency column in Table 1 shows that the Nav A3C+D2 agents achieve the lowest latency to goal once the goal has been discovered (the first number shows the time in seconds to find the goal the first time, and the second number is the average time for subsequent finds). Figure 5 shows clearly how the agent finds the goal, and directly returns to that goal for the rest of the episode. For Random Goal 2, none of the agents achieve lower latency after initial goal acquisition; this is presumably due to the larger, more challenging environment.

5.2 STACKED LSTM GOAL ANALYSIS
Figure 7(a) shows shows the trajectories traversed by an agent for each of the four goal locations. After an initial exploratory phase to find the goal, the agent consistently returns to the goal location. We visualize the agent’s policy by applying tSNE dimension reduction (Maaten & Hinton, 2008) to the cell activations at each step of the agent for each of the four goal locations. Whilst clusters corresponding to each of the four goal locations are clearly distinct in the LSTM A3C agent, there are 2 main clusters in the Nav A3C agent – with trajectories to diagonally opposite arms of the maze represented similarly. Given that the action sequence to opposite arms is equivalent (e.g. straight, turn left twice for top left and bottom right goal locations), this suggests that the Nav A3C policy-dictating LSTM maintains an efficient representation of 2 sub-policies (i.e. rather than 4 independent policies) – with critical information about the currently relevant goal provided by the additional LSTM.

5.3 INVESTIGATING DIFFERENT COMBINATIONS OF AUXILIARY TASKS
Our results suggest that depth prediction from the policy LSTM yields optimal results. However, several other auxiliary tasks have been concurrently introduced in (Jaderberg et al., 2017), and thus we provide a comparison of reward prediction against depth prediction. Following that paper, we implemented two additional agent architectures, one performing reward prediction from the convnet using a replay buffer, called Nav A3C*+R, and one combining reward prediction from the convnet and depth prediction from the LSTM (Nav A3C+RD2). Table 2 suggests that reward prediction (Nav A3C*+R) improves upon the plain stacked LSTM architecture (Nav A3C*) but not as much as depth prediction from the policy LSTM (Nav A3C+D2). Combining reward prediction and depth prediction (Nav A3C+RD2) yields comparable results to depth prediction alone (Nav A3C+D2); normalised average AUC values are respectively 0.995 vs. 0.981. Future work will explore other auxiliary tasks.

6 CONCLUSION
We proposed a deep RL method, augmented with memory and auxiliary learning targets, for training agents to navigate within large and visually rich environments that include frequently changing start and goal locations. Our results and analysis highlight the utility of un/self-supervised auxiliary objectives, namely depth prediction and loop closure, in providing richer training signals that bootstrap learning and enhance data efficiency. Further, we examine the behavior of trained agents, their ability to localise, and their network activity dynamics, in order to analyse their navigational abilities.
Our approach of augmenting deep RL with auxiliary objectives allows end-end learning and may encourage the development of more general navigation strategies. Notably, our work with auxiliary losses is related to (Jaderberg et al., 2017) which independently looks at data efficiency when exploiting auxiliary losses. One difference between the two works is that our auxiliary losses are online (for the current frame) and do not rely on any form of replay. Also the explored losses are very different in nature. Finally our focus is on the navigation domain and understanding if navigation emerges as a bi-product of solving an RL problem, while Jaderberg et al. (2017) is concerned with data efficiency for any RL-task.
Whilst our best performing agents are relatively successful at navigation, their abilities would be stretched if larger demands were placed on rapid memory (e.g. in procedurally generated mazes), due to the limited capacity of the stacked LSTM in this regard. It will be important in the future to combine visually complex environments with architectures that make use of external memory (Graves et al., 2016; Weston et al., 2014; Olton et al., 1979) to enhance the navigational abilities of agents. Further, whilst this work has focused on investigating the benefits of auxiliary tasks for developing the ability to navigate through end-to-end deep reinforcement learning, it would be interesting for future work to compare these techniques with SLAM-based approaches.
ACKNOWLEDGEMENTS
We would like to thank Alexander Pritzel, Thomas Degris and Joseph Modayil for useful discussions, Charles Beattie, Julian Schrittwieser, Marcus Wainwright, and Stig Petersen for environment design and development, and Amir Sadik and Sarah York for expert human game testing.

Supplementary Material
A VIDEOS OF TRAINED NAVIGATION AGENTS
We show the behaviour of Nav A3C*+D1L agent in 5 videos, corresponding to the 5 navigation environments: I-maze5, (small) static maze6, (large) static maze7, (small) random goal maze8 and (large) random goal maze9. Each video shows a high-resolution video (the actual inputs to the agent are down-sampled to 84×84 RGB images), the value function over time (with fruit reward and goal acquisitions), the layout of the mazes with consecutive trajectories of the agent marked in different colours and the output of the trained position decoder, overlayed on top of the maze layout.

B NETWORK ARCHITECTURE AND TRAINING

B.1 THE ONLINE MULTI-LEARNER ALGORITHM FOR MULTI-TASK LEARNING
We introduce a class of neural network-based agents that have modular structures and that are trained on multiple tasks, with inputs coming from different modalities (vision, depth, past rewards and past actions). Implementing our agent architecture is simplified by its modular nature. Essentially, we construct multiple networks, one per task, using shared building blocks, and optimise these networks jointly. Some modules, such as the conv-net used for perceiving visual inputs, or the LSTMs used for learning the navigation policy, are shared among multiple tasks, while other modules, such as depth predictor gd or loop closure predictor gl, are task-specific. The navigation network that outputs the policy and the value function is trained using reinforcement learning, while the depth prediction and loop closure prediction networks are trained using self-supervised learning.
Within each thread of the asynchronous training environment, the agent plays on its own episode of the game environment, and therefore sees observation and reward pairs {(st, rt)} and takes actions that are different from those experienced by agents from the other, parallel threads. Within a thread, the multiple tasks (navigation, depth and loop closure prediction) can be trained at their own schedule, and they add gradients to the shared parameter vector as they arrive. Within each thread, we use a flag-based system to subordinate gradient updates to the A3C reinforcement learning procedure.

B.2 NETWORK AND TRAINING DETAILS
For all the experiments we use an encoder model with 2 convolutional layers followed by a fully connected layer, or recurrent layer(s), from which we predict the policy and value function. The architecture is similar to the one in (Mnih et al., 2016). The convolutional layers are as follows. The first convolutional layer has a kernel of size 8x8 and a stride of 4x4, and 16 feature maps. The second layer has a kernel of size 4x4 and a stride of 2x2, and 32 feature maps. The fully connected layer, in the FF A3C architecture in Figure 2a has 256 hidden units (and outputs visual features ft). The LSTM in the LSTM A3C architecture has 256 hidden units (and outputs LSTM hidden activations ht). The LSTMs in Figure 2c and 2d are fed extra inputs (past reward rt−1, previous action at expressed as a one-hot vector of dimension 8 and agent-relative lateral and rotational velocity vt encoded by a 6-dimensional vector), which are all concatenated to vector ft. The Nav A3C architectures (Figure 2c,d) have a first LSTM with 64 or 128 hiddens and a second LSTM with 256 hiddens. The depth predictor modules gd, g′d and the loop closure detection module gl are all single-layer MLPs with 128 hidden units. The depth MLPs are followed by 64 independent 8-dimensional softmax outputs (one per depth pixel). The loop closure MLP is followed by a 2-dimensional softmax output. We illustrate on Figure 8 the architecture of the Nav A3C+D+L+Dr agent.
Depth is taken as the Z-buffer from the Labyrinth environment (with values between 0 and 255), divided by 255 and taken to power 10 to spread the values in interval [0, 1]. We empirically decided to use the following quantization: {0, 0.05, 0.175, 0.3, 0.425, 0.55, 0.675, 0.8, 1} to ensure a uniform
5Video of the Nav A3C*+D1L agent on the I-maze: https://youtu.be/PS4iJ7Hk_BU 6Video of the Nav A3C*+D1L agent on static maze 1: https://youtu.be/-HsjQoIou_c 7Video of the Nav A3C*+D1L agent on static maze 2: https://youtu.be/kH1AvRAYkbI 8Video of the Nav A3C*+D1L agent on random goal maze 1: https://youtu.be/5IBT2UADJY0 9Video of the Nav A3C*+D1L agent on random goal maze 2: https://youtu.be/e10mXgBG9yo
binning across 8 classes. The previous version of the agent had a single depth prediction MLP gd for regressing 8× 16 = 128 depth pixels from the convnet outputs ft. The parameters of each of the modules point to a subset of a common vector of parameters. We optimise these parameters using an asynchronous version of RMSProp (Tieleman & Hinton, 2012). (Nair et al., 2015) was a recent example of asynchronous and parallel gradient updates in deep reinforcement learning; in our case, we focus on the specific Asynchronous Advantage Actor Critic (A3C) reinforcement learning procedure in (Mnih et al., 2016).
Learning follows closely the paradigm described in (Mnih et al., 2016). We use 16 workers and the same RMSProp algorithm without momentum or centering of the variance. Gradients are computed over non-overlaping chunks of the episode. The score for each point of a training curve is the average over all the episodes the model gets to finish in 5e4 environment steps.
The whole experiments are run for a maximum of 1e8 environment step. The agent has an action repeat of 4 as in (Mnih et al., 2016), which means that for 4 consecutive steps the agent will use the same action picked at the beginning of the series. For this reason through out the paper we actually report results in terms of agent perceived steps rather than environment steps. That is, the maximal number of agent perceived step that we do for any particular run is 2.5e7.
In our grid we sample hyper-parameters from categorical distributions:
• Learning rate was sampled from [10−4, 5 · 10−4]. • Strength of the entropy regularization from [10−4, 10−3]. • Rewards were not scaled and not clipped in the new set of experiments. In our previous set
of experiments, rewards were scaled by a factor from {0.3, 0.5} and clipped to 1 prior to back-propagation in the Advantage Actor-Critic algorithm. • Gradients are computed over non-overlaping chunks of 50 or 75 steps of the episode. In our
previous set of experiments, we used chunks of 100 steps.
The auxiliary tasks, when used, have hyperparameters sampled from:
• Coefficient βd of the depth prediction loss from convnet features Ld sampled from {3.33, 10, 33}. • Coefficient β′d of the depth prediction loss from LSTM hiddens Ld′ sampled from {1, 3.33, 10}. • Coefficient βl of the loop closure prediction loss Ll sampled from {1, 3.33, 10}.
Loop closure uses the following thresholds: maximum distance for position similarity η1 = 1 square and minimum distance for removing trivial loop-closures η2 = 2 squares.

C ADDITIONAL RESULTS

C.1 REWARD CLIPPING
Figure 9 shows additional learning curves. In particular in the left plot we show that the baselines (A3C FF and A3C LSTM) as well as Nav A3C agent without auxiliary losses, perform worse without reward clipping than with reward clipping. It seems that removing reward clipping makes learning unstable in absence of auxiliary tasks. For this particular reason we chose to show the baselines with reward clipping in our main results.

C.2 DEPTH PREDICTION AS REGRESSION OR CLASSIFICATION TASKS
The right subplot of Figure 9 compares having depth as an input versus as a target. Note that using RGBD inputs to the Nav A3C agent performs even worse than predicting depth as a regression task, and in general is worse than predicting depth as a classification task.

C.3 NON-NAVIGATION TASKS IN 3D MAZE ENVIRONMENTS
We have evaluated the behaviour of the agents introduced in this paper, as well as agents with reward prediction, introduced in (Jaderberg et al., 2017) (Nav A3C*+R) and with a combination of reward prediction from the convnet and depth prediction from the policy LSTM (Nav A3C+RD2), on different 3D maze environments with non-navigation specific tasks. In the first environment, Seek-Avoid Arena, there are apples (yielding 1 point) and lemons (yielding -1 point) disposed in an arena, and the agents needs to pick all the apples before respawning; episodes last 20 seconds. The second environment, Stairway to Melon, is a thin square corridor; in one direction, there is a lemon followed by a stairway to a melon (10 points, resets the level) and in the other direction are 7 apples and a dead end, with the melon visible but not reachable. The agent spawns between the lemon and the apples with a random orientation. Both environments have been released in DeepMind Lab (Beattie et al., 2016). These environments do not require navigation skills such as shortest path planning, but a simple reward identification (lemon vs. apple or melon) and persistent exploration. As Figure 10 shows, there is no major difference between auxiliary tasks related to depth prediction or reward prediction. Depth prediction boosts the performance of the agent beyond that of the stacked LSTM architecture, hinting at a more general applicability of depth prediction beyond navigation tasks.

C.4 SENSITIVITY TOWARDS HYPER-PARAMETER SAMPLING
For each of the experiments in this paper, 64 replicas were run with hyperparameters (learning rate, entropy cost) sampled from the same interval. Figure 11 shows that the Nav architectures with
auxiliary tasks achieve higher results for a comparatively larger number of replicas, hinting at the fact that auxiliary tasks make learning more robust to the choice of hyperparameters.

C.5 ASYMPTOTIC PERFORMANCE OF THE AGENTS
Finally, we compared the asymptotic performance of the agents, both in terms of navigation (final rewards obtained at the end of the episode) and in terms of their representation in the policy LSTM. Rather than visualising the convolutional filters, we quantify the change in representation, with and
without auxiliary task, in terms of position decoding, following the approach explained in Section 5.1. Specifically, we compare the baseline agent (LSTM A3C*) to a navigation agent with one auxiliary task (depth prediction), that gets about twice as many gradient updates for the same number of frames seen in the environment: once for the RL task and once for the auxiliary depth prediction task. As Table 3 shows, the performance of the baseline agent as well as the position decoding accuracy do significantly increase after twice the number of training steps (going from 57 points to 90 points, and from 33.4% to 66.5%, but do not reach the performance and position decoding accuracy of the Nav A3C+D2 agent after half the number of training frames. For this reason, we believe that the auxiliary task do more than simply accelerate training.
","Learning to navigate in complex environments with dynamic elements is an important milestone in developing AI agents. In this work we formulate the navigation question as a reinforcement learning problem and show that data efficiency and task performance can be dramatically improved by relying on additional auxiliary tasks leveraging multimodal sensory inputs. In particular we consider jointly learning the goal-driven reinforcement learning problem with auxiliary depth prediction and loop closure classification tasks. This approach can learn to navigate from raw sensory input in complicated 3D mazes, approaching human-level performance even under conditions where the goal location changes frequently. We provide detailed analysis of the agent behaviour1, its ability to localise, and its network activity dynamics, showing that the agent implicitly learns key navigation abilities.",ICLR 2017 conference submission,True,,"This paper shows that a deep RL approach augmented with auxiliary tasks improves performance on navigation in complex environments. Specifically, A3C is used for the RL problem, and the agent is simultaneously trained on an unsupervised depth prediction task and a self-supervised loop closure classification task. While the use of auxiliary tasks to improve training of models including RL agents is not new, the main contribution here is the use of tasks that encourage learning an intrinsic representation of space and movement that enables significant improvements on maze navigation tasks.

The paper is well written, experiments are convincing, and the value of the auxiliary tasks for the problem are clear. However, the contribution is relatively incremental given previous work on RL for navigation and on auxiliary tasks. The work could become of greater interest provided broader analysis and insights on either optimal combinations of tasks for visual navigation (e.g. the value of other visual / geometry-based tasks), or on auxiliary tasks with RL in general.  As it is, it is a useful demonstration of the benefit of geometry-based auxiliary tasks for navigation, but of relatively narrow interest.

---

The paper proposes an approach to navigating in complex environments using RL agents that have auxiliary tasks besides just the successful navigation itself (for instance, the task of predicting depth from images). The idea is a nice one, and the demonstration is fairly compelling. The one aspect that seems a bit unsatisfying is that the the approach does seem a bit ad-hoc, and could be made more formal, but presenting these results on a challenging task like this navigation problem is certainly sufficient for the paper to be worth accepting. The pros and cons are as follows:
 
 
 Pros:
 + Idea of formulating auxiliary tasks is a nice one and the precise form in which it is done here appears novel
 + Good results on a challenge task of maze navigation from visual data
 
 Cons:
 - Methodology does seem a bit ad-hoc, it would be nice to see if some of the auxiliary task mechanisms could be formalized beyond simple ""this is what worked for this domain""

---

We have addressed the points/suggestions raised through several additional experiments which have been added to the paper in the latest revision.

1) Exploring the optimal combinations of auxiliary tasks: we now include an additional auxiliary task, reward prediction (shown to be effective across a range of tasks in Jaderberg et al. 2016, ICLR submission). Results show that depth prediction is superior to reward prediction in the navigational settings examined, with the combination of the two being no more effective (section 5.3, table 2).

2) The focus of our paper on navigation and the depth prediction auxiliary task: we now present results showing that auxiliary depth prediction is beneficial in scenarios that have minimal navigational demands, suggesting its general effectiveness (appendix C.3; figure 10).

3) Whether auxiliary tasks increase robustness to hyperparameters: we provide a more systematic analysis to show that this does seem to be the case (appendix C.4; figure 11).

4) Whether auxiliary tasks simply accelerate training: we provide evidence that they do more that this through analyses of asymptotic performance (appendix C.5; table 3), effects on the representations learnt (i.e. indexed by position decoding), demonstration that depth prediction shows benefits compared to the reward prediction auxiliary task in the navigation domain.

---

This paper shows that a deep RL approach augmented with auxiliary tasks improves performance on navigation in complex environments. Specifically, A3C is used for the RL problem, and the agent is simultaneously trained on an unsupervised depth prediction task and a self-supervised loop closure classification task. While the use of auxiliary tasks to improve training of models including RL agents is not new, the main contribution here is the use of tasks that encourage learning an intrinsic representation of space and movement that enables significant improvements on maze navigation tasks.

The paper is well written, experiments are convincing, and the value of the auxiliary tasks for the problem are clear. However, the contribution is relatively incremental given previous work on RL for navigation and on auxiliary tasks. The work could become of greater interest provided broader analysis and insights on either optimal combinations of tasks for visual navigation (e.g. the value of other visual / geometry-based tasks), or on auxiliary tasks with RL in general.  As it is, it is a useful demonstration of the benefit of geometry-based auxiliary tasks for navigation, but of relatively narrow interest.

---

I do like the demonstration that including learning of auxiliary tasks does not interfere with the RL tasks but even helps. This is also not so surprising with deep networks. The deep structure of the model allows the model to learn first a good representation of the world on which it can base its solutions for specific goals. While even early representations do of course depend on the task performance itself, it is clear that there are common first stages in sensory representations like the need for edge detection etc. Thus, training by additional tasks will at least increase the effective training size. It is of course unclear how to adjust for this to make a fair comparison, but the paper could have included some more insights such as the change in representation with and without auxiliary training. 

I still strongly disagree with the implied definition of supervised or even self-supervised learning. The definition of unsupervised is learning without external labels. It does not matter if this comes from a human or for example from an expensive machine that is used to train a network so that a task can be solved later without this expensive machine. I would call EM a self-supervised method where labels are predicted from the model itself and used to bootstrap parameter learning. In this case you are using externally supplied labels, which is clearly a supervised learning task!

---

This relatively novel work proposes to augment current RL models by adding self-supervised tasks encouraging better internal representations. 
The proposed tasks are depth prediction and loop closure detection. While these tasks assume a 3D environment as well some position information, such priors are well suited to a large variety of tasks pertaining to navigation and robotics.

Extensive experiments suggest to incorporating such auxiliary tasks increase performance and to a large extent learning speed.
Additional analysis of value functions and internal representations suggest that some structure is being discovered by the model, which would not be without the auxiliary tasks.


While specific to 3D-environment tasks, this work provides additional proof that using input data in addition to sparse external reward signals helps to boost learning speed as well as learning better internal representation. It is original, clearly presented, and strongly supported by empirical evidence.

One small downside of the experimental method (or maybe just the results shown) is that by picking top-5 runs, it is hard to judge whether such a model is better suited to the particular hyperparameter range that was chosen, or is simply more robust to these hyperparameter settings. Maybe an analysis of performance as a function of hyperparameters would help confirm the superiority of the approach to the baselines. My own suspicion is that adding auxiliary tasks would make the model robust to bad hyperparameters.

Another downside is that the authors dismiss navigation literature as ""not RL"". I sympathize with the limit on the number of things that can fit in a paper, but some experimental comparison with such literature may have proven insightful, if just in measuring the quality of the learned representations.

---

We have just submitted an updated version of the paper, including additional references as well as new results on those agents that are enhanced with auxiliary tasks.
Specifically, we investigated:
1) a new way of performing depth prediction, by formulating it as a classification task (over the quantized depth image) and compared it to depth regression.
2) the use of depth prediction as an auxiliary task for the policy LSTM, instead of the convnet.
3) the effect, during actor critic training, of reward clipping on the performance of the agent as well as on the stability of RL learning.

---

I like the approach (and more thorough insights) into making the RL problem easier by providing additional, related SL tasks to learn; however I think that ""Recurrent Reinforcement Learning: A Hybrid Approach"" should be cited as previous work in this regard (on top of Lample & Chaplot).

---

This paper shows that a deep RL approach augmented with auxiliary tasks improves performance on navigation in complex environments. Specifically, A3C is used for the RL problem, and the agent is simultaneously trained on an unsupervised depth prediction task and a self-supervised loop closure classification task. While the use of auxiliary tasks to improve training of models including RL agents is not new, the main contribution here is the use of tasks that encourage learning an intrinsic representation of space and movement that enables significant improvements on maze navigation tasks.

The paper is well written, experiments are convincing, and the value of the auxiliary tasks for the problem are clear. However, the contribution is relatively incremental given previous work on RL for navigation and on auxiliary tasks. The work could become of greater interest provided broader analysis and insights on either optimal combinations of tasks for visual navigation (e.g. the value of other visual / geometry-based tasks), or on auxiliary tasks with RL in general.  As it is, it is a useful demonstration of the benefit of geometry-based auxiliary tasks for navigation, but of relatively narrow interest.

---

The paper proposes an approach to navigating in complex environments using RL agents that have auxiliary tasks besides just the successful navigation itself (for instance, the task of predicting depth from images). The idea is a nice one, and the demonstration is fairly compelling. The one aspect that seems a bit unsatisfying is that the the approach does seem a bit ad-hoc, and could be made more formal, but presenting these results on a challenging task like this navigation problem is certainly sufficient for the paper to be worth accepting. The pros and cons are as follows:
 
 
 Pros:
 + Idea of formulating auxiliary tasks is a nice one and the precise form in which it is done here appears novel
 + Good results on a challenge task of maze navigation from visual data
 
 Cons:
 - Methodology does seem a bit ad-hoc, it would be nice to see if some of the auxiliary task mechanisms could be formalized beyond simple ""this is what worked for this domain""

---

We have addressed the points/suggestions raised through several additional experiments which have been added to the paper in the latest revision.

1) Exploring the optimal combinations of auxiliary tasks: we now include an additional auxiliary task, reward prediction (shown to be effective across a range of tasks in Jaderberg et al. 2016, ICLR submission). Results show that depth prediction is superior to reward prediction in the navigational settings examined, with the combination of the two being no more effective (section 5.3, table 2).

2) The focus of our paper on navigation and the depth prediction auxiliary task: we now present results showing that auxiliary depth prediction is beneficial in scenarios that have minimal navigational demands, suggesting its general effectiveness (appendix C.3; figure 10).

3) Whether auxiliary tasks increase robustness to hyperparameters: we provide a more systematic analysis to show that this does seem to be the case (appendix C.4; figure 11).

4) Whether auxiliary tasks simply accelerate training: we provide evidence that they do more that this through analyses of asymptotic performance (appendix C.5; table 3), effects on the representations learnt (i.e. indexed by position decoding), demonstration that depth prediction shows benefits compared to the reward prediction auxiliary task in the navigation domain.

---

This paper shows that a deep RL approach augmented with auxiliary tasks improves performance on navigation in complex environments. Specifically, A3C is used for the RL problem, and the agent is simultaneously trained on an unsupervised depth prediction task and a self-supervised loop closure classification task. While the use of auxiliary tasks to improve training of models including RL agents is not new, the main contribution here is the use of tasks that encourage learning an intrinsic representation of space and movement that enables significant improvements on maze navigation tasks.

The paper is well written, experiments are convincing, and the value of the auxiliary tasks for the problem are clear. However, the contribution is relatively incremental given previous work on RL for navigation and on auxiliary tasks. The work could become of greater interest provided broader analysis and insights on either optimal combinations of tasks for visual navigation (e.g. the value of other visual / geometry-based tasks), or on auxiliary tasks with RL in general.  As it is, it is a useful demonstration of the benefit of geometry-based auxiliary tasks for navigation, but of relatively narrow interest.

---

I do like the demonstration that including learning of auxiliary tasks does not interfere with the RL tasks but even helps. This is also not so surprising with deep networks. The deep structure of the model allows the model to learn first a good representation of the world on which it can base its solutions for specific goals. While even early representations do of course depend on the task performance itself, it is clear that there are common first stages in sensory representations like the need for edge detection etc. Thus, training by additional tasks will at least increase the effective training size. It is of course unclear how to adjust for this to make a fair comparison, but the paper could have included some more insights such as the change in representation with and without auxiliary training. 

I still strongly disagree with the implied definition of supervised or even self-supervised learning. The definition of unsupervised is learning without external labels. It does not matter if this comes from a human or for example from an expensive machine that is used to train a network so that a task can be solved later without this expensive machine. I would call EM a self-supervised method where labels are predicted from the model itself and used to bootstrap parameter learning. In this case you are using externally supplied labels, which is clearly a supervised learning task!

---

This relatively novel work proposes to augment current RL models by adding self-supervised tasks encouraging better internal representations. 
The proposed tasks are depth prediction and loop closure detection. While these tasks assume a 3D environment as well some position information, such priors are well suited to a large variety of tasks pertaining to navigation and robotics.

Extensive experiments suggest to incorporating such auxiliary tasks increase performance and to a large extent learning speed.
Additional analysis of value functions and internal representations suggest that some structure is being discovered by the model, which would not be without the auxiliary tasks.


While specific to 3D-environment tasks, this work provides additional proof that using input data in addition to sparse external reward signals helps to boost learning speed as well as learning better internal representation. It is original, clearly presented, and strongly supported by empirical evidence.

One small downside of the experimental method (or maybe just the results shown) is that by picking top-5 runs, it is hard to judge whether such a model is better suited to the particular hyperparameter range that was chosen, or is simply more robust to these hyperparameter settings. Maybe an analysis of performance as a function of hyperparameters would help confirm the superiority of the approach to the baselines. My own suspicion is that adding auxiliary tasks would make the model robust to bad hyperparameters.

Another downside is that the authors dismiss navigation literature as ""not RL"". I sympathize with the limit on the number of things that can fit in a paper, but some experimental comparison with such literature may have proven insightful, if just in measuring the quality of the learned representations.

---

We have just submitted an updated version of the paper, including additional references as well as new results on those agents that are enhanced with auxiliary tasks.
Specifically, we investigated:
1) a new way of performing depth prediction, by formulating it as a classification task (over the quantized depth image) and compared it to depth regression.
2) the use of depth prediction as an auxiliary task for the policy LSTM, instead of the convnet.
3) the effect, during actor critic training, of reward clipping on the performance of the agent as well as on the stability of RL learning.

---

I like the approach (and more thorough insights) into making the RL problem easier by providing additional, related SL tasks to learn; however I think that ""Recurrent Reinforcement Learning: A Hybrid Approach"" should be cited as previous work in this regard (on top of Lample & Chaplot).",,,,,,6.333333333333333,,,4.0,,
444,"Authors: W. James Murdoch
Source file: 444.pdf

ABSTRACT
Although deep learning models have proven effective at solving problems in natural language processing, the mechanism by which they come to their conclusions is often unclear. As a result, these models are generally treated as black boxes, yielding no insight of the underlying learned patterns. In this paper we consider Long Short Term Memory networks (LSTMs) and demonstrate a new approach for tracking the importance of a given input to the LSTM for a given output. By identifying consistently important patterns of words, we are able to distill state of the art LSTMs on sentiment analysis and question answering into a set of representative phrases. This representation is then quantitatively validated by using the extracted phrases to construct a simple, rule-based classifier which approximates the output of the LSTM.

1 INTRODUCTION
Neural network language models, especially recurrent neural networks (RNN), are now standard tools for natural language processing. Amongst other things, they are used for translation Sutskever et al. (2014), language modelling Jozefowicz et al. (2016), and question answering Hewlett et al. (2016). In particular, the Long Short Term Memory (LSTM) Hochreiter & Schmidhuber (1997) architecture has become a basic building block of neural NLP. Although LSTM’s are regularly used in state of the art systems, their operation is not well understood. Besides the basic desire from a scientific viewpoint to clarify their workings, it is often the case that it is important to understand why a machine learning algorithm made a particular choice. Moreover, LSTM’s are computationally intensive compared to discrete models with lookup tables and pattern matching.
In this work, we describe a novel method for visualizing the importance of specific inputs for determining the output of an LSTM. We then demonstrate that, by searching for phrases which are consistently important, the importance scores can be used to extract simple phrase patterns consisting of one to five words from a trained LSTM. The phrase extraction is first done in a general document classification framework on two different sentiment analysis datasets. We then demonstrate that it can also be specialized to more complex models by applying it to WikiMovies, a recently introduced question answer dataset. To concretely validate the extracted patterns, we use them as input to a rules-based classifier which approximates the performance of the original LSTM.

2 RELATED WORK
There are two lines of related work on visualizing LSTMs. First, Hendrik et al. (2016) and Karpathy et al. (2016) analyse the movement of the raw gate activations over a sequence. Karpathy et al. (2016) is able to identify co-ordinates of ct that correspond to semantically meaningful attributes such as whether the text is in quotes and how far along the sentence a word is. However, most of the cell co-ordinates are harder to interpret, and in particular, it is often not obvious from their activations which inputs are important for specific outputs.
∗Work started during an internship at Facebook AI Research
Another approach that has emerged in the literature Alikaniotis et al. (2016) Denil et al. (2015) Bansal et al. (2016) is for each word in the document, looking at the norm of the derivative of the loss function with respect to the embedding parameters for that word. This bridges the gap between high-dimensional cell state and low-dimensional outputs. These techniques are general- they are applicable to visualizing the importance of sets of input coordinates to output coordinates of any differentiable function. In this work, we describe techniques that are designed around the structure of LSTM’s, and show that they can give better results in that setting.
A recent line of work Li et al. (2016) Hewlett et al. (2016) Rajpurkar et al. (2016) Miller et al. (2016) has focused on neural network techniques for extracting answers directly from documents. Previous work had focused on Knowledge Bases (KBs), and techniques to map questions to logical forms suitable for querying them. Although they are effective within their domain, KBs are inevitably incomplete, and are thus an unsatisfactory solution to the general problem of question-answering. Wikipedia, in contrast, has enough information to answer a far broader array of questions, but is not as easy to query. Originally introduced in Miller et al. (2016), the WikiMovies dataset consists of questions about movies paired with Wikipedia articles.

3 WORD IMPORTANCE SCORES IN LSTMS
We present a novel decomposition of the output of an LSTM into a product of factors, where each term in the product can be interpreted as the contribution of a particular word. Thus, we can assign importance scores to words according to their contribution to the LSTM’s prediction

3.1 LONG SHORT TERM MEMORY NETWORKS
Over the past few years, LSTMs have become an important part of neural NLP systems. Given a sequence of word embeddings x1, ..., xT ∈ Rd, an LSTM processes one word at a time, keeping track of cell and state vectors (c1, h1), ..., (cT , hT ) which contain information in the sentence up to word i. ht and ct are computed as a function of xt, ct−1 using the below updates
ft = σ(Wfxt + Vfht−1 + bf ) (1) it = σ(Wixt + Viht−1 + bi) (2) ot = σ(Woxt + Voht−1 + bo) (3) c̃t = tanh(Wcxt + Vcht−1 + bc) (4) ct = ftct−1 + itc̃t (5) ht = ot tanh(ct) (6)
As initial values, we define c0 = h0 = 0. After processing the full sequence, a probability distribution over C classes is specified by p, with
pi = SoftMax(WhT ) = eWihT∑C j=1 e Wjht (7)
where Wi is the i’th row of the matrix W

3.2 DECOMPOSING THE OUTPUT OF A LSTM
We now show that we can decompose the numerator of pi in Equation 7 into a product of factors, and interpret those factors as the contribution of individual words to the predicted probability of class i. Define βi,j = exp (Wi(oT (tanh(cj)− tanh(cj−1))) , (8) so that
exp(WihT ) = exp  T∑ j=1 Wi(oT (tanh(cj)− tanh(cj−1))  = T∏ j=1 βi,j .
As tanh(cj)− tanh(cj−1) can be viewed as the update resulting from word j, so βi,j can be interpreted as the multiplicative contribution to pi by word j.

3.3 AN ADDITIVE DECOMPOSITION OF THE LSTM CELL
We will show below that the βi,j capture some notion of the importance of a word to the LSTM’s output. However, these terms fail to account for how the information contributed by word j is affected by the LSTM’s forget gates between words j and T . Consequently, we empirically found that the importance scores from this approach often yield a considerable amount of false positives. A more nuanced approach is obtained by considering the additive decomposition of cT in equation (9), where each term ej can be interpreted as the contribution to the cell state cT by word j. By iterating the equation ct = ftct−1 + itc̃t, we get that
cT = T∑ i=1 ( T∏ j=i+1 fj)iic̃i = T∑ i=1 ei,T (9)
This suggests a natural definition of an alternative score to the βi,j , corresponding to augmenting the cj terms with products of forget gates to reflect the upstream changes made to cj after initially processing word j.
exp(WihT ) = T∏ j=1 exp
( Wi(oT (tanh(
j∑ k=1 ek,T )− tanh( j−1∑ k=1 ek,T )))
) (10)
= T∏ j=1 exp Wi(oT (tanh(( t∏ k=j+1 fk)cj)− tanh(( t∏ k=j fk)cj−1)))  (11) =
T∏ j=1 γi,j (12)

4 PHRASE EXTRACTION FOR DOCUMENT CLASSIFICATION
We now introduce a technique for using our variable importance scores to extract phrases from a trained LSTM. To do so, we search for phrases which consistently provide a large contribution to the prediction of a particular class relative to other classes. The utility of these patterns is validated by using them as input for a rules based classifier. For simplicity, we focus on the binary classification case.

4.1 PHRASE EXTRACTION
A phrase can be reasonably described as predictive if, whenever it occurs, it causes a document to both be labelled as a particular class, and not be labelled as any other. As our importance scores introduced above correspond to the contribution of particular words to class predictions, they can be used to score potential patterns by looking at a pattern’s average contribution to the prediction of a given class relative to other classes. More precisely, given a collection of D documents {{xi,j}Ndi=1}Dj=1, for a given phrase w1, ..., wk we can compute scores S1, S2 for classes 1 and 2, as well as a combined score S and class C as
S1(w1, ..., wk) = Averagej,b
{∏k l=1 β1,b+l,j |xb+i,j = wi, i = 1, ..., k } Averagej,b {∏k l=1 β2,b+l,j |xb+i,j = wi, i = 1, ..., k
} (13) S2(w1, .., wk) = 1
S1(w1, ..., wk) (14)
S(w1, ..., wk) = max i (Si(w1, ..., wk)) (15)
C(w1, ..., wk) = argmaxi(Si(w1, ..., wk)) (16)
where βi,j,k denotes βi,j applied to document k.
The numerator of S1 denotes the average contribution of the phrase to the prediction of class 1 across all occurrences of the phrase. The denominator denotes the same statistic, but for class 2. Thus, if
S1 is high, then w1, ..., wk is a strong signal for class 1, and likewise for S2. We propose to use S as a score function in order to search for high scoring, representative, phrases which provide insight into the trained LSTM, and C to denote the class corresponding to a phrase.
In practice, the number of phrases is too large to feasibly compute the score of them all. Thus, we approximate a brute force search through a two step procedure. First, we construct a list of candidate phrases by searching for strings of consecutive words j with importance scores βi,j > c for any i and some threshold c; in the experiments below we use c = 1.1. Then, we score and rank the set of candidate phrases, which is much smaller than the set of all phrases.

4.2 RULES BASED CLASSIFIER
The extracted patterns from Section 4.1 can be used to construct a simple, rules-based classifier which approximates the output of the original LSTM. Given a document and a list of patterns sorted by descending score given by S, the classifier sequentially searches for each pattern within the document using simple string matching. Once it finds a pattern, the classifier returns the associated class given by C, ignoring the lower ranked patterns. The resulting classifier is interpretable, and despite its simplicity, retains much of the accuracy of the LSTM used to build it.

5 EXPERIMENTS
We now present the results of our experiments.

5.1 TRAINING DETAILS
We implemented all models in Torch using default hyperparameters for weight initializations. For WikiMovies, all documents and questions were pre-processed so that multiple word entities were concatenated into a single word. For a given question, relevant articles were found by first extracting from the question the rarest entity, then returning a list of Wikipedia articles containing any of those words. We use the pre-defined splits into train, validation and test sets, containing 96k, 10k and 10k questions, respectively. The word and hidden representations of the LSTM were both set to dimension 200 for WikiMovies, 300 and 512 for Yelp, and 300 and 150 for Stanford Sentiment Treebank. All models were optimized using Adam Kingma & Ba (2015) with the default learning rate of 0.001 using early stopping on the validation set. For rule extraction using gradient scores, the product in the reward function is replaced by a sum. In both datasets, we found that normalizing the gradient scores by the largest gradient improved results.

5.2 SENTIMENT ANALYSIS
We first applied the document classification framework to two different sentiment analysis datasets. Originally introduced in Zhang et al. (2015), the Yelp review polarity dataset was obtained from the Yelp Dataset Challenge and has train and test sets of size 560,000 and 38,000. The task is binary prediction for whether the review is positive (four or five stars) or negative (one or two stars). The reviews are relatively long, with an average length of 160.1 words. We also used the binary classification task from the Stanford Sentiment Treebank (SST) Socher et al. (2013), which has less data with train/dev/test sizes of 6920/872/1821, and is done at a sentence level, so has much shorter document lengths.
We report results in Table 1 for seven different models. We report state of the art results from prior work using convolutional neural networks; Kim (2014) for SST and Zhang et al. (2015) for Yelp. We also report our LSTM baselines, which are competitive with state of the art, along with the three different pattern matching models described above. For SST, we also report prior results using bag of words features with Naive Bayes.
The additive cell decomposition pattern equals or outperforms the cell-difference patterns, which handily beat the gradient results. This coincides with our empirical observations regarding the information contained within the importance measures, and validates our introduced measure. The differences between measures become more pronounced in Yelp, as the longer document sizes provide more opportunities for false positives.
Although our pattern matching algorithms underperform other methods, we emphasize that pure performance is not our goal, nor would we expect more from such a simple model. Rather, the fact that our method provides reasonable accuracy is one piece of evidence, in addition to the qualitative evidence given later, that our word importance scores and extracted patterns contain useful information for understanding the actions of a LSTM.

5.3 WIKIMOVIES
Although document classification comprises a sizeable portion of current research in natural language processing, much recent work focuses on more complex problems and models. In this section, we examine WikiMovies, a recently introduced question answer dataset, and show that with some simple modifications our approach can be adapted to this problem.

5.3.1 DATASET
WikiMovies is a dataset consisting of more than 100,000 questions about movies, paired with relevant Wikipedia articles. It was constructed using the pre-existing dataset MovieLens, paired with templates extracted from the SimpleQuestions dataset Bordes et al. (2015), a open-domain question answering dataset based on Freebase. They then selected a set of Wikipedia articles about movies by identifying a set of movies from OMDb that had an associated article by title match, and kept the title and first section for each article.
For a given question, the task is to read through the relevant articles and extract the answer, which is contained somewhere within the text. The dataset also provides a list of 43k entities containing all possible answers.

5.3.2 LSTMS FOR WIKIMOVIES
We propose a simplified version of recent work Li et al. (2016). Given a pair of question xq1, ..., x q N and document xd1, ..., x d T , we first compute an embedding for the question using a LSTM. Then, for each word t in the document, we augment the word embedding xt with the computed question embedding. This is equivalent to adding an additional term which is linear in the question embedding into the gate equations 3-6, allowing the patterns an LSTM absorbs to be directly conditioned upon the question at hand.
hqt = LSTM(x q t ) (17)
ht = LSTM(xdt ‖h q N ) (18)
Having run the above model over the document while conditioning on a question, we are given contextual representations h1, ..., hT of the words in the document. For each entity t in the document
we use pt to conduct a binary prediction for whether or not the entity is the answer. At test time, we return the entity with the highest probability as the answer.
pt = SoftMax(Wht) (19)

5.3.3 PHRASE EXTRACTION
We now introduce some simple modifications that were useful in adapting our pattern extraction framework to this specific task. First, in order to define the set of classifications problems to search over, we treat each entity t within each document as a separate binary classification task with corresponding predictor pt. Given this set of classification problems, rather than search over the space of all possible phrases, we restrict ourselves to those ending at the entity in question. We also distinguish patterns starting at the beginning of the document with those that do not and introduce an entity character into our pattern vocabulary, which can be matched by any entity. Template examples can be seen below, in Table 4. Once we have extracted a list of patterns, in the rules-based classifier we only search for positive examples, and return as the answer the entity matched to the highest ranked positive pattern.

5.3.4 RESULTS
We report results on six different models in Tables 2 and 3. We show the results from Miller et al. (2016), which fit a key-value memory network (KV-MemNN) on representations from information extraction (IE) and raw text (Doc). Next, we report the results of the LSTM described in Section 5.3.2. Finally, we show the results of using three variants of the pattern matching algorithm described in Section 5.3.3: using patterns extracted using the additive decomposition (cell decomposition), difference in cells approaches (cell-difference) and gradient importance scores (gradient), as discussed in Section 2. Performance is reported using the accuracy of the top hit over all possible answers (all entities), i.e. the hits@1 metric.
As shown in Table 2, our LSTM model surpasses the prior state of the art by nearly 4%. Moreover, our automatic pattern matching model approximates the LSTM with less than 6% error, which is surprisingly small for such a simple model, and falls within 2% of the prior state of the art. Similarly to sentiment analysis, we observe a clear ordering of the results across question categories, with our cell decomposition scores providing the best performance, followed by the cell difference and gradient scores.

6 DISCUSSION

6.1 LEARNED PATTERNS
We present extracted patterns for both sentiment tasks, and some WikiMovies question categories in Table 4. These patterns are qualitatively sensible, providing further validation of our approach. The increased size of the Yelp dataset allowed for longer phrases to be extracted relative to SST.
Category Top Patterns Yelp Polarity Positive definitely come back again., love love love this
place, great food and great service., highly recommended!, will definitely be coming back, overall great experience, love everything about, hidden gem.
Yelp Polarity Negative worst customer service ever, horrible horrible horrible, won’t be back, disappointed in this place, never go back there, not worth the money, not recommend this place SST Positive riveting documentary, is a real charmer, funny and touching, well worth your time, journey of the heart, emotional wallop, pleasure to watch, the whole family, cast is uniformly superb, comes from the heart, best films of the year, surprisingly funny, deeply satisfying SST Negative pretentious mess ..., plain bad, worst film of the year, disappointingly generic, fart jokes, banal dialogue, poorly executed, waste of time, a weak script, dullard, how bad it is, platitudes, never catches fire, tries too hard to be, bad acting, untalented artistes, derivative horror film, lackluster WikiMovies movie to writer film adaptation of Charles Dickens’, film adapted from ENT, by journalist ENT, written by ENT WikiMovies movie to actor western film starring ENT, starring Ben Affleck, . The movie stars ENT, that stars ENT WikiMovies movie to language is a 2014 french, icelandic, finnish, russian, danish, bengali, dutch, original german, zulu,czech, estonian, mandarin, filipino, hungarian
Table 4: Selected top patterns using cell decomposition scores, ENT denotes an entity placeholder

6.2 APPROXIMATION ERROR BETWEEN LSTM AND PATTERN MATCHING
Although our approach is able to extract sensible patterns and achieve reasonable performance, there is still an approximation gap between our algorithm and the LSTM. In Table 5 we present some examples of instances where the LSTM was able to correctly classify a sentence, and our algorithm was not, along with the pattern used by our algorithm. At first glance, the extracted patterns are sensible, as ”gets the job done” or ”witty dialogue” are phrases you’d expect to see in a positive review of a movie. However, when placed in the broader context of these particular reviews, they cease to be predictive. This demonstrates that, although our work is useful as a firstorder approximation, there are still additional relationships that an LSTM is able to learn from data.

6.3 COMPARISON BETWEEN WORD IMPORTANCE MEASURES
While the prediction accuracy of our rules-based classifier provides quantitative validation of the relative merits of our visualizations, the qualitative differences are also insightful. In Table 6, we provide a side-by-side comparison between the different measures. As discussed before, the difference in cells technique fails to account for how the updates resulting from word j are affected by the LSTM’s forget gates between when the word is initially processed and the answer. Consequently, we empirically found that without the interluding forget gates to dampen cell movements, the variable importance scores were far noisier than in additive cell decomposition approach. Under the additive cell decomposition, it identifies the phrase ’it stars’, as well as the actor’s name Aqib Khan as being important, a sensible conclusion. Moreover, the vast majority of words are labelled with an importance score of 1, corresponding to irrelevant. On the other hand, the difference in cells approach yields widely changing importance scores, which are challenging to interpret. In terms of noise, the gradient measures seem to lie somewhere in the middle. These patterns are broadly consistent with what we have observed, and provide qualitative validation of our metrics.

7 CONCLUSION
In this paper, we introduced a novel method for visualizing the importance of specific inputs in determining the output of an LSTM. By searching for phrases which consistently provide large contributions, we are able to distill trained, state of the art, LSTMs into an ordered set of representative phrases. We quantitatively validate the extracted phrases through their performance in a simple, rules-based classifier. Results are shown in a general document classification framework, then specialized to a more complex, recently introduced, question answer dataset. Our introduced measures provide superior predictive ability and cleaner visualizations relative to prior work. We believe that this represents an exciting new paradigm for analysing the behaviour of LSTM’s.
Additive cell decomposition Difference in cell values Gradient
west is west is a 2010 british comedy - drama film , which is a sequel to the 1999 comedy ”
east is east ” . it stars aqib khan
west is west is a 2010 british comedy -
drama film , which is a sequel to the 1999 comedy ” east is east ” . it starsaqib khan
west is west is a 2010 british comedy - drama film , which is a sequel to the 1999 comedy ” east is east ”. itstars aqib khan
Table 6: Comparison of importance scores acquired by three different approaches, conditioning on the question ”the film west is west starred which actors?”. Bigger and darker means more important.

ACKNOWLEDGEMENTS
This research was partially funded by Air Force grant FA9550-14-1-0016. It was also supported by the Center for Science of Information (CSoI), an US NSF Science and Technology Center, under grant agreement CCF-0939370.

8 APPENDIX - HEAT MAPS
We provide an example heat map using the cell decomposition metric for each class in both sentiment analysis datasets, and selected WikiMovie question categories
Dataset Category Heat Map
Yelp Polarity Positive we went here twice for breakfast . had the bananas foster waffles with fresh whipped cream , they were amazing ! ! perfect seat out side on the terrace
Yelp Polarity Negative call me spoiled ...this sushi is gross and the orange chicken , well it was so thin i don ’t think it had chicken in it. gosomewhereelse
Stanford Sentiment Positive Whether or not you ’re enlightened by any of Derrida ’s lectures on “ the other ” and “ the self ,
” Derrida is an undeniablyfascinatingandplayfulfellow
Stanford Sentiment Negative ... begins with promise , but runs aground after being snared in its own tangled plot
Pattern Question Heat Map
Movie to Year What was the release year of another 48 hours?
another 48 hrs is a 1990
Movie to Writer Which person wrote the movie last of the dogmen? last of the dogmen is a 1995 western adventure film written and directed by tab murphy
Movie to Actor Who acted in the movie thunderbolt? thunderbolt ( ) ( ” piklik foh ” ) is a 1995 hong kong action filmstarring jackie chan
Movie to Director Who directed bloody bloody bible camp? bloody bloody bible cam p is a 2012 american horror - comedy /s platter film . the film
was directed by vito trabucco
Movie to Genre What genre is trespass in? trespassisa 1992 action Movie to Votes How would people rate the pool? though filmed in hindi , a language smith didn ’t know , the film earned
good∗ Movie to Rating How popular was les miserables? les mis rables is a 1935 american drama film starring fredric march and charles laughton
based upon thefamous Movie to Tags Describe rough magic?
rough magic is a 1995 comedy film directed by clare peploe and starring bridget fonda , russell crowe
Movie to Language What is the main language in fate?
fate ( ) is a 2001 turkish
","Although deep learning models have proven effective at solving problems in natural language processing, the mechanism by which they come to their conclusions is often unclear. As a result, these models are generally treated as black boxes, yielding no insight of the underlying learned patterns. In this paper we consider Long Short Term Memory networks (LSTMs) and demonstrate a new approach for tracking the importance of a given input to the LSTM for a given output. By identifying consistently important patterns of words, we are able to distill state of the art LSTMs on sentiment analysis and question answering into a set of representative phrases. This representation is then quantitatively validated by using the extracted phrases to construct a simple, rule-based classifier which approximates the output of the LSTM.",ICLR 2017 conference submission,True,,"This paper proposes a novel method for extracting rule-based classifiers from trained LSTM models. The proposed method is applied to a factoid question-answering task, where it is demonstrated that the extracted rules perform comparatively to the original LSTM. The analysis of the extracted rules illustrate the features the LSTM model picks up on.

Analyzing and visualizing the computations carried out by RNNs in order to understand the functions they compute is an important direction of research. This sort of analysis will help us understand the pitfalls of RNNs, and how we can improve them. Although the approach taken is relatively inflexible - each rule is defined as an ordered sequence of words - the authors experiment with three different scores for picking salient words (state-difference, cell-difference and gradient) and their approach yields comparable performance, which suggests that the extracted rules mimic the RNN closely. The results are also somewhat surprising, since most of the rules consist only of two or three words.

It would have been interesting to try extend the approach on other natural language processing tasks, such as machine translation. Presumably the rules learned here will be quite different.

Other comments:
- Eq. (12) is over-parametrized with two vectors $P$ and $Q$. The same function can be computed with a single vector. This becomes clear when you divide both the numerator and denominator by $e^{P h_t}$.
- Section 4.1. Is it correct that this section is focused on the forward LSTM? If so, please clarify it in the text.
- In Eq. (13), define $c_0 = 0$.
- Eq. (13) is exactly the same as Eq. (15). Is there a mistake?
- In Table 1, third column should have word ""film"" highlighted.
- ""are shown in 2"" -> ""are shown in Table 2"".
- Since there are some problems representing numbers, it may help to replace each digit with the hashtag symbol #.

---

The equation between (8) - (9) seems to be incorrect, as the left hand side of which should be p_i.

---

Timely topic (interpretability of neural models for NLP), interesting approach, surprising results.

---

In response to helpful comments from reviewers, we have just uploaded a revision. The main changes are as follows

- In response to requests for extensions to other datasets, we now have results on 2 different binary sentiment analysis datasets - Stanford Sentiment Treebank and Yelp reviews

- We introduced a simpler, more general approach for extracting rules from LSTMs trained on document classification, and demonstrate that with some easy modifications it can replace our prior, more complex, rule extraction mechanism on WikiMovies.

- Our rules now take the form of simple phrases, rather than allowing for variable-sized gaps between words as before

- Our LSTM baseline on WikiMovies is now SOTA by nearly 4%, and the automatically extracted patterns outperform the manual patterns in the earlier version.

- We added a discussion on instances correctly classified by the LSTM, but incorrectly classified by our rules-based algorithm

- For simplicity, we changed our WikiMovies baseline to a unidirectional LSTM, and removed bidirectional LSTMs from the paper. Extension of our new approach to bidirectional LSTMs would be straightforward, but we feel would add unneeded complexity to the presentation

All told, we feel that these algorithms and results are simpler, more powerful and more general than our prior work, and we look forward to discussing them.

---

EDIT: the revisions made to this paper are very thorough and address many of my concerns, and the paper is also easier to understand. i recommend the latest version of this paper for acceptance and have increased my score.

This paper presents a way of interpreting LSTM models, which are notable for their opaqueness. In particular, the authors propose decomposing the LSTM's predictions for a QA task into importance scores for words, which are then used to generate patterns that are used to find answers with a simple matching algorithm. On the WikiMovies dataset, the extracted pattern matching method achieves accuracies competitive with a normal LSTM, which shows the power of the proposed approach. 

I really like the motivation of the paper, as interpreting LSTMs is definitely still a work-in-progress, and the high performance of the pattern matching was surprising. However, several details of the pattern extraction process are not very clear, and  the evaluation is conducted on a very specific task, where predictions are made at every word. As such, I recommend the paper in its current form as a weak accept but hope that the authors clarify their approach, as I believe the proposed method is potentially useful for NLP researchers.

Comments:
- Please introduce in more detail the specific QA tasks you are applying your models on before section 3.3, as it's not clear at that point that the answer is an entity within the document.
- 3.3: is the softmax predicting a 0/1 value (e.g., is this word the answer or not?)
- 3.3: what are the P and Q vectors? do you just mean that you are transforming the hidden state into a 2-dimensional vector for binary prediction?
- how does performance of the pattern matching change with different cutoff constant values?
- 5.2: are there questions whose answers are not entities? 
- how could the proposed approach be used when predictions aren't made at every word? is there any extension for, say, sentence-level sentiment classification?

---

This work proposes a pattern extraction method to both understand what a trained LSTM has learnt and to allow implementation of a hand-coded algorithm that performs similarly to the LSTM. Good results are shown on one dataset for one model architecture so it is unclear how well this approach will generalize, however, it seems it will be a useful way to understand and debug models.

The questions in WikiMovies seem to be generated from templates and so this pattern matching approach will likely work well. However, from the experiments it's not clear if this will extend to other types of Q&A tasks where the answer may be free form text and not be a substring in the document. Is the model required to produce a continuous span over the original document?

The approach also seems to have some deficiencies in how it handles word types such as numbers or entity names. This can be encoded in the embedding for the word but from the description of the algorithm, it seems that the approach requires an entity detector. Does this mean that the approach is unable to determine when it has reached an entity from the decomposition of the output of the LSTM? The results where 'manual pattern matching' where explicit year annotations are used, seem to show that the automatic method is unable to deal with word types.

It would also be good to see an attention model as a baseline in addition to the gradient-based baseline.

Minor comments:
- P and Q seem to be undefined.
- Some references seem to be bad, e.g. in section 5.1: 'in 1' instead of 'in table 1'. Similarly above section 7: 'as shown in 3' and in section 7.1.
- In the paragraph above section 6.3: 'adam' -> 'Adam'.

---

This paper proposes a novel method for extracting rule-based classifiers from trained LSTM models. The proposed method is applied to a factoid question-answering task, where it is demonstrated that the extracted rules perform comparatively to the original LSTM. The analysis of the extracted rules illustrate the features the LSTM model picks up on.

Analyzing and visualizing the computations carried out by RNNs in order to understand the functions they compute is an important direction of research. This sort of analysis will help us understand the pitfalls of RNNs, and how we can improve them. Although the approach taken is relatively inflexible - each rule is defined as an ordered sequence of words - the authors experiment with three different scores for picking salient words (state-difference, cell-difference and gradient) and their approach yields comparable performance, which suggests that the extracted rules mimic the RNN closely. The results are also somewhat surprising, since most of the rules consist only of two or three words.

It would have been interesting to try extend the approach on other natural language processing tasks, such as machine translation. Presumably the rules learned here will be quite different.

Other comments:
- Eq. (12) is over-parametrized with two vectors $P$ and $Q$. The same function can be computed with a single vector. This becomes clear when you divide both the numerator and denominator by $e^{P h_t}$.
- Section 4.1. Is it correct that this section is focused on the forward LSTM? If so, please clarify it in the text.
- In Eq. (13), define $c_0 = 0$.
- Eq. (13) is exactly the same as Eq. (15). Is there a mistake?
- In Table 1, third column should have word ""film"" highlighted.
- ""are shown in 2"" -> ""are shown in Table 2"".
- Since there are some problems representing numbers, it may help to replace each digit with the hashtag symbol #.

---

This paper proposes a novel method for extracting rule-based classifiers from trained LSTM models. The proposed method is applied to a factoid question-answering task, where it is demonstrated that the extracted rules perform comparatively to the original LSTM. The analysis of the extracted rules illustrate the features the LSTM model picks up on.

Analyzing and visualizing the computations carried out by RNNs in order to understand the functions they compute is an important direction of research. This sort of analysis will help us understand the pitfalls of RNNs, and how we can improve them. Although the approach taken is relatively inflexible - each rule is defined as an ordered sequence of words - the authors experiment with three different scores for picking salient words (state-difference, cell-difference and gradient) and their approach yields comparable performance, which suggests that the extracted rules mimic the RNN closely. The results are also somewhat surprising, since most of the rules consist only of two or three words.

It would have been interesting to try extend the approach on other natural language processing tasks, such as machine translation. Presumably the rules learned here will be quite different.

Other comments:
- Eq. (12) is over-parametrized with two vectors $P$ and $Q$. The same function can be computed with a single vector. This becomes clear when you divide both the numerator and denominator by $e^{P h_t}$.
- Section 4.1. Is it correct that this section is focused on the forward LSTM? If so, please clarify it in the text.
- In Eq. (13), define $c_0 = 0$.
- Eq. (13) is exactly the same as Eq. (15). Is there a mistake?
- In Table 1, third column should have word ""film"" highlighted.
- ""are shown in 2"" -> ""are shown in Table 2"".
- Since there are some problems representing numbers, it may help to replace each digit with the hashtag symbol #.

---

The equation between (8) - (9) seems to be incorrect, as the left hand side of which should be p_i.

---

Timely topic (interpretability of neural models for NLP), interesting approach, surprising results.

---

In response to helpful comments from reviewers, we have just uploaded a revision. The main changes are as follows

- In response to requests for extensions to other datasets, we now have results on 2 different binary sentiment analysis datasets - Stanford Sentiment Treebank and Yelp reviews

- We introduced a simpler, more general approach for extracting rules from LSTMs trained on document classification, and demonstrate that with some easy modifications it can replace our prior, more complex, rule extraction mechanism on WikiMovies.

- Our rules now take the form of simple phrases, rather than allowing for variable-sized gaps between words as before

- Our LSTM baseline on WikiMovies is now SOTA by nearly 4%, and the automatically extracted patterns outperform the manual patterns in the earlier version.

- We added a discussion on instances correctly classified by the LSTM, but incorrectly classified by our rules-based algorithm

- For simplicity, we changed our WikiMovies baseline to a unidirectional LSTM, and removed bidirectional LSTMs from the paper. Extension of our new approach to bidirectional LSTMs would be straightforward, but we feel would add unneeded complexity to the presentation

All told, we feel that these algorithms and results are simpler, more powerful and more general than our prior work, and we look forward to discussing them.

---

EDIT: the revisions made to this paper are very thorough and address many of my concerns, and the paper is also easier to understand. i recommend the latest version of this paper for acceptance and have increased my score.

This paper presents a way of interpreting LSTM models, which are notable for their opaqueness. In particular, the authors propose decomposing the LSTM's predictions for a QA task into importance scores for words, which are then used to generate patterns that are used to find answers with a simple matching algorithm. On the WikiMovies dataset, the extracted pattern matching method achieves accuracies competitive with a normal LSTM, which shows the power of the proposed approach. 

I really like the motivation of the paper, as interpreting LSTMs is definitely still a work-in-progress, and the high performance of the pattern matching was surprising. However, several details of the pattern extraction process are not very clear, and  the evaluation is conducted on a very specific task, where predictions are made at every word. As such, I recommend the paper in its current form as a weak accept but hope that the authors clarify their approach, as I believe the proposed method is potentially useful for NLP researchers.

Comments:
- Please introduce in more detail the specific QA tasks you are applying your models on before section 3.3, as it's not clear at that point that the answer is an entity within the document.
- 3.3: is the softmax predicting a 0/1 value (e.g., is this word the answer or not?)
- 3.3: what are the P and Q vectors? do you just mean that you are transforming the hidden state into a 2-dimensional vector for binary prediction?
- how does performance of the pattern matching change with different cutoff constant values?
- 5.2: are there questions whose answers are not entities? 
- how could the proposed approach be used when predictions aren't made at every word? is there any extension for, say, sentence-level sentiment classification?

---

This work proposes a pattern extraction method to both understand what a trained LSTM has learnt and to allow implementation of a hand-coded algorithm that performs similarly to the LSTM. Good results are shown on one dataset for one model architecture so it is unclear how well this approach will generalize, however, it seems it will be a useful way to understand and debug models.

The questions in WikiMovies seem to be generated from templates and so this pattern matching approach will likely work well. However, from the experiments it's not clear if this will extend to other types of Q&A tasks where the answer may be free form text and not be a substring in the document. Is the model required to produce a continuous span over the original document?

The approach also seems to have some deficiencies in how it handles word types such as numbers or entity names. This can be encoded in the embedding for the word but from the description of the algorithm, it seems that the approach requires an entity detector. Does this mean that the approach is unable to determine when it has reached an entity from the decomposition of the output of the LSTM? The results where 'manual pattern matching' where explicit year annotations are used, seem to show that the automatic method is unable to deal with word types.

It would also be good to see an attention model as a baseline in addition to the gradient-based baseline.

Minor comments:
- P and Q seem to be undefined.
- Some references seem to be bad, e.g. in section 5.1: 'in 1' instead of 'in table 1'. Similarly above section 7: 'as shown in 3' and in section 7.1.
- In the paragraph above section 6.3: 'adam' -> 'Adam'.

---

This paper proposes a novel method for extracting rule-based classifiers from trained LSTM models. The proposed method is applied to a factoid question-answering task, where it is demonstrated that the extracted rules perform comparatively to the original LSTM. The analysis of the extracted rules illustrate the features the LSTM model picks up on.

Analyzing and visualizing the computations carried out by RNNs in order to understand the functions they compute is an important direction of research. This sort of analysis will help us understand the pitfalls of RNNs, and how we can improve them. Although the approach taken is relatively inflexible - each rule is defined as an ordered sequence of words - the authors experiment with three different scores for picking salient words (state-difference, cell-difference and gradient) and their approach yields comparable performance, which suggests that the extracted rules mimic the RNN closely. The results are also somewhat surprising, since most of the rules consist only of two or three words.

It would have been interesting to try extend the approach on other natural language processing tasks, such as machine translation. Presumably the rules learned here will be quite different.

Other comments:
- Eq. (12) is over-parametrized with two vectors $P$ and $Q$. The same function can be computed with a single vector. This becomes clear when you divide both the numerator and denominator by $e^{P h_t}$.
- Section 4.1. Is it correct that this section is focused on the forward LSTM? If so, please clarify it in the text.
- In Eq. (13), define $c_0 = 0$.
- Eq. (13) is exactly the same as Eq. (15). Is there a mistake?
- In Table 1, third column should have word ""film"" highlighted.
- ""are shown in 2"" -> ""are shown in Table 2"".
- Since there are some problems representing numbers, it may help to replace each digit with the hashtag symbol #.",,,,,,7.0,,,3.3333333333333335,,
457,"INCREMENTAL NETWORK QUANTIZATION: TOWARDS LOSSLESS CNNS WITH LOW-PRECISION WEIGHTS
Authors: Aojun Zhou, Anbang Yao, Yiwen Guo, Lin Xu, Yurong Chen
Source file: 457.pdf

ABSTRACT
This paper presents incremental network quantization (INQ), a novel method, targeting to efficiently convert any pre-trained full-precision convolutional neural network (CNN) model into a low-precision version whose weights are constrained to be either powers of two or zero. Unlike existing methods which are struggled in noticeable accuracy loss, our INQ has the potential to resolve this issue, as benefiting from two innovations. On one hand, we introduce three interdependent operations, namely weight partition, group-wise quantization and re-training. A well-proven measure is employed to divide the weights in each layer of a pretrained CNN model into two disjoint groups. The weights in the first group are responsible to form a low-precision base, thus they are quantized by a variablelength encoding method. The weights in the other group are responsible to compensate for the accuracy loss from the quantization, thus they are the ones to be re-trained. On the other hand, these three operations are repeated on the latest re-trained group in an iterative manner until all the weights are converted into low-precision ones, acting as an incremental network quantization and accuracy enhancement procedure. Extensive experiments on the ImageNet classification task using almost all known deep CNN architectures including AlexNet, VGG-16, GoogleNet and ResNets well testify the efficacy of the proposed method. Specifically, at 5-bit quantization (a variable-length encoding: 1 bit for representing zero value, and the remaining 4 bits represent at most 16 different values for the powers of two) 1, our models have improved accuracy than the 32-bit floating-point references. Taking ResNet-18 as an example, we further show that our quantized models with 4-bit, 3-bit and 2-bit ternary weights have improved or very similar accuracy against its 32-bit floating-point baseline. Besides, impressive results with the combination of network pruning and INQ are also reported. We believe that our method sheds new insights on how to make deep CNNs to be applicable on mobile or embedded devices. The code will be made publicly available.

1 INTRODUCTION
Deep convolutional neural networks (CNNs) have demonstrated record breaking results on a variety of computer vision tasks such as image classification (Krizhevsky et al., 2012; Simonyan & Zisserman, 2015), face recognition (Taigman et al., 2014; Sun et al., 2014), semantic segmentation (Long et al., 2015; Chen et al., 2015a) and object detection (Girshick, 2015; Ren et al., 2015). Regardless of the availability of significantly improved training resources such as abundant annotated data, powerful computational platforms and diverse training frameworks, the promising results of deep CNNs are mainly attributed to the large number of learnable parameters, ranging from tens of millions to even hundreds of millions. Recent progress further shows clear evidence that CNNs could easily enjoy the accuracy gain from the increased network depth and width (He et al., 2016; Szegedy et al., 2015; 2016). However, this in turn lays heavy burdens on the memory and other
∗This work was done when Aojun Zhou was an intern at Intel Labs China, supervised by Anbang Yao who proposed the original idea and is responsible for correspondence. The first three authors contributed equally to the writing of the paper.
1This notation applies to our method throughout the paper.
computational resources. For instance, ResNet-152, a specific instance of the latest residual network architecture wining ImageNet classification challenge in 2015, has a model size of about 230 MB and needs to perform about 11.3 billion FLOPs to classify a 224× 224 image crop. Therefore, it is very challenging to deploy deep CNNs on the devices with limited computation and power budgets.
Substantial efforts have been made to the speed-up and compression on CNNs during training, feedforward test or both of them. Among existing methods, the category of network quantization methods attracts great attention from researches and developers. Some network quantization works try to compress pre-trained full-precision CNN models directly. Gong et al. (2014) address the storage problem of AlexNet (Krizhevsky et al., 2012) with vector quantization techniques. By replacing the weights in each of the three fully connected layers with respective floating-point centroid values obtained from the clustering, they can get over 20× model compression at about 1% loss in top-5 recognition rate. HashedNet (Chen et al., 2015b) uses a hash function to randomly map pre-trained weights into hash buckets, and all the weights in the same hash bucket are constrained to share a single floating-point value. In HashedNet, only the fully connected layers of several shallow CNN models are considered. For better compression, Han et al. (2016) present deep compression method which combines the pruning (Han et al., 2015), vector quantization and Huffman coding, and reduce the model storage by 35× on AlexNet and 49× on VGG-16 (Simonyan & Zisserman, 2015). Vanhoucke et al. (2011) use an SSE 8-bit fixed-point implementation to improve the computation of neural networks on the modern Intel x86 CPUs in feed-forward test, yielding 3× speed-up over an optimized floating-point baseline. Training CNNs by substituting the 32-bit floating-point representation with the 16-bit fixed-point representation has also been explored in Gupta et al. (2015). Other seminal works attempt to restrict CNNs into low-precision versions during training phase. Soudry et al. (2014) propose expectation backpropagation (EBP) to estimate the posterior distribution of deterministic network weights. With EBP, the network weights can be constrained to +1 and -1 during feed-forward test in a probabilistic way. BinaryConnect (Courbariaux et al., 2015) further extends the idea behind EBP to binarize network weights during training phase directly. It has two versions of network weights: floating-point and binary. The floating-point version is used as the reference for weight binarization. BinaryConnect achieves state-of-the-art accuracy using shallow CNNs for small datasets such as MNIST (LeCun et al., 1998) and CIFAR-10. Later on, a series of efforts have been invested to train CNNs with low-precision weights, low-precision activations and even low-precision gradients, including but not limited to BinaryNet (Courbariaux et al., 2016), XNOR-Net (Rastegari et al., 2016), ternary weight network (TWN) (Li & Liu, 2016), DoReFa-Net (Zhou et al., 2016) and quantized neural network (QNN) (Hubara et al., 2016).
Despite these tremendous advances, CNN quantization still remains an open problem due to two critical issues which have not been well resolved yet, especially under scenarios of using low-precision weights for quantization. The first issue is the non-negligible accuracy loss for CNN quantization methods, and the other issue is the increased number of training iterations for ensuring convergence. In this paper, we attempt to address these two issues by presenting a novel incremental network quantization (INQ) method.
In our INQ, there is no assumption on the CNN architecture, and its basic goal is to efficiently convert any pre-trained full-precision (i.e., 32-bit floating-point) CNN model into a low-precision version whose weights are constrained to be either powers of two or zero. The advantage of such kind of low-precision models is that the original floating-point multiplication operations can be replaced by cheaper binary bit shift operations on dedicated hardware like FPGA. We noticed that most existing network quantization methods adopt a global strategy in which all the weights are simultaneously converted to low-precision ones (that are usually in the floating-point types). That is, they have not considered the different importance of network weights, leaving the room to retain network accuracy limited. In sharp contrast to existing methods, our INQ makes a very careful handling for the model accuracy drop from network quantization. To be more specific, it incorporates three interdependent operations: weight partition, group-wise quantization and re-training. Weight partition uses a pruning-inspired measure (Han et al., 2015; Guo et al., 2016) to divide the weights in each layer of a pre-trained full-precision CNN model into two disjoint groups which play complementary roles in our INQ. The weights in the first group are quantized to be either powers of two or zero by a variable-length encoding method, forming a low-precision base for the original model. The weights in the other group are re-trained while keeping the quantized weights fixed, compensating for the accuracy loss resulted from the quantization. Furthermore, these three operations are repeated on the
latest re-trained weight group in an iterative manner until all the weights are quantized, acting as an incremental network quantization and accuracy enhancement procedure (as illustrated in Figure 1).
The main insight of our INQ is that a compact combination of the proposed weight partition, groupwise quantization and re-training operations has the potential to get a lossless low-precision CNN model from any full-precision reference. We conduct extensive experiments on the ImageNet large scale classification task using almost all known deep CNN architectures to validate the effectiveness of our method. We show that: (1) For AlexNet, VGG-16, GoogleNet and ResNets with 5-bit quantization, INQ achieves improved accuracy in comparison with their respective full-precision baselines. The absolute top-1 accuracy gain ranges from 0.13% to 2.28%, and the absolute top-5 accuracy gain is in the range of 0.23% to 1.65%. (2) INQ has the property of easy convergence in training. In general, re-training with less than 8 epochs could consistently generate a lossless model with 5-bit weights in the experiments. (3) Taking ResNet-18 as an example, our quantized models with 4-bit, 3-bit and 2-bit ternary weights also have improved or very similar accuracy compared with its 32-bit floating-point baseline. (4) Taking AlexNet as an example, the combination of our network pruning and INQ outperforms deep compression method (Han et al., 2016) with significant margins.

2 INCREMENTAL NETWORK QUANTIZATION
In this section, we clarify the insight of our INQ, describe its key components, and detail its implementation.

2.1 WEIGHT QUANTIZATION WITH VARIABLE-LENGTH ENCODING
Suppose a pre-trained full-precision (i.e., 32-bit floating-point) CNN model can be represented by {Wl : 1 ≤ l ≤ L}, where Wl denotes the weight set of the lth layer, and L denotes the number of learnable layers in the model. To simplify the explanation, we only consider convolutional layers and fully connected layers. For CNN models like AlexNet, VGG-16, GoogleNet and ResNets as tested in this paper, Wl can be a 4D tensor for the convolutional layer, or a 2D matrix for the fully connected layer. For simplicity, here the dimension difference is not considered in the expression. Given a pre-trained full-precision CNN model, the main goal of our INQ is to convert all 32-bit floating-point weights to be either powers of two or zero without loss of model accuracy. Besides, we also attempt to explore the limit of the expected bit-width under the premise of guaranteeing lossless network quantization. Here, we start with our basic network quantization method on how to
convert Wl to be a low-precision version Ŵl, and each of its entries is chosen from
Pl = {±2n1 , · · · ,±2n2 , 0}, (1)
where n1 and n2 are two integer numbers, and they satisfy n2 ≤ n1. Mathematically, n1 and n2 help to bound Pl in the sense that its non-zero elements are constrained to be in the range of either [−2n1 ,−2n2 ] or [2n2 , 2n1 ]. That is, network weights with absolute values smaller than 2n2 will be pruned away (i.e., set to zero) in the final low-precision model. Obviously, the problem is how to determine n1 and n2. In our INQ, the expected bit-width b for storing the indices in Pl is set beforehand, thus the only hyper-parameter shall be determined is n1 because n2 can be naturally computed once b and n1 are available. Here, n1 is calculated by using a tricky yet practically effective formula as
n1 = floor(log2(4s/3)), (2)
where floor(·) indicates the round down operation and s is calculated by using
s = max(abs(Wl)), (3)
where abs(·) is an element-wise operation and max(·) outputs the largest element of its input. In fact, Equation (2) helps to match the rounding power of 2 for s, and it could be easily implemented in practical programming. After n1 is obtained, n2 can be naturally determined as n2 = n1 + 1 − 2(b−1)/2. For instance, if b = 3 and n1 = −1, it is easy to get n2 = −2. Once Pl is determined, we further use the ladder of powers to convert every entry of Wl into a low-precision one by using
Ŵl(i, j) = { βsgn(Wl(i, j)) if (α+ β)/2 ≤ abs(Wl(i, j)) < 3β/2 0 otherwise,
(4)
where α and β are two adjacent elements in the sorted Pl, making the above equation as a numerical rounding to the quantum values. It should be emphasized that factor 4/3 in Equation (2) is set to make sure that all the elements in Pl correspond with the quantization rule defined in Equation (4). In other words, factor 4/3 in Equation (2) highly correlates with factor 3/2 in Equation (4).
Here, an important thing we want to clarify is the definition of the expected bit-width b. Taking 5-bit quantization as an example, since zero value cannot be written as the power of two, we use 1 bit to represent zero value, and the remaining 4 bits to represent at most 16 different values for the powers of two. That is, the number of candidate quantum values is at most 2b−1 + 1, so our quantization method actually adopts a variable-length encoding scheme. It is clear that the quantization described above is performed in a linear scale. An alternative solution is to perform the quantization in the log scale. Although it may also be effective, it should be a little bit more difficult in implementation and may cause some extra computational overhead in comparison to our method.

2.2 INCREMENTAL QUANTIZATION STRATEGY
We can naturally use the above described method to quantize any pre-trained full-precision CNN model. However, noticeable accuracy loss appeared in the experiments when using small bit-width values (e.g., 5-bit, 4-bit, 3-bit and 2-bit).
In the literature, there are many existing network quantization works such as HashedNet (Chen et al., 2015b), vector quantization (Gong et al., 2014), fixed-point representation (Vanhoucke et al., 2011; Gupta et al., 2015), BinaryConnect (Courbariaux et al., 2015), BinaryNet (Courbariaux et al., 2016), XNOR-Net (Rastegari et al., 2016), TWN (Li & Liu, 2016), DoReFa-Net (Zhou et al., 2016) and QNN (Hubara et al., 2016). Similar to our basic network quantization method, they also suffer from non-negligible accuracy loss on deep CNNs, especially when being applied on the ImageNet large scale classification dataset. For all these methods, a common fact is that they adopt a global strategy in which all the weights are simultaneously converted into low-precision ones, which in turn causes accuracy loss. Compared with the methods focusing on the pre-trained models, accuracy loss becomes worse for the methods such as XNOR-Net, TWN, DoReFa-Net and QNN which intend to train low-precision CNNs from scratch.
Recall that our main goal is to achieve lossless low-precision quantization for any pre-trained fullprecision CNN model with no assumption on its architecture. To this end, our INQ makes a special
handling of the strategy for suppressing resulting quantization loss in model accuracy. We are partially inspired by the latest progress in network pruning (Han et al., 2015; Guo et al., 2016). In these methods, the accuracy loss from removing less important network weights of a pre-trained neural network model could be well compensated by following re-training steps. Therefore, we conjecture that the nature of changing network weight importance is critical to achieve lossless network quantization.
Base on this assumption, we present INQ which incorporates three interdependent operations: weight partition, group-wise quantization and re-training. Weight partition is to divide the weights in each layer of a pre-trained full-precision CNN model into two disjoint groups which play complementary roles in our INQ. The weights in the first group are responsible for forming a low-precision base for the original model, thus they are quantized by using Equation (4). The weights in the second group adapt to compensate for the loss in model accuracy, thus they are the ones to be re-trained. Once the first run of the quantization and re-training operations is finished, all the three operations are further conducted on the second weight group in an iterative manner, until all the weights are converted to be either powers of two or zero, acting as an incremental network quantization and accuracy enhancement procedure. As a result, accuracy loss under low-precision CNN quantization can be well suppressed by our INQ. Illustrative results at iterative steps of our INQ are provided in Figure 2.
For the lth layer, weight partition can be defined as
A (1) l ∪A (2) l = {Wl(i, j)}, and A (1) l ∩A (2) l = ∅, (5)
where A(1)l denotes the first weight group that needs to be quantized, and A2 denotes the other weight group that needs to be re-trained. We leave the strategies for group partition to be chosen in the experiment section. Here, we define a binary matrix Tl to help distinguish above two categories of weights. That is, Tl(i, j) = 0 means Wl(i, j) ∈ A(1)l , and Tl(i, j) = 1 means Wl(i, j) ∈ A(2)l .

2.3 INCREMENTAL NETWORK QUANTIZATION ALGORITHM
Now, we come to the training method. Taking the lth layer as an example, the basic optimization problem of making its weights to be either powers of two or zero can be expressed as
min Wl E(Wl) = L(Wl) + λR(Wl)
s.t. Wl(i, j) ∈ Pl, 1 ≤ l ≤ L, (6)
where L(Wl) is the network loss, R(Wl) is the regularization term, λ is a positive coefficient, and the constraint term indicates each weight entry Wl(i, j) should be chosen from the set Pl consisting of a fixed number of the values of powers of two plus zero. Direct solving above optimization problem in training from scratch is challenging since it is very easy to undergo convergence problem.
By performing weight partition and group-wise quantization operations beforehand, the optimization problem defined in (6) can be reshaped into a easier version. That is, we only need to optimize the following objective function
min Wl E(Wl) = L(Wl) + λR(Wl)
s.t. Wl(i, j) ∈ Pl, if Tl(i, j) = 0, 1 ≤ l ≤ L, (7)
where Pl is determined at group-wise quantization operation, and the binary matrix Tl acts as a mask which is determined by weight partition operation. Since Pl and Tl are known, the optimization problem (7) can be solved using popular stochastic gradient decent (SGD) method. That is, in INQ, we can get the update scheme for the re-training as
Wl(i, j)←Wl(i, j)− γ ∂E
∂(Wl(i, j)) Tl(i, j), (8)
where γ is a positive learning rate. Note that the binary matrix Tl forces zero update to the weights that have been quantized. That is, only the weights still keep with floating-point values are updated, akin to the latest pruning methods (Han et al., 2015; Guo et al., 2016) in which only the weights that are not currently removed are re-trained to enhance network accuracy. The whole procedure of our INQ is summarized as Algorithm 1.
We would like to highlight that the merits of our INQ are in three aspects: (1) Weight partition introduces the importance-aware weight quantization. (2) Group-wise weight quantization introduces much less accuracy loss than simultaneously quantizing all the network weights, thus making retraining have larger room to recover model accuracy. (3) By integrating the operations of weight partition, group-wise quantization and re-training into a nested loop, our INQ has the potential to obtain lossless low-precision CNN model from the pre-trained full-precision reference.
Algorithm 1 Incremental network quantization for lossless CNNs with low-precision weights. Input: X: the training data, {Wl : 1 ≤ l ≤ L}: the pre-trained full-precision CNN model, {σ1, σ2, · · · , σN}: the accumulated portions of weights quantized at iterative steps Output: {Ŵl : 1 ≤ l ≤ L}: the final low-precision model with the weights constrained to be either powers of two or zero
1: Initialize A(1)l ← ∅, A (2) l ← {Wl(i, j)}, Tl ← 1, for 1 ≤ l ≤ L 2: for n = 1, 2, . . . , N do 3: Reset the base learning rate and the learning policy 4: According to σn, perform layer-wise weight partition and update A (1) l , A (2) l and Tl 5: Based on A(1)l , determine Pl layer-wisely 6: Quantize the weights in A(1)l by Equation (4) layer-wisely 7: Calculate feed-forward loss, and update weights in {A(2)l : 1 ≤ l ≤ L} by Equation (8) 8: end for

3 EXPERIMENTAL RESULTS
To analyze the performance of our INQ, we perform extensive experiments on the ImageNet large scale classification task, which is known as the most challenging image classification benchmark so far. ImageNet dataset has about 1.2 million training images and 50 thousand validation images. Each image is annotated as one of 1000 object classes. We apply our INQ to AlexNet, VGG-16, GoogleNet, ResNet-18 and ResNet-50, covering almost all known deep CNN architectures. Using the center crops of validation images, we report the results with two standard measures: top-1 error rate and top-5 error rate. For fair comparison, all pre-trained full-precision (i.e., 32-bit floatingpoint) CNN models except ResNet-18 are taken from the Caffe model zoo2. Note that He et al. (2016) do not release their pre-trained ResNet-18 model to the public, so we use a publicly available re-implementation by Facebook3. Since our method is implemented with Caffe, we make use of an open source tool4 to convert the pre-trained ResNet-18 model from Torch to Caffe.

3.1 RESULTS ON IMAGENET
Setting expected bit-width to 5, the first set of experiments is performed to testify the efficacy of our INQ on different CNN architectures. Regarding weight partition, there are several candidate strategies as we tried in our previous work for efficient network pruning (Guo et al., 2016). In Guo et al. (2016), we found random partition and pruning-inspired partition are the two best choices compared with the others. Thus in this paper, we directly compare these two strategies for weight partition. In random strategy, the weights in each layer of any pre-trained full-precision deep CNN model are randomly split into two disjoint groups. In pruning-inspired strategy, the weights are divided into two disjoint groups by comparing their absolute values with layer-wise thresholds which are automatically determined by a given splitting ratio. Here we directly use pruning-inspired strategy and the experimental results in Section 3.2 will show why. After the re-training with no more than 8 epochs over each pre-trained full-precision model, we obtain the results as shown in Table 1. It can be concluded that the 5-bit CNN models generated by our INQ show consistently improved top-1 and top-5 recognition rates compared with respective full-precision references. Parameter settings are described below.
AlexNet: AlexNet has 5 convolutional layers and 3 fully-connected layers. We set the accumulated portions of quantized weights at iterative steps as {0.3, 0.6, 0.8, 1}, the batch size as 256, the weight decay as 0.0005, and the momentum as 0.9.
VGG-16: Compared with AlexNet, VGG-16 has 13 convolutional layers and more parameters. We set the accumulated portions of quantized weights at iterative steps as {0.5, 0.75, 0.875, 1}, the batch size as 32, the weight decay as 0.0005, and the momentum as 0.9.
2https://github.com/BVLC/caffe/wiki/Model-Zoo 3https://github.com/facebook/fb.resnet.torch/tree/master/pretrained 4https://github.com/zhanghang1989/fb-caffe-exts
GoogleNet: Compared with AlexNet and VGG-16, GoogleNet is more difficult to quantize due to a smaller number of parameters and the increased network width. We set the accumulated portions of quantized weights at iterative steps as {0.2, 0.4, 0.6, 0.8, 1}, the batch size as 80, the weight decay as 0.0002, and the momentum as 0.9.
ResNet-18: Different from above three networks, ResNets have batch normalization layers and relief the vanishing gradient problem by using shortcut connections. We first test the 18-layer version for exploratory purpose and test the 50-layer version later on. The network architectures of ResNet18 and ResNet-34 are very similar. The only difference is the number of filters in every convolutional layer. We set the accumulated portions of quantized weights at iterative steps as {0.5, 0.75, 0.875, 1}, the batch size as 80, the weight decay as 0.0005, and the momentum as 0.9. ResNet-50: Besides significantly increased network depth, ResNet-50 has a more complex network architecture in comparison to ResNet-18. However, regarding network architecture, ResNet-50 is very similar to ResNet-101 and ResNet-152. The only difference is the number of filters in every convolutional layer. We set the accumulated portions of quantized weights at iterative steps as {0.5, 0.75, 0.875, 1}, the batch size as 32, the weight decay as 0.0005, and the momentum as 0.9.

3.2 ANALYSIS OF WEIGHT PARTITION STRATEGIES
In our INQ, the first operation is weight partition whose result will directly affect the following group-wise quantization and re-training operations. Therefore, the second set of experiments is conducted to analyze two candidate strategies for weight partition. As mentioned in the previous section, we use pruning-inspired strategy for weight partition. Unlike random strategy in which all the weights have equal probability to fall into the two disjoint groups, pruning-inspired strategy considers that the weights with larger absolute values are more important than the smaller ones to form a low-precision base for the original CNN model. We use ResNet-18 as a test case to compare the performance of these two strategies. In the experiments, the parameter settings are completely the same as described in Section 3.1. We set 4 epochs for weight re-training. Table 2 summarizes the results of our INQ with 5-bit quantization. It can be seen that our INQ achieves top-1 error rate of 32.11% and top-5 error rate of 11.73% by using random partition. Comparatively, pruning-inspired partition brings 1.09% and 0.83% decrease in top-1 and top-5 error rates, respectively. Apparently, pruning-inspired partition is better than random partition, and this is the reason why we use it in this paper. For future works, weight partition based on quantization error could also be an option worth exploring.

3.3 THE TRADE-OFF BETWEEN EXPECTED BIT-WIDTH AND MODEL ACCURACY
The third set of experiments is performed to explore the limit of the expected bit-width under which our INQ can still achieve lossless network quantization. Similar to the second set of experiments, we also use ResNet-18 as a test case, and the parameter settings for the batch size, the weight decay and the momentum are completely the same. Finally, lower-precision models with 4-bit, 3-bit and even 2-bit ternary weights are generated for comparisons. As the expected bit-width goes down, the number of candidate quantum values will be decreased significantly, thus we shall increase the number of iterative steps accordingly for enhancing the accuracy of final low-precision model. Specifically, we set the accumulated portions of quantized weights at iterative steps as {0.3, 0.5, 0.8, 0.9, 0.95, 1}, {0.2, 0.4, 0.6, 0.7, 0.8, 0.9, 0.95, 1} and {0.2, 0.4, 0.6, 0.7, 0.8, 0.85, 0.9, 0.95, 0.975, 1} for 4-bit, 3-bit and 2-bit ternary models, respectively. The required number of epochs also increases when the expected bit-width goes down, and it reaches 30 when training our 2-bit ternary model. Although our 4-bit model shows slightly decreased accuracy when compared with the 5-bit model, its accuracy is still better than that of the pre-trained full-precision model. Comparatively, even when the expected bit-width goes down to 3, our low-precision model shows only 0.19% and
0.33% losses in top-1 and top-5 recognition rates, respectively. As for our 2-bit ternary model, although it incurs 2.25% decrease in top-1 error rate and 1.56% decrease in top-5 error rate in comparison to the pre-trained full-precision reference, its accuracy is considerably better than stateof-the-art results reported for binary-weight network (BWN) (Rastegari et al., 2016) and ternary weight network (TWN) (Li & Liu, 2016). Detailed results are summarized in Table 3 and Table 4.

3.4 LOW-BIT DEEP COMPRESSION
In the literature, recently proposed deep compression method (Han et al., 2016) reports so far best results on network compression without loss of model accuracy. Therefore, the last set of experiments is conducted to explore the potential of our INQ for much better deep compression. Note that Han et al. (2016) is a hybrid network compression solution combining three different techniques, namely network pruning (Han et al., 2015), vector quantization (Gong et al., 2014) and Huffman coding. Taking AlexNet as an example, network pruning gets 9× compression, however this result is mainly obtained from the fully connected layers. Actually its compression performance on the convolutional layers is less than 3× (as can be seen in the Table 4 of Han et al. (2016)). Besides, network pruning is realized by separately performing pruning and re-training in an iterative way, which is very time-consuming. It will cost at least several weeks for compressing AlexNet. We solved this problem by our dynamic network surgery (DNS) method (Guo et al., 2016) which achieves about 7× speed-up in training and improves the performance of network pruning from 9× to 17.7×. In Han et al. (2016), after network pruning, vector quantization further improves compression ratio from 9× to 27×, and Huffman coding finally boosts compression ratio up to 35×. For fair comparison, we combine our proposed INQ and DNS, and compare the resulting method with Han et al. (2016). Detailed results are summarized in Table 5. When combing our proposed INQ and DNS, we achieve much better compression results compared with Han et al. (2016). Specifically, with 5-bit quantization, we can achieve 53× compression with slightly larger gains both in top-5 and top-1 recognition rates, yielding 51.43%/96.30% absolute improvement in compression performance compared with full version/fair version (i.e., the combination of network pruning and vector quantization) of Han et al. (2016), respectively. Consistently better results have also obtained for our 4-bit and 3-bit models.
Besides, we also perform a set of experiments on AlexNet to compare the performance of our INQ and vector quantization (Gong et al., 2014). For fair comparison, re-training is also used to enhance the performance of vector quantization, and we set the number of cluster centers for all of 5 convolutional layers and 3 fully connect layers to 32 (i.e., 5-bit quantization). In the experiment, vector quantization incurs over 3% loss in model accuracy. When we change the number of cluster centers for convolutional layers from 32 to 128, it gets an accuracy loss of 0.98%. This is consistent with the results reported in (Gong et al., 2014). Comparatively, vector quantization is mainly proposed
to compress the parameters in the fully connected layers of a pre-trained full-precision CNN model, while our INQ addresses all network layers simultaneously and has no accuracy loss for 5-bit and 4-bit quantization. Therefore, it is evident that our INQ is much better than vector quantization. Last but not least, the final weights for vector quantization (Gong et al., 2014), network pruning (Han et al., 2015) and deep compression (Han et al., 2016) are still floating-point values, but the final weights for our INQ are in the form of either powers of two or zero. The direct advantage of our INQ is that the original floating-point multiplication operations can be replaced by cheaper binary bit shift operations on dedicated hardware like FPGA.

4 CONCLUSIONS
In this paper, we present INQ, a new network quantization method, to address the problem of how to convert any pre-trained full-precision (i.e., 32-bit floating-point) CNN model into a lossless lowprecision version whose weights are constrained to be either powers of two or zero. Unlike existing methods which usually quantize all the network weights simultaneously, INQ is a more compact quantization framework. It incorporates three interdependent operations: weight partition, groupwise quantization and re-training. Weight partition splits the weights in each layer of a pre-trained full-precision CNN model into two disjoint groups which play complementary roles in INQ. The weights in the first group is directly quantized by a variable-length encoding method, forming a low-precision base for the original CNN model. The weights in the other group are re-trained while keeping all the quantized weights fixed, compensating for the accuracy loss from network quantization. More importantly, the operations of weight partition, group-wise quantization and re-training are repeated on the latest re-trained weight group in an iterative manner until all the weights are quantized, acting as an incremental network quantization and accuracy enhancement procedure. On the ImageNet large scale classification task, we conduct extensive experiments and show that our quantized CNN models with 5-bit, 4-bit, 3-bit and even 2-bit ternary weights have improved or at least comparable accuracy against their full-precision baselines, including AlexNet, VGG-16, GoogleNet and ResNets. As for future works, we plan to extend incremental idea behind INQ from low-precision weights to low-precision activations and low-precision gradients (we have actually already made some good progress on it, as shown in our supplementary materials). We will also investigate computation and power efficiency by implementing our low-precision CNN models on hardware platforms.

A APPENDIX 1: STATISTICAL ANALYSIS OF THE QUANTIZED WEIGHTS
Taking our 5-bit AlexNet model as an example, we analyze the distribution of the quantized weights. Detailed statistical results are summarized in Table 6. We can find: (1) in the 1st and 2nd convolutional layers, the values of {−2−6, −2−5, −2−4, 2−6, 2−5, 2−4} and {−2−8, −2−7, −2−6, −2−5, 0, 2−8, 2−7, 2−6, 2−5} occupy over 60% and 94% of all quantized weights, respectively; (2) the distributions of the quantized weights in the 3rd, 4th and 5th convolutional layers are similar to that of the 2nd convolutional layer, and more weights are quantized into zero in the 2nd, 3rd, 4th and 5th convolutional layers compared with the 1st convolutional layer; (3) in the 1st fully connected layer, the values of {−2−10, −2−9, −2−8, −2−7, 0, 2−10, 2−9, 2−8, 2−7} occupy about 98% of all quantized weights, and similar results can be seen for the 2nd fully connected layer; (4) generally, the distributions of the quantized weights in the convolutional layers are usually more scattered compared with the fully connected layers. This may be partially the reason why it is much easier to get good compression performance on fully connected layers in comparison to convolutional layers, when using methods such as network hashing (Chen et al., 2015b) and vector quantization (Gong et al., 2014); (5) for 5-bit AlexNet model, the required bit-width for each layer is actually 4 but not 5.

B APPENDIX 2: LOSSLESS CNNS WITH LOW-PRECISION WEIGHTS AND LOW-PRECISION ACTIVATIONS
Recently, we have made some good progress on developing our INQ for lossless CNNs with both low-precision weights and low-precision activations. According to the results summarized in Table 7, it can be seen that our VGG-16 model with 5-bit weights and 4-bit activations shows improved top-5 and top-1 recognition rates in comparison to the pre-trained reference with 32-bit floating-point weights and 32-bit floating-point activations. To the best of our knowledge, this should be the best results reported on VGG-16 architecture so far.
","This paper presents incremental network quantization (INQ), a novel method, targeting to efficiently convert any pre-trained full-precision convolutional neural network (CNN) model into a low-precision version whose weights are constrained to be either powers of two or zero. Unlike existing methods which are struggled in noticeable accuracy loss, our INQ has the potential to resolve this issue, as benefiting from two innovations. On one hand, we introduce three interdependent operations, namely weight partition, group-wise quantization and re-training. A well-proven measure is employed to divide the weights in each layer of a pretrained CNN model into two disjoint groups. The weights in the first group are responsible to form a low-precision base, thus they are quantized by a variablelength encoding method. The weights in the other group are responsible to compensate for the accuracy loss from the quantization, thus they are the ones to be re-trained. On the other hand, these three operations are repeated on the latest re-trained group in an iterative manner until all the weights are converted into low-precision ones, acting as an incremental network quantization and accuracy enhancement procedure. Extensive experiments on the ImageNet classification task using almost all known deep CNN architectures including AlexNet, VGG-16, GoogleNet and ResNets well testify the efficacy of the proposed method. Specifically, at 5-bit quantization (a variable-length encoding: 1 bit for representing zero value, and the remaining 4 bits represent at most 16 different values for the powers of two) 1, our models have improved accuracy than the 32-bit floating-point references. Taking ResNet-18 as an example, we further show that our quantized models with 4-bit, 3-bit and 2-bit ternary weights have improved or very similar accuracy against its 32-bit floating-point baseline. Besides, impressive results with the combination of network pruning and INQ are also reported. We believe that our method sheds new insights on how to make deep CNNs to be applicable on mobile or embedded devices. The code will be made publicly available.",ICLR 2017 conference submission,True,,"Nice idea but not complete, model size is not reduced by the large factors found in one of your references (Song 2016), where they go to 5 bits, but this is ontop of pruning which gives overall 49X reduction in model size of VGG (without loss of accuracy). You may achieve similar reductions with inclusion of pruning (or better since you go to 4 bits with no loss) but we should see this in the paper, so at the moment it is difficult to compare

---

The paper presents a method for iterative quantization of neural networks weights to powers of 2. The technique is simple, but novel and effective, with thorough evaluation on a variety of ImageNet classification models.

---

Thanks to all the reviewers for constructive suggestions and comments. We are really excited that the novelty of our paper has been well recognized.

In this updated version, we carefully considered all reviewers’ suggestions to improve the paper. Generally, we performed four aspects of works: (1) the result comparison of pruning + quantization between our method and Han et al.’s method [1] was incorporated into the paper (please see Section 3.4 for details); (2) the result comparison of weight quantization between our method and vector quantization [2] was also incorporated into the paper (please see Section 3.4 for details); (3) we tried our best to improve the clarifications of our encoding method for weight quantization, definition of bit-width, detailed experimental settings and so on, and several rounds of proof-reading and revising were also conducted; (4) more experimental results (including the statistical analyses on the distribution of weights after quantization and our latest progress on developing INQ for deep CNNs with low-precision weights and low-precision activations) that reviewers may be interested were added to the paper as the supplementary materials.
 
Moreover, to make our work fully reproducible, the code (along with an instruction manual) will be released to public as we promised in the paper submission.

(1) To reviewer 1:

Question1: “Also, the description of the pruning-inspired partitioning strategy could be clarified somewhat... e.g., the chosen splitting ratio of 50% only seems to be referenced in a figure caption and not the main text.”

Following your suggestion, we added detailed parameter settings (such as splitting ratio and etc.) to the respective sets of experiments described in Section 3 accordingly.

Question 2: “The paper could use another second pass for writing style and grammar.”

Following your suggestion, we tried our best to do a much better work on revising and proof-reading, with the helps from the native colleagues in USA.

(2) To reviewer 2:

Thanks for your recognition of the novelty of our method. We believe that our responses posted on Dec. 16, 2016 should well address your concern on the result comparison of pruning + quantization between our method and Han et al.’s method [1]. Furthermore, detailed result comparisons can be found in Section 3.4 of the paper. It can be clearly seen that our method outperforms Han et al.’s method [1] with significant margins.

(3) To reviewer 3:

Question 1: “1) It would be good to incorporate some of the answers into the paper, mainly the results with pruning + this method as that can be compared fairly to Han et al. and outperforms it.”

Following your suggestion, we incorporated related results into the paper (please see Section 3.4 for details).

Question 2: “It would be good to better explain the encoding method (my question 4) as it is not that clear from the paper (e.g. made me make a mistake in question 5 for the computation of n2).”

Following your suggestion, we revised related parts, especially the clarification of our encoding method based on our previous responses to your questions accordingly (please see Section 2.1 for details).

Question 3: ""The ""5 bits"" is misleading as in fact what is used is variable length encoding (which is on average close to 5 bits) where: - 0 is represented with 1 bit, e.g. 0 - other values are represented with 5 bits, where the first bit is needed to distinguish from 0, and the remaining 4 bits represent the 16 different values for the powers of 2.”

Following your suggestion, we made a clear clarification on the definition of bit-width accordingly.

References:
Song Han, Jeff Pool, John Tran, and William J. Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. ICLR, 2016.
Yunchao Gong, Liu Liu, Ming Yang, and Lubomir Bourdev. Compressing deep convolutional networks using vector quantization. arXiv preprint arXiv:1412.6115v1, 2014.

---

There is a great deal of ongoing interest in compressing neural network models. One line of work has focused on using low-precision representations of the model weights, even down to 1 or 2 bits. However, so far these approaches have been accompanied by a significant impact on accuracy. The paper proposes an iterative quantization scheme, in which the network weights are quantized in stages---the largest weights (in absolute value) are quantized and fixed, while unquantized weights can adapt to compensate for any resulting error. The experimental results show this is extremely effective, yielding models with 4 bit or 3 bit weights with essentially no reduction in accuracy. While at 2 bits the accuracy decreases slightly, the results are substantially better than those achieved with other quantization approaches.

Overall this paper is clear, the technique is as far as I am aware novel, the experiments are thorough and the results are very compelling, so I recommend acceptance. The paper could use another second pass for writing style and grammar. Also, the description of the pruning-inspired partitioning strategy could be clarified somewhat... e.g., the chosen splitting ratio of 50% only seems to be referenced in a figure caption and not the main text.

---

The idea of this paper is reasonable - gradually go from original weights to compressed weights by compressing a part of them and fine-tuning the rest. Everything seems fine, results look good, and my questions have been addressed.

To improve the paper:

1) It would be good to incorporate some of the answers into the paper, mainly the results with pruning + this method as that can be compared fairly to Han et al. and outperforms it.

2) It would be good to better explain the encoding method (my question 4) as it is not that clear from the paper (e.g. made me make a mistake in question 5 for the computation of n2). The ""5 bits"" is misleading as in fact what is used is variable length encoding (which is on average close to 5 bits) where:
- 0 is represented with 1 bit, e.g. 0
- other values are represented with 5 bits, where the first bit is needed to distinguish from 0, and the remaining 4 bits represent the 16 different values for the powers of 2.

---

Nice idea but not complete, model size is not reduced by the large factors found in one of your references (Song 2016), where they go to 5 bits, but this is ontop of pruning which gives overall 49X reduction in model size of VGG (without loss of accuracy). You may achieve similar reductions with inclusion of pruning (or better since you go to 4 bits with no loss) but we should see this in the paper, so at the moment it is difficult to compare

---

Dear Reviewers,

Please take a look through the paper and ask the authors to clarify any questions you might have. The deadline for this part of the review process is December 2, 2016.

Thanks!

---

Nice idea but not complete, model size is not reduced by the large factors found in one of your references (Song 2016), where they go to 5 bits, but this is ontop of pruning which gives overall 49X reduction in model size of VGG (without loss of accuracy). You may achieve similar reductions with inclusion of pruning (or better since you go to 4 bits with no loss) but we should see this in the paper, so at the moment it is difficult to compare

---

The paper presents a method for iterative quantization of neural networks weights to powers of 2. The technique is simple, but novel and effective, with thorough evaluation on a variety of ImageNet classification models.

---

Thanks to all the reviewers for constructive suggestions and comments. We are really excited that the novelty of our paper has been well recognized.

In this updated version, we carefully considered all reviewers’ suggestions to improve the paper. Generally, we performed four aspects of works: (1) the result comparison of pruning + quantization between our method and Han et al.’s method [1] was incorporated into the paper (please see Section 3.4 for details); (2) the result comparison of weight quantization between our method and vector quantization [2] was also incorporated into the paper (please see Section 3.4 for details); (3) we tried our best to improve the clarifications of our encoding method for weight quantization, definition of bit-width, detailed experimental settings and so on, and several rounds of proof-reading and revising were also conducted; (4) more experimental results (including the statistical analyses on the distribution of weights after quantization and our latest progress on developing INQ for deep CNNs with low-precision weights and low-precision activations) that reviewers may be interested were added to the paper as the supplementary materials.
 
Moreover, to make our work fully reproducible, the code (along with an instruction manual) will be released to public as we promised in the paper submission.

(1) To reviewer 1:

Question1: “Also, the description of the pruning-inspired partitioning strategy could be clarified somewhat... e.g., the chosen splitting ratio of 50% only seems to be referenced in a figure caption and not the main text.”

Following your suggestion, we added detailed parameter settings (such as splitting ratio and etc.) to the respective sets of experiments described in Section 3 accordingly.

Question 2: “The paper could use another second pass for writing style and grammar.”

Following your suggestion, we tried our best to do a much better work on revising and proof-reading, with the helps from the native colleagues in USA.

(2) To reviewer 2:

Thanks for your recognition of the novelty of our method. We believe that our responses posted on Dec. 16, 2016 should well address your concern on the result comparison of pruning + quantization between our method and Han et al.’s method [1]. Furthermore, detailed result comparisons can be found in Section 3.4 of the paper. It can be clearly seen that our method outperforms Han et al.’s method [1] with significant margins.

(3) To reviewer 3:

Question 1: “1) It would be good to incorporate some of the answers into the paper, mainly the results with pruning + this method as that can be compared fairly to Han et al. and outperforms it.”

Following your suggestion, we incorporated related results into the paper (please see Section 3.4 for details).

Question 2: “It would be good to better explain the encoding method (my question 4) as it is not that clear from the paper (e.g. made me make a mistake in question 5 for the computation of n2).”

Following your suggestion, we revised related parts, especially the clarification of our encoding method based on our previous responses to your questions accordingly (please see Section 2.1 for details).

Question 3: ""The ""5 bits"" is misleading as in fact what is used is variable length encoding (which is on average close to 5 bits) where: - 0 is represented with 1 bit, e.g. 0 - other values are represented with 5 bits, where the first bit is needed to distinguish from 0, and the remaining 4 bits represent the 16 different values for the powers of 2.”

Following your suggestion, we made a clear clarification on the definition of bit-width accordingly.

References:
Song Han, Jeff Pool, John Tran, and William J. Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. ICLR, 2016.
Yunchao Gong, Liu Liu, Ming Yang, and Lubomir Bourdev. Compressing deep convolutional networks using vector quantization. arXiv preprint arXiv:1412.6115v1, 2014.

---

There is a great deal of ongoing interest in compressing neural network models. One line of work has focused on using low-precision representations of the model weights, even down to 1 or 2 bits. However, so far these approaches have been accompanied by a significant impact on accuracy. The paper proposes an iterative quantization scheme, in which the network weights are quantized in stages---the largest weights (in absolute value) are quantized and fixed, while unquantized weights can adapt to compensate for any resulting error. The experimental results show this is extremely effective, yielding models with 4 bit or 3 bit weights with essentially no reduction in accuracy. While at 2 bits the accuracy decreases slightly, the results are substantially better than those achieved with other quantization approaches.

Overall this paper is clear, the technique is as far as I am aware novel, the experiments are thorough and the results are very compelling, so I recommend acceptance. The paper could use another second pass for writing style and grammar. Also, the description of the pruning-inspired partitioning strategy could be clarified somewhat... e.g., the chosen splitting ratio of 50% only seems to be referenced in a figure caption and not the main text.

---

The idea of this paper is reasonable - gradually go from original weights to compressed weights by compressing a part of them and fine-tuning the rest. Everything seems fine, results look good, and my questions have been addressed.

To improve the paper:

1) It would be good to incorporate some of the answers into the paper, mainly the results with pruning + this method as that can be compared fairly to Han et al. and outperforms it.

2) It would be good to better explain the encoding method (my question 4) as it is not that clear from the paper (e.g. made me make a mistake in question 5 for the computation of n2). The ""5 bits"" is misleading as in fact what is used is variable length encoding (which is on average close to 5 bits) where:
- 0 is represented with 1 bit, e.g. 0
- other values are represented with 5 bits, where the first bit is needed to distinguish from 0, and the remaining 4 bits represent the 16 different values for the powers of 2.

---

Nice idea but not complete, model size is not reduced by the large factors found in one of your references (Song 2016), where they go to 5 bits, but this is ontop of pruning which gives overall 49X reduction in model size of VGG (without loss of accuracy). You may achieve similar reductions with inclusion of pruning (or better since you go to 4 bits with no loss) but we should see this in the paper, so at the moment it is difficult to compare

---

Dear Reviewers,

Please take a look through the paper and ask the authors to clarify any questions you might have. The deadline for this part of the review process is December 2, 2016.

Thanks!",,,,,,7.333333333333333,,,3.6666666666666665,,
460,"Authors: EXPERIENCE REPLAY, Ziyu Wang, Victor Bapst, Koray Kavukcuoglu
Source file: 460.pdf

ABSTRACT
This paper presents an actor-critic deep reinforcement learning agent with experience replay that is stable, sample efficient, and performs remarkably well on challenging environments, including the discrete 57-game Atari domain and several continuous control problems. To achieve this, the paper introduces several innovations, including truncated importance sampling with bias correction, stochastic dueling network architectures, and a new trust region policy optimization method.

1 INTRODUCTION
Realistic simulated environments, where agents can be trained to learn a large repertoire of cognitive skills, are at the core of recent breakthroughs in AI (Bellemare et al., 2013; Mnih et al., 2015; Schulman et al., 2015a; Narasimhan et al., 2015; Mnih et al., 2016; Brockman et al., 2016; Oh et al., 2016). With richer realistic environments, the capabilities of our agents have increased and improved. Unfortunately, these advances have been accompanied by a substantial increase in the cost of simulation. In particular, every time an agent acts upon the environment, an expensive simulation step is conducted. Thus to reduce the cost of simulation, we need to reduce the number of simulation steps (i.e. samples of the environment). This need for sample efficiency is even more compelling when agents are deployed in the real world.
Experience replay (Lin, 1992) has gained popularity in deep Q-learning (Mnih et al., 2015; Schaul et al., 2016; Wang et al., 2016; Narasimhan et al., 2015), where it is often motivated as a technique for reducing sample correlation. Replay is actually a valuable tool for improving sample efficiency and, as we will see in our experiments, state-of-the-art deep Q-learning methods (Schaul et al., 2016; Wang et al., 2016) have been up to this point the most sample efficient techniques on Atari by a significant margin. However, we need to do better than deep Q-learning, because it has two important limitations. First, the deterministic nature of the optimal policy limits its use in adversarial domains. Second, finding the greedy action with respect to the Q function is costly for large action spaces.
Policy gradient methods have been at the heart of significant advances in AI and robotics (Silver et al., 2014; Lillicrap et al., 2015; Silver et al., 2016; Levine et al., 2015; Mnih et al., 2016; Schulman et al., 2015a; Heess et al., 2015). Many of these methods are restricted to continuous domains or to very specific tasks such as playing Go. The existing variants applicable to both continuous and discrete domains, such as the on-policy asynchronous advantage actor critic (A3C) of Mnih et al. (2016), are sample inefficient.
The design of stable, sample efficient actor critic methods that apply to both continuous and discrete action spaces has been a long-standing hurdle of reinforcement learning (RL). We believe this paper
is the first to address this challenge successfully at scale. More specifically, we introduce an actor critic with experience replay (ACER) that nearly matches the state-of-the-art performance of deep Q-networks with prioritized replay on Atari, and substantially outperforms A3C in terms of sample efficiency on both Atari and continuous control domains.
ACER capitalizes on recent advances in deep neural networks, variance reduction techniques, the off-policy Retrace algorithm (Munos et al., 2016) and parallel training of RL agents (Mnih et al., 2016). Yet, crucially, its success hinges on innovations advanced in this paper: truncated importance sampling with bias correction, stochastic dueling network architectures, and efficient trust region policy optimization.
On the theoretical front, the paper proves that the Retrace operator can be rewritten from our proposed truncated importance sampling with bias correction technique.

2 BACKGROUND AND PROBLEM SETUP
Consider an agent interacting with its environment over discrete time steps. At time step t, the agent observes the nx-dimensional state vector xt ∈ X ⊆ Rnx , chooses an action at according to a policy π(a|xt) and observes a reward signal rt ∈ R produced by the environment. We will consider discrete actions at ∈ {1, 2, . . . , Na} in Sections 3 and 4, and continuous actions at ∈ A ⊆ Rna in Section 5. The goal of the agent is to maximize the discounted return Rt = ∑ i≥0 γ
irt+i in expectation. The discount factor γ ∈ [0, 1) trades-off the importance of immediate and future rewards. For an agent following policy π, we use the standard definitions of the state-action and state only value functions:
Qπ(xt, at) = Ext+1:∞,at+1:∞ [Rt|xt, at] and V π(xt) = Eat [Qπ(xt, at)|xt] . Here, the expectations are with respect to the observed environment states xt and the actions generated by the policy π, where xt+1:∞ denotes a state trajectory starting at time t+ 1.
We also need to define the advantage function Aπ(xt, at) = Qπ(xt, at)− V π(xt), which provides a relative measure of value of each action since Eat [Aπ(xt, at)] = 0.
The parameters θ of the differentiable policy πθ(at|xt) can be updated using the discounted approximation to the policy gradient (Sutton et al., 2000), which borrowing notation from Schulman et al. (2015b), is defined as:
g = Ex0:∞,a0:∞
 ∑
t≥0
Aπ(xt, at)∇θ log πθ(at|xt)   . (1)
Following Proposition 1 of Schulman et al. (2015b), we can replaceAπ(xt, at) in the above expression with the state-action value Qπ(xt, at), the discounted return Rt, or the temporal difference residual rt + γV
π(xt+1) − V π(xt), without introducing bias. These choices will however have different variance. Moreover, in practice we will approximate these quantities with neural networks thus introducing additional approximation errors and biases. Typically, the policy gradient estimator using Rt will have higher variance and lower bias whereas the estimators using function approximation will have higher bias and lower variance. Combining Rt with the current value function approximation to minimize bias while maintaining bounded variance is one of the central design principles behind ACER.
To trade-off bias and variance, the asynchronous advantage actor critic (A3C) of Mnih et al. (2016) uses a single trajectory sample to obtain the following gradient approximation:
ĝa3c = ∑
t≥0
(( k−1∑
i=0
γirt+i ) + γkV πθv (xt+k)− V πθv (xt) ) ∇θ log πθ(at|xt). (2)
A3C combines both k-step returns and function approximation to trade-off variance and bias. We may think of V πθv (xt) as a policy gradient baseline used to reduce variance.
In the following section, we will introduce the discrete-action version of ACER. ACER may be understood as the off-policy counterpart of the A3C method of Mnih et al. (2016). As such, ACER builds on all the engineering innovations of A3C, including efficient parallel CPU computation.
ACER uses a single deep neural network to estimate the policy πθ(at|xt) and the value function V πθv (xt). (For clarity and generality, we are using two different symbols to denote the parameters of the policy and value function, θ and θv , but most of these parameters are shared in the single neural network.) Our neural networks, though building on the networks used in A3C, will introduce several modifications and new modules.

3 DISCRETE ACTOR CRITIC WITH EXPERIENCE REPLAY
Off-policy learning with experience replay may appear to be an obvious strategy for improving the sample efficiency of actor-critics. However, controlling the variance and stability of off-policy estimators is notoriously hard. Importance sampling is one of the most popular approaches for offpolicy learning (Meuleau et al., 2000; Jie & Abbeel, 2010; Levine & Koltun, 2013). In our context, it proceeds as follows. Suppose we retrieve a trajectory {x0, a0, r0, µ(·|x0), · · · , xk, ak, rk, µ(·|xk)}, where the actions have been sampled according to the behavior policy µ, from our memory of experiences. Then, the importance weighted policy gradient is given by:
ĝimp =
( k∏
t=0
ρt
) k∑
t=0
( k∑
i=0
γirt+i ) ∇θ log πθ(at|xt), (3)
where ρt = π(at|xt) µ(at|xt) denotes the importance weight. This estimator is unbiased, but it suffers from very high variance as it involves a product of many potentially unbounded importance weights. To prevent the product of importance weights from exploding, Wawrzyński (2009) truncates this product. Truncated importance sampling over entire trajectories, although bounded in variance, could suffer from significant bias.
Recently, Degris et al. (2012) attacked this problem by using marginal value functions over the limiting distribution of the process to yield the following approximation of the gradient:
gmarg = Ext∼β,at∼µ [ρt∇θ log πθ(at|xt)Qπ(xt, at)] , (4) where Ext∼β,at∼µ[·] is the expectation with respect to the limiting distribution β(x) = limt→∞ P (xt = x|x0, µ) with behavior policy µ. To keep the notation succinct, we will replace Ext∼β,at∼µ[·] with Extat [·] and ensure we remind readers of this when necessary. Two important facts about equation (4) must be highlighted. First, note that it depends on Qπ and not on Qµ, consequently we must be able to estimate Qπ. Second, we no longer have a product of importance weights, but instead only need to estimate the marginal importance weight ρt. Importance sampling in this lower dimensional space (over marginals as opposed to trajectories) is expected to exhibit lower variance.
Degris et al. (2012) estimateQπ in equation (4) using lambda returns: Rλt = rt+(1−λ)γV (xt+1)+ λγρt+1R λ t+1. This estimator requires that we know how to choose λ ahead of time to trade off bias and variance. Moreover, when using small values of λ to reduce variance, occasional large importance weights can still cause instability.
In the following subsection, we adopt the Retrace algorithm of Munos et al. (2016) to estimate Qπ. Subsequently, we propose an importance weight truncation technique to improve the stability of the off-policy actor critic of Degris et al. (2012), and introduce a computationally efficient trust region scheme for policy optimization. The formulation of ACER for continuous action spaces will require further innovations that are advanced in Section 5.

3.1 MULTI-STEP ESTIMATION OF THE STATE-ACTION VALUE FUNCTION
In this paper, we estimate Qπ(xt, at) using Retrace (Munos et al., 2016). (We also experimented with the related tree backup method of Precup et al. (2000) but found Retrace to perform better in practice.) Given a trajectory generated under the behavior policy µ, the Retrace estimator can be expressed recursively as follows1:
Qret(xt, at) = rt + γρ̄t+1[Q ret(xt+1, at+1)−Q(xt+1, at+1)] + γV (xt+1), (5)
1For ease of presentation, we consider only λ = 1 for Retrace.
where ρ̄t is the truncated importance weight, ρ̄t = min {c, ρt} with ρt = π(at|xt)µ(at|xt) , Q is the current value estimate of Qπ, and V (x) = Ea∼πQ(x, a). Retrace is an off-policy, return-based algorithm which has low variance and is proven to converge (in the tabular case) to the value function of the target policy for any behavior policy, see Munos et al. (2016).
The recursive Retrace equation depends on the estimate Q. To compute it, in discrete action spaces, we adopt a convolutional neural network with “two heads” that outputs the estimate Qθv (xt, at), as well as the policy πθ(at|xt). This neural representation is the same as in (Mnih et al., 2016), with the exception that we output the vector Qθv (xt, at) instead of the scalar Vθv (xt). The estimate Vθv (xt) can be easily derived by taking the expectation of Qθv under πθ.
To approximate the policy gradient gmarg, ACER uses Qret to estimate Qπ. As Retrace uses multistep returns, it can significantly reduce bias in the estimation of the policy gradient 2.
To learn the critic Qθv (xt, at), we again use Q ret(xt, at) as a target in a mean squared error loss and update its parameters θv with the following standard gradient:
(Qret(xt, at)−Qθv (xt, at))∇θvQθv (xt, at)). (6) Because Retrace is return-based, it also enables faster learning of the critic. Thus the purpose of the multi-step estimator Qret in our setting is twofold: to reduce bias in the policy gradient, and to enable faster learning of the critic, hence further reducing bias.

3.2 IMPORTANCE WEIGHT TRUNCATION WITH BIAS CORRECTION
The marginal importance weights in Equation (4) can become large, thus causing instability. To safe-guard against high variance, we propose to truncate the importance weights and introduce a correction term via the following decomposition of gmarg:
gmarg =Extat [ρt∇θlog πθ(at|xt)Qπ(xt, at)]
=Ext [ Eat[ρ̄t∇θlog πθ(at|xt)Qπ(xt, at)]+E
a∼π ([ ρt(a)− c ρt(a) ]
+
∇θlog πθ(a|xt)Qπ(xt, a) )] ,(7)
where ρ̄t = min {c, ρt} with ρt = π(at|xt)µ(at|xt) as before. We have also introduced the notation ρt(a) =
π(a|xt) µ(a|xt) , and [x]+ = x if x > 0 and it is zero otherwise. We remind readers that the above
expectations are with respect to the limiting state distribution under the behavior policy: xt ∼ β and at ∼ µ. The clipping of the importance weight in the first term of equation (7) ensures that the variance of the gradient estimate is bounded. The correction term (second term in equation (7)) ensures that our estimate is unbiased. Note that the correction term is only active for actions such that ρt(a) > c. In particular, if we choose a large value for c, the correction term only comes into effect when the variance of the original off-policy estimator of equation (4) is very high. When this happens, our decomposition has the nice property that the truncated weight in the first term is at most c while the correction weight [ ρt(a)−c ρt(a) ] + in the second term is at most 1.
We model Qπ(xt, a) in the correction term with our neural network approximation Qθv (xt, at). This modification results in what we call the truncation with bias correction trick, in this case applied to the function ∇θ log πθ(at|xt)Qπ(xt, at):
ĝmarg =Ext [ Eat [ ρ̄t∇θlog πθ(at|xt)Qret(xt, at) ] +E a∼π ([ ρt(a)− c ρt(a) ]
+
∇θlog πθ(a|xt)Qθv (xt, a) )] .(8)
Equation (8) involves an expectation over the stationary distribution of the Markov process. We can however approximate it by sampling trajectories {x0, a0, r0, µ(·|x0), · · · , xk, ak, rk, µ(·|xk)}
2An alternative to Retrace here is Q(λ) with off-policy corrections (Harutyunyan et al., 2016) which we discuss in more detail in Appendix B.
generated from the behavior policy µ. Here the terms µ(·|xt) are the policy vectors. Given these trajectories, we can compute the off-policy ACER gradient:
ĝacert = ρ̄t∇θ log πθ(at|xt)[Qret(xt, at)− Vθv (xt)]
+ E a∼π ([ ρt(a)− c ρt(a) ]
+
∇θ log πθ(a|xt)[Qθv (xt, a)− Vθv (xt)] ) . (9)
In the above expression, we have subtracted the classical baseline Vθv (xt) to reduce variance.
It is interesting to note that, when c = ∞, (9) recovers (off-policy) policy gradient up to the use of Retrace. When c = 0, (9) recovers an actor critic update that depends entirely on Q estimates. In the continuous control domain, (9) also generalizes Stochastic Value Gradients if c = 0 and the reparametrization trick is used to estimate its second term (Heess et al., 2015).

3.3 EFFICIENT TRUST REGION POLICY OPTIMIZATION
The policy updates of actor-critic methods do often exhibit high variance. Hence, to ensure stability, we must limit the per-step changes to the policy. Simply using smaller learning rates is insufficient as they cannot guard against the occasional large updates while maintaining a desired learning speed. Trust Region Policy Optimization (TRPO) (Schulman et al., 2015a) provides a more adequate solution.
Schulman et al. (2015a) approximately limit the difference between the updated policy and the current policy to ensure safety. Despite the effectiveness of their TRPO method, it requires repeated computation of Fisher-vector products for each update. This can prove to be prohibitively expensive in large domains.
In this section we introduce a new trust region policy optimization method that scales well to large problems. Instead of constraining the updated policy to be close to the current policy (as in TRPO), we propose to maintain an average policy network that represents a running average of past policies and forces the updated policy to not deviate far from this average.
We decompose our policy network in two parts: a distribution f , and a deep neural network that generates the statistics φθ(x) of this distribution. That is, given f , the policy is completely characterized by the network φθ: π(·|x) = f(·|φθ(x)). For example, in the discrete domain, we choose f to be the categorical distribution with a probability vector φθ(x) as its statistics. The probability vector is of course parameterised by θ.
We denote the average policy network as φθa and update its parameters θa “softly” after each update to the policy parameter θ: θa ← αθa + (1− α)θ. Consider, for example, the ACER policy gradient as defined in Equation (9), but with respect to φ:
ĝacert = ρ̄t∇φθ(xt) log f(at|φθ(x))[Qret(xt, at)− Vθv (xt)]
+ E a∼π ([ ρt(a)− c ρt(a) ]
+
∇φθ(xt) log f(at|φθ(x))[Qθv (xt, a)− Vθv (xt)] ) . (10)
Given the averaged policy network, our proposed trust region update involves two stages. In the first stage, we solve the following optimization problem with a linearized KL divergence constraint:
minimize z
1 2 ‖ĝacert − z‖22
subject to ∇φθ(xt)DKL [f(·|φθa(xt))‖f(·|φθ(xt))] T z ≤ δ
(11)
Since the constraint is linear, the overall optimization problem reduces to a simple quadratic programming problem, the solution of which can be easily derived in closed form using the KKT conditions. Letting k = ∇φθ(xt)DKL [f(·|φθa(xt)‖f(·|φθ(xt)], the solution is:
z∗ = ĝacert −max {
0, kT ĝacert − δ ‖k‖22
} k (12)
This transformation of the gradient has a very natural form. If the constraint is satisfied, there is no change to the gradient with respect to φθ(xt). Otherwise, the update is scaled down in the direction
of k, thus effectively lowering rate of change between the activations of the current policy and the average policy network.
In the second stage, we take advantage of back-propagation. Specifically, the updated gradient with respect to φθ, that is z∗, is back-propagated through the network to compute the derivatives with respect to the parameters. The parameter updates for the policy network follow from the chain rule: ∂φθ(x) ∂θ z ∗.
The trust region step is carried out in the space of the statistics of the distribution f , and not in the space of the policy parameters. This is done deliberately so as to avoid an additional back-propagation step through the policy network.
We would like to remark that the algorithm advanced in this section can be thought of as a general strategy for modifying the backward messages in back-propagation so as to stabilize the activations.
Instead of a trust region update, one could alternatively add an appropriately scaled KL cost to the objective function as proposed by Heess et al. (2015). This approach, however, is less robust to the choice of hyper-parameters in our experience.
The ACER algorithm results from a combination of the above ideas, with the precise pseudo-code appearing in Appendix A. A master algorithm (Algorithm 1) calls ACER on-policy to perform updates and propose trajectories. It then calls ACER off-policy component to conduct several replay steps. When on-policy, ACER effectively becomes a modified version of A3C where Q instead of V baselines are employed and trust region optimization is used.

4 RESULTS ON ATARI
We use the Arcade Learning Environment of Bellemare et al. (2013) to conduct an extensive evaluation. We deploy one single algorithm and network architecture, with fixed hyper-parameters, to learn to play 57 Atari games given only raw pixel observations and game rewards. This task is highly demanding because of the diversity of games, and high-dimensional pixel-level observations.
Our experimental setup uses 16 actor-learner threads running on a single machine with no GPUs. We adopt the same input pre-processing and network architecture as Mnih et al. (2015). Specifically, the network consists of a convolutional layer with 32 8× 8 filters with stride 4 followed by another convolutional layer with 64 4× 4 filters with stride 2, followed by a final convolutional layer with 64 3× 3 filters with stride 1, followed by a fully-connected layer of size 512. Each of the hidden layers is followed by a rectifier nonlinearity. The network outputs a softmax policy and Q values.
When using replay, we add to each thread a replay memory that is up to 50 000 frames in size. The total amount of memory used across all threads is thus similar in size to that of DQN (Mnih et al., 2015). For all Atari experiments, we use a single learning rate adopted from an earlier implementation of A3C without further tuning. We do not anneal the learning rates over the course of training as in Mnih et al. (2016). We otherwise adopt the same optimization procedure as in Mnih et al. (2016). Specifically, we adopt entropy regularization with weight 0.001, discount the rewards with γ = 0.99, and perform updates every 20 steps (k = 20 in the notation of Section 2). In all our experiments with experience replay, we use importance weight truncation with c = 10. We consider training ACER both with and without trust region updating as described in Section 3.3. When trust region updating is used, we use δ = 1 and α = 0.99 for all experiments.
To compare different agents, we adopt as our metric the median of the human normalized score over all 57 games. The normalization is calculated such that, for each game, human scores and random scores are evaluated to 1, and 0 respectively. The normalized score for a given game at time t is computed as the average normalized score over the past 1 million consecutive frames encountered until time t. For each agent, we plot its cumulative maximum median score over time. The result is summarized in Figure 1.
The four colors in Figure 1 correspond to four replay ratios (0, 1, 4 and 8) with a ratio of 4 meaning that we use the off-policy component of ACER 4 times after using the on-policy component (A3C). That is, a replay ratio of 0 means that we are using A3C. The solid and dashed lines represent ACER with and without trust region updating respectively. The gray and black curves are the original DQN (Mnih et al., 2015) and Prioritized Replay agent of Schaul et al. (2016) agents respectively.
As shown on the left panel of Figure 1, replay significantly increases data efficiency. We observe that when using the trust region optimizer, the average reward as a function of the number of environmental steps increases with the ratio of replay. This increase has diminishing returns, but with enough replay, ACER can match the performance of the best DQN agents. Moreover, it is clear that the off-policy actor critics (ACER) are much more sample efficient than their on-policy counterpart (A3C).
The right panel of Figure 1 shows that ACER agents perform similarly to A3C when measured by wall clock time. Thus, in this case, it is possible to achieve better data-efficiency without necessarily compromising on computation time. In particular, ACER with a replay ratio of 4 is an appealing alternative to either the prioritized DQN agent or A3C.

5 CONTINUOUS ACTOR CRITIC WITH EXPERIENCE REPLAY
Retrace requires estimates of both Q and V , but we cannot easily integrate over Q to derive V in continuous action spaces. In this section, we propose a solution to this problem in the form of a novel representation for RL, as well as modifications necessary for trust region updating.

5.1 POLICY EVALUATION
Retrace provides a target for learning Qθv , but not for learning Vθv . We could use importance sampling to compute Vθv given Qθv , but this estimator has high variance.
We propose a new architecture which we call Stochastic Dueling Networks (SDNs), inspired by the Dueling networks of Wang et al. (2016), which is designed to estimate both V π and Qπ off-policy while maintaining consistency between the two estimates. At each time step, an SDN outputs a stochastic estimate Q̃θv of Q π and a deterministic estimate Vθv of V π , such that
Q̃θv (xt, at) ∼ Vθv (xt) +Aθv (xt, at)− 1
n
n∑
i=1
Aθv (xt, ui), and ui ∼ πθ(·|xt) (13)
where n is a parameter, see Figure 2. The two estimates are consistent in the sense that Ea∼π(·|xt) [ Eu1:n∼π(·|xt) ( Q̃θv (xt, a) )] = Vθv (xt). Furthermore, we can learn about V π by learn-
ing Q̃θv . To see this, assume we have learned Q π perfectly such that Eu1:n∼π(·|xt) ( Q̃θv (xt, at) ) = Qπ(xt, at), then Vθv (xt) = Ea∼π(·|xt) [ Eu1:n∼π(·|xt) ( Q̃θv (xt, a) )] = Ea∼π(·|xt) [Qπ(xt, a)] =
V π(xt). Therefore, a target on Q̃θv (xt, at) also provides an error signal for updating Vθv .
In addition to SDNs, however, we also construct the following novel target for estimating V π:
V target(xt) = min { 1, π(at|xt) µ(at|xt) }( Qret(xt, at)−Qθv (xt, at) ) + Vθv (xt). (14)
The above target is also derived via the truncation and bias correction trick; for more details, see Appendix D.
Finally, when estimating Qret in continuous domains, we implement a slightly different formulation of the truncated importance weights ρ̄t = min { 1, ( π(at|xt) µ(at|xt) ) 1 d } , where d is the dimensionality of
the action space. Although not essential, we have found this formulation to lead to faster learning.

5.2 TRUST REGION UPDATING
To adopt the trust region updating scheme (Section 3.3) in the continuous control domain, one simply has to choose a distribution f and a gradient specification ĝacert suitable for continuous action spaces.
For the distribution f , we choose Gaussian distributions with fixed diagonal covariance and mean φθ(x).
To derive ĝacert in continuous action spaces, consider the ACER policy gradient for the stochastic dueling network, but with respect to φ:
gacert = Ext [ Eat [ ρ̄t∇φθ(xt) log f(at|φθ(xt))(Qopc(xt, at)− Vθv (xt)) ]
+ E a∼π ([ ρt(a)− c ρt(a) ]
+
(Q̃θv (xt, a)− Vθv (xt))∇φθ(xt) log f(a|φθ(xt)) )] . (15)
In the above definition, we are using Qopc instead of Qret. Here, Qopc(xt, at) is the same as Retrace with the exception that the truncated importance ratio is replaced with 1 (Harutyunyan et al., 2016). Please refer to Appendix B an expanded discussion on this design choice. Given an observation xt, we can sample a′t ∼ πθ(·|xt) to obtain the following Monte Carlo approximation
ĝacert = ρ̄t∇φθ(xt) log f(at|φθ(xt))(Qopc(xt, at)− Vθv (xt))
+
[ ρt(a ′ t)− c
ρt(a′t)
]
+
(Q̃θv (xt, a ′ t)− Vθv (xt))∇φθ(xt) log f(a′t|φθ(xt)). (16)
Given f and ĝacert , we apply the same steps as detailed in Section 3.3 to complete the update.
The precise pseudo-code of ACER algorithm for continuous spaces results is presented in Appendix A.

6 RESULTS ON MUJOCO
We evaluate our algorithms on 6 continuous control tasks, all of which are simulated using the MuJoCo physics engine (Todorov et al., 2012). For descriptions of the tasks, please refer to Appendix E.1. Briefly, the tasks with action dimensionality in brackets are: cartpole (1D), reacher (3D), cheetah (6D), fish (5D), walker (6D) and humanoid (21D). These tasks are illustrated in Figure 3.
To benchmark ACER for continuous control, we compare it to its on-policy counterpart both with and without trust region updating. We refer to these two baselines as A3C and Trust-A3C. Additionally, we also compare to a baseline with replay where we truncate the importance weights over trajectories as in (Wawrzyński, 2009). For a detailed description of this baseline, please refer to Appendix E. Again, we run this baseline both with and without trust region updating, and refer to these choices as Trust-TIS and TIS respectively. Last but not least, we refer to our proposed approach with SDN and trust region updating as simply ACER. All five setups are implemented in the asynchronous A3C framework.
All the aforementioned setups share the same network architecture that computes the policy and state values. We maintain an additional small network that computes the stochastic A values in the case of ACER. We use n = 5 (using the notation in Equation (13)) in all SDNs. Instead of mixing on-policy and replay learning as done in the Atari domain, ACER for continuous actions is entirely off-policy, with experiences generated from the simulator (4 times on average). When using replay, we add to each thread a replay memory that is 5, 000 frames in size and perform updates every 50 steps (k = 50 in the notation of Section 2). The rate of the soft updating (α as in Section 3.3) is set to 0.995 in all setups involving trust region updating. The truncation threshold c is set to 5 for ACER.
We use diagonal Gaussian policies with fixed diagonal covariances where the diagonal standard deviation is set to 0.3. For all setups, we sample the learning rates log-uniformly in the range [10−4, 10−3.3]. For setups involving trust region updating, we also sample δ uniformly in the range [0.1, 2]. With all setups, we use 30 sampled hyper-parameter settings.
The empirical results for all continuous control tasks are shown Figure 3, where we show the mean and standard deviation of the best 5 out of 30 hyper-parameter settings over which we searched 3. For sensitivity analyses with respect to the hyper-parameters, please refer to Figures 5 and 6 in the Appendix.
In continuous control, ACER outperforms the A3C and truncated importance sampling baselines by a very significant margin.
Here, we also find that the proposed trust region optimization method can result in huge improvements over the baselines. The high-dimensional continuous action policies are much harder to optimize than the small discrete action policies in Atari, and hence we observe much higher gains for trust region optimization in the continuous control domains. In spite of the improvements brought in by trust region optimization, ACER still outperforms all other methods, specially in higher dimensions.

6.1 ABLATIONS
To further tease apart the contributions of the different components of ACER, we conduct an ablation analysis where we individually remove Retrace / Q(λ) off-policy correction, SDNs, trust region, and truncation with bias correction from the algorithm. As shown in Figure 4, Retrace and offpolicy correction, SDNs, and trust region are critical: removing any one of them leads to a clear deterioration of the performance. Truncation with bias correction did not alter the results in the Fish and Walker2d tasks. However, in Humanoid, where the dimensionality of the action space is much higher, including truncation and bias correction brings a significant boost which makes the originally kneeling humanoid stand. Presumably, the high dimensionality of the action space increases the variance of the importance weights which makes truncation with bias correction important. For more details on the experimental setup please see Appendix E.4.

7 THEORETICAL ANALYSIS
Retrace is a very recent development in reinforcement learning. In fact, this work is the first to consider Retrace in the policy gradients setting. For this reason, and given the core role that Retrace plays in ACER, it is valuable to shed more light on this technique. In this section, we will prove that Retrace can be interpreted as an application of the importance weight truncation and bias correction trick advanced in this paper.
Consider the following equation:
Qπ(xt, at) = Ext+1at+1 [rt + γρt+1Qπ(xt+1, at+1)] . (17)
If we apply the weight truncation and bias correction trick to the above equation we obtain
Qπ(xt, at) = Ext+1at+1 [ rt + γρ̄t+1Q
π(xt+1, at+1) + γ E a∼π ([ ρt+1(a)− c ρt+1(a) ]
+
Qπ(xt+1, a) )] .
(18) By recursively expanding Qπ as in Equation (18), we can represent Qπ(x, a) as:
Qπ(x, a) = Eµ
 ∑
t≥0
γt
( t∏
i=1
ρ̄i )( rt + γ E
b∼π ([ ρt+1(b)− c ρt+1(b) ]
+
Qπ(xt+1, b) ))  . (19)
The expectation Eµ is taken over trajectories starting from x with actions generated with respect to µ. When Qπ is not available, we can replace it with our current estimate Q to get a return-based
3 For videos of the policies learned with ACER, please see: https://www.youtube.com/watch?v= NmbeQYoVv5g&list=PLkmHIkhlFjiTlvwxEnsJMs3v7seR5HSP-.
esitmate of Qπ . This operation also defines an operator:
BQ(x, a) = Eµ
 ∑
t≥0
γt
( t∏
i=1
ρ̄i )( rt + γ E
b∼π ([ ρt+1(b)− c ρt+1(b) ]
+
Q(xt+1, b)
))  . (20)
In the following proposition, we show that B is a contraction operator with a unique fixed point Qπ and that it is equivalent to the Retrace operator. Proposition 1. The operator B is a contraction operator such that ‖BQ−Qπ‖∞ ≤ γ‖Q−Qπ‖∞ and B is equivalent to Retrace.
The above proposition not only shows an alternative way of arriving at the same operator, but also provides a different proof of contraction for Retrace. Please refer to Appendix C for the regularization conditions and proof of the above proposition.
Finally, B, and therefore Retrace, generalizes both the Bellman operator T π and importance sampling. Specifically, when c = 0, B = T π and when c = ∞, B recovers importance sampling; see Appendix C.

8 CONCLUDING REMARKS
We have introduced a stable off-policy actor critic that scales to both continuous and discrete action spaces. This approach integrates several recent advances in RL in a principle manner. In addition, it integrates three innovations advanced in this paper: truncated importance sampling with bias correction, stochastic dueling networks and an efficient trust region policy optimization method.
We showed that the method not only matches the performance of the best known methods on Atari, but that it also outperforms popular techniques on several continuous control problems.
The efficient trust region optimization method advanced in this paper performs remarkably well in continuous domains. It could prove very useful in other deep learning domains, where it is hard to stabilize the training process.

ACKNOWLEDGMENTS
We are very thankful to Marc Bellemare, Jascha Sohl-Dickstein, and Sébastien Racaniere for proofreading and valuable suggestions.

A ACER PSEUDO-CODE FOR DISCRETE ACTIONS
Algorithm 1 ACER for discrete actions (master algorithm) // Assume global shared parameter vectors θ and θv . // Assume ratio of replay r. repeat
Call ACER on-policy, Algorithm 2. n← Possion(r) for i ∈ {1, · · · , n} do
Call ACER off-policy, Algorithm 2. end for
until Max iteration or time reached.
Algorithm 2 ACER for discrete actions Reset gradients dθ ← 0 and dθv ← 0. Initialize parameters θ′ ← θ and θ′v ← θv . if not On-Policy then
Sample the trajectory {x0, a0, r0, µ(·|x0), · · · , xk, ak, rk, µ(·|xk)} from the replay memory. else
Get state x0 end if for i ∈ {0, · · · , k} do
Compute f(·|φθ′(xi)), Qθ′v (xi, ·) and f(·|φθa(xi)). if On-Policy then
Perform ai according to f(·|φθ′(xi)) Receive reward ri and new state xi+1 µ(·|xi)← f(·|φθ′(xi))
end if ρ̄i ← min { 1,
f(ai|φθ′ (xi)) µ(ai|xi)
} .
end for
Qret ← { 0 for terminal xk∑ aQθ′v (xk, a)f(a|φθ′(xk)) otherwise for i ∈ {k − 1, · · · , 0} do Qret ← ri + γQret Vi ← ∑ aQθ′v (xi, a)f(a|φθ′(xi))
Computing quantities needed for trust region updating:
g ← min {c, ρi(ai)}∇φθ′ (xi) log f(ai|φθ′(xi))(Q ret − Vi)
+ ∑ a [ 1− c ρi(a) ] + f(a|φθ′(xi))∇φθ′ (xi) log f(a|φθ′(xi))(Qθ′v (xi, ai)− Vi)
k ← ∇φθ′ (xi)DKL [f(·|φθa(xi)‖f(·|φθ′(xi)]
Accumulate gradients wrt θ′: dθ′ ← dθ′ + ∂φθ′ (xi) ∂θ′
( g −max { 0, k
T g−δ ‖k‖22
} k )
Accumulate gradients wrt θ′v: dθv ← dθv +∇θ′v (Q ret −Qθ′v (xi, a)) 2 Update Retrace target: Qret ← ρ̄i ( Qret −Qθ′v (xi, ai) ) + Vi
end for Perform asynchronous update of θ using dθ and of θv using dθv . Updating the average policy network: θa ← αθa + (1− α)θ
B Q(λ) WITH OFF-POLICY CORRECTIONS
Given a trajectory generated under the behavior policy µ, the Q(λ) with off-policy corrections estimator (Harutyunyan et al., 2016) can be expressed recursively as follows:
Qopc(xt, at) = rt + γ[Q opc(xt+1, at+1)−Q(xt+1, at+1)] + γV (xt+1). (21)
Notice that Qopc(xt, at) is the same as Retrace with the exception that the truncated importance ratio is replaced with 1.
Algorithm 3 ACER for Continuous Actions Reset gradients dθ ← 0 and dθv ← 0. Initialize parameters θ′ ← θ and θ′v ← θv . Sample the trajectory {x0, a0, r0, µ(·|x0), · · · , xk, ak, rk, µ(·|xk)} from the replay memory. for i ∈ {0, · · · , k} do
Compute f(·|φθ′(xi)), Vθ′v (xi), Q̃θ′v (xi, ai), and f(·|φθa(xi)). Sample a′i ∼ f(·|φθ′(xi)) ρi ← f(ai|φθ′ (xi))µ(ai|xi) and ρ ′ i ← f(a′i|φθ′ (xi)) µ(a′i|xi)
ci ← min { 1, (ρi) 1 d } .
end for
Qret ← { 0 for terminal xk Vθ′v (xk) otherwise Qopc ← Qret for i ∈ {k − 1, · · · , 0} do Qret ← ri + γQret Qopc ← ri + γQopc Computing quantities needed for trust region updating:
g ← min {c, ρi}∇φθ′ (xi) log f(ai|φθ′(xi)) ( Qopc(xi, ai)− Vθ′v (xi) ) + [ 1− c
ρ′i ] + (Q̃θ′v (xi, a ′ i)− Vθ′v (xi))∇φθ′ (xi) log f(a ′ i|φθ′(xi))
k ← ∇φθ′ (xi)DKL [f(·|φθa(xi)‖f(·|φθ′(xi)]
Accumulate gradients wrt θ: dθ ← dθ + ∂φθ′ (xi) ∂θ′
( g −max { 0, k
T g−δ ‖k‖22
} k )
Accumulate gradients wrt θ′v: dθv ← dθv + (Qret − Q̃θ′v (xi, ai))∇θ′v Q̃θ′v (xi, ai) dθv ← dθv + min {1, ρi} ( Qret(xt, ai)− Q̃θ′v (xt, ai) ) ∇θ′vVθ′v (xi)
Update Retrace target: Qret ← ci ( Qret − Q̃θ′v (xi, ai) ) + Vθ′v (xi)
Update Retrace target: Qopc ← ( Qopc − Q̃θ′v (xi, ai) ) + Vθ′v (xi)
end for Perform asynchronous update of θ using dθ and of θv using dθv . Updating the average policy network: θa ← αθa + (1− α)θ
Because of the lack of the truncated importance ratio, the operator defined by Qopc is only a contraction if the target and behavior policies are close to each other (Harutyunyan et al., 2016). Q(λ) with off-policy corrections is therefore less stable compared to Retrace and unsafe for policy evaluation.
Qopc, however, could better utilize the returns as the traces are not cut by the truncated importance weights. As a result,Qopc could be used efficiently to estimateQπ in policy gradient (e.g. in Equation (16)). In our continuous control experiments, we have found that Qopc leads to faster learning.

C RETRACE AS TRUNCATED IMPORTANCE SAMPLING WITH BIAS CORRECTION
For the purpose of proving proposition 1, we assume our environment to be a Markov Decision Process (X ,A, γ, P, r). We restrict X to be a finite state space. For notational simplicity, we also restrict A to be a finite action space. P : X ,A → X defines the state transition probabilities and r : X ,A → [−RMAX, RMAX] defines a reward function. Finally, γ ∈ [0, 1) is the discount factor.
Proof of proposition 1. First we show that B is a contraction operator. |BQ(x, a)−Qπ(x, a)|
= ∣∣∣∣∣∣ Eµ  ∑
t≥0
γt
( t∏
i=1
ρ̄i )( γ E b∼π ([ ρt+1(b)− c ρt+1(b) ]
+ (Q(xt+1, b)−Qπ(xt+1, b)) ))  ∣∣∣∣∣∣
≤ Eµ
 ∑
t≥0
γt
( t∏
i=1
ρ̄i )[ γ E b∼π ([ ρt+1(b)− c ρt+1(b) ]
+
|Q(xt+1, b)−Qπ(xt+1, b)| )] 
≤ Eµ
 ∑
t≥0
γt
( t∏
i=1
ρ̄i )( γ(1− P̄t+1) sup
b |Q(xt+1, b)−Qπ(xt+1, b)|
)  (22)
Where P̄t+1 = 1− E b∼π [[ ρt+1(b)−c ρt+1(b) ] + ] = E b∼µ [ρ̄t+1(b)]. The last inequality in the above equation is due to Hölder’s inequality.
(22) ≤ sup x,b |Q(x, b)−Qπ(x, b)|Eµ
 ∑
t≥0
γt
( t∏
i=1
ρ̄i ) ( γ(1− P̄t+1) )  
= sup x,b |Q(x, b)−Qπ(x, b)|Eµ
 γ ∑
t≥0
γt
( t∏
i=1
ρ̄i
) − ∑
t≥0
γt
( t∏
i=1
ρ̄i ) ( γP̄t+1 )  
= sup x,b |Q(x, b)−Qπ(x, b)|Eµ
 γ ∑
t≥0
γt
( t∏
i=1
ρ̄i
) − ∑
t≥0
γt+1
( t+1∏
i=1
ρ̄i
) 
= sup x,b |Q(x, b)−Qπ(x, b)| (γC − (C − 1))
whereC = ∑ t≥0 γ
t (∏t
i=1 ρ̄i ) . SinceC ≥∑0t=0 γt (∏t i=1 ρ̄i ) = 1, we have that γC−(C−1) ≤
γ. Therefore, we have shown that B is a contraction operator. Now we show that B is the same as Retrace. By apply the trunction and bias correction trick, we have
E b∼π [Q(xt+1, b)] = E b∼µ [ρ̄t+1(b)Q(xt+1, b)] + E b∼π ([ ρt+1(b)− c ρt+1(b) ]
+
Q(xt+1, b)
) . (23)
By adding and subtracting the two sides of Equation (23) inside the summand of Equation (20), we have BQ(x, a) = Eµ [∑
t≥0
γt ( t∏
i=1
ρ̄i )[ rt + γ E
b∼π ([ ρt+1(b)− c ρt+1(b) ]
+
Q(xt+1, b)
) +γ E
b∼π [Q(xt+1, b)]
−γ E b∼µ [ρ̄t+1(b)Q(xt+1, b)]− γ E b∼π ([ ρt+1(b)− c ρt+1(b) ]
+
Q(xt+1, b)
)]]
= Eµ
 ∑
t≥0
γt
( t∏
i=1
ρ̄i )( rt + γ E
b∼π [Q(xt+1, b)]− γ E b∼µ [ρ̄t+1(b)Q(xt+1, b)]
) 
= Eµ
 ∑
t≥0
γt
( t∏
i=1
ρ̄i )( rt + γ E
b∼π [Q(xt+1, b)]− γρ̄t+1Q(xt+1, at+1)
) 
= Eµ
 ∑
t≥0
γt
( t∏
i=1
ρ̄i )( rt + γ E
b∼π [Q(xt+1, b)]−Q(xt, at)
) +Q(x, a) = RQ(x, a)
In the remainder of this appendix, we show that B generalizes both the Bellman operator and importance sampling. First, we reproduce the definition of B:
BQ(x, a) = Eµ
 ∑
t≥0
γt
( t∏
i=1
ρ̄i )( rt + γ E
b∼π ([ ρt+1(b)− c ρt+1(b) ]
+
Q(xt+1, b)
))  .
When c = 0, we have that ρ̄i = 0 ∀i. Therefore only the first summand of the sum remains:
BQ(x, a) = Eµ [ rt + γ E
b∼π (Q(xt+1, b))
] .
In this case B = T . When c =∞, the compensation term disappears and ρ̄i = ρi ∀i:
BQ(x, a) = Eµ
 ∑
t≥0
γt
( t∏
i=1
ρi )( rt + γ E
b∼π (0×Q(xt+1, b))
)  = Eµ  ∑
t≥0
γt
( t∏
i=1
ρi ) rt   .
In this case B is the same operator defined by importance sampling.
D DERIVATION OF V target
By using the truncation and bias correction trick, we can derive the following:
V π(xt) = E a∼µ
[ min { 1, π(a|xt) µ(a|xt) } Qπ(xt, a) ] + E a∼π ([ ρt(a)− 1 ρt(a) ]
+
Qπ(xt+1, a) ) .
We, however, cannot use the above equation as a target as we do not have access to Qπ . To derive a target, we can take a Monte Carlo approximation of the first expectation in the RHS of the above equation and replace the first occurrence of Qπ with Qret and the second with our current neural net approximation Qθv (xt, ·):
V targetpre (xt) := min { 1, π(at|xt) µ(at|xt) } Qret(xt, at) + E a∼π ([ ρt(a)− 1 ρt(a) ]
+
Qθv (xt, a)
) . (24)
Through the truncation and bias correction trick again, we have the following identity:
E a∼π [Qθv (xt, a)] = E a∼µ
[ min { 1, π(a|xt) µ(a|xt) } Qθv (xt, a) ] + E a∼π ([ ρt(a)− 1 ρt(a) ]
+
Qθv (xt, a)
) . (25)
Adding and subtracting both sides of Equation (25) to the RHS of (24) while taking a Monte Carlo approximation, we arrive at V target(xt):
V target(xt) := min { 1, π(at|xt) µ(at|xt) }( Qret(xt, at)−Qθv (xt, at) ) + Vθv (xt).

E CONTINUOUS CONTROL EXPERIMENTS
E.1 DESCRIPTION OF THE CONTINUOUS CONTROL PROBLEMS
Our continuous control tasks were simulated using the MuJoCo physics engine (Todorov et al. (2012)). For all experiments we considered an episodic setup with an episode length of T = 500 steps and a discount factor of 0.99.
Cartpole swingup This is an instance of the classic cart-pole swing-up task. It consists of a pole attached to a cart running on a finite track. The agent is required to balance the pole near the center of the track by applying a force to the cart only. An episode starts with the pole at a random angle and zero velocity. A reward zero is given except when the pole is approximately upright (within ±5 deg) and the track approximately in the center of the track (±0.05) for a track length of 2.4. The observations include position and velocity of the cart, angle and angular velocity of the pole. a sine/cosine of the angle, the position of the tip of the pole, and Cartesian velocities of the pole. The dimension of the action space is 1.
Reacher3 The agent needs to control a planar 3-link robotic arm in order to minimize the distance between the end effector of the arm and a target. Both arm and target position are chosen randomly at the beginning of each episode. The reward is zero except when the tip of the arm is within 0.05 of the target, where it is one. The 8-dimensional observation consists of the angles and angular velocity of all joints as well as the displacement between target and the end effector of the arm. The 3-dimensional action are the torques applied to the joints.
Cheetah The Half-Cheetah (Wawrzyński (2009); Heess et al. (2015)) is a planar locomotion task where the agent is required to control a 9-DoF cheetah-like body (in the vertical plane) to move in the direction of the x-axis as quickly as possible. The reward is given by the velocity along the x-axis and a control cost: r = vx + 0.1‖a‖2. The observation vector consists of the z-position of the torso and its x, z velocities as well as the joint angles and angular velocities. The action dimension is 6.
Fish The goal of this task is to control a 13-DoF fish-like body to swim to a random target in 3D space. The reward is given by the distance between the head of the fish and the target, a small penalty for the body not being upright, and a control cost. At the beginning of an episode the fish is initialized facing in a random direction relative to the target. The 24-dimensional observation is given by the displacement between the fish and the target projected onto the torso coordinate frame, the joint angles and velocities, the cosine of the angle between the z-axis of the torso and the world z-axis, and the velocities of the torso in the torso coordinate frame. The 5-dimensional actions control the position of the side fins and the tail.
Walker The 9-DoF planar walker is inspired by (Schulman et al. (2015a)) and is required to move forward along the x-axis as quickly as possible without falling. The reward consists of the x-velocity of the torso, a quadratic control cost, and terms that penalize deviations of the torso from the preferred height and orientation (i.e. terms that encourage the walker to stay standing and upright). The 24-dimensional observation includes the torso height, velocities of all DoFs, as well as sines and cosines of all body orientations in the x-z plane. The 6-dimensional action controls the torques applied at the joints. Episodes are terminated early with a negative reward when the torso exceeds upper and lower limits on its height and orientation.
Humanoid The humanoid is a 27 degrees-of-freedom body with 21 actuators (21 action dimensions). It is initialized lying on the ground in a random configuration and the task requires it to achieve a standing position. The reward function penalizes deviations from the height of the head when standing, and includes additional terms that encourage upright standing, as well as a quadratic action penalty. The 94 dimensional observation contains information about joint angles and velocities and several derived features reflecting the body’s pose.
E.2 UPDATE EQUATIONS OF THE BASELINE TIS
The baseline TIS follows the following update equations,
updates to the policy: min { 5, ( k−1∏
i=0
ρt+i
)}[ k−1∑
i=0
γirt+i + γ kVθv (xk+t)− Vθv (xt) ] ∇θ log πθ(at|xt),
updates to the value: min { 5, ( k−1∏
i=0
ρt+i
)}[ k−1∑
i=0
γirt+i + γ kVθv (xk+t)− Vθv (xt) ] ∇θvVθv (xt).
The baseline Trust-TIS is appropriately modified according to the trust region update described in Section 3.3.
E.3 SENSITIVITY ANALYSIS
In this section, we assess the sensitivity of ACER to hyper-parameters. In Figures 5 and 6, we show, for each game, the final performance of our ACER agent versus the choice of learning rates, and the trust region constraint δ respectively.
Note, as we are doing random hyper-parameter search, each learning rate is associated with a random δ and vice versa. It is therefore difficult to tease out the effect of either hyper-parameter independently.
We observe, however, that ACER is not very sensitive to the hyper-parameters overall. In addition, smaller δ’s do not seem to adversely affect the final performance while larger δ’s do in domains of higher action dimensionality. Similarly, smaller learning rates perform well while bigger learning rates tend to hurt final performance in domains of higher action dimensionality.
E.4 EXPERIMENTAL SETUP OF ABLATION ANALYSIS
For the ablation analysis, we use the same experimental setup as in the continuous control experiments while removing one component at a time.
To evaluate the effectiveness of Retrace/Q(λ) with off-policy correction, we replace both with importance sampling based estimates (following Degris et al. (2012)) which can be expressed recursively: Rt = rt + ρt+1Rt+1.
To evaluate the Stochastic Dueling Networks, we replace it with two separate networks: one computing the state values and the other Q values. Given Qret(xt, at), the naive way of estimating the state values is to use the following update rule:
( ρtQ ret(xt, at)− Vθv (xt) ) ∇θvVθv (xt).
The above update rule, however, suffers from high variance. We consider instead the following update rule:
ρt ( Qret(xt, at)− Vθv (xt) ) ∇θvVθv (xt)
which has markedly lower variance. We update our Q estimates as before.
To evaluate the effects of the truncation and bias correction trick, we change our c parameter (see Equation (16)) to∞ so as to use pure importance sampling.
","This paper presents an actor-critic deep reinforcement learning agent with experience replay that is stable, sample efficient, and performs remarkably well on challenging environments, including the discrete 57-game Atari domain and several continuous control problems. To achieve this, the paper introduces several innovations, including truncated importance sampling with bias correction, stochastic dueling network architectures, and a new trust region policy optimization method.",ICLR 2017 conference submission,True,,"First of all, thanks for this excellent work.

My question is about eq. 4. In Degris et al (2012) the policy gradient is computed as the expectation under the off-policy behavior of \rho(s_t, a_t) \psi(s_t, a_t) (R_t^\lambda - V(s_t))
With \rho(s_t,a_t) = \pi(a_t | s_t) / \mu(a_t | s_t) and \psi(s_t, a_t) = \grad_\theta ( log \pi (a_t | s_t) ) /  \pi (a_t | s_t)
The last division by \pi (a_t | s_t) is missing in equation (4).

Am I mistaken or is the reference wrong?
Thanks for your time.

---

pros:
 - set of contributions leading to SOTA for sample complexity wrt Atari (discrete) and continuous domain problems
 - significant experimental analysis
 - long all-in-one paper
 
 cons:
 - builds on existing ideas, although ablation analysis shows each to be essential
 - long paper
 
The PCs believe this paper will be a good contribution to the conference track.

---

Dear reviewers, we would really appreciate it if you can take a look at the paper again in light of our replies, the updated paper, and the comments from Xi Chen. Thanks very much for your time!

---

This submission has a couple important contributions and it'd be actually easy to split it into 2 strong papers.

Roughly:
1. Especially in deep rl, policy gradient methods have suffered from worse sample complexity compared to value-based methods like DQN. Learning a critic to improve sample efficiency for policy gradient methods is a straightforward idea but this is the first convincing demonstration (by carefully combing different elements like Retrace(\lambda) and experience replay). This represents an important step towards making policy gradient methods more sample efficient and alone, I believe, merits acceptance. It's worth noting that there is another ICLR submission Q-Prop (

---

We thank the three reviewers. The one common concern is ablations. This paper proposes several new ideas, and then goes on to combine these ideas.  

To answer the reviewers concerns about ablations, we added a new figure (Figure 4). This is an extremely important figure and we urge the reviewers and readers to consult it as it should answer any concerns and highlight the value of the many contributions made in this paper. The figure shows that each ingredient (Retrace/Q-lambda with off-policy correction, stochastic dueling nets, and the NEW trust region method) on its own leads to a massive improvement. Likewise, truncation with bias correction plays an important role for large action spaces (control of humanoid). This figure indicates that this paper is not about making 4 small contributions and combining them. Rather it is about making 4 important contributions, which are all essential to obtain a stable, scalable, general, off-policy actor critic. Attaining this has been a holy grail, and this paper shows how to do it.

Given our good results, we could easily have written several papers; one for each contribution. Instead, we chose to do the honest thing and write a single solid 20-page paper aimed at truly building powerful deep RL agents for both continuous and discrete action spaces. The paper also presents novel theoretical results for RL, and a very comprehensive experimental study. 

We did not want to claim state-of-the-art on Atari because this often depends on how one chooses to measure what should be state-of-the-art (eg sample complexity, highest median, highest mean, etc.). But clearly, in terms of median, ACER with 1 replay achieves a higher median score that any previously reported result. Note that this result is not just for a few games, but for the entire set of 57 games. The UNREAL agent submitted to this conference is the only method we know that achieves a higher median, but it does so by adding auxiliary tasks to A3C and massive hyper-parameter sweeps. We could also add auxiliary tasks to ACER and do hyper-parameter sweeps to further improve it, but this is left for future work as we wanted to focus on designing a powerful core RL agent.

We hope this reply and in particular the ablations clearly answer your concerns. With 3 6’s this thorough paper will be rejected despite the several novel contributions it makes, new theoretical analysis, and excellent results on a comprehensive set of tasks. We hope you take the ablations and this reply into consideration to choose your final scores.

---

This paper studies the off-policy learning of actor-critic with experience replay. This is an important and challenging problem in order to improve the sample efficiency of the reinforcement learning algorithms. The paper attacks the problem by introducing a new way to truncate importance weight, a modified trust region optimization, and by combining retrace method. The combination of the above techniques performs well on Atari and MuJoCo in terms of improving sample efficiency. My main comment is how does each of the technique contribute to the performance gain? If some experiments could be carried out to evaluate the separate gains from these tricks, it would be helpful.

---

This paper introduces an actor-critic deep RL approach with experience replay, which combines truncated importance sampling and trust region policy optimization. The paper also proposes a new method called stochastic duelling networks to estimate the critic for continuous action spaces. The method is applied to Atari games and continuous control problems, where it yields performance comparable to state-of-the-art methods.

As mentioned in the beginning of the paper, the main contributions of this work lies in combining 1) truncated importance sampling with retrace, 2) trust region policy optimization, and 3) stochastic duelling networks. These improvements work well and may be beneficial to future work in RL.

However, each improvement appears to be quite incremental. Moreover, the ACER framework seems much more complex and fragile to implement compared to the standard deep q-learning with prioritized replay (which appears to perform just as well on Atari games). So for the Atari domain, I would still put my money on prioritized replay due to its simplicity. Thirdly, improving sample efficiency for deep RL is a laudable goal, but really this goal should be pursued in a problem setting where sample efficiency is important. Unfortunately, the paper only evaluates sample efficiency in the Atari and continuous control tasks domain; two domains where sample efficiency is not important. Thus, it is not clear that the proposed method ACER will generalize to problems where we really care about sample efficiency.

Some technical aspects which need clarifications:
- For Retrace, I assume that you compute recursively $Q^{ret}$ starting from the end of each trajectory? Please comment on this.
- It's not clear to me how to derive eq. (7). Is an approximation (double tilde) sign missing?
- In section 3.1 the paper argued that $Q^{ret}$ gives a lower-variance estimate of the action-value function. Then why not use it in eq. (8) for the bias correction term?
- The paper states that it uses a replay memory of 50000 frames, so that across threads it is comparable in size to previous work. However, for each thread this is much smaller compared to earlier experiments on Atari games. For example, one million experience replay transitions were used in the paper ""Prioritized Experience Replay"" by Schaul et al. This may have a huge impact on performance of the models (both for ACER and for the competing models). In order to properly assess the improvements of ACER over previous work, the authors need to also experiment with larger experience replay memories.


Other comments:
- Please move Section 7 to the appendix.
- ""Moreover, when using small values of lambda to reduce variance, occasional large importance weights can still cause instability"": I think what is meant is using *large* values of lambda.
- Above eq. (6) mention that the squared error is used.
- Missing a ""t"" subscript at the beginning of eq. (9)?
- It was hard to understand the stochastic duelling networks. Please rephrase this part.
- Please clarify this sentence ""To compare different agents, we adopt as our metric the median of the human normalized score over all 57 games.""
- Figure 2 (Bottom): Please add label to vertical axes.

---

The paper looks at several innovations for deep RL, and evaluates their effect on solving games in the Atari domain.  The paper reads a bit like a laundry list of the researcher’s latest tricks.  It is written clearly enough, but lacks a compelling message.  I expect the work will be interesting to people already implementing deep RL methods, but will probably not get much attention from the broader community.

The claims on p.1 suggest the approach is stable and sample efficience, and so I expected to see some theoretical analysis with respect to these properties. But this is an empirical claim; it would help to clarify that in the abstract.

The proposed innovations are based on sound methods.  It is particularly nice to see the same approach working for both discrete and continuous domains.

The paper has reasonably complete empirical results. It would be nice to see confidence intervals on more of the plots. Also, the results don’t really tease apart the effect of each of the various innovations, so it’s harder to understand the impact of each piece and to really get intuition, for example about why ACER outperforms A3C.  Also, it wasn’t clear to me why you only get matching results on discrete tasks, but get state-of-the-art on continuous tasks.

The paper has good coverage of the related literature. It is nice to see this work draw more attention to Retrace, including the theoretical characterization in Sec.7.

---

Should the inside summations of equation (3) should go from i = 0 to (k - t)?

---

Dear Authors,

Please resubmit your paper in the ICLR 2017 format with the correct marging spacing for your submission to be considered. Thank you!

---

First of all, thanks for this excellent work.

My question is about eq. 4. In Degris et al (2012) the policy gradient is computed as the expectation under the off-policy behavior of \rho(s_t, a_t) \psi(s_t, a_t) (R_t^\lambda - V(s_t))
With \rho(s_t,a_t) = \pi(a_t | s_t) / \mu(a_t | s_t) and \psi(s_t, a_t) = \grad_\theta ( log \pi (a_t | s_t) ) /  \pi (a_t | s_t)
The last division by \pi (a_t | s_t) is missing in equation (4).

Am I mistaken or is the reference wrong?
Thanks for your time.

---

pros:
 - set of contributions leading to SOTA for sample complexity wrt Atari (discrete) and continuous domain problems
 - significant experimental analysis
 - long all-in-one paper
 
 cons:
 - builds on existing ideas, although ablation analysis shows each to be essential
 - long paper
 
The PCs believe this paper will be a good contribution to the conference track.

---

Dear reviewers, we would really appreciate it if you can take a look at the paper again in light of our replies, the updated paper, and the comments from Xi Chen. Thanks very much for your time!

---

This submission has a couple important contributions and it'd be actually easy to split it into 2 strong papers.

Roughly:
1. Especially in deep rl, policy gradient methods have suffered from worse sample complexity compared to value-based methods like DQN. Learning a critic to improve sample efficiency for policy gradient methods is a straightforward idea but this is the first convincing demonstration (by carefully combing different elements like Retrace(\lambda) and experience replay). This represents an important step towards making policy gradient methods more sample efficient and alone, I believe, merits acceptance. It's worth noting that there is another ICLR submission Q-Prop (

---

We thank the three reviewers. The one common concern is ablations. This paper proposes several new ideas, and then goes on to combine these ideas.  

To answer the reviewers concerns about ablations, we added a new figure (Figure 4). This is an extremely important figure and we urge the reviewers and readers to consult it as it should answer any concerns and highlight the value of the many contributions made in this paper. The figure shows that each ingredient (Retrace/Q-lambda with off-policy correction, stochastic dueling nets, and the NEW trust region method) on its own leads to a massive improvement. Likewise, truncation with bias correction plays an important role for large action spaces (control of humanoid). This figure indicates that this paper is not about making 4 small contributions and combining them. Rather it is about making 4 important contributions, which are all essential to obtain a stable, scalable, general, off-policy actor critic. Attaining this has been a holy grail, and this paper shows how to do it.

Given our good results, we could easily have written several papers; one for each contribution. Instead, we chose to do the honest thing and write a single solid 20-page paper aimed at truly building powerful deep RL agents for both continuous and discrete action spaces. The paper also presents novel theoretical results for RL, and a very comprehensive experimental study. 

We did not want to claim state-of-the-art on Atari because this often depends on how one chooses to measure what should be state-of-the-art (eg sample complexity, highest median, highest mean, etc.). But clearly, in terms of median, ACER with 1 replay achieves a higher median score that any previously reported result. Note that this result is not just for a few games, but for the entire set of 57 games. The UNREAL agent submitted to this conference is the only method we know that achieves a higher median, but it does so by adding auxiliary tasks to A3C and massive hyper-parameter sweeps. We could also add auxiliary tasks to ACER and do hyper-parameter sweeps to further improve it, but this is left for future work as we wanted to focus on designing a powerful core RL agent.

We hope this reply and in particular the ablations clearly answer your concerns. With 3 6’s this thorough paper will be rejected despite the several novel contributions it makes, new theoretical analysis, and excellent results on a comprehensive set of tasks. We hope you take the ablations and this reply into consideration to choose your final scores.

---

This paper studies the off-policy learning of actor-critic with experience replay. This is an important and challenging problem in order to improve the sample efficiency of the reinforcement learning algorithms. The paper attacks the problem by introducing a new way to truncate importance weight, a modified trust region optimization, and by combining retrace method. The combination of the above techniques performs well on Atari and MuJoCo in terms of improving sample efficiency. My main comment is how does each of the technique contribute to the performance gain? If some experiments could be carried out to evaluate the separate gains from these tricks, it would be helpful.

---

This paper introduces an actor-critic deep RL approach with experience replay, which combines truncated importance sampling and trust region policy optimization. The paper also proposes a new method called stochastic duelling networks to estimate the critic for continuous action spaces. The method is applied to Atari games and continuous control problems, where it yields performance comparable to state-of-the-art methods.

As mentioned in the beginning of the paper, the main contributions of this work lies in combining 1) truncated importance sampling with retrace, 2) trust region policy optimization, and 3) stochastic duelling networks. These improvements work well and may be beneficial to future work in RL.

However, each improvement appears to be quite incremental. Moreover, the ACER framework seems much more complex and fragile to implement compared to the standard deep q-learning with prioritized replay (which appears to perform just as well on Atari games). So for the Atari domain, I would still put my money on prioritized replay due to its simplicity. Thirdly, improving sample efficiency for deep RL is a laudable goal, but really this goal should be pursued in a problem setting where sample efficiency is important. Unfortunately, the paper only evaluates sample efficiency in the Atari and continuous control tasks domain; two domains where sample efficiency is not important. Thus, it is not clear that the proposed method ACER will generalize to problems where we really care about sample efficiency.

Some technical aspects which need clarifications:
- For Retrace, I assume that you compute recursively $Q^{ret}$ starting from the end of each trajectory? Please comment on this.
- It's not clear to me how to derive eq. (7). Is an approximation (double tilde) sign missing?
- In section 3.1 the paper argued that $Q^{ret}$ gives a lower-variance estimate of the action-value function. Then why not use it in eq. (8) for the bias correction term?
- The paper states that it uses a replay memory of 50000 frames, so that across threads it is comparable in size to previous work. However, for each thread this is much smaller compared to earlier experiments on Atari games. For example, one million experience replay transitions were used in the paper ""Prioritized Experience Replay"" by Schaul et al. This may have a huge impact on performance of the models (both for ACER and for the competing models). In order to properly assess the improvements of ACER over previous work, the authors need to also experiment with larger experience replay memories.


Other comments:
- Please move Section 7 to the appendix.
- ""Moreover, when using small values of lambda to reduce variance, occasional large importance weights can still cause instability"": I think what is meant is using *large* values of lambda.
- Above eq. (6) mention that the squared error is used.
- Missing a ""t"" subscript at the beginning of eq. (9)?
- It was hard to understand the stochastic duelling networks. Please rephrase this part.
- Please clarify this sentence ""To compare different agents, we adopt as our metric the median of the human normalized score over all 57 games.""
- Figure 2 (Bottom): Please add label to vertical axes.

---

The paper looks at several innovations for deep RL, and evaluates their effect on solving games in the Atari domain.  The paper reads a bit like a laundry list of the researcher’s latest tricks.  It is written clearly enough, but lacks a compelling message.  I expect the work will be interesting to people already implementing deep RL methods, but will probably not get much attention from the broader community.

The claims on p.1 suggest the approach is stable and sample efficience, and so I expected to see some theoretical analysis with respect to these properties. But this is an empirical claim; it would help to clarify that in the abstract.

The proposed innovations are based on sound methods.  It is particularly nice to see the same approach working for both discrete and continuous domains.

The paper has reasonably complete empirical results. It would be nice to see confidence intervals on more of the plots. Also, the results don’t really tease apart the effect of each of the various innovations, so it’s harder to understand the impact of each piece and to really get intuition, for example about why ACER outperforms A3C.  Also, it wasn’t clear to me why you only get matching results on discrete tasks, but get state-of-the-art on continuous tasks.

The paper has good coverage of the related literature. It is nice to see this work draw more attention to Retrace, including the theoretical characterization in Sec.7.

---

Should the inside summations of equation (3) should go from i = 0 to (k - t)?

---

Dear Authors,

Please resubmit your paper in the ICLR 2017 format with the correct marging spacing for your submission to be considered. Thank you!",,,,,,6.333333333333333,,,3.3333333333333335,,
471,"IMPROVING NEURAL CONVERSATION MODELS
Authors: Kirthevasan Kandasamy, Yoram Bachrach, Ryota Tomioka, Daniel Tarlow, David Carter
Source file: 471.pdf

ABSTRACT
We study reinforcement learning of chatbots with recurrent neural network architectures when the rewards are noisy and expensive to obtain. For instance, a chatbot used in automated customer service support can be scored by quality assurance agents, but this process can be expensive, time consuming and noisy. Previous reinforcement learning work for natural language processing uses on-policy updates and/or is designed for on-line learning settings. We demonstrate empirically that such strategies are not appropriate for this setting and develop an off-policy batch policy gradient method (BPG). We demonstrate the efficacy of our method via a series of synthetic experiments and an Amazon Mechanical Turk experiment on a restaurant recommendations dataset.

1 INTRODUCTION
Chatbots are one of the classical applications of artificial intelligence and are now ubiquitous in technology, business and everyday life. Many corporate entities are now increasingly using chatbots to either replace or assist humans in customer service contexts. For example, Microsoft is currently actively building a chat bot to optimise and streamline its technical support service.
In these scenarios, there is usually an abundance of historical data since past conversations between customers and human customer service agents are usually recorded by organisations. An apparently straightforward solution would be to train chatbots to reproduce the responses by human agents using standard techniques such as maximum likelihood. While this seems natural, it is far from desirable for several reasons. It has been observed that such procedures have a tendency to produce very generic responses (Sordoni et al., 2015). For instance, when we trained chatbots via maximum likelihood on a restaurant recommendations dataset, they repeatedly output responses to the effect of How large is your group?, What is your budget? etc. Further, they also produce responses such as Let me look that up. or Give me a second. which, although permissible for a human agent to say, are not appropriate for a chatbot. Although there are ways to increase the diversity of responses (Li et al., 2015), our focus is on encouraging the bot to meaningfully advance the conversation. One way to address this problem is to provide some form of weak supervision for responses generated by a chatbot. For example, a human labeller, such as a quality assurance agent, could score each response generated by a chatbot in a conversation with a customer. This brings us to the reinforcement learning (RL) paradigm where these rewards (scores) are to be used to train a good chatbot. In this paper we will use the terms score, label, and reward interchangeably. Labelled data will mean conversations which have been assigned a reward of some form as explained above.
Nonetheless, there are some important differences in the above scenario when compared to the more popular approaches for RL.
• Noisy and expensive rewards: Obtaining labels for each conversation can be time consuming and economically expensive. As a result, there is a limited amount of labelled data available. Moreover, labels produced by humans are invariably noisy due to human error and subjectivity.
• Off-line evaluations: Unlike conventional RL settings, such as games, where we try to find the optimal policy while interacting with the system, the rewards here are not immediately available. Previous conversations are collected, labelled by human experts, and then given to an algorithm which has to manage with the data it has.
• Unlabelled Data: While labelled data is limited, a large amount of unlabelled data is available.
If labelled data is in short supply, reinforcement learning could be hopeless. However, if unlabelled data can be used to train a decent initial bot, say via maximum likelihood, we can use policy iteration techniques to refine this bot by making local improvements using the labelled data (Bellman, 1956). Besides chatbots, this framework also finds applications in tasks such as question answering (Ferrucci et al., 2010; Hermann et al., 2015; Sachan et al., 2016), generating image descriptions (Karpathy & Fei-Fei, 2015) and machine translation (Bahdanau et al., 2014) where a human labeller can provide weak supervision in the form of a score to a sentence generated by a bot.
To contextualise the work in this paper, we make two important distinctions in policy iteration methods in reinforcement learning. The first is on-policy vs off-policy. In on-policy settings, the goal is to improve the current policy on the fly while exploring the space. On-policy methods are used in applications where it is necessary to be competitive (achieve high rewards) while simultaneously exploring the environment. In off-policy, the environment is explored using a behaviour policy, but the goal is to improve a different target policy. The second distinction is on-line vs batch (off-line). In on-line settings one can interact with the environment. In batch methods, which is the setting for this work, one is given past exploration data from possibly several behaviour policies and the goal is to improve a target policy using this data. On-line methods can be either on-policy or off-policy whereas batch methods are necessarily off-policy.
In this paper, we study reinforcement learning in batch settings, for improving chat bots with Seq2Seq recurrent neural network (RNN) architectures. One of the challenges when compared to on-line learning is that we do not have interactive control over the environment. We can only hope to do as well as our data permits us to. On the other hand, the batch setting affords us some luxuries. We can reuse existing data and use standard techniques for hyper-parameter tuning based on cross validation. Further, in on-line policy updates, we have to be able to “guess” how an episode will play out, i.e. actions the behaviour/target policies would take in the future and corresponding rewards. However, in batch learning, the future actions and rewards are directly available in the data. This enables us to make more informed choices when updating our policy.

RELATED WORK
Recently there has been a surge of interest in deep learning approaches to reinforcement learning, many of them adopting Q-learning, e.g. (He et al., 2015; Mnih et al., 2013; Narasimhan et al., 2015). In Q-learning, the goal is to estimate the optimal action value function Q∗. Then, when an agent is at a given state, it chooses the best greedy action according to Q∗. While Q-learning has been successful in several applications, it is challenging in the settings we consider since estimating Q∗ over large action and state spaces will require a vast number of samples. In this context, policy iteration methods are more promising since we can start with an initial policy and make incremental local improvements using the data we have. This is especially true given that we can use maximum likelihood techniques to estimate a good initial bot using unlabelled data.
Policy gradient methods, which fall within the paradigm of policy iteration, make changes to the parameters of a policy along the gradient of a desired objective (Sutton et al., 1999). Recently, the natural language processing (NLP) literature has turned its attention to policy gradient methods for improving language models. Ranzato et al. (2015) present a method based on the classical REINFORCE algorithm (Williams, 1992) for improving machine translation after preliminary training with maximum likelihood objectives. Bahdanau et al. (2016) present an actor-critic method also for machine translation. In both cases, as the reward, the authors use the BLEU (bilingual evaluation understudy) score of the output and the translation in the training dataset. This setting, where the rewards are deterministic and cheaply computable, does not reflect difficulties inherent to training chatbots where labels are noisy and expensive. Li et al. (2016) develop a policy gradient method bot for chatbots. However, they use user defined rewards (based on some simple rules) which, once again, are cheaply obtained and deterministic. Perhaps the closest to our work is that of Williams & Zweig (2016) who use a REINFORCE based method for chat bots. We discuss the differences of
this and other methods in greater detail in Section 3. The crucial difference between all of the above efforts and ours is that they use on-policy and/or on-line updates in their methods.
The remainder of this manuscript is organised as follows. In Section 2 we review Seq2Seq models and Markov decision processes (MDP) and describe our framework for batch reinforcement learning. Section 3 presents our method BPG and compares it with prior work in the RL and NLP literature. Section 4 presents experiments on a synthetic task and a customer service dataset for restaurant recommendations.

2 PRELIMINARIES

2.1 A REVIEW OF SEQ2SEQ MODELS
The goal of a Seq2Seq model in natural language processing is to produce an output sequence y = [a1, a2, . . . , aT ] given an input sequence x (Cho et al., 2014; Kalchbrenner & Blunsom, 2013; Sutskever et al., 2014). Here ai ∈ A where A is a vocabulary of words. For example, in machine translation from French to English, x is the input sequence in French, and y is its translation in English. In customer service chatbots, x is the conversation history until the customer’s last query and y is the response by an agent/chatbot. In a Seq2Seq model, we use an encoder network to represent the input sequence as a euclidean vector and then a decoder network to convert this vector to an output sequence. Typically, both the encoder and decoder networks are recurrent neural networks (RNN) (Mikolov et al., 2010) where the recurrent unit processes each word in the input/output sequences one at a time. In this work, we will use the LSTM (long short term memory) (Hochreiter & Schmidhuber, 1997) as our recurrent unit due to its empirical success in several applications.
In its most basic form, the decoder RNN can be interpreted as assigning a probability distribution over A given the current “state”. At time t, the state st is the input sequence x and the words yt−1 = [a1, . . . , at−1] produced by the decoder thus far, i.e. st = (x, yt−1). We sample the next word at from this probability distribution π(·|st), then update our state st+1 = (x, yt) where yt = [yt−1, at], and proceed in a similar fashion. The vocabulary A contains an end-of-statement token <EOS>. If we sample <EOS> at time T + 1, we terminate the sequence and output yT .

2.2 A REVIEW OF MARKOV DECISION PROCESSES (MDP)
We present a formalism for MDPs simplified to our setting. In an MDP, an agent takes an action a in a state s and transitions to a state s′. An episode refers to a sequence of transitions s1 → a1 → s2 → a2 → · · · → aT → sT+1 until the agent reaches a terminal state sT+1. At a terminal state, the agent receives a reward. Formally, an MDP is the triplet (S,A, R). Here, S is a set of states andA is a set of actions. When we take an action a at state s we transition to a new state s′ = s′(s, a) which, in this work, will be deterministic. A will be a finite but large discrete set and S will be discrete but potentially infinite. R : S → R is the expected reward function such that when we receive a reward r at state s ∈ S, E[r] = R(s). Let S0 ⊂ S be a set of terminal states. When we transition to any s ∈ S0, the episode ends. In this work, we will assume that the rewards are received only at a terminal state, i.e R(s) is nonzero only on S0. A policy π is a rule to select an action at a given state. We will be focusing on stochastic policies π : A×S → R+ where π(a|s) denotes the probability an agent will execute action a at state s. We define the value function V π : S → R of policy π, where V (s) is the expected reward at the end of the episode when we follow policy π from state s. For any terminal state s ∈ S0, V π(s) = R(s) regardless of π. We will also find it useful to define the action-value function Qπ : S × A :→ R, where Qπ(s, a) is the expected reward of taking action a at state s and then following policy π. With deterministic state transitions this is simply Qπ(s, a) = V π(s′(s, a)). It can be verified that V π(s) = Ea∼π(·|s) [Qπ(s, a)] (Sutton & Barto, 1998).

2.3 SET UP
We now frame our learning from labels scenario for RNN chatbots as an MDP. The treatment has similarities to some recent RL work in the NLP literature discussed above.
Let x be the input and yt−1 = [a1, . . . , at−1] be the words output by the decoder until time t. The state of our MDP at time t of the current episode will be st = (x, yt−1). Therefore, the set of states S will be all possible pairs of inputs and partial output sequences. The actions A will be the vocabulary. The terminal states S0 will be (x, y) such that the last literal of y is <EOS>. The stochastic policy π will be a Seq2Seq RNN which produces a distribution overA given state st. When we wish to make the dependence of the policy on the RNN parameters θ explicit, we will write πθ. When we sample an action at ∼ π(·|st), we deterministically transition to state (x, [yt−1, at]). If we sample aT+1 = <EOS> at time T + 1, the episode terminates and we observe a stochastic reward.
We are given a dataset of input-output-reward triples {(x(i), y(i), r(i))}ni=1 where y(i) = (a
(i) 1 , . . . , a (i) Ti , <EOS>) is the sequence of output words. This data was collected from possibly multiple behaviour policies which output y(i) for the given input x(i). In the above customer service example, the behaviour policies could be chatbots, or even humans, which were used for conversations with a customer. The rewards ri are scores assigned by a human quality assurance agent to each response of the chatbot. Our goal is to use this data to improve a given target policy πθ. We will use q to denote the distribution of the data. q(s) is the distribution of the states in the dataset, q(a|s) is the conditional distribution of an action given a state, and q(s, a) = q(s)q(a|s) is the joint distribution over states and actions. q will be determined by the initial distribution of the inputs x(i) and the behaviour policies used to collect the training data. Our aim is to find a policy that does well with respect to q. Specifically, we wish to maximise the following objective,
J(θ) = ∑ s∈S q(s)V πθ (s). (1)
Here, the value function V πθ is not available to us but has to be estimated from the data. This is similar to objectives used in on-line off-policy policy gradient literature where q is replaced by the limiting distribution of the behaviour policy (Degris et al., 2012). In the derivation of our algorithm, we will need to know q(a|s) to compute the gradient of our objective. In off-policy reinforcement learning settings this is given by the behaviour policy which is readily available. If the behaviour policy if available to us, then we can use this too. Otherwise, a simple alternative is to “learn” a behaviour policy. For example, in our experiments we used an RNN trained using the unlabelled data to obtain values for q(a|s). As long as this learned policy can capture the semantics of natural language (for example, the word apple is more likely than car when the current state is (x, I ate an)), then it can be expected to do reasonably well. In the following section, we will derive a stochastic gradient descent (SGD) procedure that will approximately minimise (1).
Before we proceed, we note that it is customary in the RL literature to assume stochastic transitions between states and use rewards at all time steps instead of the terminal step. Further, the future rewards are usually discounted by a discount factor γ < 1. While we use the above formalism to simplify the exposition, the ideas presented here extend naturally to more conventional settings.

3 BATCH POLICY GRADIENT
Our derivation follows the blueprint in Degris et al. (2012) who derive an off-policy on-line actor critic algorithm. Following standard policy gradient methods, we will aim to update the policy by taking steps along the gradient of the objective∇J(θ).
∇J(θ) = ∇Es∼q [∑ a∈A πθ(a|s)Qπθ (s, a) ] = Es∼q [∑ a∈A ∇πθ(a|s)Qπθ (s, a) + πθ(a|s)∇Qπθ (s, a) ] .
The latter term inside the above summation is difficult to work with, so the first step is to ignore it and work with the approximate gradient g(θ) = Es∼q[ ∑ a∈A∇πθ(a|s)Qπθ (s, a)] ≈ ∇J(θ). Degris et al. (2012) provide theoretical justification for this approximation in off policy settings by establishing that J(θ) ≤ J(θ + αg(θ)) for all small enough α. Expanding on g(θ), we obtain:
g(θ) = Es∼q [∑ a∈A πθ(a|s) ∇πθ(a|s) πθ(a|s) Qπθ (s, a) ] = E s∼q a∼q(·|s) [ ρ(s, a)ψ(a, s)Qπθ (s, a) ] = E(st,at)∼q(·,·) [ρ(st, at)ψ(at, st)(Q πθ (st, at)− V πθ (st))] . (2)
Here ψ(a, s) = ∇πθ(a|s)πθ(a|s) = ∇ log πθ(a|s) is the score function of the policy and ρ(s, a) = πθ(a|s)/q(a|s) is the importance sampling coefficient. In the last step, we have used the fact that E[π(a|s)ψ(a|s)h(s)] = 0 for any function h : S → R of the current state (Szepesvári, 2010). The purpose of introducing the value function V πθ is to reduce the variance of the SGD updates – we want to assess how good/bad action at is relative to how well πθ will do at state st in expectation. If at is a good action (Qπθ (st, at) is large relative to V πθ (st)), the coefficient of the score function is positive and it will change θ so as to assign a higher probability to action at at state st.
The Qπθ , V πθ functions are not available to us so we will replace them with estimates. For V πθ (st) we will use an estimate V̂ (st) – we will discuss choices for this shortly. However, the action value function is usually not estimated in RL policy gradient settings to avoid the high sample complexity. A sensible stochastic approximation for Qπθ (st, at) is to use the sum of future rewards from the current state (Sutton & Barto, 1998)1. If we receive reward r at the end of the episode, we can then use Qπθ (st, at) ≈ r for all time steps t in the episode. However, since q(at|st) is different from πθ(at|st) we will need to re-weight future rewards via importance sampling r ∏T i=t ρ(si, ai). This is to account for the fact that an action a given s may have been more likely under the policy πθ(·|s) than it was under q(·|s) or vice versa. Instead of directly using the re-weighted rewards, we will use the so called λ–return which is a convex combination of the re-weighted rewards and the value function (Sutton, 1988; 1984). In our setting, they are defined recursively from the end of the episode t = T + 1 to t = 1 as follows. For λ ∈ (0, 1],
r̃λT+1 = r, r̃ λ t = (1− λ)V πθ (st+1) + λρ(st, at)r̃λt+1 for t = T, . . . , 1. (3)
The purpose of introducing λ is to reduce the variance of using the future rewards alone as an estimate for Qπθ (st, at). This is primarily useful when rewards are noisy. If the rewards are deterministic, λ = 1 which ignores the value function is the best choice. In noisy settings, it is recommended to use λ < 1 (see Sec 3.1 of (Szepesvári, 2010)). In our algorithm, we will replace r̃λt with r λ t where V πθ is replaced with the estimate V̂ . Putting it all together, and letting α denote the step size, we have the following update rule for the parameters θ of our policy:
θ ← θ + αρ(st, at)ψ(st, at)(rλt − V̂ (st)). In Algorithm 1, we have summarised the procedure where the updates are performed after an entire pass through the dataset. In practice, we perform the updates in mini-batches.
An Estimator for the Value Function: All that is left to do is to specify an estimator V̂ for the value function. We first need to acknowledge that this is a difficult problem: S is quite large and for typical applications for this work there might not be enough data since labels are expensive. That said, the purpose of V̂ in (2), (3) is to reduce the variance of our SGD updates and speed up convergence so it is not critical that this be precise – even a bad estimator will converge eventually. Secondly, standard methods for estimating the value function based on minimising the projected Bellman error require the second derivatives, which might be intractable for highly nonlinear parametrisations of V̂ (Maei, 2011). For these two statistical and computational reasons, we resort to simple estimators for V πθ . We will study two options. The first is a simple heuristic used previously in the RL literature, namely a constant estimator for V̂ which is equal to the mean of all rewards in the dataset (Williams, 1992). The second uses the parametrisation V̂ (s) = σ(ξ>φ(s)) where σ is the logistic function and φ(s) ∈ Rd is a Euclidean representation of the state. For V̂ (s) of the above form, the Hessian ∇2ξV̂ (s) can be computed in O(d) time. To estimate this value function, we use the GTD(λ) estimator from Maei (2011). As φ(s) we will be using the hidden state of the LSTM. The rationale for this is as follows. In an LSTM trained using maximum likelihood, the hidden state contains useful information about the objective. If there is overlap between the maximum likelihood and reinforcement learning objectives, we can expect the hidden state to also carry useful information about the RL objective. Therefore, we can use the hidden state to estimate the value function whose expectation is the RL objective. We have described our implementation of GTD(λ) in Appendix A and specified some implementation details in Section 4.
1 Note Qπθ (st, at) = V πθ (st+1) for deterministic transitions. However, it is important not to interpret the term in (2) as the difference in the value function between successive states. Conditioned on the current time step, V πθ (st) is deterministic, while V πθ (st+1) is stochastic. In particular, while a crude estimate suffices for the former, the latter is critical and should reflect the rewards received during the remainder of the episode.
Algorithm 1 Batch Policy Gradient (BPG) Given: Data {(xi, yi, ri)}ni=1, step size α, return coefficient λ, initial θ0.
– Set θ ← θ0. – For each epoch k = 1, 2, . . .
I Set ∆θ ← 0 I For each episode i = 1, . . . , n • rλT+1 ← ri • ρt ← πθ(a(i)t |s (i) t )/q(a (i) t |s (i) t ) for t = 1, . . . , T
(i). • For each time step in reverse t = T (i), . . . , 1
(i) rλt ← (1− λ)V̂ (s (i) t+1) + λρtr λ t+1
(ii) ∆θ ← ∆θ + 1 T (i) ρtψ(s (i) t , a (i) t )(r λ t − V̂ (s (i) t ))
(iii) Compute updates for the value function estimate V̂ . I Update the policy θ ← θ + α∆θ I Update the value function estimate V̂ .

COMPARISON WITH OTHER RL APPROACHES IN NLP
Policy gradient methods have been studied extensively in on policy settings where the goal is to improve the current policy on the fly (Amari, 1998; Williams, 1992). To our knowledge, all RL approaches in Seq2Seq models have also adopted on-policy policy gradient updates (Bahdanau et al., 2016; Li et al., 2016; Ranzato et al., 2015; Williams & Zweig, 2016). However, on policy methods break down in off-policy settings, because any update must account for the probability of the action under the target policy. For example, suppose the behaviour policy took action a at state s and received a low reward. Then we should modify the target policy θ so as to reduce πθ(a|s). However, if the target policy is already assigning low probability to a|s then we should not be as aggressive when making the updates. The re-weighting ρ(s, a) via importance sampling does precisely this.
A second difference is that we study batch RL. Standard on-line methods are designed for settings where we have to continually improve the target while exploring using the behaviour policy. Critical to such methods are the estimation of future rewards at the current state and the future actions that will be taken by both the behaviour and target policies. In order to tackle this, previous research either ignore future rewards altogether (Williams, 1992), resort to heuristics to distribute a delayed reward to previous time steps (Bahdanau et al., 2016; Williams & Zweig, 2016), or make additional assumptions about the distribution of the states such as stationarity of the Markov process (Degris et al., 2012; Maei, 2011). However, in batch settings, the λ-return from a given time step can be computed directly (3) since the future action and rewards are available in the dataset. Access to this information provides a crucial advantage over techniques designed for on-line settings.

4 EXPERIMENTS
Implementation Details: We implement our methods using Chainer (Tokui et al., 2015), and group sentences of the same length together in the same batch to make use of GPU parallelisation. Since different batches could be of different length, we do not normalise the gradients by the batch size as we should take larger steps after seeing more data. However, we normalise by the length of the output sequence to allocate equal weight to all sentences. We truncate all output sequences to length 64 and use a maximum batch size of 32. We found it necessary to use a very small step size (10−5), otherwise the algorithm has a tendency to get stuck at bad parameter values. While importance reweighting is necessary in off-policy settings, it can increase the variance of the updates, especially when q(at|st) is very small. A common technique to alleviate this problem is to clip the ρ(st, at) value (Swaminathan & Joachims, 2015). In addition to single ρ(st, at) values, our procedure has a product of ρ(st, at) values when computing the future rewards (3). The effect of large ρ values is a large weight ρt(rλt −V̂ (st)) for the score function in step (ii) of Algorithm 1. In our implementation,
LSTM LSTM LSTM
LSTM LSTM LSTM
softmax softmax softmax
LSTM LSTM
LSTM LSTM
Encoder Decoder
we clip this weight at 5 which controls the variance of the updates and ensures that a single example does not disproportionately affect the gradient.
RNN Design: In both experiments we use deep LSTMs with two layers for the encoder and decoder RNNs. The output of the bottom layer is fed to the top layer and in the decoder RNN, the output of the top layer is fed to a softmax layer of size |A|. When we implement GTD(λ) to estimate V πθ we use the hidden state of the bottom LSTM as φ(s). When performing our policy updates, we only change the parameters of the top LSTM and the softmax layer in our decoder RNN. If we were to change the bottom LSTM too, then the state representation φ(s) would also change as the policy changes. This violates the MDP framework. In other words, we treat the bottom layer as part of the environment in our MDP. To facilitate a fair comparison, we only modify the top LSTM and softmax layers in all methods. We have illustrated this set up in Fig. 1. We note that if one is content with using the constant estimator, then one can change all parameters of the RNN.

4.1 SOME SYNTHETIC EXPERIMENTS ON THE EUROPARL DATASET
To convey the main intuitions of our method, we compare our methods against other baselines on a synthetic task on the European parliament proceedings corpus (Koehn, 2005). We describe the experimental set up briefly, deferring details to Appendix B.1. The input sequence to the RNN was each sentence in the dataset. Given an input, the goal was to reproduce the words in the input without repeating words in a list of forbidden words. The RL algorithm does not explicitly know either goal of the objective but has to infer it from the stochastic rewards assigned to input output sequences in the dataset. We used a training set of 500 input-output-reward triplets for the RL methods.
We initialised all methods by maximum likelihood training on 6000 input output sequences where the output sequence was the reverse of the input sequence. The maximum likelihood objective captures part of the RL objective. This set up reflects naturally occurring practical scenarios for the algorithm where a large amount unlabelled data can be used to bootstrap a policy if the maximum likelihood and reinforcement learning objectives are at least partially aligned. We trained the RL algorithms for 200 epochs on the training set. At the end of each epoch, we generated outputs from the policy on test set of 500 inputs and scored them according to our criterion. We plot the test set error against the number of epochs for various methods in Fig. 2.
Fig. 2(a) compares 3 methods: BPG with and without maximum likelihood initialisation and a version of BPG which does not use importance sampling. Clearly, bootstrapping an RL algorithm with ML can be advantageous especially if data is abundantly available for ML training. Further, without importance sampling, the algorithm is not as competitive for reasons described in Section 3. In all 3 cases, we used a constant estimator for V̂ and λ = 0.5. The dashed line indicates the performance of ML training alone. BPG-NIS is similar to the algorithms of Ranzato et al. (2015); Williams & Zweig (2016) except that there, their methods implicitly use λ = 1.
Fig. 2(b) compares 4 methods: BPG and its on-line version OPG with constant (CONST) and GTD(λ) estimators for V̂ . The on-line versions of the algorithms are a direct implementation of the method in Degris et al. (2012) which do not use the future rewards as we do. The first observation is that while GTD(λ) is slightly better in the early iterations, it performs roughly the same as using a constant estimator in the long run. Next, BPG performs significantly better than OPG. We believe this is due to the following two reasons. First, the online updates assume stationarity of the MDP. When this does not hold, such as in limited data instances like ours, the SGD updates can be
very noisy. Secondly, the value function estimate plays a critical role in the online version. While obtaining a reliable estimate V̂ is reasonable in on-line settings where we can explore indefinitely to collect a large number of samples, it is difficult when one only has a limited number of labelled samples. Finally, we compare BPG with different choices for λ in Fig. 2(c). As noted previously, λ < 1 is useful with stochastic rewards, but choosing too small a value is detrimental. The optimal λ value may depend on the problem.

4.2 RESTAURANT RECOMMENDATIONS
We use data from an on-line restaurant recommendation service. Customers log into the service and chat with a human agent asking recommendations for restaurants. The agents ask a series of questions such as food preferences, group size etc. before recommending a restaurant. The goal is to train a chatbot (policy) which can replace or assist the agent. For reasons explained in Section 1, maximum likelihood training alone will not be adequate. By obtaining reward labels for responses produced by various other bots, we hope to improve on a bot initialised using maximum likelihood.
Data Collection: We collected data for RL as follows. We trained five different RNN chatbots with different LSTM parameters via maximum likelihood on a dataset of 6000 conversations from this dataset. The bots were trained to reproduce what the human agent said (output y) given the past conversation history (input x). While the dataset is relatively small, we can still expect our bots to do reasonably well since we work in a restricted domain. Next, we generated responses from these bots on 1216 separate conversations and had them scored by workers on Amazon Mechanical Turk (AMT). For each response by the bots in each conversation, the workers were shown the history before the particular response and asked to score (label) each response on a scale of 0− 1− 2. We collected scores from three different workers for each response and used the mean as the reward.
Policies and RL Application: Next, we initialised 2 bots via maximum likelihood and then used BPG to improve them using the labels collected from AMT. For the 2 bots we used the following LSTM hidden state size H , word embedding size E and BPG parameters. These parameters were chosen arbitrarily and are different from those of the bots used in data collection described above.
• Bot-1: H = 512, E = 256. BPG: λ = 0.5, GTD(λ) estimator for V̂ .
• Bot-2: H = 400, E = 400. BPG: λ = 0.5, constant estimator for V̂ .
Testing: We used a separate test set of 500 conversations which had a total of more than 3500 inputoutput (conversation history - response) pairs. For each Bot-1 and Bot-2 we generated responses before and after applying BPG, totalling 4 responses per input. We then had them scored by workers on AMT using the same set up described above. The same worker labels the before-BPG and afterBPG responses from the same bot. This controls spurious noise effects and allows us to conduct a paired test. We collected 16, 808 before and after label pairs each for Bot-1 and Bot-2 and compare them using a paired t-test and a Wilcoxon signed rank test.
Results: The results are shown in Table 1. The improvements on Bot-2 are statistically significant at the 10% level on both tests, while Bot-1 is significant on the Wilcoxon test. The large p-values for Bot-1 are due to the noisy nature of AMT experiments and we believe that we can attain significance if we collect more labels which will reduce the standard error in both tests. In Appendix B.2 we present some examples of conversation histories and the responses generated by the bots before and after applying BPG. We qualitatively discuss specific kinds of issues that we were able to overcome via reinforcement learning.

5 CONCLUSION
We presented a policy gradient method for batch reinforcement learning to train chatbots. The data to this algorithm are input-output sequences generated using other chatbots/humans and stochastic rewards for each output in the dataset. This setting arises in many applications, such as customer service systems, where there is usually an abundance of unlabelled data, but labels (rewards) are expensive to obtain and can be noisy. Our algorithm is able to efficiently use minimal labelled data to improve chatbots previously trained through maximum likelihood on unlabelled data. While our method draws its ideas from previous policy gradient work in the RL and NLP literature, there are some important distinctions that contribute to its success in the settings of interest for this work. Via importance sampling we ensure that the probability of an action is properly accounted for in off-policy updates. By explicitly working in the batch setting, we are able to use knowledge of future actions and rewards to converge faster to the optimum. Further, we use the unlabelled data to initialise our method and also learn a reasonable behaviour policy. Our method outperforms baselines on a series of synthetic and real experiments.
The ideas presented in this work extend beyond chatbots. They can be used in applications such as question answering, generating image descriptions and machine translation where an output sentence generated by a policy is scored by a human labeller to provide a weak supervision signal.

ACKNOWLEDGEMENTS
We would like to thank Christoph Dann for the helpful conversations and Michael Armstrong for helping us with the Amazon Mechanical Turk experiments.

APPENDIX
A IMPLEMENTATION OF GTD(λ)
We present the details of the GTD(λ) algorithm (Maei, 2011) to estimate a value function in Algorithm 2. However, while Maei (2011) give an on-line version we present the batch version here where the future rewards of an episode are known. We use a parametrisation of the form V̂ (s) = V̂ξ(s) = σ(ξ
>φ(s)) where ξ ∈ Rd is the parameter to be estimated. σ(z) = 1/(1 + e−z) is the logistic function.
The algorithm requires two step sizes α′, α′′ below for the updates to ξ and the ancillary parameter w. Following the recommendations in Borkar (1997), we use α′′ α. In our implementations, we used α′ = 10−5 and α′′ = 10−6. When we run BPG, we perform steps (a)-(f) of Algorithm 2 in step (iii) of Algorithm 1 and the last two update steps of Algorithm 2 in the last update step of Algorithm 1.
The gradient and Hessian of V̂ξ have the following forms,
∇ξV̂ξ(s) = V̂ξ(s)(1− V̂ξ(s))φ(s), ∇2ξV̂ξ(s) = V̂ξ(s)(1− V̂ξ(s))(1− 2V̂ξ(s))φ(s)φ(s)>.
The Hessian product in step (d) of Algorithm 2 can be computed in O(d) time via, ∇2ξV̂ξ(s) · w = [ V̂ξ(s)(1− V̂ξ(s))(1− 2V̂ξ(s))(φ(s)>w) ] φ(s).
Algorithm 2 GTD(λ) Given: Data {(xi, yi, ri)}ni=1, step sizes α′, α′′, return coefficient λ, initial ξ0.
– Set ξ ← ξ0, w ← 0. – For each epoch k = 1, 2, . . .
I Set ∆ξ ← 0, ∆w ← 0. I For each episode i = 1, . . . , n • Set rλT+1 ← ri, gλT+1 ← 0, qλT+1 ← 0 • ρt ← πθ(a(i)t |s (i) t )/q(a (i) t |s (i) t ) for t = 1, . . . , T
(i). • For each time step in reverse t = T (i), . . . , 1:
(a) gλt ← ρt ( (1− λ)V̂ξ(s(i)t+1) + λρtrλt+1 )
(b) qλt ← ρt ( (1− λ)∇ξV̂ξ(s(i)t+1) + λqλt+1 )
(c) δt ← gλt − V̂ξ(s (i) t ) (d) ht ← ( δt − w>∇ξV̂ξ(s(i)t ) ) ∇2ξV̂ξ(s (i) t ) · w
(e) ∆w ← ∆w + 1 T (i) ( δt − w>∇ξV̂ξ(s(i)t ) ) ∇ξV̂ξ(s(i)t )
(f) ∆ξ ← ∆ξ + 1 T (i) ( δt∇ξV̂ξ(s(i)t )− qλt w>∇ξV̂ξ(s (i) t )− ht ) I w ← w + α′′∆w. I ξ ← ξ + α′∆ξ.

B ADDENDUM TO EXPERIMENTS

B.1 DETAILS OF THE SYNTHETIC EXPERIMENT SET UP
Given an input and output sequence, we used the average of five Bernoulli rewards Bern(r), where the parameter r was r = 0.75 × rr + 0.25 × rf. Here rr was the fraction of common words in the input and output sequences while rf = 0.01pf where pf is the fraction of forbidden words in the dataset. As the forbidden words, we used the 50 most common words in the dataset. So if an input
had 10 words of which 2 were forbidden, an output sequence repeating 7 of the allowed words and 1 forbidden word would receive an expected score of 0.75× (8/10) + 0.25× 0.01(1/8) = 0.7406. The training and testing set for reinforcement learning were obtained as follows. We trained 4 bots using maximum likelihood on 6000 input output sequences as indicated in Section 4.1. The LSTM hidden state size H and word embedding size E for the 4 bots were, (H,E) = (256, 128), (128, 64), (64, 32), (32, 16). The vocabulary size was |A| = 12000. We used these bots to generate outputs for 500 different input sequences each. This collection of input and output pairs was scored stochastically as described above to produce a pool of 2000 input-output-score triplets. From this pool we use a fixed set of 500 triplets for testing across all our experiments. From the remaining 1500 data points, we randomly select 500 for training for each execution of an algorithm. For all RL algorithms, we used an LSTM with 16 layers and 16 dimensional word embeddings.

B.2 ADDENDUM TO THE AMT RESTAURANT RECOMMENDATIONS EXPERIMENT

MORE DETAILS ON THE EXPERIMENTAL SET UP
We collected the initial batch of training data for RL as follows: We trained, via maximum likelihood on 6000 conversations, five RNN bots whose LSTM hidden sizeH and word embedding sizeE were (H,E) = (512, 512), (256, 256), (128, 128), (512, 256), (256, 64). The inputs x were all words from the history of the conversation truncated at length 64, i.e. the most recent 64 words in the conversation history. The outputs were the actual responses of the agent which were truncated to length 64. As the vocabulary we use the |A| = 4000 most commonly occurring words in the dataset and replace the rest with an <UNK> token.
Using the bots trained this way we generate responses on 1216 separate conversations. This data was sent to AMT workers who were asked to label the conversations on the following scale.
• 2: The response is coherent and appropriate given the history and advances the conversation forward.
• 1: The response has some minor flaws but is discernible and appropriate. • 0: The response is either completely incoherent or inappropriate and fails to advance the
conversation forward.

SOME QUALITATIVE RESULTS
In Tables 2 and 3 we have presented some examples. The text in black/grey shows the conversation history, the response in blue is by the bot trained via maximum likelihood (ML) alone and in red is the bot after improvement using our BPG reinforcement learning algorithm. The first two examples of Table 2 present examples where the ML algorithm repeated generic questions (on budget, group size etc.) even though they had already been answered previously. After applying BPG, we are able to correct such issues, even though there are some grammatical errors. In the second, third and fourth example, we see that the ML+BPG bot is able to take context into consideration well when responding. For example, the customer asks for oriental/Mexican/Italian food. While the ML bot doesn’t take this into consideration, the ML+BPG bot is able to provide relevant answers. However, in the third example, the name of the restaurant suggests that the food might be Indian and not Mexican. In the final example of Table 2 the customer asks a direct question about smoking. The ML bot provides an irrelevant answer where as the ML+BPG bot directly responds to the question.
In some examples, the ML bot had a tendency to produce sentences that were grammatically correct but nonsensical, sensible but grammatically incorrect, or just complete gibberish. We were able to correct such issues via RL. The first three examples of Table 3 present such cases. Occasionally the opposite happened. The last example of Table 3 is one such instance.
","We study reinforcement learning of chatbots with recurrent neural network architectures when the rewards are noisy and expensive to obtain. For instance, a chatbot used in automated customer service support can be scored by quality assurance agents, but this process can be expensive, time consuming and noisy. Previous reinforcement learning work for natural language processing uses on-policy updates and/or is designed for on-line learning settings. We demonstrate empirically that such strategies are not appropriate for this setting and develop an off-policy batch policy gradient method (BPG). We demonstrate the efficacy of our method via a series of synthetic experiments and an Amazon Mechanical Turk experiment on a restaurant recommendations dataset.",ICLR 2017 conference submission,True,,"The paper discuss a ""batch"" method for RL setup to improve chat-bots.
The authors provide nice overview of the RL setup they are using and present an algorithm which is similar to previously published on line setup for the same problem. They make a comparison to the online version and explore several modeling choices. 

I find the writing clear, and the algorithm a natural extension of the online version.

Below are some constructive remarks:
- Comparison of the constant vs. per-state value function: In the artificial experiment there was no difference between the two while on the real-life task there was. It will be good to understand why, and add this to the discussion. Here is one option:
- For the artificial task it seems like you are giving the constant value function an unfair advantage, as it can update all the weights of the model, and not just the top layer, like the per-state value function.
- section 2.2:
   sentence before last: s' is not defined. 
   last sentence: missing ""... in the stochastic case."" at the end.
- Section 4.1 last paragraph: ""While Bot-1 is not significant ..."" => ""While Bot-1 is not significantly different from ML ...""

---

This is an interesting and timely paper combining off-policy learning with seq2seq models to train a chatbot on a restaurant reservation task, using labels collected through Amazon Mechanical Turk while using the bot with a baseline maximum likelihood policy. 
 The paper is clear, well-written and well-executed. Although the improvements are modest and the actual novelty of the paper is limited (combining known pieces in a rather straightforward way), this is still an interesting and informative read, and will probably be of interest to many people at ICLR.

---

Once again, we'd like to thank all reviewers for the feedback. We have updated the manuscript accordingly. The changes are done in magenta so that it would be easier to identify them.

@Reviewer2:
s' is defined in the first para of section 2.2. Also, the expectation in the statement at the end of section 2.2 is to account for the stochasticity in the rewards.

---

This paper extends neural conversational models into the batch reinforcement learning setting. The idea is that you can collect human scoring data for some responses from a dialogue model, however such scores are expensive. Thus, it is natural to use off-policy learning – training a base policy on unsupervised data, deploying that policy to collect human scores, and then learning off-line from those scores.

While the overall contribution is modest (extending off-policy actor-critic to the application of dialogue generation), the approach is well-motivated, and the paper is written clearly and is easy to understand. 

My main concern is that the primary dataset used (restaurant recommendations) is very small (6000 conversations). In fact, it is several orders of magnitude smaller than other datasets used in the literature (e.g. Twitter, the Ubuntu Dialogue Corpus) for dialogue generation. It is a bit surprising to me that RNN chatbots (with no additional structure) are able to generate reasonable utterances on such a small dataset. Wen et al. (2016) are able to do this on a similarly small restaurant dataset, but this is mostly because they map directly from dialogue states to surface form, rather than some embedding representation of the context. Thus, it remains to be seen if the approaches in this paper also result in improvements when much more unsupervised data is available.

References:

Wen, Tsung-Hsien, Milica Gasic, Nikola Mrksic, Lina M. Rojas-Barahona, Pei-Hao Su, Stefan Ultes, David Vandyke, and Steve Young. ""A Network-based End-to-End Trainable Task-oriented Dialogue System."" arXiv preprint arXiv:1604.04562 (2016).

---

The author propose to use a off-policy actor-critic algorithm in a batch-setting to improve chat-bots.
The approach is well motivated and the paper is well written, except for some intuitions for why the batch version outperforms the on-line version (see comments on ""clarification regarding batch vs. online setting"").
The artificial experiments are instructive, and the real-world experiments were performed very thoroughly although the results show only modest improvement.

---

The paper discuss a ""batch"" method for RL setup to improve chat-bots.
The authors provide nice overview of the RL setup they are using and present an algorithm which is similar to previously published on line setup for the same problem. They make a comparison to the online version and explore several modeling choices. 

I find the writing clear, and the algorithm a natural extension of the online version.

Below are some constructive remarks:
- Comparison of the constant vs. per-state value function: In the artificial experiment there was no difference between the two while on the real-life task there was. It will be good to understand why, and add this to the discussion. Here is one option:
- For the artificial task it seems like you are giving the constant value function an unfair advantage, as it can update all the weights of the model, and not just the top layer, like the per-state value function.
- section 2.2:
   sentence before last: s' is not defined. 
   last sentence: missing ""... in the stochastic case."" at the end.
- Section 4.1 last paragraph: ""While Bot-1 is not significant ..."" => ""While Bot-1 is not significantly different from ML ...""

---

The paper discuss a ""batch"" method for RL setup to improve chat-bots.
The authors provide nice overview of the RL setup they are using and present an algorithm which is similar to previously published on line setup for the same problem. They make a comparison to the online version and explore several modeling choices. 

I find the writing clear, and the algorithm a natural extension of the online version.

Below are some constructive remarks:
- Comparison of the constant vs. per-state value function: In the artificial experiment there was no difference between the two while on the real-life task there was. It will be good to understand why, and add this to the discussion. Here is one option:
- For the artificial task it seems like you are giving the constant value function an unfair advantage, as it can update all the weights of the model, and not just the top layer, like the per-state value function.
- section 2.2:
   sentence before last: s' is not defined. 
   last sentence: missing ""... in the stochastic case."" at the end.
- Section 4.1 last paragraph: ""While Bot-1 is not significant ..."" => ""While Bot-1 is not significantly different from ML ...""

---

This is an interesting and timely paper combining off-policy learning with seq2seq models to train a chatbot on a restaurant reservation task, using labels collected through Amazon Mechanical Turk while using the bot with a baseline maximum likelihood policy. 
 The paper is clear, well-written and well-executed. Although the improvements are modest and the actual novelty of the paper is limited (combining known pieces in a rather straightforward way), this is still an interesting and informative read, and will probably be of interest to many people at ICLR.

---

Once again, we'd like to thank all reviewers for the feedback. We have updated the manuscript accordingly. The changes are done in magenta so that it would be easier to identify them.

@Reviewer2:
s' is defined in the first para of section 2.2. Also, the expectation in the statement at the end of section 2.2 is to account for the stochasticity in the rewards.

---

This paper extends neural conversational models into the batch reinforcement learning setting. The idea is that you can collect human scoring data for some responses from a dialogue model, however such scores are expensive. Thus, it is natural to use off-policy learning – training a base policy on unsupervised data, deploying that policy to collect human scores, and then learning off-line from those scores.

While the overall contribution is modest (extending off-policy actor-critic to the application of dialogue generation), the approach is well-motivated, and the paper is written clearly and is easy to understand. 

My main concern is that the primary dataset used (restaurant recommendations) is very small (6000 conversations). In fact, it is several orders of magnitude smaller than other datasets used in the literature (e.g. Twitter, the Ubuntu Dialogue Corpus) for dialogue generation. It is a bit surprising to me that RNN chatbots (with no additional structure) are able to generate reasonable utterances on such a small dataset. Wen et al. (2016) are able to do this on a similarly small restaurant dataset, but this is mostly because they map directly from dialogue states to surface form, rather than some embedding representation of the context. Thus, it remains to be seen if the approaches in this paper also result in improvements when much more unsupervised data is available.

References:

Wen, Tsung-Hsien, Milica Gasic, Nikola Mrksic, Lina M. Rojas-Barahona, Pei-Hao Su, Stefan Ultes, David Vandyke, and Steve Young. ""A Network-based End-to-End Trainable Task-oriented Dialogue System."" arXiv preprint arXiv:1604.04562 (2016).

---

The author propose to use a off-policy actor-critic algorithm in a batch-setting to improve chat-bots.
The approach is well motivated and the paper is well written, except for some intuitions for why the batch version outperforms the on-line version (see comments on ""clarification regarding batch vs. online setting"").
The artificial experiments are instructive, and the real-world experiments were performed very thoroughly although the results show only modest improvement.

---

The paper discuss a ""batch"" method for RL setup to improve chat-bots.
The authors provide nice overview of the RL setup they are using and present an algorithm which is similar to previously published on line setup for the same problem. They make a comparison to the online version and explore several modeling choices. 

I find the writing clear, and the algorithm a natural extension of the online version.

Below are some constructive remarks:
- Comparison of the constant vs. per-state value function: In the artificial experiment there was no difference between the two while on the real-life task there was. It will be good to understand why, and add this to the discussion. Here is one option:
- For the artificial task it seems like you are giving the constant value function an unfair advantage, as it can update all the weights of the model, and not just the top layer, like the per-state value function.
- section 2.2:
   sentence before last: s' is not defined. 
   last sentence: missing ""... in the stochastic case."" at the end.
- Section 4.1 last paragraph: ""While Bot-1 is not significant ..."" => ""While Bot-1 is not significantly different from ML ...""",,,,,,7.0,,,3.0,,
482,"Authors: Jacek M. Bajor, Thomas A. Lasko
Source file: 482.pdf

ABSTRACT
It is a surprising fact that electronic medical records are failing at one of their primary purposes, that of tracking the set of medications that the patient is actively taking. Studies estimate that up to 50% of such lists omit active drugs, and that up to 25% of all active medications do not appear on the appropriate patient list. Manual efforts to maintain these lists involve a great deal of tedious human labor, which could be reduced by computational tools to suggest likely missing or incorrect medications on a patient’s list. We report here an application of recurrent neural networks to predict the likely therapeutic classes of medications that a patient is taking, given a sequence of the last 100 billing codes in their record. Our best model was a GRU that achieved high prediction accuracy (micro-averaged AUC 0.93, Label Ranking Loss 0.076), limited by hardware constraints on model size. Additionally, examining individual cases revealed that many of the predictions marked incorrect were likely to be examples of either omitted medications or omitted billing codes, supporting our assertion of a substantial number of errors and omissions in the data, and the likelihood of models such as these to help correct them.

1 INTRODUCTION
The idea of exploiting the large amounts of data captured in electronic medical records for both clinical care and secondary research holds great promise, but its potential is weakened by errors and omissions in those records (Safran et al., 2007; de Lusignan & van Weel, 2006). Among many other problems, accurately capturing the list of medications currently taken by a given patient is extremely challenging (Velo & Minuz, 2009). In one study, over 50% of electronic medication lists contained omissions (Caglar et al., 2011), and in another, 25% of all medications taken by patients were not recorded (Kaboli et al., 2004). Even medication lists provided by the patients themselves contain multiple errors and omissions (Green et al., 2010) .
Many efforts have been made to ensure the correctness of medication lists, most of them involving improved communication between patients and providers (Keogh et al., 2016), but these efforts have not yet been successful, and incorrect or incomplete medication documentation continues to be a source of error in computational medical research. In this work we attempt to identify likely errors and omissions in the record, predicting the set of active medications from the sequence of most recent disease-based billing codes in the record. Predictions from such a model could be used either in manual medication reconciliation (a common process undertaken to correct the medication record) or to provide a prior to other models, such as an NLP model attempting to extract medication use from the narrative clinical text.
Given the sequential nature of clinical data, we suspected that recurrent neural networks would be a good architecture for making these predictions. In this work we investigate this potential, comparing the performance of recurrent networks to that of similarly-configured feed forward networks.
The input for each case is a sequence of ICD-9 billing codes (Section 2.1), for which the model produces a single, multi-label prediction of the therapeutic classes (Section 3.1) of medications taken by the patient during the period of time covered by the billing code sequence.
This work is designed to test how well the complete set of medications a patient is actively taking at a given moment can be predicted by the sequence of diagnostic billing codes leading up to that moment, in the context of non-trivial label noise. It also explores whether sequence-oriented recursive neural nets can do a better job of that prediction than standard feed-forward networks.

2 BACKGROUND

2.1 MEDICAL BILLING CODES
Each time a patient has billable contact with the healthcare system, one or more date-stamped billing codes are attached to the patient record, indicating the medical conditions that are associated (or suspected to be associated) with the reason for the visit. While these codes are notoriously unreliable because they are only used for billing and not actual clinical practice (O’Malley et al., 2005), they are nevertheless useful in a research context (Bastarache & Denny, 2011; Denny et al., 2010), especially if they are used probabilistically (Lasko, 2014). In our institution, codes from the International Classification of Diseases, Ninth Revision (ICD-9) have historically been used, although we have recently transitioned to the tenth revision (ICD-10). For this project, we used ICD-9 codes.
The ICD-9 hierarchy consists of 21 chapters roughly corresponding to a single organ system or pathologic class (Appendix B). Leaf-level codes in that tree represent single diseases or disease subtypes. For this project, we used a subset of the two thousand most common leaf-level codes as our input data.

2.2 RECURRENT NEURAL NETWORKS AND VARIATIONS
Most of the ICLR community are very familiar with recurrent neural networks and their variations, but we include a conceptual description of them here for readers coming from other fields. More thorough descriptions are available elsewhere (Graves, 2012; Olah, 2015).
A recurrent neural network is a variation in which the output of one node on input xt loops around to become an input to another node on input xt+1, allowing information to be preserved as it iterates over an input data sequence (Figure 1). They were introduced in the 1980s (Rumelhart et al., 1986), but achieved explosive popularity only recently, after the development of methods to more reliably capture long-term dependencies, which significantly improved their performance on sequence-tosequence mapping (Hochreiter & Schmidhuber, 1997; Sutskever et al., 2014).
The basic RNN unit has a simple internal structure (Figure 2a). Output from the previous iteration ht−1 and the next input in a sequence xt are both fed to the network on the next iteration. The Long Short-Term Memory configuration (LSTM) introduces new, more complex internal structure (Figure 2b) consisting of four neural network layers and a cell state (ct), which is carried from one iteration to another. The additional layers form forget, input and output gates, which allow for the information to be forgotten (reset) or passed on to varying degrees.
The LSTM model and its variations are commonly used in applications where sequence and temporal data are involved, such as in image captioning (Vinyals et al., 2014), language translation (Sutskever et al., 2014), and speech recognition (Graves et al., 2013). In many cases LSTM models define the state of the art, such as with a recent conversational speech recognizer that (slightly) outperforms professional transcriptionists (Xiong et al., 2016).
A recent variation on the LSTM architecture is the Gated Recurrent Unit (GRU) (Cho et al., 2014), which introduces a single update gate in place of input and forget gates (Figure 2c). GRUs perform as well as or better than LSTMs in many cases (Chung et al., 2014; Jozefowicz et al., 2015), and have the additional advantage of a simpler structure.
In this work we try both an LSTM and a GRU on our learning problem.

2.3 RELATED WORK
Little research in the computational medical domain has used recurrent neural networks. The earliest example we are aware of is the use of an LSTM model that produced reasonable accuracy
(micro-AUC 0.86) in a 128-dimensional multi-label prediction of diagnoses from regularly sampled, continuously-monitored, real-valued physiologic variables in an Intensive Care Unit setting. This was an interesting initial application, but it turned out to be only 0.001 better than the baseline classifier, which was a multi-layer perceptron with expert-designed features (Lipton et al., 2016). Given the dataset size (10,401 patient records) the lack of improvement may have been due to insufficient data to power accurate feature learning in the recurrent network.
Very recent work, contemporary with ours, used a GRU model with a semantic embedding in 32,787 patient records to predict the development of heart failure 3 - 6 months in the future, from medication orders and billing codes in an 18-month window. The model achieved respectable accuracy (0.88 AUC), and demonstrated a meaningful 0.05 AUC improvement over a deep feedforward network (Choi et al., 2016b).
Other recent work from the same group used a GRU model in a multi-label context to predict the medications, billing codes, and time of the next patient visit from a sequence of that same information for previous visits, using 263,706 patient records. It achieved a recall@30 of 72.4 for the task, an improvement of 20 over a single-hidden-layer MLP with 2000 units (Choi et al., 2016a). This is an example of using one of the strengths of a recurrent network - predicting the next element in a sequence. It contrasts with our work that exploits a different strength of recurrent networks - predicting a sequence or class that is semantically distinct from but parallel to the elements of the input sequence.
The closest work to ours from a medical domain perspective is a series of collaborative filter models (including co-occurrence counting, k-nearest neighbors, and logistic regression) that predict missing medications using a leave-one-drug-out evaluation design, with predictions based on the rest of the medications, ICD-9 billing codes, and demographic data. The models were trained and tested on data from 419 patients in three different clinics, with accuracy varying by clinic, as expected, but not appreciably by model. Most models ranked the missing drug in the top 10 results between 40 and 50% of the time, and ranked the therapeutic class of the drug in the top 10 results between 50 and 65% of the time.
Many aspects of our work can be found in these prior efforts, but none addresses our particular problem in the same way. Our work is unique in its learning problem of identifying all drugs a patient is likely to be taking, based only on the billing codes in the record. Like most others cited, we use recurrent neural networks in a multi-label predictive context, but in contrast to them we compare
to the most similar non-recurrent model we can construct, in order to evaluate the contribution of the temporal sequence information to the solution. Finally, we use one to four orders of magnitude more data (3.3 million instances, see Section 3.1) than these prior efforts, which we hope will give us a more realistic assessment of the various deep architectures we use on our problem.

3 EXPERIMENTS

3.1 DATA
Our source database was the deidentified mirror of Vanderbilt’s Electronic Medical Record, which contains billing codes, medication histories, laboratory test results, narrative text and medical imaging data for over 2 million patients, reaching back nearly 30 years (Roden et al., 2008). We obtained IRB approval to use this data in this research.
For this experiment we filtered all records in our database to include only the top 1,000 most common medications and the top m = 2000 most common billing codes, which cover 99.5% of all medication occurrences and 85.1% of all billing code occurrences. We then included all records from the filtered data that had at least one medication occurrence and at least ten billing code occurrences. This resulted in 610,076 complete patient records, which we divided 80/5/15 into training, validation, and final test sets.
A data instance d = {E, T, y} consisted of a sequence E = {e1, . . . , en}, of one-hot billing code vectors ei ∈ {0, 1}m and their associated times T = {t1, . . . , tn}, ti ∈ R as input, and a multi-label vector y ∈ {0, 1}k of medication classes as the output target. The most recent n = 100 billing codes to a selected reference time point in a given patient record were collected into the input sequence E, and their occurrence times into T , zero padding if necessary. All medications that occurred during the time span of T were then collected into the output vector y. Practice patterns change over time, so simply taking the most recent 100 codes for each patient could produce a biased result. To avoid this, we chose random reference points, stratified by medication. In other words, the reference points were randomly chosen from the occurrences of each medication in the entire dataset, up to 10,000 points per medication. This resulted in 3.3 million data instances, an average of 5.4 instances per patient record. Each patient’s data was included in at most one of the training, validation, or test sets.
Because there are often many approximately equivalent medication choices for a given therapeutic purpose, we converted medication names to their therapeutic class (beta blocker, immunosuppressant, corticosteroid, etc.) as a synonym reduction step. This step also aggregated generic with brand names, as well as different formulations of the same active ingredient. For this task we used the Anatomical Chemical Classification System (ATC)1, which is a multi-level ontology of medications, organized by both anatomic and therapeutic class. The top level is a broad categorization of medications (Appendix B), the bottom (fifth) level is individual medications, and we used the third level, which contains 287 therapeutic classes of the approximately appropriate abstraction level for our purpose. We used a publicly available mapping2 to translate between our medication names and ATC codes, with manual mapping for the minority of medications that had no mapping entry. Our set of medications used k = 182 third-level ATC codes, rendering our output label a 182-elementlong multi-label vector, in which an element is set yi = 1 if a medication in that class appeared in the set of medications identified for that instance, yi = 0 otherwise. Some medications mapped to more than one class, and we set yi = 1 for all of them.
Our medication data was collected from structured order entry records and extracted using NLP (Xu et al., 2010) from mentions in the narrative text of a patient record that included the medication name, dose, route and frequency. As discussed above, we assumed (and our results demonstrate) that the medication data is incomplete, and our hope was that a model learned from a sufficiently large dataset will be robust to the missing data.
This configuration represents the input billing codes in a sequence, but the output medications as a multi-label vector. This is because ICD-9 codes are represented sequentially in our source data, but medications are not. They are represented as a list that changes over time in the record. The
1http://www.whocc.no/atc/structure and principles/ 2https://www.nlm.nih.gov/research/umls/rxnorm/
usual goal of clinicians is to verify the list of medications at each visit, and if omissions or additions are indicated by the patient, to change the list to reflect that. But in the time-constrained reality of clinical practice, this reconciliation happens sporadically, and many clinicians are hesitant to change an entry on the medication list for which they were not the original prescriber, so the timing of the changes in the documentation do not reflect the timing of changes in reality. Therefore we are reduced to predicting a single multi-label vector, representing the medications that the patient probably took during the span of time represented by the input codes. (We actually did attempt some full sequence-to-sequence mappings, with various orderings of the medication sequences, but we did not achieve any promising results in that direction.)

3.2 CLASSIFIERS
Our main technical goal was to test the performance of recurrent neural networks on this sequencecentric prediction problem. To evaluate the specific gains provided by the recurrent architectures, we compare performance against a fully connected feed-forward network configured as similarly as possible to the recurrent networks, and (as baselines) a random forest and a constant-prevalence model. We discuss the specific configurations of these classifiers in this section.

3.2.1 RECURRENT NEURAL NETWORKS
We tested both LSTMs and GRUs in this experiment. We configured both architectures to first compute a semantic embedding xi ∈ Rb of each input ei vector, before appending the times ti (Figure 3) and feeding the result to three layers of recurrent units. The final output from the last pass of recurrent unit is as a multi-label prediction for each candidate medication.
The optimal hyperparameters for the model were selected in the randomized parameter optimization (Bergstra & Bengio, 2012), with the embedding dimension b = 32, number of layers, and number of nodes optimized by a few trials of human-guided search. Other optimized parameters included the fraction of dropout (between layers, input gates and recurrent connections), and L1 and L2 regularization coefficients (final values are presented in Appendix A).
Both models were implemented using Keras (Chollet, 2015) and trained for 300 iterations using cross-entropy under the Adadelta optimizer (Zeiler, 2012).

3.2.2 FULLY CONNECTED NEURAL NETWORK
The fully connected network used as similar an architecture as possible to the recurrent networks, in an attempt to isolate the gain achieved from the recurrence property. Specifically, we used the same architecture for embedding and timestamp appending (Figure 3).
Hyperparameters were optimized using random search over the number of layers, number of nodes, dropout, activation function between layers, L1 and L2 regularization coefficients (Appendix A). (Surprisingly, the optimizer chose tanh over ReLU as the optimal activation function.)
The models were also implemented using Keras, and were trained using cross-entropy for 500 iterations under the Adadelta optimizer.

3.2.3 RANDOM FOREST
Because the random forest model is not easily structured to operate on sequences, we represented the input data as either binary occurrence vectors v ∈ {0, 1}m, or bag-of-codes vectors w ∈ Nm (counts of each code value in the sequence) rather than as sequences of codes with associated times. No embedding was used, because random forest code was not able to cope with the large size of the data in the (dense) embedded space.
Even in the (sparse) original space, the full dataset was too large for the random forest code, so we implemented it as an ensemble of ten independent forests, each trained on one tenth of the training data, and their average score used for test predictions.
Models were implemented using scikit-learn (Pedregosa et al., 2011) with parameters optimized under random search (Appendix A).
While other models could reasonably serve as a baseline for this work, we chose a random forest because they tend to perform well on widely varying datasets (Fernández-Delgado et al., 2014), they are efficient to train and test, and they don’t require a huge effort to optimize (in order to produce a fair comparison).

3.3 CONSTANT-PREVALENCE MODEL
This minimum baseline model simply predicts the prevalence of each label for all instances. For example, if there were three possible medications, with prevalences of 0.3, 0.9, and 0.2, then the prediction of this model would be a constant [0.3, 0.9, 0.2] for each instance. We include this model in order to mitigate the fact that while all of our evaluation measures are suitable for comparing models on the same data, some are not well suited for external comparison because they depend, for example, on the prevalence of positive labels (Section 3.4). By including this model we can at least establish a true minimum baseline for reference.

3.4 EVALUATION
Our main evaluation focused on the models, although we also performed a separate evaluation of the embedding.

3.4.1 MODELS
There are several possibilities for evaluation in a multi-label classification context (Sechidis et al., 2011; Zhang & Zhou, 2014). We chose micro-averaged area under the ROC curve (AUC) and label ranking loss as the primary methods of evaluation, because they treat each instance with equal weight, regardless of the nature of the positive labels for that instance. In other words, we wanted primary measures that did not give a scoring advantage to instances with either very many or very few positive labels, or that included very rare or very prevalent labels. Additionally, both of these measures appeal to us as intuitive extensions of the usual binary AUC, when seen from the perspective of a single instance. However, because these two measures don’t reflect all aspects of multi-label prediction performance, we also include macro-averaged AUC, label ranking average precision and coverage error measures.
Micro-averaged AUC considers each of the multiple label predictions in each instance as either true or false, and then computes the binary AUC as if they all belonged to the same 2-class problem (Zhang & Zhou, 2014). In other words, micro-averaged AUC Aµ is:
Aµ = ∣∣(x, x′, l, l′) : f(x, l) ≥ f(x′, l′), (x, l),∈ S, (x′, l′) ∈ S̄∣∣∣∣S∣∣∣∣S̄∣∣ , (1) where S = {(x, l) : l ∈ Y } is the set of (instance, label) pairs with a positive label, and Y = {yi : yi = 1, i = 1 . . . k} is the set of positive labels for input x. Label ranking loss LR gives the average fraction of all possible (positive, negative) label pairs for each instance in which the negative label has a higher score than the positive label (Tsoumakas et al., 2010):
LR = 1
N N∑ j=1
1
|Y (j)||Y (j)| ∣∣∣{(l, l′) : r(j)(l) > r(j)(l′), (l, l′) ∈ Y (j) × Y (j) }∣∣∣ (2) where the superscript (j) refers to the jth test instance (of N total instances) and r(l) is the predicted rank of a label l.
Macro-averaged AUC can be thought of as averaging the AUC performance of several one-vs-all classifiers, one model for each label. It treats each model equally, regardless of the prevalence of positive labels for that model. This gives a score of 0.5 to the constant-prevalence model, at the cost of weighting instances differently in order to achieve that. This is in contrast to micro-averaged AUC, which can be thought of as averaging across instances rather than labels. It weighs each instance equally, at the cost of a 0.5 score no longer being the random-guessing baseline.
Label ranking average precision gives the mean fraction of correct positive labels among all positive labels with lower scores for each label. The coverage error function calculates the mean number of labels on the ranked list that are needed to cover all the positive labels of the sample. Both of these depend on the prevalence of positive labels in a test instance.

3.4.2 EMBEDDING
We evaluated the embedding based on how strongly related in a clinical semantic sense the nearest neighbor to each code is (in the embedding space). A licensed physician manually annotated the list of all 2000 codes with its match category m ∈ {strongly related,loosely related,unrelated}, and we computed the empirical marginal probability P (m) of each category, the empirical conditional probability P (m|d) of the match category given the nearest neighbor (Manhattan) distance d and the empirical marginal probability P (d). For comparison, we computed P (m) under 100 random code pairings.

4 RESULTS AND DISCUSSION
The GRU model had the top performance by all measures, although the LSTM was a close second (Table 1), a performance pattern consistent with previous reports (Chung et al., 2014). The deep neural net performance was about 0.01 worse in both measures, suggesting that the recurrent models were able to use the sequence information, but only to a small advantage over the most similar nontemporal architecture. However, we note that both RNNs’ performance peaked at the top end of our tractable range for model size, while the feed-forward network peaked using a model about one third that size (Appendix A). Experimenting with the architecture, we found that increasing the number of nodes or layers for the feed-forward network increased training time but not performance. This suggests that the RNN performance was limited by the hardware available, and increasing the size of the model may further increase performance, and that the feed-forward network was limited by something else.
Both random forest models were weaker than the deep neural net, as might be expected from the need to resort to binary and bag-of-codes representations of the input data.
A natural question is what performance is good enough for clinical use. While there is little clinical experience with multi-label classifiers, we would generally expect clinicians using a binary classifier in an advisory role to find an AUC & 0.9 to be useful, and AUC & 0.95 to be very useful. An AUC difference of 0.01, and perhaps 0.005 are potentially noticeable in clinical use.
This 0.9/0.01 rule of thumb may loosely translate to our AUC variants, but it can directly translate to Label Ranking Loss LR (2). If we think of a single output prediction ŷ ∈ [0, 1]k as a set of predictions for k binary labels, then 1− AUC for that set of predictions is equivalent to LR for the original instance ŷ. Therefore, values of LR . 0.1 may be clinically useful, and LR . 0.05 may be very useful.
Subjectively examining performance on 20 randomly selected cases, we find very good detailed predictions, but also evidence of both missing medications and missing billing codes. An example of a good set of detailed predictions is from a complex patient suffering from multiple myeloma (a type of cancer) with various complications. This patient was taking 26 medications, 24 of which had moderate to high probability predictions (Figure 4). (We have found by eyeball that a prediction cutoff of 0.2 gives a reasonable balance between sensitivity and specificity for our model.) In the other direction, only two of the high-prediction classes were not actually being taken, but those classes, along with several of the other moderately-predicted classes, are commonly used for cancer and are clinically reasonable for the case. (Details of this and the two cases below are in Appendix C).
A good example of missing medications is a case in which the record has multiple billing codes for both osteoporosis (which is very commonly treated with medication) and postablative hypothyroidism (a deliberately induced condition that is always treated with medication), but no medications of the appropriate classes were in the record. The GRU model predicted both of these classes, which the patient was almost surely taking.
A good example of either missing billing codes or discontinued medications that remain documented as active is a case in which the record has at least five years of data consisting only of codes for Parkinson’s disease, but which lists medications for high cholesterol, hypertension, and other heart disease. The GRU model predicted a reasonable set of medications for Parkinson’s disease and its complications, but did not predict the other medications that are not suggested by the record.
Given how easy it was to find cases with apparently missing codes and medications, we conclude that there is indeed a substantial amount of label noise in our data, and we therefore interpret our models’ performance as lower bounds on the actual performance. We are encouraged that this kind of a model may actually be useful for identifying missing medications in the record, but of course a more thorough validation, and possibly a more accurate model, would be necessary before using in a clinical scenario. A definitive experiment would use off-line research, including reconciling information from various electronic and human sources to establish the ground truth of which medications were being taken on a particular day, but such efforts are labor intensive and expensive, and can only be conducted on a very small scale.
An interesting byproduct of these models is the semantic embedding of ICD-9 codes used in the recurrent networks (Figure 5). Transforming input to a semantic embedding is a common pre-
processing step to improve performance, but clearly the semantic understanding it provides to an algorithm can be useful beyond the immediate learning problem (Mikolov et al., 2013). Investigating the embedding learned in this experiment shows some generalizable potential, but it also reveals the need for further refinement before it can be truly useful. Specifically, while it’s easy to find tight groups of ICD-9 codes that are strongly clinically related in our embedding, we also find groups for which we cannot see a meaningful clinical relationship.
For example, we see two groups of codes relating to kidney failure and diabetes mellitus, two classes of very prevalent disease (Figure 5, insets). In other iterations with different parameter settings, the kidney failure codes were even embedded in a sequence reflecting the natural progression of the disease, with the code for dialysis (an intensive treatment for end-stage kidney failure) embedded at the appropriate place. Interestingly, these were not the parameter settings that optimized overall prediction performance. In other settings, such as our performance-optimal setting, the sequence is close to the natural progression of the disease, but not quite identical. Nevertheless, this is an exciting result that suggests great potential.
Further evaluation of the embedding found that 49% of codes were strongly related semantically to their nearest neighbor, 10% were loosely related, and 41% unrelated. This fraction of strongly related nearest neighbors was lower than we had hoped, but much higher than expected by chance (Figure 6), and it definitely improved classification performance. Furthermore, it was obvious by inspection that in general, codes closer in the embedding were more semantically related than distant codes, but interestingly, the distance to the nearest such neighbor showed the opposite relationship — nearest neighbors that were very close were less likely to be semantically related than nearest neighbors that were far, and this trend is roughly linear across the full range of d (Figure 6). So the sparser the points are in the embedded space, the more semantically related they are to their nearest neighbor, but the causal direction of that effect and the technical reason for it are beyond the scope of this initial work.
For this prediction problem, we settled on predicting the medications that occurred in the record during the same time span as the billing codes used. Originally, we intended to predict only the medications listed on the day of the reference point, but that turned out to greatly exacerbate the missing medication problem. After trying medications that fell on the reference day only, the week prior to the reference day, and the six months prior, our best performance both subjectively and objectively was achieved using the full time range of the input data.
While the performance of the recurrent networks was quite good, we believe it could be improved by including additional input data, such as laboratory test results, demographics, and perhaps vital
signs. We also suspect that if we can devise a way to convert our medication data into reliablyordered sequences, we can more fully exploit the strengths of recurrent networks for medication prediction. We look forward to trying these and other variations in future work.

ACKNOWLEDGMENTS
This work was funded by grants from the Edward Mallinckrodt, Jr. Foundation and the National Institutes of Health R21LM011664 and R01EB020666. Clinical data was provided by the Vanderbilt Synthetic Derivative, which is supported by institutional funding and by the Vanderbilt CTSA grant ULTR000445.

APPENDIX A.
This appendix lists the optimized parameters for the different models. Except where noted, parameters were optimized under random search.
Recurrent Neural Network Models: (parameters marked with an asterisk were optimized with human-guided search.)
Parameter Model
GRU LSTM
Dropout for input gates 0.1 0.25 Dropout for recurrent connections 0.75 0.75 L1 applied to the input weights matrices 0 0 L1 applied to the recurrent weights matrices 0 0 L2 applied to the input weights matrices 0.0001 0.0001 L2 applied to the recurrent weights matrices 0.0001 0.001 L2 applied to the output layer’s weights matrices 0.0001 0.001 Dropout before the output layer 0.5 0.5 *Number of recurrent layers 3 3 *Number of nodes in recurrent units 400 400
Feed Forward Neural Network Model:
Parameter Value
Dropout before the output layer 0.1 Dropout between feed-forward layers 0.1 Number of feed-forward layers 3 Activation function between feed-forward layers tanh Number of nodes in feed-forward layers 128
Random Forest Model (binary input):
Parameter Value
Number of estimators 800 Ratio of features to consider when looking for the best split 0.4666 Minimum number of samples required to split an internal node 87 Minimum number of samples required to be at a leaf node 3 The function to measure the quality of a split entropy

APPENDIX B.
This appendix lists the top level classes for International Statistical Classification of Diseases and Related Health Problems, Ninth Revision (ICD-9) and Anatomical Chemical Classification System (ATC).
ICD-9 chapters.
Code range Description
001-139 Infectious and parasitic diseases 140-239 Neoplasms 240-279 Endocrine, nutritional and metabolic diseases, and immunity disorders 280-289 Diseases of the blood and blood-forming organs 290-319 Mental disorders 320-359 Diseases of the nervous system 360-389 Diseases of the sense organs 390-459 Diseases of the circulatory system 460-519 Diseases of the respiratory system 520-579 Diseases of the digestive system 580-629 Diseases of the genitourinary system 630-679 Complications of pregnancy, childbirth, and the puerperium 680-709 Diseases of the skin and subcutaneous tissue 710-739 Diseases of the musculoskeletal system and connective tissue 740-759 Congenital anomalies 760-779 Certain conditions originating in the perinatal period 780-799 Symptoms, signs, and ill-defined conditions 800-999 Injury and poisoning
V01-V91 Supplementary - factors influencing health status and contact with health services E000-E999 Supplementary - external causes of injury and poisoning
Top level groups ATC codes and their corresponding colors used in Figure 4 and Appendix C.
Code Contents Color
A Alimentary tract and metabolism B Blood and blood forming organs C Cardiovascular system D Dermatologicals G Genito-urinary system and sex hormones H Systemic hormonal preparations, excluding sex hormones and insulins J Antiinfectives for systemic use L Antineoplastic and immunomodulating agents M Musculo-skeletal system N Nervous system P Antiparasitic products, insecticides and repellents R Respiratory system S Sensory organs V Various

APPENDIX C.
This appendix presents results from three illustrative cases from the dozen cases randomly selected for individual evaluation.
CASE 1.
ICD-9 code Code description Time estimate (ago)
203.00 Multiple myeloma, without mention of having achieved remission 4.8 months ago 273.1 Monoclonal paraproteinemia 4.8 months ago 285.9 Anemia, unspecified 4.8 months ago 276.50 Volume depletion, unspecified 4.8 months ago 733.00 Osteoporosis, unspecified 4.8 months ago 203.00 Multiple myeloma, without mention of having achieved remission 4.8 months ago 203.00 Multiple myeloma, without mention of having achieved remission 2.9 months ago 203.01 Multiple myeloma, in remission 2.9 months ago 273.1 Monoclonal paraproteinemia 2.9 months ago 273.1 Monoclonal paraproteinemia 1.6 months ago 279.3 Unspecified immunity deficiency 1.6 months ago 203.00 Multiple myeloma, without mention of having achieved remission 1.6 months ago 781.2 Abnormality of gait 3.7 weeks ago 203.00 Multiple myeloma, without mention of having achieved remission 3.7 weeks ago 401.9 Unspecified essential hypertension 3.7 weeks ago V12.54 Personal history of transient ischemic attack (TIA), and cerebral infarction without residual deficits 3.7 weeks ago 794.31 Nonspecific abnormal electrocardiogram [ECG] [EKG] 3.7 weeks ago 786.09 Other respiratory abnormalities 3.7 weeks ago 273.1 Monoclonal paraproteinemia 3.7 weeks ago 203.00 Multiple myeloma, without mention of having achieved remission 3.6 weeks ago V58.69 Long-term (current) use of other medications 3.6 weeks ago 794.31 Nonspecific abnormal electrocardiogram [ECG] [EKG] 3.4 weeks ago 203.00 Multiple myeloma, without mention of having achieved remission 4 days ago V42.82 Peripheral stem cells replaced by transplant 4 days ago 203.01 Multiple myeloma, in remission 3 days ago
38.97 Central venous catheter placement with guidance 3 days ago V42.82 Peripheral stem cells replaced by transplant 3 days ago V58.81 Fitting and adjustment of vascular catheter 3 days ago 203.00 Multiple myeloma, without mention of having achieved remission 3 days ago V42.82 Peripheral stem cells replaced by transplant 2 days ago 203.01 Multiple myeloma, in remission 2 days ago 203.00 Multiple myeloma, without mention of having achieved remission 1 day ago V42.82 Peripheral stem cells replaced by transplant 1 day ago 203.00 Multiple myeloma, without mention of having achieved remission now V42.82 Peripheral stem cells replaced by transplant now
Medication predictions for a complicated patient. Each vertical bar represents the prediction for a single medication class, with the height of the bar representing the confidence of the prediction. Black labels above arrows indicate ATC therapeutic classes for medications the patient was actually taking. Colors and letters below the axis indicate high-level therapeutic class groups.
Predicted vs. actual medication classes for the patient in Case 1. The four-character sequence in the first and fourth columns is the ATC code for the medication therapeutic class, and an asterisk in the first column indicates that the predicted medication is in the actual medication list. Probabilities listed are the model predictions for the listed therapeutic class. In the predicted medications column, all predictions with probability at least 0.2 are listed.
Top predictions Prob. True labels Prob.
S03B* Corticosteroids 97.01% S03B Corticosteroids 97.01% S01C* Antiinflammatory agents and antiinfectives in combi-
nation 95.54% S01C Antiinflammatory agents and antiinfectives in combination 95.54%
S02B* Corticosteroids 95.54% S02B Corticosteroids 95.54% L01A Alkylating agents 94.00% D07X Corticosteroids, other combinations 93.37% D07X* Corticosteroids, other combinations 93.37% H02A Corticosteroids for systemic use, plain 91.06% H02A* Corticosteroids for systemic use, plain 91.06% D07A Corticosteroids, plain 90.83% D07A* Corticosteroids, plain 90.83% S01B Antiinflammatory agents 90.79% S01B* Antiinflammatory agents 90.79% D10A Anti-acne preparations for topical use 88.56% D10A* Anti-acne preparations for topical use 88.56% C05A Agents for treatment of hemorrhoids and anal fissures
for topical use 88.52%
C05A* Agents for treatment of hemorrhoids and anal fissures for topical use 88.52% R01A Decongestants and other nasal preparations for topical use 87.02% A04A Antiemetics and antinauseants 87.95% J05A Direct acting antivirals 86.83% R01A* Decongestants and other nasal preparations for topi-
cal use 87.02% A01A Stomatological preparations 86.11%
J05A* Direct acting antivirals 86.8% N02A Opioids 84.86% A01A* Stomatological preparations 86.11% B05C Irrigating solutions 82.56% N02A* Opioids 84.86% A12C Other mineral supplements 79.50% B05C* Irrigating solutions 82.56% B05X I.V. solution additives 74.84% A12C* Other mineral supplements 79.50% L04A Immunosuppressants 68.76% B05X* I.v. solution additives 74.84% N02B Other analgesics and antipyretics 57.24% L04A* Immunosuppressants 68.76% S01A Antiinfectives 54.59% N05A Antipsychotics 58.64% J01D Other beta-lactam antibacterials 43.40% N02B* Other analgesics and antipyretics 57.24% C03C High-ceiling diuretics 39.88% S01A* Antiinfectives 54.59% J01M Quinolone antibacterials 29.78% L03A Immunostimulants 45.96% C07A Beta blocking agents 27.08% A02B Drugs for peptic ulcer and gastro-oesophageal reflux
disease 44.56%
J01D* Other beta-lactam antibacterials 43.40% N03A Antiepileptics 20.00% C03C* High-ceiling diuretics 39.88% J01X Other antibacterials 5.88% B01A Antithrombotic agents 37.80% M03B Muscle relaxants, centrally acting agents 5.09% V03A All other therapeutic products 34.18% R06A Antihistamines for systemic use 31.78% A06A Drugs for constipation 31.57% J01M* Quinolone antibacterials 29.78% N05B Anxiolytics 29.42% D04A Antipruritics, incl. antihistamines, anesthetics, etc. 27.62% C07A* Beta blocking agents 27.08% L01X Other antineoplastic agents 24.72% R05C Expectorants, excl. combinations with cough sup-
pressants 20.43%
N03A* Antiepileptics 20.00%
CASE 2.
ICD-9 code Code description Time estimate (ago)
735.4 Other hammer toe (acquired) 2.4 years ago 729.5 Pain in limb 2.4 years ago 244.1 Other postablative hypothyroidism 1.5 years ago 285.9 Anemia, unspecified 1.5 years ago 244.1 Other postablative hypothyroidism 1.2 years ago 244.1 Other postablative hypothyroidism 11.5 months ago
733.00 Osteoporosis, unspecified 11.5 months ago 733.01 Senile osteoporosis 7.7 months ago 268.9 Unspecified vitamin D deficiency 7.7 months ago 729.5 Pain in limb 7.7 months ago 174.9 Malignant neoplasm of breast (female), unspecified 7.7 months ago 722.52 Degeneration of lumbar or lumbosacral intervertebral disc 7.7 months ago 279.3 Unspecified immunity deficiency 7.7 months ago 733.01 Senile osteoporosis 6.4 months ago 733.01 Senile osteoporosis 6.2 months ago 244.1 Other postablative hypothyroidism 6.0 months ago 401.1 Benign essential hypertension 6.0 months ago V58.69 Long-term (current) use of other medications 1.9 weeks ago 733.01 Senile osteoporosis now 244.1 Other postablative hypothyroidism now V58.69 Long-term (current) use of other medications now
Predicted vs. actual medication classes for Case 2. Table structure as in case 1.
Top predictions Prob. True labels Prob.
M05B Drugs affecting bone structure and mineralization 88.18% A11C Vitamin a and d, incl. combinations of the two 39.42% H03A Thyroid preparations 84.82% N06A Antidepressants 20.88% H05A Parathyroid hormones and analogues 66.33% C10A Lipid modifying agents, plain 17.05% A11C* Vitamin a and d, incl. combinations of the two 39.42% N03A Antiepileptics 15.61% N02B Other analgesics and antipyretics 37.58% C09C Angiotensin ii antagonists, plain 10.38% A01A Stomatological preparations 23.05% L02B Hormone antagonists and related agents 4.22% A12A Calcium 21.59% N06A* Antidepressants 20.88% C07A Beta blocking agents 20.81%
Medication predictions for a simpler patient. Note that the high-prediction medications are clinically reasonable given the billing codes in the sequence. Figure representation as in case 1.
CASE 3.
ICD-9 code Code description Time estimate (ago)
332.0 Paralysis agitans 5.0 years ago 332.0 Paralysis agitans 4.7 years ago 332.0 Paralysis agitans 4.5 years ago 332.0 Paralysis agitans 4.0 years ago 332.0 Paralysis agitans 3.5 years ago 332.0 Paralysis agitans 3.0 years ago 332.0 Paralysis agitans 2.7 years ago 332.0 Paralysis agitans 2.4 years ago 332.0 Paralysis agitans 2.0 years ago 332.0 Paralysis agitans 1.7 years ago 332.0 Paralysis agitans 1.0 years ago 332.0 Paralysis agitans 9.9 months ago 332.0 Paralysis agitans 4.1 months ago 332.0 Paralysis agitans now
Predicted vs. actual medication classes for Case 3. Table structure as in case 1.
Top predictions Prob. True labels Prob.
N04B Dopaminergic agents 97.66% C10A Lipid modifying agents, plain 13.90% N03A Antiepileptics 34.01% C09A Ace inhibitors, plain 9.21% N02B Other analgesics and antipyretics 32.81% C01E Other cardiac preparations 5.56% N06A Antidepressants 26.10% C02C Antiadrenergic agents, peripherally acting 0.72% N02A Opioids 20.33% G03B Androgens 0.32%
A14A Anabolic steroids 0.08%
Medication predictions for a patient with only one ICD-9 code, repeated many times over five years. The medications listed under true labels are not indicated for paralysis agitans (Parkinson’s disease), but the patient was surely taking them for reasons not documented in the ICD-9 sequence. The model predicted mostly reasonable medications for a patient with Parkinson’s disease, especially Dopaminergic agents, which is the primary treatment for the disease. Figure representation as in case 1, above.
","It is a surprising fact that electronic medical records are failing at one of their primary purposes, that of tracking the set of medications that the patient is actively taking. Studies estimate that up to 50% of such lists omit active drugs, and that up to 25% of all active medications do not appear on the appropriate patient list. Manual efforts to maintain these lists involve a great deal of tedious human labor, which could be reduced by computational tools to suggest likely missing or incorrect medications on a patient’s list. We report here an application of recurrent neural networks to predict the likely therapeutic classes of medications that a patient is taking, given a sequence of the last 100 billing codes in their record. Our best model was a GRU that achieved high prediction accuracy (micro-averaged AUC 0.93, Label Ranking Loss 0.076), limited by hardware constraints on model size. Additionally, examining individual cases revealed that many of the predictions marked incorrect were likely to be examples of either omitted medications or omitted billing codes, supporting our assertion of a substantial number of errors and omissions in the data, and the likelihood of models such as these to help correct them.",ICLR 2017 conference submission,True,,"This is a well-conducted and well-written study on the prediction of medication from diagnostic codes. The authors compared GRUs, LSTMs, feed-forward networks and random forests (making a case for why random forests should be used, instead of SVMs) and analysed the predictions and embeddings.

The authors also did address the questions of the reviewers.

My only negative point is that this work might be more relevant for a data science or medical venue rather than at ICLR.

---

This paper applies RNNs to predict medications from billing costs. While this paper does not have technical novelty, it is well done and well organized. It demonstrates a creative use of recent models in a very important domain, and I think many people in our community are interested and inspired by well-done applications that branch to socially important domains. Moreover, I think an advantage to accepting it at ICLR is that it gives our ""expert"" stamp of approval -- I see a lot of questionable / badly applied / antiquated machine learning methods in domain conferences, so I think it would be helpful for those domains to have examples of application papers that are considered sound.

---

This is a well written, organized, and presented paper that I enjoyed reading.  I commend the authors on their attention to the narrative and the explanations.  While it did not present any new methodology or architecture, it instead addressed an important application of predicting the medications a patient is using, given the record of billing codes.  The dataset they use is impressive and useful and, frankly, more interesting than the typical toy datasets in machine learning.  That said, the investigation of those results was not as deep as I thought it should have been in an empirical/applications paper.  Despite their focus on the application, I was encouraged to see the authors use cutting edge choices (eg Keras, adadelta, etc) in their architecture.  A few points of criticism:

-The numerical results are in my view too brief.  Fig 4 is anecdotal, Fig 5 is essentially a negative result (tSNE is only in some places interpretable), so that leaves Table 1.  I recognize there is only one dataset, but this does not offer a vast amount of empirical evidence and analysis that one might expect out of a paper with no major algorithmic/theoretical advances.  To be clear I don't think this is disqualifying or deeply concerning; I simply found it a bit underwhelming.

- To be constructive, re the results I would recommend removing Fig 5 and replacing that with some more meaningful analysis of performance.  I found Fig 5 to be mostly uninformative, other than as a negative result, which I think can be stated in a sentence rather than in a large figure.

- There is a bit of jargon used and expertise required that may not be familiar to the typical ICLR reader.  I saw that another reviewer suggested perhaps ICLR is not the right venue for this work.  While I certainly see the reviewer's point that a medical or healthcare venue may be more suitable, I do want to cast my vote of keeping this paper here... our community benefits from more thoughtful and in depth applications. Instead I think this can be addressed by tightening up those points of jargon and making the results more easy to evaluate by an ICLR reader (that is, as it stands now researchers without medical experience have to take your results after Table 1 on faith, rather than getting to apply their well-trained quantitative eye). 

Overall, a nice paper.

---

In light of the detailed author responses and further updates to the manuscript, I am raising my score to an 8 and reiterating my support for this paper. I think it will be among the strongest non-traditional applied deep learning work at ICLR and will receive a great deal of interest and attention from attendees.

-----

This paper describes modern deep learning approach to the problem of predicting the medications taken by a patient during a period of time based solely upon the sequence of ICD-9 codes assigned to the patient during that same time period. This problem is formulated as a multilabel sequence classification (in contrast to language modeling, which is multiclass classification). They propose to use standard LSTM and GRU architectures with embedding layers to handle the sparse categorical inputs, similar to that described in related work by Choi, et al. In experiments using a cohort of ~610K patient records, they find that RNN models outperform strong baselines including an MLP and a random forest, as well as a common sense baseline. The differences in performance between the recurrent models and the MLP appear to be large enough to be significant, given the size of the test set.

Strengths:
- Very important problem. As the authors point out, two the value propositions of EHRs -- which have been widely adopted throughout the US due to a combination of legislation and billions of dollars in incentives from the federal government -- included more accurate records and fewer medication mistakes. These two benefits have largely failed to materialize. This seems like a major opportunity for data mining and machine learning.
- Paper is well-written with lucid introduction and motivation, thorough discussion of related work, clear description of experiments and metrics, and interesting qualitative analysis of results.
- Empirical results are solid with a strong win for RNNs over convincing baselines. This is in contrast to some recent related papers, including Lipton & Kale et al, ICLR 2016, where the gap between the RNN and MLP was relatively small, and Choi et al, MLHC 2016, which omitted many obvious baselines.
- Discussion is thorough and thoughtful. The authors are right about the kidney code embedding results: this is a very promising result.

Weaknesses:
- The authors make several unintuitive decisions related to data preprocessing and experimental design, foremost among them the choice NOT to use full patient sequences but instead only truncated patient sequences that each ends at randomly chosen time point. This does not necessarily invalidate their results, but it is somewhat unnatural and the explanation is difficult to follow, reducing the paper's potential impact. It is also reduces the RNN's potential advantage.
- The chosen metrics seem appropriate, but non-experts may have trouble interpreting the absolute and relative performances (beyond the superficial, e.g., RNN score 0.01 more than NN!). The authors should invest some space in explaining (1) what level of performance -- for each metric -- would be necessary for the model to be useful in a real clinical setting and (2) whether the gaps between the various models are ""significant"" (even in an informal sense).
- The paper proposes nothing novel in terms of methods, which is a serious weakness for a methods conference like ICLR. I think it is strong enough empirically (and sufficiently interesting in application) to warrant acceptance regardless, but there may be things the authors can do to make it more competitive. For example, one potential hypothesis is that higher capacity models are more prone to overfitting noisy targets. Is there some way to investigate this, perhaps by looking at the kinds of errors each model makes?

I have a final comment: as a piece of clinical work, the paper has a huge weakness: the lack of ground truth labels for missing medications. Models are both trained and tested on data with noisy labels. For training, the authors are right that this shouldn't be a huge problem, provided the label noise is random (even class conditional isn't too big of a problem). For testing, though, this seems like it could skew metrics. Further, the assumption that the label noise is not systemic seems very unlikely given that these data are recorded by human clinicians. The cases shown in Appendix C lend some credence to this assertion: for Case 1, 7/26 actual medications received probabilities < 0.5. My hunch is that clinical reviewers would view the paper with great skepticism. The authors will need to get creative about evaluation -- or invest a lot of time/money in labeling data -- to really prove that this works.

For what it is worth, I hope that this paper is accepted as I think it will be of great interest to the ICLR community. However, I am borderline about whether I'd be willing to fight for its acceptance. If the authors can address the reviewers' critiques -- and in particular, dive into the question of overfitting the imperfect labels and provide some insights -- I might be willing to raise my score and lobby for acceptance.

---

This is a well-conducted and well-written study on the prediction of medication from diagnostic codes. The authors compared GRUs, LSTMs, feed-forward networks and random forests (making a case for why random forests should be used, instead of SVMs) and analysed the predictions and embeddings.

The authors also did address the questions of the reviewers.

My only negative point is that this work might be more relevant for a data science or medical venue rather than at ICLR.

---

This is a well-conducted and well-written study on the prediction of medication from diagnostic codes. The authors compared GRUs, LSTMs, feed-forward networks and random forests (making a case for why random forests should be used, instead of SVMs) and analysed the predictions and embeddings.

The authors also did address the questions of the reviewers.

My only negative point is that this work might be more relevant for a data science or medical venue rather than at ICLR.

---

This paper applies RNNs to predict medications from billing costs. While this paper does not have technical novelty, it is well done and well organized. It demonstrates a creative use of recent models in a very important domain, and I think many people in our community are interested and inspired by well-done applications that branch to socially important domains. Moreover, I think an advantage to accepting it at ICLR is that it gives our ""expert"" stamp of approval -- I see a lot of questionable / badly applied / antiquated machine learning methods in domain conferences, so I think it would be helpful for those domains to have examples of application papers that are considered sound.

---

This is a well written, organized, and presented paper that I enjoyed reading.  I commend the authors on their attention to the narrative and the explanations.  While it did not present any new methodology or architecture, it instead addressed an important application of predicting the medications a patient is using, given the record of billing codes.  The dataset they use is impressive and useful and, frankly, more interesting than the typical toy datasets in machine learning.  That said, the investigation of those results was not as deep as I thought it should have been in an empirical/applications paper.  Despite their focus on the application, I was encouraged to see the authors use cutting edge choices (eg Keras, adadelta, etc) in their architecture.  A few points of criticism:

-The numerical results are in my view too brief.  Fig 4 is anecdotal, Fig 5 is essentially a negative result (tSNE is only in some places interpretable), so that leaves Table 1.  I recognize there is only one dataset, but this does not offer a vast amount of empirical evidence and analysis that one might expect out of a paper with no major algorithmic/theoretical advances.  To be clear I don't think this is disqualifying or deeply concerning; I simply found it a bit underwhelming.

- To be constructive, re the results I would recommend removing Fig 5 and replacing that with some more meaningful analysis of performance.  I found Fig 5 to be mostly uninformative, other than as a negative result, which I think can be stated in a sentence rather than in a large figure.

- There is a bit of jargon used and expertise required that may not be familiar to the typical ICLR reader.  I saw that another reviewer suggested perhaps ICLR is not the right venue for this work.  While I certainly see the reviewer's point that a medical or healthcare venue may be more suitable, I do want to cast my vote of keeping this paper here... our community benefits from more thoughtful and in depth applications. Instead I think this can be addressed by tightening up those points of jargon and making the results more easy to evaluate by an ICLR reader (that is, as it stands now researchers without medical experience have to take your results after Table 1 on faith, rather than getting to apply their well-trained quantitative eye). 

Overall, a nice paper.

---

In light of the detailed author responses and further updates to the manuscript, I am raising my score to an 8 and reiterating my support for this paper. I think it will be among the strongest non-traditional applied deep learning work at ICLR and will receive a great deal of interest and attention from attendees.

-----

This paper describes modern deep learning approach to the problem of predicting the medications taken by a patient during a period of time based solely upon the sequence of ICD-9 codes assigned to the patient during that same time period. This problem is formulated as a multilabel sequence classification (in contrast to language modeling, which is multiclass classification). They propose to use standard LSTM and GRU architectures with embedding layers to handle the sparse categorical inputs, similar to that described in related work by Choi, et al. In experiments using a cohort of ~610K patient records, they find that RNN models outperform strong baselines including an MLP and a random forest, as well as a common sense baseline. The differences in performance between the recurrent models and the MLP appear to be large enough to be significant, given the size of the test set.

Strengths:
- Very important problem. As the authors point out, two the value propositions of EHRs -- which have been widely adopted throughout the US due to a combination of legislation and billions of dollars in incentives from the federal government -- included more accurate records and fewer medication mistakes. These two benefits have largely failed to materialize. This seems like a major opportunity for data mining and machine learning.
- Paper is well-written with lucid introduction and motivation, thorough discussion of related work, clear description of experiments and metrics, and interesting qualitative analysis of results.
- Empirical results are solid with a strong win for RNNs over convincing baselines. This is in contrast to some recent related papers, including Lipton & Kale et al, ICLR 2016, where the gap between the RNN and MLP was relatively small, and Choi et al, MLHC 2016, which omitted many obvious baselines.
- Discussion is thorough and thoughtful. The authors are right about the kidney code embedding results: this is a very promising result.

Weaknesses:
- The authors make several unintuitive decisions related to data preprocessing and experimental design, foremost among them the choice NOT to use full patient sequences but instead only truncated patient sequences that each ends at randomly chosen time point. This does not necessarily invalidate their results, but it is somewhat unnatural and the explanation is difficult to follow, reducing the paper's potential impact. It is also reduces the RNN's potential advantage.
- The chosen metrics seem appropriate, but non-experts may have trouble interpreting the absolute and relative performances (beyond the superficial, e.g., RNN score 0.01 more than NN!). The authors should invest some space in explaining (1) what level of performance -- for each metric -- would be necessary for the model to be useful in a real clinical setting and (2) whether the gaps between the various models are ""significant"" (even in an informal sense).
- The paper proposes nothing novel in terms of methods, which is a serious weakness for a methods conference like ICLR. I think it is strong enough empirically (and sufficiently interesting in application) to warrant acceptance regardless, but there may be things the authors can do to make it more competitive. For example, one potential hypothesis is that higher capacity models are more prone to overfitting noisy targets. Is there some way to investigate this, perhaps by looking at the kinds of errors each model makes?

I have a final comment: as a piece of clinical work, the paper has a huge weakness: the lack of ground truth labels for missing medications. Models are both trained and tested on data with noisy labels. For training, the authors are right that this shouldn't be a huge problem, provided the label noise is random (even class conditional isn't too big of a problem). For testing, though, this seems like it could skew metrics. Further, the assumption that the label noise is not systemic seems very unlikely given that these data are recorded by human clinicians. The cases shown in Appendix C lend some credence to this assertion: for Case 1, 7/26 actual medications received probabilities < 0.5. My hunch is that clinical reviewers would view the paper with great skepticism. The authors will need to get creative about evaluation -- or invest a lot of time/money in labeling data -- to really prove that this works.

For what it is worth, I hope that this paper is accepted as I think it will be of great interest to the ICLR community. However, I am borderline about whether I'd be willing to fight for its acceptance. If the authors can address the reviewers' critiques -- and in particular, dive into the question of overfitting the imperfect labels and provide some insights -- I might be willing to raise my score and lobby for acceptance.

---

This is a well-conducted and well-written study on the prediction of medication from diagnostic codes. The authors compared GRUs, LSTMs, feed-forward networks and random forests (making a case for why random forests should be used, instead of SVMs) and analysed the predictions and embeddings.

The authors also did address the questions of the reviewers.

My only negative point is that this work might be more relevant for a data science or medical venue rather than at ICLR.",,,,,,7.0,,,4.0,,
486,"Authors: Thomas N. Kipf
Source file: 486.pdf

ABSTRACT
We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.

We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.

1 INTRODUCTION
We consider the problem of classifying nodes (such as documents) in a graph (such as a citation network), where labels are only available for a small subset of nodes. This problem can be framed as graph-based semi-supervised learning, where label information is smoothed over the graph via some form of explicit graph-based regularization (Zhu et al., 2003; Zhou et al., 2004; Belkin et al., 2006; Weston et al., 2012), e.g. by using a graph Laplacian regularization term in the loss function:
L = L0 + λLreg , with Lreg = ∑ i,j Aij‖f(Xi)− f(Xj)‖2 = f(X)>∆f(X) . (1)
Here, L0 denotes the supervised loss w.r.t. the labeled part of the graph, f(·) can be a neural networklike differentiable function, λ is a weighing factor and X is a matrix of node feature vectors Xi. ∆ = D − A denotes the unnormalized graph Laplacian of an undirected graph G = (V, E) with N nodes vi ∈ V , edges (vi, vj) ∈ E , an adjacency matrix A ∈ RN×N (binary or weighted) and a degree matrix Dii = ∑ j Aij . The formulation of Eq. 1 relies on the assumption that connected nodes in the graph are likely to share the same label. This assumption, however, might restrict modeling capacity, as graph edges need not necessarily encode node similarity, but could contain additional information.
In this work, we encode the graph structure directly using a neural network model f(X,A) and train on a supervised target L0 for all nodes with labels, thereby avoiding explicit graph-based regularization in the loss function. Conditioning f(·) on the adjacency matrix of the graph will allow the model to distribute gradient information from the supervised loss L0 and will enable it to learn representations of nodes both with and without labels.
Our contributions are two-fold. Firstly, we introduce a simple and well-behaved layer-wise propagation rule for neural network models which operate directly on graphs and show how it can be motivated from a first-order approximation of spectral graph convolutions (Hammond et al., 2011). Secondly, we demonstrate how this form of a graph-based neural network model can be used for fast and scalable semi-supervised classification of nodes in a graph. Experiments on a number of datasets demonstrate that our model compares favorably both in classification accuracy and efficiency (measured in wall-clock time) against state-of-the-art methods for semi-supervised learning.

2 FAST APPROXIMATE CONVOLUTIONS ON GRAPHS
In this section, we provide theoretical motivation for a specific graph-based neural network model f(X,A) that we will use in the rest of this paper. We consider a multi-layer Graph Convolutional Network (GCN) with the following layer-wise propagation rule:
H(l+1) = σ ( D̃− 1 2 ÃD̃− 1 2H(l)W (l) ) . (2)
Here, Ã = A + IN is the adjacency matrix of the undirected graph G with added self-connections. IN is the identity matrix, D̃ii = ∑ j Ãij and W
(l) is a layer-specific trainable weight matrix. σ(·) denotes an activation function, such as the ReLU(·) = max(0, ·). H(l) ∈ RN×D is the matrix of activations in the lth layer; H(0) = X . In the following, we show that the form of this propagation rule can be motivated1 via a first-order approximation of localized spectral filters on graphs (Hammond et al., 2011; Defferrard et al., 2016).

2.1 SPECTRAL GRAPH CONVOLUTIONS
We consider spectral convolutions on graphs defined as the multiplication of a signal x ∈ RN (a scalar for every node) with a filter gθ = diag(θ) parameterized by θ ∈ RN in the Fourier domain, i.e.:
gθ ? x = UgθU >x , (3)
where U is the matrix of eigenvectors of the normalized graph Laplacian L = IN −D− 1 2AD− 1 2 = UΛU>, with a diagonal matrix of its eigenvalues Λ and U>x being the graph Fourier transform of x. We can understand gθ as a function of the eigenvalues of L, i.e. gθ(Λ). Evaluating Eq. 3 is computationally expensive, as multiplication with the eigenvector matrix U isO(N2). Furthermore, computing the eigendecomposition of L in the first place might be prohibitively expensive for large graphs. To circumvent this problem, it was suggested in Hammond et al. (2011) that gθ(Λ) can be well-approximated by a truncated expansion in terms of Chebyshev polynomials Tk(x) up to K th order:
gθ′(Λ) ≈ K∑ k=0 θ′kTk(Λ̃) , (4)
with a rescaled Λ̃ = 2λmax Λ − IN . λmax denotes the largest eigenvalue of L. θ ′ ∈ RK is now a vector of Chebyshev coefficients. The Chebyshev polynomials are recursively defined as Tk(x) = 2xTk−1(x) − Tk−2(x), with T0(x) = 1 and T1(x) = x. The reader is referred to Hammond et al. (2011) for an in-depth discussion of this approximation.
Going back to our definition of a convolution of a signal x with a filter gθ′ , we now have:
gθ′ ? x ≈ K∑ k=0 θ′kTk(L̃)x , (5)
with L̃ = 2λmaxL − IN ; as can easily be verified by noticing that (UΛU >)k = UΛkU>. Note that this expression is nowK-localized since it is aK th-order polynomial in the Laplacian, i.e. it depends only on nodes that are at maximum K steps away from the central node (K th-order neighborhood). The complexity of evaluating Eq. 5 is O(|E|), i.e. linear in the number of edges. Defferrard et al. (2016) use this K-localized convolution to define a convolutional neural network on graphs.

2.2 LAYER-WISE LINEAR MODEL
A neural network model based on graph convolutions can therefore be built by stacking multiple convolutional layers of the form of Eq. 5, each layer followed by a point-wise non-linearity. Now, imagine we limited the layer-wise convolution operation to K = 1 (see Eq. 5), i.e. a function that is linear w.r.t. L and therefore a linear function on the graph Laplacian spectrum.
1We provide an alternative interpretation of this propagation rule based on the Weisfeiler-Lehman algorithm (Weisfeiler & Lehmann, 1968) in Appendix A.
In this way, we can still recover a rich class of convolutional filter functions by stacking multiple such layers, but we are not limited to the explicit parameterization given by, e.g., the Chebyshev polynomials. We intuitively expect that such a model can alleviate the problem of overfitting on local neighborhood structures for graphs with very wide node degree distributions, such as social networks, citation networks, knowledge graphs and many other real-world graph datasets. Additionally, for a fixed computational budget, this layer-wise linear formulation allows us to build deeper models, a practice that is known to improve modeling capacity on a number of domains (He et al., 2016).
In this linear formulation of a GCN we further approximate λmax ≈ 2, as we can expect that neural network parameters will adapt to this change in scale during training. Under these approximations Eq. 5 simplifies to:
gθ′ ? x ≈ θ′0x+ θ′1 (L− IN )x = θ′0x− θ′1D− 1 2AD− 1 2x , (6)
with two free parameters θ′0 and θ ′ 1. The filter parameters can be shared over the whole graph. Successive application of filters of this form then effectively convolve the kth-order neighborhood of a node, where k is the number of successive filtering operations or convolutional layers in the neural network model.
In practice, it can be beneficial to constrain the number of parameters further to address overfitting and to minimize the number of operations (such as matrix multiplications) per layer. This leaves us with the following expression:
gθ ? x ≈ θ ( IN +D − 12AD− 1 2 ) x , (7)
with a single parameter θ = θ′0 = −θ′1. Note that IN + D− 1 2AD− 1 2 now has eigenvalues in the range [0, 2]. Repeated application of this operator can therefore lead to numerical instabilities and exploding/vanishing gradients when used in a deep neural network model. To alleviate this problem, we introduce the following renormalization trick: IN +D− 1 2AD− 1 2 → D̃− 12 ÃD̃− 12 , with
Ã = A+ IN and D̃ii = ∑ j Ãij .
We can generalize this definition to a signalX ∈ RN×C withC input channels (i.e. aC-dimensional feature vector for every node) and F filters or feature maps as follows:
Z = D̃− 1 2 ÃD̃− 1 2XΘ , (8)
where Θ ∈ RC×F is now a matrix of filter parameters and Z ∈ RN×F is the convolved signal matrix. This filtering operation has complexity O(|E|FC), as ÃX can be efficiently implemented as a product of a sparse matrix with a dense matrix.

3 SEMI-SUPERVISED NODE CLASSIFICATION
Having introduced a simple, yet flexible model f(X,A) for efficient information propagation on graphs, we can return to the problem of semi-supervised node classification. As outlined in the introduction, we can relax certain assumptions typically made in graph-based semi-supervised learning by conditioning our model f(X,A) both on the data X and on the adjacency matrix A of the underlying graph structure. We expect this setting to be especially powerful in scenarios where the adjacency matrix contains information not present in the data X , such as citation links between documents in a citation network or relations in a knowledge graph. The overall model, a multi-layer GCN for semi-supervised learning, is schematically depicted in Figure 1.

3.1 EXAMPLE
In the following, we consider a two-layer GCN for semi-supervised node classification on a graph with a symmetric adjacency matrix A (binary or weighted). We first calculate Â = D̃− 1 2 ÃD̃− 1 2 in a pre-processing step. Our forward model then takes the simple form:
Z = f(X,A) = softmax ( Â ReLU ( ÂXW (0) ) W (1) ) . (9)
1
30
Here, W (0) ∈ RC×H is an input-to-hidden weight matrix for a hidden layer with H feature maps. W (1) ∈ RH×F is a hidden-to-output weight matrix. The softmax activation function, defined as softmax(xi) = 1 Z exp(xi) with Z = ∑ i exp(xi), is applied row-wise. For semi-supervised multiclass classification, we then evaluate the cross-entropy error over all labeled examples:
L = − ∑ l∈YL F∑ f=1 Ylf lnZlf , (10)
where YL is the set of node indices that have labels. The neural network weights W (0) and W (1) are trained using gradient descent. In this work, we perform batch gradient descent using the full dataset for every training iteration, which is a viable option as long as datasets fit in memory. Using a sparse representation for A, memory requirement is O(|E|), i.e. linear in the number of edges. Stochasticity in the training process is introduced via dropout (Srivastava et al., 2014). We leave memory-efficient extensions with mini-batch stochastic gradient descent for future work.

3.2 IMPLEMENTATION
In practice, we make use of TensorFlow (Abadi et al., 2015) for an efficient GPU-based implementation2 of Eq. 9 using sparse-dense matrix multiplications. The computational complexity of evaluating Eq. 9 is then O(|E|CHF ), i.e. linear in the number of graph edges.

4 RELATED WORK
Our model draws inspiration both from the field of graph-based semi-supervised learning and from recent work on neural networks that operate on graphs. In what follows, we provide a brief overview on related work in both fields.

4.1 GRAPH-BASED SEMI-SUPERVISED LEARNING
A large number of approaches for semi-supervised learning using graph representations have been proposed in recent years, most of which fall into two broad categories: methods that use some form of explicit graph Laplacian regularization and graph embedding-based approaches. Prominent examples for graph Laplacian regularization include label propagation (Zhu et al., 2003), manifold regularization (Belkin et al., 2006) and deep semi-supervised embedding (Weston et al., 2012).
2Code to reproduce our experiments is available at https://github.com/tkipf/gcn.
Recently, attention has shifted to models that learn graph embeddings with methods inspired by the skip-gram model (Mikolov et al., 2013). DeepWalk (Perozzi et al., 2014) learns embeddings via the prediction of the local neighborhood of nodes, sampled from random walks on the graph. LINE (Tang et al., 2015) and node2vec (Grover & Leskovec, 2016) extend DeepWalk with more sophisticated random walk or breadth-first search schemes. For all these methods, however, a multistep pipeline including random walk generation and semi-supervised training is required where each step has to be optimized separately. Planetoid (Yang et al., 2016) alleviates this by injecting label information in the process of learning embeddings.

4.2 NEURAL NETWORKS ON GRAPHS
Neural networks that operate on graphs have previously been introduced in Gori et al. (2005); Scarselli et al. (2009) as a form of recurrent neural network. Their framework requires the repeated application of contraction maps as propagation functions until node representations reach a stable fixed point. This restriction was later alleviated in Li et al. (2016) by introducing modern practices for recurrent neural network training to the original graph neural network framework. Duvenaud et al. (2015) introduced a convolution-like propagation rule on graphs and methods for graph-level classification. Their approach requires to learn node degree-specific weight matrices which does not scale to large graphs with wide node degree distributions. Our model instead uses a single weight matrix per layer and deals with varying node degrees through an appropriate normalization of the adjacency matrix (see Section 3.1).
A related approach to node classification with a graph-based neural network was recently introduced in Atwood & Towsley (2016). They report O(N2) complexity, limiting the range of possible applications. In a different yet related model, Niepert et al. (2016) convert graphs locally into sequences that are fed into a conventional 1D convolutional neural network, which requires the definition of a node ordering in a pre-processing step.
Our method is based on spectral graph convolutional neural networks, introduced in Bruna et al. (2014) and later extended by Defferrard et al. (2016) with fast localized convolutions. In contrast to these works, we consider here the task of transductive node classification within networks of significantly larger scale. We show that in this setting, a number of simplifications (see Section 2.2) can be introduced to the original frameworks of Bruna et al. (2014) and Defferrard et al. (2016) that improve scalability and classification performance in large-scale networks.

5 EXPERIMENTS
We test our model in a number of experiments: semi-supervised document classification in citation networks, semi-supervised entity classification in a bipartite graph extracted from a knowledge graph, an evaluation of various graph propagation models and a run-time analysis on random graphs.

5.1 DATASETS
We closely follow the experimental setup in Yang et al. (2016). Dataset statistics are summarized in Table 1. In the citation network datasets—Citeseer, Cora and Pubmed (Sen et al., 2008)—nodes are documents and edges are citation links. Label rate denotes the number of labeled nodes that are used for training divided by the total number of nodes in each dataset. NELL (Carlson et al., 2010; Yang et al., 2016) is a bipartite graph dataset extracted from a knowledge graph with 55,864 relation nodes and 9,891 entity nodes.
Citation networks We consider three citation network datasets: Citeseer, Cora and Pubmed (Sen et al., 2008). The datasets contain sparse bag-of-words feature vectors for each document and a list of citation links between documents. We treat the citation links as (undirected) edges and construct a binary, symmetric adjacency matrix A. Each document has a class label. For training, we only use 20 labels per class, but all feature vectors.
NELL NELL is a dataset extracted from the knowledge graph introduced in (Carlson et al., 2010). A knowledge graph is a set of entities connected with directed, labeled edges (relations). We follow the pre-processing scheme as described in Yang et al. (2016). We assign separate relation nodes r1 and r2 for each entity pair (e1, r, e2) as (e1, r1) and (e2, r2). Entity nodes are described by sparse feature vectors. We extend the number of features in NELL by assigning a unique one-hot representation for every relation node, effectively resulting in a 61,278-dim sparse feature vector per node. The semi-supervised task here considers the extreme case of only a single labeled example per class in the training set. We construct a binary, symmetric adjacency matrix from this graph by setting entries Aij = 1, if one or more edges are present between nodes i and j.
Random graphs We simulate random graph datasets of various sizes for experiments where we measure training time per epoch. For a dataset with N nodes we create a random graph assigning 2N edges uniformly at random. We take the identity matrix IN as input feature matrix X , thereby implicitly taking a featureless approach where the model is only informed about the identity of each node, specified by a unique one-hot vector. We add dummy labels Yi = 1 for every node.

5.2 EXPERIMENTAL SET-UP
Unless otherwise noted, we train a two-layer GCN as described in Section 3.1 and evaluate prediction accuracy on a test set of 1,000 labeled examples. We provide additional experiments using deeper models with up to 10 layers in Appendix B. We choose the same dataset splits as in Yang et al. (2016) with an additional validation set of 500 labeled examples for hyperparameter optimization (dropout rate for all layers, L2 regularization factor for the first GCN layer and number of hidden units). We do not use the validation set labels for training.
For the citation network datasets, we optimize hyperparameters on Cora only and use the same set of parameters for Citeseer and Pubmed. We train all models for a maximum of 200 epochs (training iterations) using Adam (Kingma & Ba, 2015) with a learning rate of 0.01 and early stopping with a window size of 10, i.e. we stop training if the validation loss does not decrease for 10 consecutive epochs. We initialize weights using the initialization described in Glorot & Bengio (2010) and accordingly (row-)normalize input feature vectors. On the random graph datasets, we use a hidden layer size of 32 units and omit regularization (i.e. neither dropout nor L2 regularization).

5.3 BASELINES
We compare against the same baseline methods as in Yang et al. (2016), i.e. label propagation (LP) (Zhu et al., 2003), semi-supervised embedding (SemiEmb) (Weston et al., 2012), manifold regularization (ManiReg) (Belkin et al., 2006) and skip-gram based graph embeddings (DeepWalk) (Perozzi et al., 2014). We omit TSVM (Joachims, 1999), as it does not scale to the large number of classes in one of our datasets.
We further compare against the iterative classification algorithm (ICA) proposed in Lu & Getoor (2003) in conjunction with two logistic regression classifiers, one for local node features alone and one for relational classification using local features and an aggregation operator as described in Sen et al. (2008). We first train the local classifier using all labeled training set nodes and use it to bootstrap class labels of unlabeled nodes for relational classifier training. We run iterative classification (relational classifier) with a random node ordering for 10 iterations on all unlabeled nodes (bootstrapped using the local classifier). L2 regularization parameter and aggregation operator (count vs. prop, see Sen et al. (2008)) are chosen based on validation set performance for each dataset separately.
Lastly, we compare against Planetoid (Yang et al., 2016), where we always choose their bestperforming model variant (transductive vs. inductive) as a baseline.

6 RESULTS

6.1 SEMI-SUPERVISED NODE CLASSIFICATION
Results are summarized in Table 2. Reported numbers denote classification accuracy in percent. For ICA, we report the mean accuracy of 100 runs with random node orderings. Results for all other baseline methods are taken from the Planetoid paper (Yang et al., 2016). Planetoid* denotes the best model for the respective dataset out of the variants presented in their paper.
We further report wall-clock training time in seconds until convergence (in brackets) for our method (incl. evaluation of validation error) and for Planetoid. For the latter, we used an implementation provided by the authors3 and trained on the same hardware (with GPU) as our GCN model. We trained and tested our model on the same dataset splits as in Yang et al. (2016) and report mean accuracy of 100 runs with random weight initializations. We used the following sets of hyperparameters for Citeseer, Cora and Pubmed: 0.5 (dropout rate), 5 · 10−4 (L2 regularization) and 16 (number of hidden units); and for NELL: 0.1 (dropout rate), 1 · 10−5 (L2 regularization) and 64 (number of hidden units).
In addition, we report performance of our model on 10 randomly drawn dataset splits of the same size as in Yang et al. (2016), denoted by GCN (rand. splits). Here, we report mean and standard error of prediction accuracy on the test set split in percent.

6.2 EVALUATION OF PROPAGATION MODEL
We compare different variants of our proposed per-layer propagation model on the citation network datasets. We follow the experimental set-up described in the previous section. Results are summarized in Table 3. The propagation model of our original GCN model is denoted by renormalization trick (in bold). In all other cases, the propagation model of both neural network layers is replaced with the model specified under propagation model. Reported numbers denote mean classification accuracy for 100 repeated runs with random weight matrix initializations. In case of multiple variables Θi per layer, we impose L2 regularization on all weight matrices of the first layer.
6.3 TRAINING TIME PER EPOCH
Here, we report results for the mean training time per epoch (forward pass, cross-entropy calculation, backward pass) for 100 epochs on simulated random graphs, measured in seconds wall-clock time. See Section 5.1 for a detailed description of the random graph dataset used in these experiments. We compare results on a GPU and on a CPU-only implementation4 in TensorFlow (Abadi et al., 2015). Figure 2 summarizes the results.

7 DISCUSSION

7.1 SEMI-SUPERVISED MODEL
In the experiments demonstrated here, our method for semi-supervised node classification outperforms recent related methods by a significant margin. Methods based on graph-Laplacian regularization (Zhu et al., 2003; Belkin et al., 2006; Weston et al., 2012) are most likely limited due to their assumption that edges encode mere similarity of nodes. Skip-gram based methods on the other hand are limited by the fact that they are based on a multi-step pipeline which is difficult to optimize. Our proposed model can overcome both limitations, while still comparing favorably in terms of efficiency (measured in wall-clock time) to related methods. Propagation of feature information from neighboring nodes in every layer improves classification performance in comparison to methods like ICA (Lu & Getoor, 2003), where only label information is aggregated.
We have further demonstrated that the proposed renormalized propagation model (Eq. 8) offers both improved efficiency (fewer parameters and operations, such as multiplication or addition) and better predictive performance on a number of datasets compared to a naı̈ve 1st-order model (Eq. 6) or higher-order graph convolutional models using Chebyshev polynomials (Eq. 5).

7.2 LIMITATIONS AND FUTURE WORK
Here, we describe several limitations of our current model and outline how these might be overcome in future work.
Memory requirement In the current setup with full-batch gradient descent, memory requirement grows linearly in the size of the dataset. We have shown that for large graphs that do not fit in GPU memory, training on CPU can still be a viable option. Mini-batch stochastic gradient descent can alleviate this issue. The procedure of generating mini-batches, however, should take into account the number of layers in the GCN model, as the K th-order neighborhood for a GCN with K layers has to be stored in memory for an exact procedure. For very large and densely connected graph datasets, further approximations might be necessary.
Directed edges and edge features Our framework currently does not naturally support edge features and is limited to undirected graphs (weighted or unweighted). Results on NELL however show that it is possible to handle both directed edges and edge features by representing the original directed graph as an undirected bipartite graph with additional nodes that represent edges in the original graph (see Section 5.1 for details).
Limiting assumptions Through the approximations introduced in Section 2, we implicitly assume locality (dependence on the K th-order neighborhood for a GCN with K layers) and equal importance of self-connections vs. edges to neighboring nodes. For some datasets, however, it might be beneficial to introduce a trade-off parameter λ in the definition of Ã:
Ã = A+ λIN . (11) 4Hardware used: 16-core Intel R© Xeon R© CPU E5-2640 v3 @ 2.60GHz, GeForce R© GTX TITAN X
This parameter now plays a similar role as the trade-off parameter between supervised and unsupervised loss in the typical semi-supervised setting (see Eq. 1). Here, however, it can be learned via gradient descent.

8 CONCLUSION
We have introduced a novel approach for semi-supervised classification on graph-structured data. Our GCN model uses an efficient layer-wise propagation rule that is based on a first-order approximation of spectral convolutions on graphs. Experiments on a number of network datasets suggest that the proposed GCN model is capable of encoding both graph structure and node features in a way useful for semi-supervised classification. In this setting, our model outperforms several recently proposed methods by a significant margin, while being computationally efficient.

ACKNOWLEDGMENTS
We would like to thank Christos Louizos, Taco Cohen, Joan Bruna, Zhilin Yang, Dave Herman, Pramod Sinha and Abdul-Saboor Sheikh for helpful discussions. This research was funded by SAP.

A RELATION TO WEISFEILER-LEHMAN ALGORITHM
A neural network model for graph-structured data should ideally be able to learn representations of nodes in a graph, taking both the graph structure and feature description of nodes into account. A well-studied framework for the unique assignment of node labels given a graph and (optionally) discrete initial node labels is provided by the 1-dim Weisfeiler-Lehman (WL-1) algorithm (Weisfeiler & Lehmann, 1968):
Algorithm 1: WL-1 algorithm (Weisfeiler & Lehmann, 1968)
Input: Initial node coloring (h(0)1 , h (0) 2 , ..., h (0) N ) Output: Final node coloring (h(T )1 , h (T ) 2 , ..., h (T ) N ) t← 0; repeat
for vi ∈ V do h (t+1) i ← hash (∑ j∈Ni h (t) j ) ;
t← t+ 1; until stable node coloring is reached;
Here, h(t)i denotes the coloring (label assignment) of node vi (at iteration t) and Ni is its set of neighboring node indices (irrespective of whether the graph includes self-connections for every node or not). hash(·) is a hash function. For an in-depth mathematical discussion of the WL-1 algorithm see, e.g., Douglas (2011).
We can replace the hash function in Algorithm 1 with a neural network layer-like differentiable function with trainable parameters as follows:
h (l+1) i = σ ∑ j∈Ni 1 cij h (l) j W (l)  , (12) where cij is an appropriately chosen normalization constant for the edge (vi, vj). Further, we can take h(l)i now to be a vector of activations of node i in the l
th neural network layer. W (l) is a layer-specific weight matrix and σ(·) denotes a differentiable, non-linear activation function.
By choosing cij = √ didj , where di = |Ni| denotes the degree of node vi, we recover the propagation rule of our Graph Convolutional Network (GCN) model in vector form (see Eq. 2)5.
This—loosely speaking—allows us to interpret our GCN model as a differentiable and parameterized generalization of the 1-dim Weisfeiler-Lehman algorithm on graphs.
A.1 NODE EMBEDDINGS WITH RANDOM WEIGHTS
From the analogy with the Weisfeiler-Lehman algorithm, we can understand that even an untrained GCN model with random weights can serve as a powerful feature extractor for nodes in a graph. As an example, consider the following 3-layer GCN model:
Z = tanh ( Â tanh ( Â tanh ( ÂXW (0) ) W (1) ) W (2) ) , (13)
with weight matricesW (l) initialized at random using the initialization described in Glorot & Bengio (2010). Â, X and Z are defined as in Section 3.1.
We apply this model on Zachary’s karate club network (Zachary, 1977). This graph contains 34 nodes, connected by 154 (undirected and unweighted) edges. Every node is labeled by one of four classes, obtained via modularity-based clustering (Brandes et al., 2008). See Figure 3a for an illustration.
5Note that we here implicitly assume that self-connections have already been added to every node in the graph (for a clutter-free notation).
We take a featureless approach by setting X = IN , where IN is the N by N identity matrix. N is the number of nodes in the graph. Note that nodes are randomly ordered (i.e. ordering contains no information). Furthermore, we choose a hidden layer dimensionality6 of 4 and a two-dimensional output (so that the output can immediately be visualized in a 2-dim plot).
Figure 3b shows a representative example of node embeddings (outputs Z) obtained from an untrained GCN model applied to the karate club network. These results are comparable to embeddings obtained from DeepWalk (Perozzi et al., 2014), which uses a more expensive unsupervised training procedure.
A.2 SEMI-SUPERVISED NODE EMBEDDINGS
On this simple example of a GCN applied to the karate club network it is interesting to observe how embeddings react during training on a semi-supervised classification task. Such a visualization (see Figure 4) provides insights into how the GCN model can make use of the graph structure (and of features extracted from the graph structure at later layers) to learn embeddings that are useful for a classification task.
We consider the following semi-supervised learning setup: we add a softmax layer on top of our model (Eq. 13) and train using only a single labeled example per class (i.e. a total number of 4 labeled nodes). We train for 300 training iterations using Adam (Kingma & Ba, 2015) with a learning rate of 0.01 on a cross-entropy loss.
Figure 4 shows the evolution of node embeddings over a number of training iterations. The model succeeds in linearly separating the communities based on minimal supervision and the graph structure alone. A video of the full training process can be found on our website7.
6We originally experimented with a hidden layer dimensionality of 2 (i.e. same as output layer), but observed that a dimensionality of 4 resulted in less frequent saturation of tanh(·) units and therefore visually more pleasing results.
7http://tkipf.github.io/graph-convolutional-networks/

B EXPERIMENTS ON MODEL DEPTH
In these experiments, we investigate the influence of model depth (number of layers) on classification performance. We report results on a 5-fold cross-validation experiment on the Cora, Citeseer and Pubmed datasets (Sen et al., 2008) using all labels. In addition to the standard GCN model (Eq. 2), we report results on a model variant where we use residual connections (He et al., 2016) between hidden layers to facilitate training of deeper models by enabling the model to carry over information from the previous layer’s input:
H(l+1) = σ ( D̃− 1 2 ÃD̃− 1 2H(l)W (l) ) +H(l) . (14)
On each cross-validation split, we train for 400 epochs (without early stopping) using the Adam optimizer (Kingma & Ba, 2015) with a learning rate of 0.01. Other hyperparameters are chosen as follows: 0.5 (dropout rate, first and last layer), 5 · 10−4 (L2 regularization, first layer), 16 (number of units for each hidden layer) and 0.01 (learning rate). Results are summarized in Figure 5.
For the datasets considered here, best results are obtained with a 2- or 3-layer model. We observe that for models deeper than 7 layers, training without the use of residual connections can become difficult, as the effective context size for each node increases by the size of its K th-order neighborhood (for a model with K layers) with each additional layer. Furthermore, overfitting can become an issue as the number of parameters increases with model depth.
",We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.,ICLR 2017 conference submission,True,,"This paper proposes the graph convolutional networks, motivated from approximating graph convolutions.  In one propagation step, what the model does can be simplified as, first linearly transform the node representations for each node, and then multiply the transformed node representations with the normalized affinity matrix (with self-connections added), and then pass through nonlinearity.

This model is used for semi-supervised learning on graphs, and in the experiments it demonstrated quite impressive results compared to other baselines, outperforming them by a significant margin.  The evaluation of propagation model is also interesting, where different variants of the model and design decisions are evaluated and compared.

It is surprising that such a simple model works so much better than all the baselines.  Considering that the model used is just a two-layer model in most experiments, this is really surprising as a two-layer model is very local, and the output of a node can only be affected by nodes in a 2-hop neighborhood, and no longer range interactions can play any roles in this.  Since computation is quite efficient (sec. 6.3), I wonder if adding more layers helped anything or not.

Even though motivated from graph convolutions, when simplified as the paper suggests, the operations the model does are quite simple.  Compared to Duvenaud et al. 2015 and Li et al. 2016, the proposed method is simpler and does almost strictly less things.  So how would the proposed GCN compare against these methods?

Overall I think this model is simple, but the connection to graph convolutions is interesting, and the experiment results are quite good.  There are a few questions that still remain, but I feel this paper can be accepted.

---

The reviewers are in agreement that this paper is well written and constitutes a solid contribution to graph-based semi-supervised learning based on variants of CNNs.

---

Dear Reviewers,

Thanks a lot for reviewing our paper and for your valuable comments. 

To incorporate your feedback, we have uploaded a revision of our paper with the following changes:

1) We have added the Iterative Classification Algorithm (ICA) from Lu & Getoor (2003) as a baseline as suggested by Reviewer 2. Thanks a lot for pointing out the references on iterative classification. ICA is indeed a powerful baseline that we have not considered previously and it compares favorably against some of the other baselines. We have put the code to reproduce the ICA baseline experiments on Github:

---

The paper develops a simple and reasonable algorithm for graph node prediction/classification. The formulations are very intuitive and lead to a simple CNN based training and can easily leverage existing GPU speedups. 

Experiments are thorough and compare with many reasonable baselines on large and real benchmark datasets. Although, I am not quite aware of the literature on other methods and there may be similar alternatives as link and node prediction is an old problem. I still think the approach is quite simple and reasonably supported by good evaluations.

---

The paper introduces a method for semi-supervised learning in graphs that exploits the spectral structure of the graph in a convolutional NN implementation. The proposed algorithm has a limited complexity and it is shown to scale well on a large dataset. The comparison with baselines on different datasets show a clear jump of performance with the proposed method.

The paper is technically fine and clear, the algorithm seems to scale well, and the results on the different datasets compare very favorably with the different baselines. The algorithm is simple and training seems easy. Concerning the originality, the proposed algorithm is a simple adaptation of graph convolutional networks (ref Defferrard 2016 in the paper) to a semi-supervised transductive setting. This is clearly mentioned in the paper, but the authors could better highlight the differences and novelty wrt this reference paper. Also, there is no comparison with the family of iterative classifiers, which usually compare favorably, both in performance and training time, with regularization based approaches, although they are mostly used in inductive settings. Below are some references for this family of methods.

The authors mention that more complex filters could be learned by stacking layers but they limit their architecture to one hidden layer. They should comment on the interest of using more layers for graph classification.


Some references on iterative classification Qing Lu and Lise Getoor. 2003. Link-based classification. In ICML, Vol. 3. 496–503.

Gideon S Mann and Andrew McCallum. 2010. Generalized expectation criteria for semi-supervised learning with weakly labeled data. The Journal of Machine Learning Research 11 (2010), 955–984.
David Jensen, Jennifer Neville, and Brian Gallagher. 2004. Why collective inference improves relational classification. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 593–598.
Joseph J Pfeiffer III, Jennifer Neville, and Paul N Bennett. 2015. Overcoming Relational Learning Biases
to Accurately Predict Preferences in Large Scale Networks. In Proceedings of the 24th International Conference on World Wide Web. International World Wide Web Conferences Steering Committee, 853–
863.
Stephane Peters, Ludovic Denoyer, and Patrick Gallinari. 2010. Iterative annotation of multi-relational social networks. In Advances in Social Networks Analysis and Mining (ASONAM), 2010 International Conference on. IEEE, 96–103.

---

This paper proposes the graph convolutional networks, motivated from approximating graph convolutions.  In one propagation step, what the model does can be simplified as, first linearly transform the node representations for each node, and then multiply the transformed node representations with the normalized affinity matrix (with self-connections added), and then pass through nonlinearity.

This model is used for semi-supervised learning on graphs, and in the experiments it demonstrated quite impressive results compared to other baselines, outperforming them by a significant margin.  The evaluation of propagation model is also interesting, where different variants of the model and design decisions are evaluated and compared.

It is surprising that such a simple model works so much better than all the baselines.  Considering that the model used is just a two-layer model in most experiments, this is really surprising as a two-layer model is very local, and the output of a node can only be affected by nodes in a 2-hop neighborhood, and no longer range interactions can play any roles in this.  Since computation is quite efficient (sec. 6.3), I wonder if adding more layers helped anything or not.

Even though motivated from graph convolutions, when simplified as the paper suggests, the operations the model does are quite simple.  Compared to Duvenaud et al. 2015 and Li et al. 2016, the proposed method is simpler and does almost strictly less things.  So how would the proposed GCN compare against these methods?

Overall I think this model is simple, but the connection to graph convolutions is interesting, and the experiment results are quite good.  There are a few questions that still remain, but I feel this paper can be accepted.

---

This paper proposes the graph convolutional networks, motivated from approximating graph convolutions.  In one propagation step, what the model does can be simplified as, first linearly transform the node representations for each node, and then multiply the transformed node representations with the normalized affinity matrix (with self-connections added), and then pass through nonlinearity.

This model is used for semi-supervised learning on graphs, and in the experiments it demonstrated quite impressive results compared to other baselines, outperforming them by a significant margin.  The evaluation of propagation model is also interesting, where different variants of the model and design decisions are evaluated and compared.

It is surprising that such a simple model works so much better than all the baselines.  Considering that the model used is just a two-layer model in most experiments, this is really surprising as a two-layer model is very local, and the output of a node can only be affected by nodes in a 2-hop neighborhood, and no longer range interactions can play any roles in this.  Since computation is quite efficient (sec. 6.3), I wonder if adding more layers helped anything or not.

Even though motivated from graph convolutions, when simplified as the paper suggests, the operations the model does are quite simple.  Compared to Duvenaud et al. 2015 and Li et al. 2016, the proposed method is simpler and does almost strictly less things.  So how would the proposed GCN compare against these methods?

Overall I think this model is simple, but the connection to graph convolutions is interesting, and the experiment results are quite good.  There are a few questions that still remain, but I feel this paper can be accepted.

---

The reviewers are in agreement that this paper is well written and constitutes a solid contribution to graph-based semi-supervised learning based on variants of CNNs.

---

Dear Reviewers,

Thanks a lot for reviewing our paper and for your valuable comments. 

To incorporate your feedback, we have uploaded a revision of our paper with the following changes:

1) We have added the Iterative Classification Algorithm (ICA) from Lu & Getoor (2003) as a baseline as suggested by Reviewer 2. Thanks a lot for pointing out the references on iterative classification. ICA is indeed a powerful baseline that we have not considered previously and it compares favorably against some of the other baselines. We have put the code to reproduce the ICA baseline experiments on Github:

---

The paper develops a simple and reasonable algorithm for graph node prediction/classification. The formulations are very intuitive and lead to a simple CNN based training and can easily leverage existing GPU speedups. 

Experiments are thorough and compare with many reasonable baselines on large and real benchmark datasets. Although, I am not quite aware of the literature on other methods and there may be similar alternatives as link and node prediction is an old problem. I still think the approach is quite simple and reasonably supported by good evaluations.

---

The paper introduces a method for semi-supervised learning in graphs that exploits the spectral structure of the graph in a convolutional NN implementation. The proposed algorithm has a limited complexity and it is shown to scale well on a large dataset. The comparison with baselines on different datasets show a clear jump of performance with the proposed method.

The paper is technically fine and clear, the algorithm seems to scale well, and the results on the different datasets compare very favorably with the different baselines. The algorithm is simple and training seems easy. Concerning the originality, the proposed algorithm is a simple adaptation of graph convolutional networks (ref Defferrard 2016 in the paper) to a semi-supervised transductive setting. This is clearly mentioned in the paper, but the authors could better highlight the differences and novelty wrt this reference paper. Also, there is no comparison with the family of iterative classifiers, which usually compare favorably, both in performance and training time, with regularization based approaches, although they are mostly used in inductive settings. Below are some references for this family of methods.

The authors mention that more complex filters could be learned by stacking layers but they limit their architecture to one hidden layer. They should comment on the interest of using more layers for graph classification.


Some references on iterative classification Qing Lu and Lise Getoor. 2003. Link-based classification. In ICML, Vol. 3. 496–503.

Gideon S Mann and Andrew McCallum. 2010. Generalized expectation criteria for semi-supervised learning with weakly labeled data. The Journal of Machine Learning Research 11 (2010), 955–984.
David Jensen, Jennifer Neville, and Brian Gallagher. 2004. Why collective inference improves relational classification. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 593–598.
Joseph J Pfeiffer III, Jennifer Neville, and Paul N Bennett. 2015. Overcoming Relational Learning Biases
to Accurately Predict Preferences in Large Scale Networks. In Proceedings of the 24th International Conference on World Wide Web. International World Wide Web Conferences Steering Committee, 853–
863.
Stephane Peters, Ludovic Denoyer, and Patrick Gallinari. 2010. Iterative annotation of multi-relational social networks. In Advances in Social Networks Analysis and Mining (ASONAM), 2010 International Conference on. IEEE, 96–103.

---

This paper proposes the graph convolutional networks, motivated from approximating graph convolutions.  In one propagation step, what the model does can be simplified as, first linearly transform the node representations for each node, and then multiply the transformed node representations with the normalized affinity matrix (with self-connections added), and then pass through nonlinearity.

This model is used for semi-supervised learning on graphs, and in the experiments it demonstrated quite impressive results compared to other baselines, outperforming them by a significant margin.  The evaluation of propagation model is also interesting, where different variants of the model and design decisions are evaluated and compared.

It is surprising that such a simple model works so much better than all the baselines.  Considering that the model used is just a two-layer model in most experiments, this is really surprising as a two-layer model is very local, and the output of a node can only be affected by nodes in a 2-hop neighborhood, and no longer range interactions can play any roles in this.  Since computation is quite efficient (sec. 6.3), I wonder if adding more layers helped anything or not.

Even though motivated from graph convolutions, when simplified as the paper suggests, the operations the model does are quite simple.  Compared to Duvenaud et al. 2015 and Li et al. 2016, the proposed method is simpler and does almost strictly less things.  So how would the proposed GCN compare against these methods?

Overall I think this model is simple, but the connection to graph convolutions is interesting, and the experiment results are quite good.  There are a few questions that still remain, but I feel this paper can be accepted.",,,,,,7.0,,,3.6666666666666665,,
498,"DROPOUT WITH EXPECTATION-LINEAR REGULARIZATION
Authors: Xuezhe Ma, Yingkai Gao, Zhiting Hu, Yaoliang Yu, Yuntian Deng
Source file: 498.pdf

ABSTRACT
Dropout, a simple and effective way to train deep neural networks, has led to a number of impressive empirical successes and spawned many recent theoretical investigations. However, the gap between dropout’s training and inference phases, introduced due to tractability considerations, has largely remained under-appreciated. In this work, we first formulate dropout as a tractable approximation of some latent variable model, leading to a clean view of parameter sharing and enabling further theoretical analysis. Then, we introduce (approximate) expectation-linear dropout neural networks, whose inference gap we are able to formally characterize. Algorithmically, we show that our proposed measure of the inference gap can be used to regularize the standard dropout training objective, resulting in an explicit control of the gap. Our method is as simple and efficient as standard dropout. We further prove the upper bounds on the loss in accuracy due to expectation-linearization, describe classes of input distributions that expectation-linearize easily. Experiments on three image classification benchmark datasets demonstrate that reducing the inference gap can indeed improve the performance consistently.

1 INTRODUCTION
Deep neural networks (DNNs, e.g., LeCun et al., 2015; Schmidhuber, 2015), if trained properly, have been demonstrated to significantly improve the benchmark performances in a wide range of application domains. As neural networks go deeper and deeper, naturally, its model complexity also increases quickly, hence the pressing need to reduce overfitting in training DNNs. A number of techniques have emerged over the years to address this challenge, among which dropout (Hinton et al., 2012; Srivastava, 2013) has stood out for its simplicity and effectiveness. In a nutshell, dropout randomly “drops” neural units during training as a means to prevent feature co-adaptation—a sign of overfitting (Hinton et al., 2012). Simple as it appears to be, dropout has led to several record-breaking performances (Hinton et al., 2012; Ma & Hovy, 2016), and thus spawned a lot of recent interests in analyzing and justifying dropout from the theoretical perspective, and also in further improving dropout from the algorithmic and practical perspective.
In their pioneering work, Hinton et al. (2012) and Srivastava et al. (2014) interpreted dropout as an extreme form of model combination (aka. model ensemble) with extensive parameter/weight sharing, and they proposed to learn the combination through minimizing an appropriate expected loss. Interestingly, they also pointed out that for a single logistic neural unit, the output of dropout is in fact the geometric mean of the outputs of the model ensemble with shared parameters. Subsequently, many theoretical justifications of dropout have been explored, and we can only mention a few here due to space limits. Building on the weight sharing perspective, Baldi & Sadowski (2013; 2014) analyzed the ensemble averaging property of dropout in deep non-linear logistic networks, and supported the view that dropout is equivalent to applying stochastic gradient descent on some regularized
loss function. Wager et al. (2013) treated dropout as an adaptive regularizer for generalized linear models (GLMs). Helmbold & Long (2016) discussed the differences between dropout and traditional weight decay regularization. In terms of statistical learning theory, Gao & Zhou (2014) studied the Rademacher complexity of different types of dropout, showing that dropout is able to reduce the Rademacher complexity polynomially for shallow neural networks (with one or no hidden layers) and exponentially for deep neural networks. This latter work (Gao & Zhou, 2014) formally demonstrated that dropout, due to its regularizing effect, contributes to reducing the inherent model complexity, in particular the variance component in the generalization error.
Seen as a model combination technique, it is intuitive that dropout contributes to reducing the variance of the model performance. Surprisingly, dropout has also been shown to play some role in reducing the model bias. For instance, Jain et al. (2015) studied the ability of dropout training to escape local minima, hence leading to reduced model bias. Other studies (Chen et al., 2014; Helmbold & Long, 2014; Wager et al., 2014) focus on the effect of the dropout noise on models with shallow architectures. We noted in passing that there are also some work (Kingma et al., 2015; Gal & Ghahramani, 2015; 2016) trying to understand dropout from the Bayesian perspective.
In this work, we first formulate dropout as a tractable approximation of a latent variable model, and give a clean view of weight sharing (§3). Then, we focus on an inference gap in dropout that has somehow gotten under-appreciated: In the inference phase, for computational tractability considerations, the model ensemble generated by dropout is approximated by a single model with scaled weights, resulting in a gap between training and inference, and rendering the many previous theoretical findings inapplicable. In general, this inference gap can be very large and no attempt (to our best knowledge) has been made to control it. We make three contributions in bridging this gap: Theoretically, we introduce expectation-linear dropout neural networks, through which we are able to explicitly quantify the inference gap (§4). In particular, our theoretical results explain why the max-norm constraint on the network weights, a standard practice in training DNNs, can lead to a small inference gap hence potentially improve performance. Algorithmically, we propose to add a sampled version of the inference gap to regularize the standard dropout training objective (expectationlinearization), hence allowing explicit control of the inference gap, and analyze the interaction between expectation-linearization and the model accuracy (§5). Experimentally, through three benchmark datasets we show that our regularized dropout is not only as simple and efficient as standard dropout but also consistently leads to improved performance (§6).

2 DROPOUT NEURAL NETWORKS
In this section we set up the notations, review the dropout neural network model, and discuss the inference gap in standard dropout training that we will attempt to study in the rest of the paper.

2.1 DNNS AND NOTATIONS
Throughout we use uppercase letters for random variables (and occasionally for matrices as well), and lowercase letters for realizations of the corresponding random variables. Let X ∈ X be the input of the neural network, Y ∈ Y be the desired output, and D = {(x1, y1), . . . , (xN , yN )} be our training sample, where xi, i = 1, . . . , N, (resp. yi) are usually i.i.d. samples of X (resp. Y ).
Let M denote a deep neural network with L hidden layers, indexed by l ∈ {1, . . . , L}. Let h(l) denote the output vector from layer l. As usual, h(0) = x is the input, and h(L) is the output of the neural network. Denote θ = {θl : l = 1, . . . , L} as the set of parameters in the network M, where θl assembles the parameters in layer l. With dropout, we need to introduce a set of dropout random variables S = {Γ(l) : l = 1, . . . , L}, where Γ(l) is the dropout random variable for layer l. Then the deep neural network M can be described as:
h(l) = fl(h (l−1) γ(l); θl), l = 1, . . . , L, (1)
where is the element-wise product and fl is the transformation function of layer l. For example, if layer l is a fully connected layer with weight matrix W , bias vector b, and sigmoid activation function σ(x) = 11+exp(−x) , then fl(x) = σ(Wx+ b)). We will also use h
(l)(x, s; θ) to denote the output of layer l with input x and dropout value s, under parameter θ.
In the simplest form of dropout, which is also called standard dropout, Γ(l) is a vector of independent Bernoulli random variables, each of which has probability pl of being 1 and 1− pl of being 0. This corresponds to dropping each of the weights independently with probability pl.

2.2 DROPOUT TRAINING
The standard dropout neural networks can be trained using stochastic gradient decent (SGD), with a sub-network sampled by dropping neural units for each training instance in a mini-batch. Forward and backward pass for that training instance are done only on the sampled sub-network. Intuitively, dropout aims at, simultaneously and jointly, training an ensemble of exponentially many neural networks (one for each configuration of dropped units) while sharing the same weights/parameters.
The goal of the stochastic training procedure of dropout can be understood as minimizing an expected loss function, after marginalizing out the dropout variables (Srivastava, 2013; Wang & Manning, 2013). In the context of maximal likelihood estimation, dropout training can be formulated as:
θ∗ = argmin θ ESD [−l(D,SD; θ)] = argmin θ ESD
[ − N∑ i=1 log p(yi|xi, Si; θ) ] , (2)
where recall that D is the training sample, SD = {S1, . . . , SN} is the dropout variable (one for each training instance), and l(D,SD; θ) is the (conditional) log-likelihood function defined by the conditional distribution p(y|x, s; θ) of output y given input x, under parameter θ and dropout variable s. Throughout we use the notation EZ to denote the conditional expectation where all random variables except Z are conditioned on.
Dropout has also been shown to work well with regularization, such as L2 weight decay (Tikhonov, 1943), Lasso (Tibshirani, 1996), KL-sparsity(Bradley & Bagnell, 2008; Hinton, 2010), and max-norm regularization (Srebro et al., 2004), among which the max-norm regularization — that constrains the norm of the incoming weight matrix to be bounded by some constant — was found to be especially useful for dropout (Srivastava, 2013; Srivastava et al., 2014).

2.3 DROPOUT INFERENCE AND GAP
As mentioned before, dropout is effectively training an ensemble of neural networks with weight sharing. Consequently, at test time, the output of each network in the ensemble should be averaged to deliver the final prediction. This averaging over exponentially many sub-networks is, however, intractable, and standard dropout typically implements an approximation by introducing a deterministic scaling factor for each layer to replace the random dropout variable:
ES [H (L)(x, S; θ)] ? ≈ h(L)(x,E[S]; θ), (3)
where the right-hand side is the output of a single deterministic neural network whose weights are scaled to match the expected number of active hidden units on the left-hand side. Importantly, the right-hand side can be easily computed since it only involves a single deterministic network.
Bulò et al. (2016) combined dropout with knowledge distillation methods (Hinton et al., 2015) to better approximate the averaging processing of the left-hand side. However, the quality of the approximation in (3) is largely unknown, and to our best knowledge, no attempt has been made to explicitly control this inference gap. The main goal of this work is to explicitly quantify, algorithmically control, and experimentally demonstrate the inference gap in (3), in the hope of improving the generalization performance of DNNs eventually. To this end, in the next section we first present a latent variable model interpretation of dropout, which will greatly facilitate our later theoretical analysis.

3 DROPOUT AS LATENT VARIABLE MODELS
With the end goal of studying the inference gap in (3) in mind, in this section, we first formulate dropout neural networks as a latent variable model (LVM) in § 3.1. Then, we point out the relation between the training procedure of LVM and that of standard dropout in § 3.2. The advantage of formulating dropout as a LVM is that we need only deal with a single model (with latent structure), instead of an ensemble of exponentially many different models (with weight sharing). This much
simplified view of dropout enables us to understand and analyze the model parameter θ in a much more straightforward and intuitive way.

3.1 AN LVM FORMULATION OF DROPOUT
A latent variable model consists of two types of variables: the observed variables that represent the empirical (observed) data and the latent variables that characterize the hidden (unobserved) structure. To formulate dropout as a latent variable model, the input x and output y are regarded as observed variables, while the dropout variable s, representing the sub-network structure, is hidden. Then, upon fixing the input space X , the output space Y , and the latent space S for dropout variables, the conditional probability of y given x under parameter θ can be written as
p(y|x; θ) = ∫ S p(y|x, s; θ)p(s)dµ(s), (4)
where p(y|x, s; θ) is the conditional distribution modeled by the neutral network with configuration s (same as in Eq. (2)), p(s) is the distribution of dropout variable S (e.g. Bernoulli), here assumed to be independent of the input x, and µ(s) is the base measure on the space S.

3.2 LVM DROPOUT TRAINING VS. STANDARD DROPOUT TRAINING
Building on the above latent variable model formulation (4) of dropout, we are now ready to point out a simple relation between the training procedure of LVM and that of standard dropout. Given an i.i.d. training sample D, the maximum likelihood estimate for the LVM formulation of dropout in (4) is equivalent to minimizing the following negative log-likelihood function:
θ∗ = argmin θ −l(D; θ) = argmin θ − N∑ i=1 log p(yi|xi; θ), (5)
where p(y|x; θ) is given in Eq. (4). Recall the dropout training objective ESD [−l(D,SD; θ)] in Eq. (2). We have the following theorem as a simple consequence of Jensen’s inequality (details in Appendix A): Theorem 1. The expected loss function of standard dropout (Eq. (2)) is an upper bound of the negative log-likelihood of LVM dropout (Eq. (5)):
−l(D; θ) ≤ ESD [−l(D,SD; θ)]. (6)
Theorem 1, in a rigorous sense, justifies dropout training as a convenient and tractable approximation of the LVM formulation in (4). Indeed, since directly minimizing the marginalized negative loglikelihood in (5) may not be easy, a standard practice is to replace the marginalized (conditional) likelihood p(y|x; θ) in (4) with its empirical Monte carlo average through drawing samples from the dropout variable S. The dropout training objective in (2) corresponds exactly to this Monte carlo approximation when a single sample Si is drawn for each training instance (xi, yi). Importantly, we note that the above LVM formulation involves only a single network parameter θ, which largely simplifies the picture and facilitates our subsequent analysis.

4 EXPECTATION-LINEAR DROPOUT NEURAL NETWORKS
Building on the latent variable model formulation in § 3, we introduce in this section the notion of expectation-linearity that essentially measures the inference gap in (3). We then characterize a general class of neural networks that exhibit expectation-linearity, either exactly or approximately over a distribution p(x) on the input space.
We start with defining expectation-linearity in the simplest single-layer neural network, then we extend the notion into general deep networks in a natural way. Definition 1 (Expectation-linear Layer). A network layer h = f(x γ; θ) is expectation-linear with respect to a set X ′ ⊆ X , if for all x ∈ X ′ we have∥∥E[f(x Γ; θ)]− f(x E[Γ]; θ)∥∥
2 = 0. (7)
In this case we say that X ′ is expectation-linearizable, and θ is expectation-linearizing w.r.t X ′.
Obviously, the condition in (7) will guarantee no gap in the dropout inference approximation (3)—an admittedly strong condition that we will relax below. Clearly, if f is an affine function, then we can choose X ′ = X and expectation-linearity is trivial. Note that expectation-linearity depends on the network parameter θ and the dropout distribution Γ.
Expectation-linearity, as defined in (7), is overly strong: under standard regularity conditions, essentially the transformation function f has to be affine over the set X ′, ruling out for instance the popular sigmoid or tanh activation functions. Moreover, in practice, downstream use of DNNs are usually robust to small errors resulting from approximate expectation-linearity (hence the empirical success of dropout), so it makes sense to define an inexact extension. We note also that the definition in (7) is uniform over the set X ′, while in a statistical setting it is perhaps more meaningful to have expectation-linearity “on average,” since inputs from lower density regions are not going to play a significant role anyway. Taking into account the aforementioned motivations, we arrive at the following inexact extension: Definition 2 (Approximately Expectation-linear Layer). A network layer h = f(x γ; θ) is δ-approximately expectation-linear with respect to a distribution p(x) over X if
EX [∥∥EΓ[f(X Γ; θ)|X]− f(X E[Γ]; θ)∥∥2] < δ. (8) In this case we say that p(x) is δ-approximately expectation-linearizable, and θ is δ-approximately expectation-linearizing.
To appreciate the power of cutting some slack from exact expectation-linearity, we remark that even non-affine activation functions often have approximately linear regions. For example, the logistic function, a commonly used non-linear activation function in DNNs, is approximately linear around the origin. Naturally, we can ask whether it is sufficient for a target distribution p(x) to be well-approximated by an approximately expectation-linearizable one. We begin by providing an appropriate measurement of the quality of this approximation. Definition 3 (Closeness, (Andreas et al., 2015)). A distribution p(x) is C-close to a set X ′ ⊆ X if
E [
inf x∗∈X ′ sup γ∈S ‖X γ − x∗ γ‖2
] ≤ C, (9)
where recall that S is the (bounded) space that the dropout variable lives in.
Intuitively, p(x) is C-close to a set X ′ if a random sample from p is no more than a distance C from X ′ in expectation and under the worst “dropout perturbation”. For example, a standard normal distribution is close to an interval centering at origin ([−α, α]) with some constant C. Our definition of closeness is similar to that in Andreas et al. (2015), who used this notion to analyze self-normalized log-linear models.
We are now ready to state our first major result that quantifies approximate expectation-linearity of a single-layered network (proof in Appendix B.1): Theorem 2. Given a network layer h = f(x γ; θ), where θ is expectation-linearizing w.r.t. X ′ ⊆ X . Suppose p(x) is C-close to X ′ and for all x ∈ X , ‖∇xf(x)‖op ≤ B, where ‖ · ‖op is the usual operator norm. Then, p(x) is 2BC-approximately expectation-linearizable.
Roughly, Theorem 2 states that the input distribution p(x) that place most of its mass on regions close to expectation-linearizable sets are approximately expectation-linearizable on a similar scale. The bounded operator norm assumption on the derivative ∇f is satisfied in most commonly used layers. For example, for a fully connected layer with weight matrix W , bias vector b, and activation function σ, ‖∇f(·)‖op = |σ′(·)| · ‖W‖op is bounded by ‖W‖op and the supremum of |σ′(·)| (1/4 when σ is sigmoid and 1 when σ is tanh).
Next, we extend the notion of approximate expectation-linearity to deep dropout neural networks. Definition 4 (Approximately Expectation-linear Network). A deep neural network with L layers (cf. Eq. (1)) is δ-approximately expectation-linear with respect to p(x) over X if
EX [∥∥ES[H(L)(X,S; θ)|X]− h(L)(X,E[S]; θ)∥∥2] < δ. (10) where h(L)(X,E[S]; θ) is the output of the deterministic neural network in standard dropout.
Lastly, we relate the level of approximate expectation-linearity of a deep neural network to the level of approximate expectation-linearity of each of its layers: Theorem 3. Given an L-layer neural network as in Eq. (1), and suppose that each layer l ∈ {1, . . . , L} is δ-approximately expectation-linear w.r.t. p(h(l)), E[Γ(l)] ≤ γ, supx ‖∇fl(x)‖op ≤ B, and E [ Var[H(l)|X] ] ≤ σ2. Then the network is ∆-approximately expectation-linear with
∆ = (Bγ)L−1δ + (δ +Bγσ)
( 1− (Bγ)L−1
1−Bγ
) . (11)
From Theorem 3 (proof in Appendix B.2) we observe that the level of approximate expectationlinearity of the network mainly depends on four factors: the level of approximate expecatationlinearity of each layer (δ), the expected variance of each layer (σ), the operator norm of the derivative of each layer’s transformation function (B), and the mean of each layer’s dropout variable (γ). In practice, γ is often a constant less than or equal to 1. For example, if Γ ∼ Bernoulli(p), then γ = p. According to the theorem, the operator norm of the derivative of each layer’s transformation function is an important factor in the level of approximate expectation-linearity: the smaller the operator norm is, the better the approximation. Interestingly, the operator norm of a layer often depends on the norm of the layer’s weight (e.g. for fully connected layers). Therefore, adding max-norm constraints to regularize dropout neural networks can lead to better approximate expectation-linearity hence smaller inference gap and the often improved model performance.
It should also be noted that when Bγ < 1, the approximation error ∆ tends to be a constant when the network becomes deeper. When Bγ = 1, ∆ grows linearly with L, and when Bγ > 1, the growth of ∆ becomes exponential. Thus, it is essential to keep Bγ < 1 to achieve good approximation, particularly for deep neural networks.

5 EXPECTATION-LINEAR REGULARIZED DROPOUT
In the previous section we have managed to bound the approximate expectation-linearity, hence the inference gap in (3), of dropout neural networks. In this section, we first prove a uniform deviation bound of the sampled approximate expectation-linearity measure from its mean, which motivates adding the sampled (hence computable) expectation-linearity measure as a regularization scheme to standard dropout, with the goal of explicitly controlling the inference gap of the learned parameter, hence potentially improving the performance. Then we give the upper bounds on the loss in accuracy due to expectation-linearization, and describe classes of distributions that expectation-linearize easily.

5.1 A UNIFORM DEVIATION BOUND FOR THE SAMPLED EXPECTATION-LINEAR MEASURE
We now show that an expectation-linear network can be found by expectation-linearizing the network on the training sample. To this end, we prove a uniform deviation bound between the empirical expectation-linearization measure using i.i.d. samples (Eq. (12)) and its mean (Eq. (13)).
Theorem 4. Let H = { h(L)(x, s; θ) : θ ∈ Θ } denote a space of L-layer dropout neural networks
indexed with θ, where h(L) : X × S → R and Θ is the space that θ lives in. Suppose that the neural networks inH satisfy the constraints: 1) ∀x ∈ X , ‖x‖2 ≤ α; 2) ∀l ∈ {1, . . . , L},E(Γ(l)) ≤ γ and ‖∇fl‖op ≤ B; 3) ‖h(L)‖ ≤ β. Denote empirical expectation-linearization measure and its mean as:
∆̂ = 1
n n∑ i=1 ∥∥ESi[H(L)(Xi, Si; θ)]− h(L)(Xi,E[Si]; θ)∥∥2, (12) ∆ = EX
[∥∥ES[H(L)(X,S; θ)]− h(L)(X,E[S]; θ)∥∥2]. (13) Then, with probability at least 1− ν, we have
sup θ∈Θ |∆− ∆̂| < 2αB L(γL/2 + 1)√ n + β
√ log(1/ν)
n . (14)
From Theorem 4 (proof in Appendix C.1) we observe that the deviation bound decreases exponentially with the number of layers L when the operator norm of the derivative of each layer’s transformation
function (B) is less than 1 (and the contrary if B ≥ 1). Importantly, the square root dependence on the number of samples (n) is standard and cannot be improved without significantly stronger assumptions.
It should be noted that Theorem 4 per se does not imply anything between expectation-linearization and the model accuracy (i.e. how well the expectation-linearized neural network actually achieves on modeling the data). Formally studying this relation is provided in § 5.3. In addition, we provide some experimental evidences in § 6 on how improved approximate expectation-linearity (equivalently smaller inference gap) does lead to better empirical performances.

5.2 EXPECTATION-LINEARIZATION AS REGULARIZATION
The uniform deviation bound in Theorem 4 motivates the possibility of obtaining an approximately expectation-linear dropout neural networks through adding the empirical measure (12) as a regularization scheme to the standard dropout training objective, as follows:
loss(D; θ) = −l(D; θ) + λV (D; θ), (15) where −l(D; θ) is the negative log-likelihood defined in Eq. (5), λ > 0 is a regularization constant, and V (D; θ) measures the level of approximate expectation-linearity:
V (D; θ) = 1
N N∑ i=1 ∥∥ESi[H(L)(xi, Si; θ)]− h(L)(xi,E[Si]; θ)∥∥22. (16) To solve (15), we can minimize loss(D; θ) via stochastic gradient descent as in standard dropout, and approximate V (D; θ) using Monte carlo:
V (D; θ) ≈ 1 N N∑ i=1 ∥∥h(L)(xi, si; θ)− h(L)(xi,E[Si]; θ)∥∥22, (17) where si is the same dropout sample as in l(D; θ) for each training instance in a mini-batch. Thus, the only additional computational cost comes from the deterministic term h(L)(xi,E[Si]; θ). Overall, our regularized dropout (15), in its Monte carlo approximate form, is as simple and efficient as the standard dropout.

5.3 ON THE ACCURACY OF EXPECTATION-LINEARIZED MODELS
So far our discussion has concentrated on the problem of finding expectation-linear neural network models, without any concerns on how well they actually perform at modeling the data. In this section, we characterize the trade-off between maximizing “data likelihood” and satisfying an expectationlinearization constraint.
To achieve the characterization, we measure the likelihood gap between the classical maximum likelihood estimator (MLE) and the MLE subject to a expectation-linearization constraint. Formally, given training data D = {(x1, y1), . . . , (xn, yn)}, we define
θ̂ = argmin θ∈Θ
−l(D; θ) (18)
θ̂δ = argmin θ∈Θ,V (D;θ)≤δ
−l(D; θ) (19)
where −l(D; θ) is the negative log-likelihood defined in Eq. (5), and V (D; θ) is the level of approximate expectation-linearity in Eq. (16).
We would like to control the loss of model accuracy by obtaining a bound on the likelihood gap defined as:
∆l(θ̂, θ̂δ) = 1
n (l(D; θ̂)− l(D; θ̂δ)) (20)
In the following, we focus on neural networks with softmax output layer for classification tasks.
p(y|x, s; θ) = h(L)y (x, s; θ) = fL(h(L−1)(x, s); η) = eη
T y h (L−1)(x,s)∑ y′∈Y e ηT y′h (L−1)(x,s) (21)
where θ = {θ1, . . . , θL−1, η}, Y = {1, . . . , k} and η = {ηy : y ∈ Y}. We claim:
Theorem 5. Given an L-layer neural network h(L)(x, s; θ) with softmax output layer in (21), where parameter θ ∈ Θ, dropout variable s ∈ S, input x ∈ X and target y ∈ Y . Suppose that for every x and s, p(y|x, s; θ̂) makes a unique best prediction—that is, for each x ∈ X , s ∈ S, there exists a unique y∗ ∈ Y such that ∀y 6= y∗, η̂Ty h(L−1)(x, s) < η̂Ty∗h(L−1)(x, s). Suppose additionally that ∀x, s, ‖h(L−1)(x, s; θ̂)‖ ≤ β, and ∀y, p(y|x; θ̂) > 0. Then
∆l(θ̂, θ̂δ) ≤ c1β2 ( ‖η̂‖2 − δ
4β
)2 e−c2δ/4β (22)
where c1 and c2 are distribution-dependent constants.
From Theorem 5 (proof in Appendix C.2) we observe that, at one extreme, distributions closed to deterministic can be expectation-linearized with little loss of likelihood.
What about the other extreme — distributions “as close to uniform distribution as possible”? With suitable assumptions about the form of p(y|x, s; θ̂) and p(y|x; θ̂), we can achieve an accuracy loss bound for distributions that are close to uniform: Theorem 6. Suppose that ∀x, s, ‖h(L−1)(x, s; θ̂)‖ ≤ β. Additionally, for each (xi, yi) ∈ D, s ∈ S , log 1k ≤ log p(yi|xi, s; θ̂) ≤ 1 k ∑ y∈Y log p(y|xi, s; θ̂). Then asymptotically as n→∞:
∆l(θ̂, θ̂δ) ≤ (
1− δ 4β‖η̂‖2
) E [KL (p(·|X; θ)‖Unif(Y))] (23)
Theorem 6 (proof in Appendix C.3) indicates that uniform distributions are also an easy class for expectation-linearization.
The next question is whether there exist any classes of conditional distributions p(y|x) for which all distributions are provably hard to expectation-linearize. It remains an open problem and might be an interesting direction for future work.

6 EXPERIMENTS
In this section, we evaluate the empirical performance of the proposed regularized dropout in (15) on a variety of network architectures for the classification task on three benchmark datasets—MNIST, CIFAR-10 and CIFAR-100. We applied the same data preprocessing procedure as in Srivastava et al. (2014). To make a thorough comparison and provide experimental evidence on how the expectationlinearization interacts with the predictive power of the learned model, we perform experiments of Monte Carlo (MC) dropout, which approximately computes the final prediction (left-hand side of (3)) via Monte Carlo sampling, w/o the proposed regularizer. In the case of MC dropout, we average m = 100 predictions using randomly sampled configurations. In addition, the network architectures and hyper-parameters for each experiment setup are the same as those in Srivastava et al. (2014), unless we explicitly claim to use different ones. Following previous works, for each data set We held out 10,000 random training images for validation to tune the hyper-parameters, including λ in Eq. (15). When the hyper-parameters are fixed, we train the final models with all the training data, including the validation data. A more detailed description of the conducted experiments can be provided in Appendix D. For each experiment, we report the mean test errors with corresponding standard deviations over 5 repetitions.

6.1 MNIST
The MNIST dataset (LeCun et al., 1998) consists of 70,000 handwritten digit images of size 28×28, where 60,000 images are used for training and the rest for testing. This task is to classify the images into 10 digit classes. For the purpose of comparison, we train 6 neural networks with different architectures. The experimental results are shown in Table 1.

6.2 CIFAR-10 AND CIFAR-100
The CIFAR-10 and CIFAR-100 datasets (Krizhevsky, 2009) consist of 60,000 color images of size 32× 32, drawn from 10 and 100 categories, respectively. 50,000 images are used for training and the
rest for testing. The neural network architecture we used for these two datasets has 3 convolutional layers, followed by two fully-connected (dense) hidden layers (again, same as that in Srivastava et al. (2014)). The experimental results are recorded in Table 1, too.
From Table 1 we can see that on MNIST data, dropout network training with expectation-linearization outperforms standard dropout on all 6 neural architectures. On CIFAR data, expectation-linearization reduces error rate from 12.82% to 12.20% for CIFAR-10, achieving 0.62% improvement. For CIFAR-100, the improvement in terms of error rate is 0.97% with reduction from 37.22% to 36.25%.
From the results we see that with or without expectation-linearization, the MC dropout networks achieve similar results. It illustrates that by achieving expectation-linear neural networks, the predictive power of the learned models has not degraded significantly. Moreover, it is interesting to see that with the regularization, on MNIST dataset, standard dropout networks achieve even better accuracy than MC dropout. It may be because that with expectation-linearization, standard dropout inference achieves better approximation of the final prediction than MC dropout with (only) 100 samples. On CIFAR datasets, MC dropout networks achieve better accuracy than the ones with the regularization. But, obviously, MC dropout requires much more inference time than standard dropout (MC dropout with m samples requires about m times the inference time of standard dropout).
6.3 EFFECT OF REGULARIZATION CONSTANT λ
In this section, we explore the effect of varying the hyper-parameter for the expectation-linearization rate λ. We train the network architectures in Table 1 with the λ value ranging from 0.1 to 10.0. Figure 1 shows the test errors obtained as a function of λ on three datasets. In addition, Figure 1, middle and right panels, also measures the empirical expectation-linearization risk ∆̂ of Eq. (12) with varying λ on CIFAR-10 and CIFAR-100, where ∆̂ is computed using Monte carlo with 100 independent samples.
From Figure 1 we can see that when λ increases, better expectation-linearity is achieved (i.e. ∆̂ decreases). The model accuracy, however, has not kept growing with increasing λ, showing that in practice considerations on the trade-off between model expectation-linearity and accuracy are needed.

6.4 COMPARISON WITH DROPOUT DISTILLATION
To make a thorough empirical comparison with the recently proposed Dropout Distillation method (Bulò et al., 2016), we also evaluate our regularization method on CIFAR-10 and CIFAR-100 datasets with the All Convolutional Network (Springenberg et al., 2014) (AllConv). To facilitate comparison, we adopt the originally reported hyper-parameters and the same setup for training.
Table 2 gives the results comparison the classification error percentages on test data under AllConv using standard dropout, Monte Carlo dropout, standard dropout with our proposed expectationlinearization, and recently proposed dropout distillation on CIFAR-10 and CIFAR-100 1. According to Table 2, our proposed expectation-linear regularization method achieves comparable performance to dropout distillation.

7 CONCLUSIONS
In this work, we attempted to establish a theoretical basis for the understanding of dropout, motivated by controlling the gap between dropout’s training and inference phases. Through formulating dropout as a latent variable model and introducing the notion of (approximate) expectation-linearity, we have formally studied the inference gap of dropout, and introduced an empirical measure as a regularization scheme to explicitly control the gap. Experiments on three benchmark datasets demonstrate that reducing the inference gap can indeed improve the end performance. In the future, we intend to formally relate the inference gap to the generalization error of the underlying network, hence providing further justification of regularized dropout.

ACKNOWLEDGEMENTS
This research was supported in part by DARPA grant FA8750-12-2-0342 funded under the DEFT program. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of DARPA.

APPENDIX: DROPOUT WITH EXPECTATION-LINEAR REGULARIZATION

A LVM DROPOUT TRAINING VS. STANDARD DROPOUT TRAINING

Proof of Theorem 1

Proof.
ESD [l(D,SD; θ)] = ∫ S N∏ i=1 p(si) ( N∑ i=1 log p(yi|xi, si; θ) ) dµ(s1) . . . dµ(sN )
= N∑ i=1 ∫ S p(si) log p(yi|xi, si; θ)dµ(si)
Because log(·) is a concave function, from Jensen’s Inequality,∫ S p(s) log p(y|x, s; θ)dµ(s) ≤ log ∫ S p(s)p(y|x, s; θ)dµ(s)
Thus
ESD [−l(D,SD; θ)] ≥ N∑ i=1 log ∫ S p(si)p(yi|xi, si; θ)dµ(si) = −l(D; θ).

B EXPECTATION-LINEAR DROPOUT NEURAL NETWORKS

B.1 PROOF OF THEOREM 2
Proof. Let γ∗ = E[Γ], and
A ∆ = {x : ‖E[f(x Γ; θ)]− f(x γ∗; θ)‖2 = 0}
Let X∗ = argmin x∈A sup γ∈S ‖X γ − x γ‖2, and X− = X −X∗. Then,
X γ = X∗ γ +X− γ
In the following, we omit the parameter θ for convenience. Moreover, we denote EΓ [ f(X Γ; θ) ] ∆ = E [ f(X Γ; θ)|X ] From Taylor Series, there exit some X ′, X ′′ ∈ X satisfy that
f(X Γ) = f(X∗ Γ) + f ′(X ′ Γ)(X− Γ) f(X γ∗) = f(X∗ γ∗) + f ′(X ′′ γ∗)(X− γ∗)
where we denote f ′(x) = (∇xf(x))T . Then,
EΓ[f(X Γ)− f(X γ∗)] = EΓ[f(X
∗ Γ +X− Γ)− f(X∗ γ∗ +X− γ∗)] = EΓ[f(X
∗ Γ)− f(X∗ γ∗) + f ′(X ′ Γ)(X− Γ)− f ′(X ′′ γ∗)(X− γ∗)] = EΓ[f(X ∗ Γ)− f(X∗ γ∗)] + EΓ[f ′(X ′ Γ)(X− Γ)− f ′(X ′′ γ∗)(X− γ∗)]
Since X∗ ∈ A, we have EΓ[f(X
∗ Γ)− f(X∗ γ∗)] = 0. Then,
EΓ[f(X Γ)− f(X γ∗)] = EΓ[f
′(X ′ Γ)(X− Γ)− f ′(X ′′ γ∗)(X− γ∗)] = EΓ[(f
′(X ′ Γ)− f ′(X ′′ γ∗))(X− Γ)] + EΓ[f ′(X ′′ γ∗)(X− Γ−X− γ∗)] = EΓ[(f ′(X ′ Γ)− f ′(X ′′ γ∗))(X− Γ)]
Then, ‖EΓ[f(X Γ)]− f(X γ∗)‖2 = ‖EΓ[(f ′(X ′ Γ)− f ′(X ′′ γ∗))(X− Γ)]‖2 Since ‖X− γ′‖2 ≤ sup
γ∈S ‖X− γ‖2 = inf x∈A sup γ∈S ‖X γ − x γ‖2, and from Jensen’s inequality
and property of operator norm,
‖EΓ[(f ′(X ′ Γ)− f ′(X ′′ γ∗))(X− Γ)]‖2 ≤ EΓ [ ‖f ′(X ′ Γ)− f ′(X ′′ γ∗)‖op‖X− Γ‖2 ] ≤ 2BEΓ [ ‖X− Γ‖2
] ≤ 2B inf
x∈A sup γ∈S ‖X γ − x γ‖2
Finally we have,
EX [ ‖EΓ[(f ′(X ′ Γ)− f ′(X ′′ γ∗))(X− Γ)]‖2 ] ≤ 2BE [ inf x∈A sup γ∈S ‖X γ − x γ‖2 ] ≤ 2BC

B.2 PROOF OF THEOREM 3
Proof. Induction on the number of the layers L. As before, we omit the parameter θ. Initial step: when L = 1, the statement is obviously true. Induction on L: Suppose that the statement is true for neural networks with L layers. Now we prove the case L+ 1. From the inductive assumption, we have,
EX [∥∥ESL[H(L)(X,SL)]− h(L)(X,E[SL])∥∥2] ≤ ∆L (1) where SL = {Γ(1), . . . ,Γ(L)} is the dropout random variables for the first L layers, and
∆L = (Bγ) L−1δ + (δ +Bγσ)
( 1− (Bγ)L−1
1−Bγ ) In addition, the L+ 1 layer is δ-approximately expectation-linear, we have:
EH(L) [∥∥EΓ(L+1)[fL+1(H(L) Γ(L+1))]− fL+1(H(L) γ(L+1))∥∥2] ≤ δ (2)
Let E[Γ(l)] = γ(l),∀l ∈ {1, . . . , L + 1}, and let H(l) and h(l) be short for H(l)(X,Sl) and h(l)(X,E(Sl)), respectively, when there is no ambiguity. Moreover, we denote
ES [ H(L)(X,S; θ) ] = ES [ H(L)(X,S; θ) ∣∣X] for convenience. Then,
EX [∥∥ESL+1[H(L+1)]− h(L+1)∥∥2] = EX
[∥∥∥ESL[EΓ(L+1)[fL+1(H(L) Γ(L+1))]− fL+1(h(L) γ(L+1))] +ESL [ fL+1(H (L) γ(L+1)) ] − fL+1(h(L) γ(L+1)) ∥∥∥ 2
] ≤ EX [∥∥∥ESL[EΓ(L+1)[fL+1(H(L) Γ(L+1))]− fL+1(h(L) γ(L+1))]∥∥∥ 2
] +EX [∥∥∥ESL[fL+1(H(L) γ(L+1))]− fL+1(h(L) γ(L+1))∥∥∥ 2
] From Eq. 2 and Jensen’s inequality, we have
EX [∥∥∥ESL[EΓ(L+1)[fL+1(H(L) Γ(L+1))]− fL+1(h(L) γ(L+1))]∥∥∥ 2 ] ≤ EH(L) [∥∥∥EΓ(L+1)[fL+1(H(L) Γ(L+1))]− fL+1(h(L) γ(L+1))∥∥∥ 2 ] ≤ δ
(3)
and
EX [∥∥∥ESL[fL+1(H(L) γ(L+1))]− fL+1(h(L) γ(L+1))∥∥∥ 2 ] = EX
[∥∥∥ESL[fL+1(H(L) γ(L+1))]− fL+1(ESL[H(L)] γ(L+1)) +fL+1(ESL [ H(L) ] γ(L+1))− fL+1(h(L) γ(L+1)) ∥∥∥ 2
] ≤ EX [∥∥∥ESL[fL+1(H(L) γ(L+1))]− fL+1(ESL[H(L)] γ(L+1))∥∥∥ 2
] +EX [∥∥∥fL+1(ESL[H(L)] γ(L+1))− fL+1(h(L) γ(L+1))∥∥∥ 2 ] (4)
Using Jensen’s inequality, property of operator norm and E [ Var[H(l)|X] ] ≤ σ2, we have
EX [∥∥∥ESL[fL+1(H(L) γ(L+1))]− fL+1(ESL[H(L)] γ(L+1))∥∥∥ 2 ] ≤ EH(L) [∥∥∥fL+1(H(L) γ(L+1))− fL+1(ESL[H(L)] γ(L+1))∥∥∥ 2
] ≤ BγEH(L)
[∥∥H(L) − ESL[H(L)]∥∥2] ≤ Bγ ( EH(L) [∥∥H(L) − ESL[H(L)]∥∥22]) 12 ≤ Bγσ (5)
From Eq. 1
EX [∥∥∥fL+1(ESL[H(L)] γ(L+1))− fL+1(h(L) γ(L+1))∥∥∥ 2 ] = BγEX
[∥∥ESL[H(L)]− h(L)∥∥2] ≤ Bγ∆L (6) Finally, to sum up with Eq. 3, Eq. 4, , Eq. 5, , Eq. 6, we have
EX [∥∥ESL+1[H(L+1)]− h(L+1)∥∥2] ≤ δ +Bγσ +Bγ∆L = (Bγ)Lδ + (δ +Bγσ) ( 1−(Bγ)L
1−Bγ
) = ∆L+1

C EXPECTATION-LINEARIZATION

C.1 PROOF OF THEOREM 4: UNIFORM DEVIATION BOUND
Before proving Theorem 4, we first define the notations.
Let Xn = {X1, . . . , Xn} be a set of n samples of input X . For a function space F : X → R, we use Radn(F , Xn) to denote the empirical Rademacher complexity of F ,
Radn(F , Xn) = Eσ [
sup f∈F ( 1 n n∑ i=1 σif(Xi) )]
and the Rademacher complexity is defined as Radn(F) = EXn [ Radn(F , Xn) ] In addition, we import the definition of dropout Rademacher complexity from Gao & Zhou (2014):
Rn(H, Xn, Sn) = Eσ [
sup h∈H
( 1 n n∑ i=1 σih(Xi, Si) )]
Rn(H) = EXn,Sn [ Radn(H, Xn, Sn) ]
where H : X × S → R is a function space defined on input space X and dropout variable space S. Rn(H, Xn, Sn) and Rn(H) are the empirical dropout Rademacher complexity and dropout Rademacher complexity, respectively. We further denoteRn(H, Xn) ∆ = ESn [ Radn(H, Xn, Sn) ] .
Now, we define the following function spaces: F = { f(x; θ) : f(x; θ) = ES [ H(L)(x, S; θ) ] , θ ∈ Θ } G = { g(x; θ) : g(x; θ) = h(L)(x,E[S]; θ), θ ∈ Θ
} H = { h(x, s; θ) : h(x, s; θ) = h(L)(x, s; θ), θ ∈ Θ
} Then, the function space of v(x) = f(x)− g(x) is V = {f(x)− g(x) : f ∈ F , g ∈ G}. Lemma 7.
Radn(F , Xn) ≤ Rn(H, Xn)
Proof. Rn(H, Xn) = ESn [ Radn(H, Xn, Sn) ] = ESn [ Eσ [ sup h∈H ( 1 n n∑ i=1 σih(Xi, Si) )]]
= Eσ [ ESn [ sup h∈H ( 1 n n∑ i=1 σih(Xi, Si) )]]
≥ Eσ [
sup h∈H
ESn [( 1 n n∑ i=1 σih(Xi, Si) )]]
= Eσ [ sup h∈H ( 1 n n∑ i=1 σiESi [ h(Xi, Si) ])] = Eσ [ sup h∈H ( 1 n n∑ i=1 σiESi [ H(L)(Xi, Si; θ) ])] = Radn(F , Xn)
From Lemma 7, we have Radn(F) ≤ Rn(H). Lemma 8.
Rn(H) ≤ αB LγL/2√ n
Radn(G) ≤ αB L
√ n
Proof. See Theorem 4 in Gao & Zhou (2014).
Now, we can prove Theorem 4.

Proof of Theorem 4
Proof. From Rademacher-based uniform bounds theorem, with probability ≥ 1− δ,
sup v∈V |∆− ∆̂| < 2Radn(V) + β
√ log(1/δ)
n
Since V = F − G, we have
Radn(V) = Radn(F − G) ≤ Radn(F) +Radn(G) ≤ αBL(γL/2 + 1)√
n
Then, finally, we have that with probability ≥ 1− δ,
sup θ∈Θ |∆− ∆̂| < 2αB L(γL/2 + 1)√ n + β
√ log(1/δ)
n

C.2 PROOF OF THEOREM 5: NON-UNIFORM BOUND OF MODEL ACCURACY
For convenience, we denote λ = {θ1, . . . , θL−1}. Then θ = {λ, η}, and MLE θ̂ = {λ̂, η̂} Lemma 9.
‖∇fL(·; η)T ‖op ≤ 2‖η‖2 (7)
Proof. denote
A = ∇fL(·; η)T = [ py(ηy − η)T ] ∣∣∣k y=1
where py = p(y|x, s; θ), η = E [ηY ] = k∑ y=1 pyηy .
For each v such that ‖v‖2 = 1,
‖Av‖22 = ∑ y∈Y ( py (ηy − η)T v )2 ≤ ∑ y∈Y ‖py (ηy − η) ‖22‖v‖22 = ∑ y∈Y ‖py (ηy − η) ‖22
≤ ∑ y∈Y py‖ηy − η‖22 ≤ ∑ y∈Y 2py ( ‖η‖22 + ∑ y′∈Y py′‖ηy′‖22 ) = 4
∑ y∈Y py‖ηy‖22 ≤ 4‖η‖22
So we have ‖A‖op ≤ 2‖η‖2.
Lemma 10. If parameter θ̃ = {λ̂, η} satisfies that ‖η‖2 ≤ δ4β , then V (D; θ̃) ≤ δ, where V (D; θ) is defined in Eq. (16).
Proof. Let SL = {Γ(1), . . . ,Γ(L)}, and let H(l) and h(l) be short for H(l)(X,Sl; θ̃) and h(l)(X,E(Sl); θ̃), respectively.
From lemma 9, we have ‖fL(x; η)− fL(y; η)‖2 ≤ 2‖η‖2‖x− y‖2. Then,∥∥ESL [HL]− hL∥∥2 = ∥∥ESL−1 [fL(H(L−1); η)]− fL(h(L−1); η)∥∥2 ≤ ESL−1
∥∥fL(H(L−1); η)− fL(h(L−1); η)∥∥2 ≤ 2‖η‖2 ∥∥H(L−1) − h(L−1)∥∥ 2 ≤ 4β‖η‖2 ≤ δ
Lemma 10 says that we can get θ satisfying the expectation-linearization constrain by explicitly scaling down η̂ while keeping λ̂.
In order to prove Theorem 5, we make the following assumptions:
• The dimension of h(L−1) is d, i.e. h(L−1) ∈ Rd. • Since ∀y ∈ Y, p(y|x; θ̂) > 0, we assume p(y|x; θ̂) ≥ 1/b, where b ≥ |Y| = k. • As in the body text, let p(y|x, s; θ̂) be nonuniform, and in particular let η̂Ty∗h (L−1)(x, s; λ̂)− η̂Ty h(L−1)(x, s; λ̂) > c‖η̂‖2,∀y 6= y∗.
For convenience, we denote ηTh(L−1)(x, s;λ) = ηTuy(x, s;λ), where uTy (x, s;λ) = (v T 0 , . . . , v T k ) and
vi = { h(L−1)(x, s;λ) if i = y 0 otherwise
To prove Theorem 5, we first prove the following lemmas.
Lemma 11. If p(y|x; θ̂) ≥ 1/b, then ∀α ∈ [0, 1], for parameter θ̃ = {λ̂, αη̂}, we have
p(y|x; θ̃) ≥ 1 b
Proof. We define
f(α) ∆ = (y|x, s; θ̃) = e αηTy h (L−1)(x,s;λ̂)∑
y′∈Y e αηT y′h (L−1)(x,s;λ̂)
=
( eη T y h (L−1)(x,s;λ̂) )α
∑ y′∈Y ( e ηT y′h (L−1)(x,s;λ̂) )α
Since Y = {1, . . . , k}, for fixed x ∈ X , s ∈ S, log f(α) is a concave function w.r.t α. Since b ≥ k, we have
log f(α) ≥ (1− α) log f(0) + α log f(1) ≥ − log b
So we have ∀x, s, p(y|x, s; θ̃) ≥ 1/b. Then
p(y|x; θ̃) = ES [ p(y|x, S; θ̂) ] ≥ 1 b
Lemma 12. if y is not the majority class, i.e. y 6= y∗, then for parameter θ̃ = {λ̂, αη̂}
p(y|x, s, θ̃) ≤ e−cα‖η̂‖2
Proof.
p(y|x, s, θ̃) = e αη̂Tuy∑
y′∈Y eαη̂
Tuy′ ≤ e
αη̂Tuy
eαη̂ Tuy∗
≤ e−cα‖η̂‖2
Lemma 13. For a fixed x and s, the absolute value of the entry of the vector under the parameter θ̃ = {λ̂, αη̂}:
|p(y|x, s; θ̃)(uy − EY [uY ])|i ≤ β(k − 1)e−cα‖η̂‖2
Proof. Suppose y is the majority class of p(y|x, s; θ̃). Then,
uy − Ey[uY ] = (vy′)ky′=1
where
vy = { (1− p(y|x, s; θ̃)h(L−1) if y = y∗ −p(y|x, s; θ̃)h(L−1) otherwise
From Lemma 12, we have
|p(y|x, s; θ̃)(uy − EY [uY ])|i ≤ |(uy − EY [uY ])|i ≤ β(k − 1)e−cα‖η̂‖2
Now, we suppose y is not the majority class of p(y|x, s; θ̃). Then,
|p(y|x, s; θ̃)(uy − EY [uY ])|i ≤ p(y|x, s; θ̃)β ≤ βe−cα‖η̂‖2
Overall, the lemma follows.
Lemma 14. We denote the matrix
A ∆ = ES [ p(y|x,s;θ̃) p(y|x;θ̃) (uy − EY [uY ])(uy − EY [uY ]) T ]
−ES [ p(y|x,s;θ̃) p(y|x;θ̃) (uy − EY [uY ]) ] ES [ p(y|x,s;θ̃) p(y|x;θ̃) (uy − EY [uY ]) ]T Then the absolute value of the entry of A under the parameter θ̃ = {λ̂, αη̂}:
|Aij | ≤ 2b(k − 1)β2e−cα‖η̂‖2
Proof. From Lemma 11, we have p(y|x; θ̃) ≥ 1/b. Additionally, the absolute value of the entry of uy − EY [uY ] is bounded by β. We have for each i∣∣∣∣∣ES [ p(y|x, s; θ̃) p(y|x; θ̃) (uy − EY [uY ]) ]∣∣∣∣∣ i ≤ ES [ p(y|x, s; θ̃) p(y|x; θ̃) β ] = β
Then from Lemma 13 |Aij | ≤ 2b(k − 1)β2e−cα‖η̂‖2
Lemma 15. We denote the matrix
B ∆ = ES [ p(y|x, s; θ̃) p(y|x; θ̃) ( EY [ uY u T Y ] − EY [uY ]EY [uY ]T )]
Then the absolute value of the entry of B under the parameter θ̃ = {λ̂, αη̂}:
|Bij | ≤ 2(k − 1)β2e−cα‖η̂‖2
Proof. We only need to prove that for fixed x and s, for each i, j:∣∣EY [uY uTY ]− EY [uY ]EY [uY ]T ∣∣ij ≤ 2(k − 1)β2e−cα‖η̂‖2 Since ∣∣EY [uY uTY ]− EY [uY ]EY [uY ]T ∣∣ij = |CovY [(uY )i, (uY )j ]| ≤ β2 k∑ y=1 p(y|x, s; θ̃)− p(y|x, s; θ̃)2
Suppose y is the majority class. Then from Lemma 12,
p(y|x, s; θ̃)− p(y|x, s; θ̃)2 ≤ 1− p(y|x, s; θ̃) ≤ (k − 1)e−cα‖η̂‖2
If y is not the majority class. Then,
p(y|x, s; θ̃)− p(y|x, s; θ̃)2 ≤ p(y|x, s; θ̃) ≤ e−cα‖η̂‖2
So we have k∑ y=1 p(y|x, s; θ̃)− p(y|x, s; θ̃)2 ≤ 2(k − 1)e−cα‖η̂‖2
The lemma follows.
Lemma 16. Under the parameter θ̃ = {λ̂, αη̂}, the largest eigenvalue of the matrix
1
n n∑ i=1 (A(xi, yi)−B(xi, yi)) (8)
is at most 2dk(k − 1)(b+ 1)β2e−cα‖η̂‖2
Proof. From Lemma 14 and Lemma 15, each entry of the matrix in (8) is at most 2(k − 1)(b + 1)β2e−cα‖η̂‖2 . Thus, by Gershgorin’s theorem, the maximum eigenvalue of the matrix in (8) is at most 2dk(k − 1)(b+ 1)β2e−cα‖η̂‖2 .
Now, we can prove Theorem 5 by constructing a scaled version of θ̂ that satisfies the expectationlinearization constraint.

Proof of Theorem 5
Proof. Consider the likelihood evaluated at θ̃ = {λ̂, αη̂}, where α = δ4β‖η̂‖2 . If α > 1, then ‖η‖2 > δ4β . We know the MLE θ̂ already satisfies the expectation-linearization constraint. So we can assume that 0 ≤ α ≤ 1, and we know that θ̃ satisfies V (D; θ̃) ≤ δ. Then,
∆l(θ̂, θ̂δ) ≤ ∆l(θ̂, θ̃) = 1
n (l(D; θ̂)− l(D; θ̃)) = g(λ̂, η̂)− g(λ̂, αη̂)
where g(λ, η) = 1n l(D; (λ, η)). Taking the second-order Taylor expansion about η, we have
g(λ̂, αη̂) = g(λ̂, η̂) +∇Tη g(λ̂, η̂)(αη̂ − η̂) + (αη̂ − η̂)T∇2ηg(λ̂, η̂)(αη̂ − η̂)
Since θ̂ is the MLE, the first-order term ∇Tη g(λ̂, η̂)(αη̂ − η̂) = 0. The Hessian in the second-order term is just Eq.(8). Thus, from Lemma 16 we have
g(λ̂, αη̂) ≤ g(λ̂, η̂)− (1− α)2‖η̂‖222dk(k − 1)(b+ 1)β2e−cα‖η̂‖2 = g(λ̂, η̂)− 2dk(k − 1)(b+ 1)β2 ( ‖η̂‖2 − δ4β )2 e−cδ/4β
= g(λ̂, η̂)− c1β2 ( ‖η̂‖2 − δ4β )2 e−c2δ/4β
with setting c1 = 2dk(k − 1)(b+ 1) and c2 = c. Then the theorem follows.

C.3 PROOF OF THEOREM 6: UNIFORM BOUND OF MODEL ACCURACY
In the following, we denote θ̃ = {λ̂, αη̂}.
Lemma 17. For each y ∈ Y , if p(y|x, s; θ̂) ≥ 1/k, then ∀α ∈ [0, 1]
p(y|x, s; θ̃) ≥ 1 k
Proof. This lemma can be regarded as a corollary of Lemma 11.
Lemma 18. For a fixed x and s, we denote eη̂ T y h (L−1)(x,s;λ̂) = wy . Then we have
p(y|x, s, θ̃) = e αη̂Ty h (L−1)(x,s;λ̂)∑ y′∈Y e αη̂T y′h (L−1)(x,s;λ̂) = (wy) α∑ y′∈Y (wy′)α
Additionally, we denote gs(α) = ∑ y′∈Y p(y′|x, s; θ̃) logwy′ − logwy . We assume gs(0) ≥ 0. Then we have ∀α ≥ 0 gs(α) ≥ 0
Proof.
∂gs(α) ∂α = ∑ y′∈Y logwy′ ∂p(y′|x, s; θ̃) ∂α = VarY [logwY |X − x, S = s] ≥ 0
So gs(α) is non-decreasing. Since gs(0) ≥ 0, we have gs(α) ≥ 0 when α ≥ 0.
From above lemma, we have for each training instance (xi, yi) ∈ D, and ∀α ∈ [0, 1],
EY [ log p(Y |xi, s; θ̃) ] ≥ log p(yi|xi, s; θ̃) (9)
For convenience, we define m(s, y) = log p(y|x, s; θ̃)− EY [ log p(Y |x, s; θ̃) ]
Lemma 19. If y satisfies Lemma 17 and gs(α) ≥ 0, then
VarY [m(s, Y )] ≥ m(s, y)2
Proof. First we have m(s, y) = log p(y|x, s; θ̃)− log 1/k −KL ( p(·|x, s; θ̃)|Unif(Y) ) ≤ 0
So we have
(VarY [m(s, Y )]) 1/2 = √ EY [( log p(Y |x, s; θ̃)− EY [ log p(Y |x, s; θ̃) ])2] ≥ EY
[∣∣∣log p(Y |x, s; θ̃)− EY [log p(Y |x, s; θ̃)]∣∣∣] = EY
[∣∣∣KL(p(·|x, s; θ̃)|Unif(Y))+ log 1/k − log p(Y |x, s; θ̃)∣∣∣] = EY [ KL ( p(·|x, s; θ̃)|Unif(Y) ) + ∣∣∣log 1/k − log p(Y |x, s; θ̃)∣∣∣]
≥ KL ( p(·|x, s; θ̃)|Unif(Y) ) + EY [ log p(Y |x, s; θ̃)− log 1/k ] = 2KL ( p(·|x, s; θ̃)|Unif(Y)
) As KL ( p(·|x, s; θ̃)|Unif(Y) ) ≥ 0 and log p(y|x, s; θ̃) ≥ log 1/k. So we have 2KL ( p(·|x, s; θ̃)|Unif(Y) ) ≥ KL ( p(·|x, s; θ̃)|Unif(Y) ) +log 1/k−log p(y|x, s; θ̃) = −m(s, y)
Then the lemma follows.
From Lemma 19 and Eq. (9), we have for each training instance (xi, yi) ∈ D, and ∀α ∈ [0, 1],
VarY [m(s, Y )] ≥ m(s, yi)2 (10)
Lemma 20. For each training instance (xi, yi) ∈ D, and ∀α ∈ [0, 1], we have
log p(yi|xi; {λ̂, αη̂}) ≥ (1− α) log p(yi|xi; {λ̂, 0}) + α log p(yi|xi; {λ̂, η̂})
Proof. We define
f(α) = log p(yi|xi; {λ̂, αη̂})− (1− α) log p(yi|xi; {λ̂, 0})− α log p(yi|xi; {λ̂, η̂})
Because f(0) = f(1) = 0, we only need to prove that f(α) is concave on [0, 1]. We have
∇2f(α) = −ES|Y=yi [VarY [m(S, Y )]] + VarS|Y=yi [m(S, yi)]
where S|Y = yi is under the probability distribution p(s|Y = yi, xi; θ̃) = p(yi|xi,S;θ̃)p(s)p(yi|xi;θ̃) From Eq. (10), we have
ES|Y=yi [VarY [m(S, Y )]] ≥ ES|Y=yi [ m(S, yi) 2 ] ≥ VarS|Y=yi [m(S, yi)]
So we have ∇2f(α) ≤ 0. The lemma follows.
Now, we can prove Theorem 6 by using the same construction of an expectation-linearizing parameter as in Theorem 5.

Proof of Theorem 6
Proof. Consider the same parameter θ̃ = {λ̂, αη̂}, where α = δ4β‖η̂‖2 ≤ 1. we know that θ̃ satisfies V (D; θ̃) ≤ δ. Then,
∆l(θ̂, θ̂δ) ≤ ∆l(θ̂, θ̃) = 1
n (l(D; θ̂)− l(D; θ̃))
From Lemma 20 we have:
l(D; θ̃) = l(D; {λ̂, αη̂}) ≥ (1− α)l(D; {λ̂, 0}) + αl(D; {λ̂, η̂})
So ∆l(θ̂, θ̂δ) ≤ (1− α) 1n ( l(D; θ̂)− l(D; {λ̂, 0}) ) = (1− α) 1n n∑ i=1 log p(yi|xi; θ̂)− log Unif(Y)
(1− α)E [KL (p(·|X; θ)‖Unif(Y))] ≤ ( 1− δ4β‖η̂‖2 ) E [KL (p(·|X; θ)‖Unif(Y))]

D DETAILED DESCRIPTION OF EXPERIMENTS

D.1 NEURAL NETWORK ARCHITECTURES
MNIST For MNIST, we train 6 different fully-connected (dense) neural networks with 2 or 3 layers (see Table 1). For all architectures, we used dropout rate p = 0.5 for all hidden layers and p = 0.2 for the input layer.
CIFAR-10 and CIFAR-100 For the two CIFAR datasets, we used the same architecture in Srivastava et al. (2014) — three convolutional layers followed by two fully-connected hidden layers. The convolutional layers have 96, 128, 265 filters respectively, with a 5× 5 receptive field applied with a stride of 1. Each convolutional layer is followed by a max pooling layer pools 3× 3 regions at strides of 2. The fully-connected layers have 2048 units each. All units use the rectified linear activation function. Dropout was applied to all the layers with dropout rate p = (0.1, 0.25, 0.25, 0.5, 0.5, 0.5) for the layers going from input to convolutional layers to fully-connected layers.

D.2 NEURAL NETWORK TRAINING
Neural network training in all the experiments is performed with mini-batch stochastic gradient descent (SGD) with momentum. We choose an initial learning rate of η0, and the learning rate is updated on each epoch of training as ηt = η0/(1 + ρt), where ρ is the decay rate and t is the number of epoch completed. We run each experiment with 2,000 epochs and choose the parameters achieving the best performance on validation sets.
Table 3 summarizes the chosen hyper-parameters for all experiments. Most of the hyper-parameters are chosen from Srivastava et al. (2014). But for some experiments, we cannot reproduce the performance reported in Srivastava et al. (2014) (We guess one of the possible reasons is that we used different library for implementation.). For these experiments, we tune the hyper-parameters on the validation sets by random search. Due to time constrains it is infeasible to do a random search across the full hyper-parameter space. Thus, we try to use as many hyper-parameters reported in Srivastava et al. (2014) as possible.
D.3 EFFECT OF EXPECTATION-LINEARIZATION RATE λ
Table 4 illustrates the detailed results of the experiments on the effect of λ. For MNIST, it lists the error rates under different λ values for six different network architectures. For two datasets of CIFAR, it gives the error rates under different λ values, among with the empirical expectation-linearization risk ∆̂.
","Dropout, a simple and effective way to train deep neural networks, has led to a number of impressive empirical successes and spawned many recent theoretical investigations. However, the gap between dropout’s training and inference phases, introduced due to tractability considerations, has largely remained under-appreciated. In this work, we first formulate dropout as a tractable approximation of some latent variable model, leading to a clean view of parameter sharing and enabling further theoretical analysis. Then, we introduce (approximate) expectation-linear dropout neural networks, whose inference gap we are able to formally characterize. Algorithmically, we show that our proposed measure of the inference gap can be used to regularize the standard dropout training objective, resulting in an explicit control of the gap. Our method is as simple and efficient as standard dropout. We further prove the upper bounds on the loss in accuracy due to expectation-linearization, describe classes of input distributions that expectation-linearize easily. Experiments on three image classification benchmark datasets demonstrate that reducing the inference gap can indeed improve the performance consistently.",ICLR 2017 conference submission,True,,"This paper puts forward a not entirely new, but also not sufficiently understood interpretation of dropout regularization. The authors derive useful theorems that estimate or put bounds on key quantities that are of interest when analyzing dropout regularized networks from their perspective. They furthermore introduce an explicit regularization term that should have a well understood impact on these key quantities. In the experimental section they convincingly show that the proposed regularization indeed has the expected effect and that their perspective on dropout is therefore useful and meaningful.

Their proposed regularization also seems to have a positive impact on the models performance but they demonstrate this only on rel. small scale benchmark problems. I therefore don’t belief that this approach will have a large impact on how practitioner train models.  But their general perspective is well aligned with the recently proposed idea of “Dropout as a bayesian approximation” and the insights and theorems in this paper might enable future work in that direction.

---

This paper presents a theoretical underpinning of dropout, and uses this derivation to both characterize its properties and to extend the method. A solid contribution. I am surprised that none of the reviewers mentioned that this work is closely related to the uncited 2015 paper ""Variational Dropout and the Local Reparameterization Trick"" by Diederik P. Kingma, Tim Salimans, Max Welling.

---

We made the following revisions:

1. We switched the section 6.3 and 6.4 to make the paper more clear.

2. We added the definition of MC dropout on page 8.

3. We fixed all the typos in the three reviewers' comments.

---

summary

The paper explains dropout with a latent variable model where the dropout variable (0 or 1 depending on which units should be dropped) is not observed and is accordingly marginalised. Maximum likelihood under this model is not tractable but standard dropout then corresponds to a simple Monte Carlo approximation of ML for this model.

The paper then introduces a theoretical framework for analysing the discrepancy (called inference gap) between the model at training (model ensemble, or here the latent variable model), and the model at testing (where usually what should be an expectation over the activations over many models becomes the activation of one model with averaged weights).
This framework introduces several notions (e.g. expectation linearity) which allow the study of which transition functions (and more generally layers) can have a small inference gap. Theorem 3 gives a bound on the inference gap.

Finally a new regularisation term is introduced to account for minimisation of the inference gap during learning.

Experiments are performed on MNIST, CIFAR-10 and CIFAR-100 and show that the method has the potential to perform better than standard dropout and at the level of Monte Carlo Dropout (the standard method to compute the real dropout outputs consistently with the training assumption of an ensemble, of course quite expensive computationally)


The study gives a very interesting theoretical model for dropout as a latent variable model where standard dropout is then a monte carlo approximation. This is very probably widely applicable to further studies of dropout.

The framework for the study of the inference gap is interesting although maybe somewhat less widely applicable.

The proposed model is convincing although 1. it is tested on simple datasets 2. the gains are relatively small and 3. there is an increased computational cost during training because a new hyper-parameter is introduced.

p6 line 8 typo: expecatation

---

This paper introduces dropout as a latent variable model (LVM). Leveraging this formulation authors analyze the dropout “inference gap” which they define to be the gap between network output during training (where an instance of dropout is used for every training sample) and test (where expected dropout values are used to scale node outputs).  They introduce the notion of expectation linearity and use this to derive bounds on the inference gap under some (mild) assumptions.  Furthermore, they propose use of per-sample based inference gap as a regularizer, and present analysis of accuracy of models with expectation-linearization constraints as compared to those without.

One relatively minor issue I see with the LVM view of dropout is that it seems applicable only to probabilistic models whereas dropout is more generally applicable to deep networks.  However I’d expect that the regularizer formulation of dropout would be effective even in non-probabilistic models.

MC dropout on page 8 is not defined, please define.

On page 9 it is mentioned that with the proposed regularizer the standard dropout networks achieve better results than when Monte Carlo dropout is used.  This seems to be the case only on MNIST dataset and not on CIFAR?

From Tables 1 and 2 it also appears that MC dropout achieves best performance across tasks and methods but it is of course an expensive procedure.  Comments on the computational efficiency of various dropout procedures - to go with the accuracy results - would be quite valuable.

Couple of typos:
- Pg. 2 “ … x is he input …” -> “ … x is the input …”
- Pg. 5 “ … as defined in (1), is …” -> ref. to (1) is not right at two places in this paragraph

Overall it is a good paper, I think should be accepted and discussed at the conference.

---

This paper puts forward a not entirely new, but also not sufficiently understood interpretation of dropout regularization. The authors derive useful theorems that estimate or put bounds on key quantities that are of interest when analyzing dropout regularized networks from their perspective. They furthermore introduce an explicit regularization term that should have a well understood impact on these key quantities. In the experimental section they convincingly show that the proposed regularization indeed has the expected effect and that their perspective on dropout is therefore useful and meaningful.

Their proposed regularization also seems to have a positive impact on the models performance but they demonstrate this only on rel. small scale benchmark problems. I therefore don’t belief that this approach will have a large impact on how practitioner train models.  But their general perspective is well aligned with the recently proposed idea of “Dropout as a bayesian approximation” and the insights and theorems in this paper might enable future work in that direction.

---

This paper puts forward a not entirely new, but also not sufficiently understood interpretation of dropout regularization. The authors derive useful theorems that estimate or put bounds on key quantities that are of interest when analyzing dropout regularized networks from their perspective. They furthermore introduce an explicit regularization term that should have a well understood impact on these key quantities. In the experimental section they convincingly show that the proposed regularization indeed has the expected effect and that their perspective on dropout is therefore useful and meaningful.

Their proposed regularization also seems to have a positive impact on the models performance but they demonstrate this only on rel. small scale benchmark problems. I therefore don’t belief that this approach will have a large impact on how practitioner train models.  But their general perspective is well aligned with the recently proposed idea of “Dropout as a bayesian approximation” and the insights and theorems in this paper might enable future work in that direction.

---

This paper presents a theoretical underpinning of dropout, and uses this derivation to both characterize its properties and to extend the method. A solid contribution. I am surprised that none of the reviewers mentioned that this work is closely related to the uncited 2015 paper ""Variational Dropout and the Local Reparameterization Trick"" by Diederik P. Kingma, Tim Salimans, Max Welling.

---

We made the following revisions:

1. We switched the section 6.3 and 6.4 to make the paper more clear.

2. We added the definition of MC dropout on page 8.

3. We fixed all the typos in the three reviewers' comments.

---

summary

The paper explains dropout with a latent variable model where the dropout variable (0 or 1 depending on which units should be dropped) is not observed and is accordingly marginalised. Maximum likelihood under this model is not tractable but standard dropout then corresponds to a simple Monte Carlo approximation of ML for this model.

The paper then introduces a theoretical framework for analysing the discrepancy (called inference gap) between the model at training (model ensemble, or here the latent variable model), and the model at testing (where usually what should be an expectation over the activations over many models becomes the activation of one model with averaged weights).
This framework introduces several notions (e.g. expectation linearity) which allow the study of which transition functions (and more generally layers) can have a small inference gap. Theorem 3 gives a bound on the inference gap.

Finally a new regularisation term is introduced to account for minimisation of the inference gap during learning.

Experiments are performed on MNIST, CIFAR-10 and CIFAR-100 and show that the method has the potential to perform better than standard dropout and at the level of Monte Carlo Dropout (the standard method to compute the real dropout outputs consistently with the training assumption of an ensemble, of course quite expensive computationally)


The study gives a very interesting theoretical model for dropout as a latent variable model where standard dropout is then a monte carlo approximation. This is very probably widely applicable to further studies of dropout.

The framework for the study of the inference gap is interesting although maybe somewhat less widely applicable.

The proposed model is convincing although 1. it is tested on simple datasets 2. the gains are relatively small and 3. there is an increased computational cost during training because a new hyper-parameter is introduced.

p6 line 8 typo: expecatation

---

This paper introduces dropout as a latent variable model (LVM). Leveraging this formulation authors analyze the dropout “inference gap” which they define to be the gap between network output during training (where an instance of dropout is used for every training sample) and test (where expected dropout values are used to scale node outputs).  They introduce the notion of expectation linearity and use this to derive bounds on the inference gap under some (mild) assumptions.  Furthermore, they propose use of per-sample based inference gap as a regularizer, and present analysis of accuracy of models with expectation-linearization constraints as compared to those without.

One relatively minor issue I see with the LVM view of dropout is that it seems applicable only to probabilistic models whereas dropout is more generally applicable to deep networks.  However I’d expect that the regularizer formulation of dropout would be effective even in non-probabilistic models.

MC dropout on page 8 is not defined, please define.

On page 9 it is mentioned that with the proposed regularizer the standard dropout networks achieve better results than when Monte Carlo dropout is used.  This seems to be the case only on MNIST dataset and not on CIFAR?

From Tables 1 and 2 it also appears that MC dropout achieves best performance across tasks and methods but it is of course an expensive procedure.  Comments on the computational efficiency of various dropout procedures - to go with the accuracy results - would be quite valuable.

Couple of typos:
- Pg. 2 “ … x is he input …” -> “ … x is the input …”
- Pg. 5 “ … as defined in (1), is …” -> ref. to (1) is not right at two places in this paragraph

Overall it is a good paper, I think should be accepted and discussed at the conference.

---

This paper puts forward a not entirely new, but also not sufficiently understood interpretation of dropout regularization. The authors derive useful theorems that estimate or put bounds on key quantities that are of interest when analyzing dropout regularized networks from their perspective. They furthermore introduce an explicit regularization term that should have a well understood impact on these key quantities. In the experimental section they convincingly show that the proposed regularization indeed has the expected effect and that their perspective on dropout is therefore useful and meaningful.

Their proposed regularization also seems to have a positive impact on the models performance but they demonstrate this only on rel. small scale benchmark problems. I therefore don’t belief that this approach will have a large impact on how practitioner train models.  But their general perspective is well aligned with the recently proposed idea of “Dropout as a bayesian approximation” and the insights and theorems in this paper might enable future work in that direction.",,,,,,7.666666666666667,,,3.3333333333333335,,
518,"Authors: GENERATIVE ADVERSAR, IAL LEARNING, Dilin Wang, Qiang Liu
Source file: 518.pdf

ABSTRACT
We propose a simple algorithm to train stochastic neural networks to draw samples from given target distributions for probabilistic inference. Our method is based on iteratively adjusting the neural network parameters so that the output changes along a Stein variational gradient (Liu & Wang, 2016) that maximumly decreases the KL divergence with the target distribution. Our method works for any target distribution specified by their unnormalized density function, and can train any black-box architectures that are differentiable in terms of the parameters we want to adapt. As an application of our method, we propose an amortized MLE algorithm for training deep energy model, where a neural sampler is adaptively trained to approximate the likelihood function. Our method mimics an adversarial game between the deep energy model and the neural sampler, and obtains realistic-looking images competitive with the state-of-the-art results.

1 INTRODUCTION
Modern machine learning increasingly relies on highly complex probabilistic models to reason about uncertainty. A key computational challenge is to develop efficient inference techniques to approximate, or draw samples from complex distributions. Currently, most inference methods, including MCMC and variational inference, are hand-designed by researchers or domain experts. This makes it difficult to fully optimize the choice of different methods and their parameters, and exploit the structures in the problems of interest in an automatic way. The hand-designed algorithm can also be inefficient when it requires to make fast inference repeatedly on a large number of different distributions with similar structures. This happens, for example, when we need to reason about a number of observed datasets in settings like online learning, or need fast inference as inner loops for other algorithms such as maximum likelihood training. Therefore, it is highly desirable to develop more intelligent probabilistic inference systems that can adaptively improve its own performance to fully the optimize computational efficiency, and generalize to new tasks with similar structures.
Specifically, denote by p(x) a probability density of interest specified up to the normalization constant, which we want to draw sample from, or marginalize to estimate its normalization constant. We want to study the following problem:
Problem 1. Given a distribution with density p(x) and a function f(η; ξ) with parameter η and random input ξ, for which we only have assess to draws of the random input ξ (without knowing its true distribution q0), and the output values of f(η; ξ) and its derivative ∂ηf(η; ξ) given η and ξ. We want to find an optimal parameter η so that the density of the random output variable x = f(η; ξ) with ξ ∼ q0 closely matches the target density p(x).
Because we have no assumption on the structure of f(η; ξ) and the distribution of random input, we can not directly calculate the actual distribution of the output random variable x = f(η; ξ); this makes it difficult to solve Problem 1 using the traditional variational inference (VI) methods. Recall that traditional VI approximates p(x) using simple proposal distributions qη(x) indexed by parameter η, and finds the optimal η by minimizing KL divergence KL(qη || p) = Eqη [log(qη/p)], which requires to calculate the density qη(x) or its derivative that is not computable by our assump-
tion (even when the Monte Carlo gradient estimation and the reparametrization trick (Kingma & Welling, 2013) are applied).
In fact, it is this requirement of calculating qη(x) that has been the major constraint for the designing of state-of-the-art variational inference methods with rich approximation families; the recent successful algorithms (e.g., Rezende & Mohamed, 2015b; Tran et al., 2015; Ranganath et al., 2015, to name only a few) have to handcraft special variational families to ensure the computational tractability of qη(x) and simultaneously obtain high approximation accuracy, which require substantial mathematical insights and research effects. Methods that do not require to explicitly calculate qη(x) can significantly simplify the design and applications of VI methods, allowing practical users to focus more on choosing proposals that work best with their specific tasks. We will use the term wild variational inference to refer to new variants of variational methods that require no tractability qη(x), to distinguish with the black-box variational inference (Ranganath et al., 2014) which refers to methods that work for generic target distributions p(x) without significant model-by-model consideration (but still require to calculate the proposal density qη(x)).
A similar problem also appears in importance sampling (IS), where it requires to calculate the IS proposal density q(x) in order to calculate the importance weight w(x) = p(x)/q(x). However, there exist methods that use no explicit information of q(x), which, seemingly counter-intuitively, give better asymptotic variance or converge rates than the typical IS that uses the proposal information (e.g., Liu & Lee, 2016; Briol et al., 2015; Henmi et al., 2007; Delyon & Portier, 2014). Discussions on this phenomenon dates back to O’Hagan (1987), who argued that “Monte Carlo (that uses the proposal information) is fundamentally unsound” for violating the Likelihood Principle, and developed Bayesian Monte Carlo (O’Hagan, 1991) as an example that uses no information on q(x), yet gives better convergence rate than the typical Monte Carlo O(n−1/2) rate (Briol et al., 2015). Despite the substantial difference between IS and VI, these results intuitively suggest the possibility of developing efficient variational inference without calculating q(x) explicitly.
In this work, we propose a simple algorithm for Problem 1 by iteratively adjusting the network parameter η to make its output random variable changes along a Stein variational gradient direction (SVGD) (Liu & Wang, 2016) that optimally decreases its KL divergence with the target distribution. Critically, the SVGD gradient includes a repulsive term to ensure that the generated samples have the right amount of variability that matches p(x). In this way, we “amortize SVGD” using a neural network, which makes it possible for our method to adaptively improve its own efficiency by leveraging fast experience, especially in cases when it needs to perform fast inference repeatedly on a large number of similar tasks. As an application, we use our method to amortize the MLE training of deep energy models, where a neural sampler is adaptively trained to approximate the likelihood function. Our method, which we call SteinGAN, mimics an adversarial game between the energy model and the neural sampler, and obtains realistic-looking images competitive with the state-of-theart results produced by generative adversarial networks (GAN) (Goodfellow et al., 2014; Radford et al., 2015).
Related Work The idea of amortized inference (Gershman & Goodman, 2014) has been recently applied in various domains of probabilistic reasoning, including both amortized variational inference (e.g., Kingma & Welling, 2013; Rezende & Mohamed, 2015a), and data-driven proposals for (sequential) Monte Carlo methods (e.g., Paige & Wood, 2016), to name only a few. Most of these methods, however, require to explicitly calculate q(x) (or its gradient). One exception is a very recent paper (Ranganath et al., 2016) that avoids calculating q(x) using an idea related to Stein discrepancy (Gorham & Mackey, 2015; Liu et al., 2016; Oates et al., 2014; Chwialkowski et al., 2016). There is also a raising interest recently on a similar problem of “learning to optimize” (e.g., Andrychowicz et al., 2016; Daniel et al., 2016; Li & Malik, 2016), which is technically easier than the more general problem of “learning to sample”. In fact, we show that our algorithm reduces to “learning to optimize” when only one particle is used in SVGD.
Generative adversarial network (GAN) and its variants have recently gained remarkable success on generating realistic-looking images (Goodfellow et al., 2014; Salimans et al., 2016; Radford et al., 2015; Li et al., 2015; Dziugaite et al., 2015; Nowozin et al., 2016). All these methods are set up to train latent variable models (the generator) under the assistant of the discriminator. Our SteinGAN instead performs traditional MLE training for a deep energy model, with the help of a neural sampler that learns to draw samples from the energy model to approximate the likelihood
function; this admits an adversarial interpretation: we can view the neural sampler as a generator that attends to fool the deep energy model, which in turn serves as a discriminator that distinguishes the real samples and the simulated samples given by the neural sampler. This idea of training MLE with neural samplers was first discussed by Kim & Bengio (2016); one of the key differences is that the neural sampler in Kim & Bengio (2016) is trained with the help of a heuristic diversity regularizer based on batch normalization, while SVGD enforces the diversity in a more principled way. Another method by Zhao et al. (2016) also trains an energy score to distinguish real and simulated samples, but within a non-probabilistic framework (see Section 5 for more discussion). Other more traditional approaches for training energy-based models (e.g., Ngiam et al., 2011; Xie et al., 2016) are often based on variants of MCMC-MLE or contrastive divergence (Geyer, 1991; Hinton, 2002; Tieleman, 2008), and have difficulty generating realistic-looking images from scratch.

2 STEIN VARIATIONAL GRADIENT DESCENT (SVGD)
Stein variational gradient descent (SVGD) (Liu & Wang, 2016) is a general purpose Bayesian inference algorithm motivated by Stein’s method (Stein, 1972; Barbour & Chen, 2005) and kernelized Stein discrepancy (Liu et al., 2016; Chwialkowski et al., 2016; Oates et al., 2014). It uses an efficient deterministic gradient-based update to iteratively evolve a set of particles {xi}ni=1 to minimize the KL divergence with the target distribution. SVGD has a simple form that reduces to the typical gradient descent for maximizing log p when using only one particle (n = 1), and hence can be easily combined with the successful tricks for gradient optimization, including stochastic gradient, adaptive learning rates (such as adagrad), and momentum.
To give a quick overview of the main idea of SVGD, let p(x) be a positive density function on Rd which we want to approximate with a set of particles {xi}ni=1. SVGD initializes the particles by sampling from some simple distribution q0, and updates the particles iteratively by
xi ← xi + φ(xi), ∀i = 1, . . . , n, (1) where is a step size, and φ(x) is a “particle gradient direction” chosen to maximumly decrease the KL divergence between the distribution of particles and the target distribution, in the sense that
φ = arg max φ∈F { − d d KL(q[ φ] || p) ∣∣ =0 } , (2)
where q[ φ] denotes the density of the updated particle x′ = x + φ(x) when the density of the original particle x is q, and F is the set of perturbation directions that we optimize over. We choose F to be the unit ball of a vector-valued reproducing kernel Hilbert space (RKHS)Hd = H×· · ·×H with eachH associating with a positive definite kernel k(x, x′); note thatH is dense in the space of continuous functions with universal kernels such as the Gaussian RBF kernel.
Critically, the gradient of KL divergence in (2) equals a simple linear functional of φ, allowing us to obtain a closed form solution for the optimal φ. Liu & Wang (2016) showed that
− d d KL(q[ φ] || p) ∣∣ =0 = Ex∼q[Tpφ(x)], (3)
with Tpφ(x) = ∇x log p(x)>φ(x) +∇x · φ(x), (4) where Tp is considered as a linear operator acting on function φ and is called the Stein operator in connection with Stein’s identity which shows that the RHS of (3) equals zero if p = q:
Ep[Tpφ] = Ep[∇x log p>φ +∇x · φ] = 0. (5) This is a result of integration by parts assuming the value of p(x)φ(x) vanishes on the boundary of the integration domain.
Therefore, the optimization in (2) reduces to
D(q || p) def= max φ∈Hd {Ex∼q[Tpφ(x)] s.t. ||φ||Hd ≤ 1}, (6)
where D(q || p) is the kernelized Stein discrepancy defined in Liu et al. (2016), which equals zero if and only if p = q under mild regularity conditions. Importantly, the optimal solution of (6) yields a closed form
φ∗(x′) ∝ Ex∼q[∇x log p(x)k(x, x′) +∇xk(x, x′)].
Algorithm 1 Amortized SVGD for Problem 1 Set batch size m, step-size scheme { t} and kernel k(x, x′). Initialize η0. for iteration t do
Draw random {ξi}mi=1, calculate xi = f(ηt; ξi), and the Stein variational gradient ∆xi in (7). Update parameter η using (8), (9) or (10).
end for
By approximating the expectation under q with the empirical average of the current particles {xi}ni=1, SVGD admits a simple form of update:
xi ← xi + ∆xi, ∀i = 1, . . . , n, where ∆xi = Êx∈{xi}ni=1 [∇x log p(x)k(x, xi) +∇xk(x, xi)], (7)
and Êx∼{xi}ni=1 [f(x)] = ∑ i f(xi)/n. The two terms in ∆xi play two different roles: the term with the gradient ∇x log p(x) drives the particles toward the high probability regions of p(x), while the term with ∇xk(x, xi) serves as a repulsive force to encourage diversity; to see this, consider a stationary kernel k(x, x′) = k(x − x′), then the second term reduces to Êx∇xk(x, xi) = −Êx∇xik(x, xi), which can be treated as the negative gradient for minimizing the average similarity Êxk(x, xi) in terms of xi. Overall, this particle update produces diverse points for distributional approximation and uncertainty assessment, and also has an interesting “momentum” effect in which the particles move collaboratively to escape the local optima.
It is easy to see from (7) that ∆xi reduces to the typical gradient ∇x log p(xi) when there is only a single particle (n = 1) and ∇xk(x, xi) when x = xi, in which case SVGD reduces to the standard gradient ascent for maximizing log p(x) (i.e., maximum a posteriori (MAP)).

3 AMORTIZED SVGD: TOWARDS AN AUTOMATIC NEURAL SAMPLER
SVGD and other particle-based methods become inefficient when we need to repeatedly infer a large number different target distributions for multiple tasks, including online learning or inner loops of other algorithms, because they can not improve based on the experience from the past tasks, and may require a large memory to restore a large number of particles. We propose to “amortize SVGD” by training a neural network f(η; ξ) to mimic the SVGD dynamics, yielding a solution for Problem 1.
One straightforward way to achieve this is to run SVGD to convergence and train f(η; ξ) to fit the SVGD results. This, however, requires to run many epochs of fully converged SVGD and can be slow in practice. We instead propose an incremental approach in which η is iteratively adjusted so that the network outputs x = f(η; ξ) changes along the Stein variational gradient direction in (7) in order to decrease the KL divergence between the target and approximation distribution.
To be specific, denote by ηt the estimated parameter at the t-th iteration of our method; each iteration of our method draws a batch of random inputs {ξi}mi=1 and calculate their corresponding output xi = f(η; ξi) based on ηt; here m is a mini-batch size (e.g., m = 100). The Stein variational gradient ∆xi in (7) would then ensure that x′i = xi + ∆xi forms a better approximation of the target distribution p. Therefore, we should adjust η to make its output matches {x′i}, that is, we want to update η by
ηt+1 ← arg min η m∑ i=1 ||f(η; ξi)− x′i||22, where x′i = xi + ∆xi. (8)
See Algorithm 1 for the summary of this procedure. If we assume is very small, then (8) reduces to a least square optimization. To see this, note that f(η; ξi) ≈ f(ηt; ξi) + ∂ηf(ηt; ξi)(η − ηt) by Taylor expansion. Since xi = f(ηt; ξi), we have
||f(η; ξi)− x′i||22 ≈ ||∂ηf(ηt; ξi)(η − ηt)− ∆xi||22. As a result, (8) reduces to the following least square optimization:
ηt+1 ← ηt + ∆ηt, where ∆ηt = arg min δ m∑ i=1 ||∂ηf(ηt; ξi)δ −∆xi||22. (9)
Update (9) can still be computationally expensive because of the matrix inversion. We can derive a further approximation by performing only one step of gradient descent of (8) (or (9)), which gives
ηt+1 ← ηt + m∑ i=1 ∂ηf(η t; ξi)∆xi. (10)
Although update (10) is derived as an approximation of (8)-(9), it is computationally faster and we find it works very effectively in practice; this is because when is small, one step of gradient update can be sufficiently close to the optimum.
Update (10) also has a simple and intuitive form: (10) can be thought as a “chain rule” that backpropagates the Stein variational gradient to the network parameter η. This can be justified by considering the special case when we use only a single particle (n = 1) in which case ∆xi in (7) reduces to the typical gradient ∇x log p(xi) of log p(x), and update (10) reduces to the typical gradient ascent for maximizing Eξ[log p(f(η; ξ))], in which case f(η; ξ) is trained to maximize log p(x) (that is, learning to optimize), instead of learning to draw samples from p for which it is crucial to use Stein variational gradient ∆xi to diversify the network outputs.
Update (10) also has a close connection with the typical variational inference with the reparameterization trick (Kingma & Welling, 2013). Let qη(x) be the density function of x = f(η; ξ), ξ ∼ q0. Using the reparameterization trick, the gradient of KL(qη || p) w.r.t. η can be shown to be
∇ηKL(qη || p) = −Eξ∼q0 [∂ηf(η; ξ)(∇x log p(x)−∇x log qη(x))].
With {ξi} i.i.d. drawn from q0 and xi = f(η; ξi), ∀i, the standard stochastic gradient descent for minimizing the KL divergence is
ηt+1 ← ηt + ∑ i ∂ηf(η t; ξi)∆̃xi, where ∆̃xi = ∇x log p(xi)−∇x log qη(xi). (11)
This is similar with (10), but replaces the Stein gradient ∆xi defined in (7) with ∆̃xi. The advantage of using ∆xi is that it does not require to explicitly calculate qη , and hence admits a solution to Problem 1 in which qη is not computable for complex network f(η; ξ) and unknown input distribution q0. Further insights can be obtained by noting that
∆xi ≈ Ex∼q[∇x log p(x)k(x, xi) +∇xk(x, xi)] = Ex∼q[(∇x log p(x)−∇x log q(x))k(x, xi)] (12) = Ex∼q[(∆̃x)k(x, xi)],
where (12) is obtained by using Stein’s identity (5). Therefore, ∆xi can be treated as a kernel smoothed version of ∆̃xi.

4 AMORTIZED MLE FOR GENERATIVE ADVERSARIAL TRAINING
Our method allows us to design efficient approximate sampling methods adaptively and automatically, and enables a host of novel applications. In this paper, we apply it in an amortized MLE method for training deep generative models.
Maximum likelihood estimator (MLE) provides a fundamental approach for learning probabilistic models from data, but can be computationally prohibitive on distributions for which drawing samples or computing likelihood is intractable due to the normalization constant. Traditional methods such as MCMC-MLE use hand-designed methods (e.g., MCMC) to approximate the intractable likelihood function but do not work efficiently in practice. We propose to adaptively train a generative neural network to draw samples from the distribution during MLE training, which not only provides computational advantage, and also allows us to generate realistic-looking images competitive with, or better than the state-of-the-art generative adversarial networks (GAN) (Goodfellow et al., 2014; Radford et al., 2015) (see Figure 1-5).
Algorithm 2 Amortized MLE as Generative Adversarial Learning Goal: MLE training for energy model p(x|θ) = exp(−φ(x, θ)− Φ(θ)). Initialize η and θ. for iteration t do
Updating η: Draw ξi ∼ q0, xi = f(η; ξi); update η using (8), (9) or (10) with p(x) = p(x|θ). Repeat several times when needed. Updating θ: Draw a mini-batch of observed data {xi,obs}, and simulated data xi = f(η; ξi), update θ by (13).
end for
To be specific, denote by {xi,obs} a set of observed data. We consider the maximum likelihood training of energy-based models of form
p(x|θ) = exp(−φ(x, θ)− Φ(θ)), Φ(θ) = log ∫ exp(−φ(x, θ))dx,
where φ(x; θ) is an energy function for x indexed by parameter θ and Φ(θ) is the log-normalization constant. The log-likelihood function of θ is
L(θ) = 1
n n∑ i=1 log p(xi,obs|θ),
whose gradient is
∇θL(θ) = −Êobs[∂θφ(x; θ)] + Eθ[∂θφ(x; θ)],
where Êobs[·] and Eθ[·] denote the empirical average on the observed data {xi,obs} and the expectation under model p(x|θ), respectively. The key computational difficulty is to approximate the model expectation Eθ[·]. To address this problem, we use a generative neural network x = f(η; ξ) trained by Algorithm 1 to approximately sample from p(x|θ), yielding a gradient update for θ of form
θ ← θ + ∇̂θL(θ), ∇̂θL(θ) = −Êobs[∂θφ(x; θ)] + Êη[∂θφ(x; θ)], (13)
where Êη denotes the empirical average on {xi} where xi = f(η; ξi), {ξi} ∼ q0. As θ is updated by gradient ascent, η is successively updated via Algorithm 1 to follow p(x|θ). See Algorithm 2. We call our method SteinGAN, because it can be intuitively interpreted as an adversarial game between the generative network f(η; ξ) and the energy model p(x|θ) which serves as a discriminator: The MLE gradient update of p(x|θ) effectively decreases the energy of the training data and increases the energy of the simulated data from f(η; ξ), while the SVGD update of f(η; ξ) decreases the energy of the simulated data to fit better with p(x|θ). Compared with the traditional methods based on MCMC-MLE or contrastive divergence, we amortize the sampler as we train, which gives much faster speed and simultaneously provides a high quality generative neural network that can generate realistic-looking images; see Kim & Bengio (2016) for a similar idea and discussions.

5 EMPIRICAL RESULTS
We evaluated our SteinGAN on four datasets, MNIST, CIFAR-10, CelebA (Liu et al., 2015), and Large-scale Scene Understanding (LSUN) (Yu et al., 2015), on which we find our method tends to generate realistic-looking images competitive with, sometimes better than DCGAN (Radford et al., 2015) (see Figure 2 - Figure 3). Our code is available at https://github.com/DartML/ SteinGAN.
Model Setup In order to generate realistic-looking images, we define our energy model based on an autoencoder:
p(x|θ) ∝ exp(−||x−D(E(x; θ); θ)||), (14)
where x denotes the image. This choice is motivated by Energy-based GAN (Zhao et al., 2016) in which the autoencoder loss is used as a discriminator but without a probabilistic interpretation. We
assume f(η; ξ) to be a neural network whose input ξ is a 100-dimensional random vector drawn by Uniform([−1, 1]). The positive definite kernel in SVGD is defined by the RBF kernel on the hidden representation obtained by the autoencoder in (14), that is,
k(x, x′) = exp(− 1 h2 ||E(x; θ)− E(x′; θ)||2).
As it is discussed in Section 3, the kernel provides a repulsive force to produce an amount of variability required for generating samples from p(x). This is similar to the heuristic repelling regularizer in Zhao et al. (2016) and the batch normalization based regularizer in Kim & Bengio (2016), but is derived in a more principled way. We take the bandwidth to be h = 0.5 ×med, where med is the median of the pairwise distances between E(x) on the image simulated by f(η; ξ). This makes the kernel change adaptively based on both θ (through E(x; θ)) and η (through bandwidth h).
Some datasets include both images x and their associated discrete labels y. In these cases, we train a joint energy model on (x, y) to capture both the inner structure of the images and its predictive relation with the label, allowing us to simulate images with a control on which category it belongs to. Our joint energy model is defined to be
p(x, y|θ) ∝ exp { − ||x−D(E(x; θ); θ)|| −max[m, σ(y, E(x; θ))] } , (15)
where σ(·, ·) is the cross entropy loss function of a fully connected output layer. In this case, our neural sampler first draws a label y randomly according to the empirical counts in the dataset, and then passes y into a neural network together with a 100 × 1 random vector ξ to generate image x. This allows us to generate images for particular categories by controlling the value of input y.
Stabilization In practice, we find it is useful to modify (13) to be θ ← θ − Êobs[∇θφ(x, θ)] + (1− γ)Êη[∇θφ(x, θ)]. (16)
where γ is a discount factor (which we take to be γ = 0.7). This is equivalent to maximizing a regularized likelihood:
max θ {log p(x|θ) + γΦ(θ)}
where Φ(θ) is the log-partition function; note that exp(γΦ(θ)) is a conjugate prior of p(x|θ). We initialize the weights of both the generator and discriminator from Gaussian distribution N (0, 0.02), and train them using Adam (Kingma & Ba, 2014) with a learning rate of 0.001 for the generator and 0.0001 for the energy model (the discriminator). In order to keep the generator and discriminator approximately aligned during training, we speed up the MLE update (16) of the discriminator (by increasing its learning rate to 0.0005) when the energy of the real data batch is larger than the energy of the simulated images, while slow down it (by freezing the MLE update of θ in (16)) if the magnitude of the energy difference between the real images and the simulated images goes above a threshold of 0.5. We used the bag of architecture guidelines for stable training suggested in DCGAN (Radford et al., 2015).
Discussion The MNIST dataset has a training set of 60, 000 examples. Both DCGAN and our model produce high quality images, both visually indistinguishable from real images; see figure 1.
CIFAR-10 is very diverse, and with only 50,000 training examples. Figure 2 shows examples of simulated images by DCGAN and SteinGAN generated conditional on each category, which look equally well visually. We also provide quantitively evaluation using a recently proposed inception score (Salimans et al., 2016), as well as the classification accuracy when training ResNet using 50, 000 simulated images as train sets, evaluated on a separate held-out testing set never seen by the GAN models. Besides DCGAN and SteinGAN, we also evaluate another simple baseline obtained by subsampling 500 real images from the training set and duplicating them 100 times. We observe that these scores capture rather different perspectives of image generation: The inception score favors images that look realistic individually and have uniformly distributed labels; as a result, the inception score of the duplicated 500 images is almost as high as the real training set. We find that the inception score of SteinGAN is comparable, or slightly lower than that of DCGAN. On the other hand, the classification accuracy measures the amount information captured in the simulated image sets; we find that SteinGAN achieves the highest classification accuracy, suggesting that it captures more information in the training set.
Figure 3 and 4 visualize the results on CelebA (with more than 200k face images) and LSUN (with nearly 3M bedroom images), respectively. We cropped and resized both dataset images into 64×64.

6 CONCLUSION
We propose a new method to train neural samplers for given distributions, together with a new SteinGAN method for generative adversarial training. Future directions involve more applications and theoretical understandings for training neural samplers.
","We propose a simple algorithm to train stochastic neural networks to draw samples from given target distributions for probabilistic inference. Our method is based on iteratively adjusting the neural network parameters so that the output changes along a Stein variational gradient (Liu & Wang, 2016) that maximumly decreases the KL divergence with the target distribution. Our method works for any target distribution specified by their unnormalized density function, and can train any black-box architectures that are differentiable in terms of the parameters we want to adapt. As an application of our method, we propose an amortized MLE algorithm for training deep energy model, where a neural sampler is adaptively trained to approximate the likelihood function. Our method mimics an adversarial game between the deep energy model and the neural sampler, and obtains realistic-looking images competitive with the state-of-the-art results.",ICLR 2017 conference submission,False,,"This paper considers the energy-based model interpretation of GAN, where the discriminator is an unnormalized model for the likelihood of a generative model p(x|theta) and the generator is a directed model that approximates this distribution. The generator is used to draw approximate negative phase samples that are used in stochastic maximum likelihood / contrastive divergence learning of the EBM / discriminator.

The main idea in the paper is to fit the generator by following the Stein variational gradient. In practice this gradient consists of the usual gradient provided by the discriminator with an added term that provides a repulsive force between the sampled data points to increase sample diversity.

The idea of using a kernel to push apart the sampled points is interesting, and will work in low dimensions, but it is hard to see how it can work in full scale images. For high dimensional samples x, the proposed kernel is unlikely to provide a useful distance measure between points. There are no convincing experiments in the paper that show otherwise. Specifically:

- There is no experiment that compares between standard GAN and GAN + repulsion, using the same architecture. (please address this in the rebuttal)
- If the Stein variational idea is taken literally, the right thing to do would be to fully optimize the generator at every step, and then taking a single optimization step on the discriminator. Instead, each is updated in turn, and the learning rates of both steps are adjusted to keep the two ""in line"".
- The kernel used to fit the generator is defined in the auto-encoder space of the discriminator, and thus depends on the discriminator parameters. The objective that is used to fit the generator thus changes at every step, and the procedure can no longer be interpreted as stochastic gradient descent with respect to any single well defined objective.

The authors obtain good results: The generated images clearly look better than those generated by DCGAN. However, their approach has a number of changes compared to DCGAN, so it is not clear where the improvement comes from. In addition, by now the DCGAN is no longer a very strong baseline, as various other techniques have been proposed.

Note: The use of phi for both the ""particle gradient direction"" and energy function is confusing

---

This paper presents an idea with a sensible core (augmenting amortized inference with per-instance optimization) but with an overcomplicated and ad-hoc execution. The reviewers provided clear guidance for how this paper could be improved, and thus I invite the authors to submit this paper to the workshop track.

---

We highly appreciate the time and feedback from all the reviewers, all of which we will take into serious consideration in our revision. We will particularly strengthen and clarify the empirical experiments. Below we address some of the major points: 

[Testing Accuracy Score]
We agree with the reviewers' point on the ""testing accuracy"" score, but think that it still provides some valuable insight about the dataset. Its blindness to the background can be a good thing in that it captures more information about the ""effective amount"" of objects the dataset contains.  The problem is that it is very difficult to obtain a *perfect* score, and reporting more than one metrics (in an objective fashion) can help to gain more comprehensive understandings. 

[Repulsive Term in High Dimension]
Our repulsive force works due to two tricks: 1) scaling the bandwidth with the data diversity using the median trick, which alleviates the exponential decay of RBF kernel. 2) define kernel on the feature space instead of the raw pixels of the images, which allows us to respect the manifold structure of the images. The framework of SVGD allows us to use any positive definite kernels and change it adaptively during iterations, because the kernel only defines the ""tangent space"" for improvement. 

SteinGAN without kernel corresponds to Viterbi training of the energy model and we find it work well with careful tuning of parameters, but tend to converge to a small number of bad-looking images after running a large number of iterations; adding the kernel under the same setting helps prevent this problem. Our current results on CIFAR10 shows that SteinGAN without kernel gives an inception score of 6.34, while that SteinGAN with kernel gives 6.76. 

[Amortized is slower than non-amortized]
Although the amortized algorithm has the overhead of updating $\xi$, it stores the information in a generative network, and allows us to simulate as many images as we need. By using the one-step gradient update we proposed, the update of $\xi$ is the same as standard backpropagation except replacing the Dlogp with the SVGD gradient.

---

This paper proposes an amortized version of the Stein variational gradient descent (SVGD) method in which ""a neural network is trained to mimic the SVGD dynamics"". It applies the method to generative adversarial training to yield a training procedure where the discriminator is interpreted as an energy-based probabilistic model.

One criticism I have of the presentation is that a lot of time and energy is spent setting the table for a method which is claimed to be widely applicable, and the scope of the empirical evaluation is narrowed down to a single specific setting. In my view, either the paper falls short of its goal of showing how widely applicable the proposed method is, or it spends too much time setting the table for SteinGAN and not enough time evaluating it.

The consequence of this is that the empirical results are insufficient in justifying the approach proposed by the paper. As another reviewer pointed out, DCGAN is becoming outdated as a benchmark for comparison.

Qualitatively, SteinGAN samples don't look significantly better than DCGAN samples, except for the CelebA dataset. In that particular case, the DCGAN samples don't appear to be the ones presented in the original paper; where do they come from?

Quantitatively, DCGAN beats SteinGAN by a small margin for the ImageNet Inception Score and SteinGAN beats DCGAN by an even smaller margin for the CIFAR10 Inception Score. Also, in my opinion, the ""testing accuracy"" score is not a convincing evaluation metric: while it is true that it measures the amount of information captured in the simulated image sets, it is only sensitive to information useful for the discrimination task, not for the more general modeling task. For instance, this score is likely completely blind to information present in the background of the image.

Because of the reasons outlined above, I don't think the paper is ready for publication at ICLR.

---

The authors propose amortized SVGD, an amortized form of prior work on SVGD, which is a particle variational method that maximally decreases the KL divergence at each update. ""amortized SVGD"" is done by training a neural network to learn this dynamic. They then apply this idea to train energy-based models, which admit a tractable unnormalized density.

In SVGD, the main difference from just MAP is the addition of a ""repulsive force"" that prevents degeneracy by encouraging probability mass to be spread to locations outside the mode. How this is able to still act as a strong enough entropy-like term in high dimensions is curious. From my understanding of their previous work, this was not a problem as the only experiments were on toy and UCI data sets.

In the experimental results here, they apply the kernel on the hidden representation of an autoencoder, which seems key, similar to Li et al. (2015) where their kernel approach for MMD would not work as well otherwise. However, unlike Li et al. (2015) the autoencoder is part of the model itself and not fixed. This breaks much of the authors' proposed motivation and criticisms of prior work, if they must autoencode onto some low-dimensional space (putting most effort then on the autoencoder, which changes per iteration) before then applying their method.

Unlike previous literature which uses inference networks, their amortized SVGD approach seems in fact slower than the non-amortized approach. This is because they must make the actual update on xi before then regressing to perform the update on eta (in previous approaches, this would be like having to perform local inferences before then updating inference network parameters, or at least partially performing the local inference). This seems quite costly during training.

I recommend the paper be rejected, and that the authors provide more comprehensive experimental results, expecially around the influence of the autoencoder, the incremental updates versus full updates, and the training time of amortized vs non-amortized approaches. The current results are promising but unclear why given the many knobs that the authors are playing with.

References

Li, Y., Swersky, K., & Zemel, R. (2015). Generative Moment Matching Networks. Presented at the International Conference on Machine Learning.

---

This paper considers the energy-based model interpretation of GAN, where the discriminator is an unnormalized model for the likelihood of a generative model p(x|theta) and the generator is a directed model that approximates this distribution. The generator is used to draw approximate negative phase samples that are used in stochastic maximum likelihood / contrastive divergence learning of the EBM / discriminator.

The main idea in the paper is to fit the generator by following the Stein variational gradient. In practice this gradient consists of the usual gradient provided by the discriminator with an added term that provides a repulsive force between the sampled data points to increase sample diversity.

The idea of using a kernel to push apart the sampled points is interesting, and will work in low dimensions, but it is hard to see how it can work in full scale images. For high dimensional samples x, the proposed kernel is unlikely to provide a useful distance measure between points. There are no convincing experiments in the paper that show otherwise. Specifically:

- There is no experiment that compares between standard GAN and GAN + repulsion, using the same architecture. (please address this in the rebuttal)
- If the Stein variational idea is taken literally, the right thing to do would be to fully optimize the generator at every step, and then taking a single optimization step on the discriminator. Instead, each is updated in turn, and the learning rates of both steps are adjusted to keep the two ""in line"".
- The kernel used to fit the generator is defined in the auto-encoder space of the discriminator, and thus depends on the discriminator parameters. The objective that is used to fit the generator thus changes at every step, and the procedure can no longer be interpreted as stochastic gradient descent with respect to any single well defined objective.

The authors obtain good results: The generated images clearly look better than those generated by DCGAN. However, their approach has a number of changes compared to DCGAN, so it is not clear where the improvement comes from. In addition, by now the DCGAN is no longer a very strong baseline, as various other techniques have been proposed.

Note: The use of phi for both the ""particle gradient direction"" and energy function is confusing

---

This paper considers the energy-based model interpretation of GAN, where the discriminator is an unnormalized model for the likelihood of a generative model p(x|theta) and the generator is a directed model that approximates this distribution. The generator is used to draw approximate negative phase samples that are used in stochastic maximum likelihood / contrastive divergence learning of the EBM / discriminator.

The main idea in the paper is to fit the generator by following the Stein variational gradient. In practice this gradient consists of the usual gradient provided by the discriminator with an added term that provides a repulsive force between the sampled data points to increase sample diversity.

The idea of using a kernel to push apart the sampled points is interesting, and will work in low dimensions, but it is hard to see how it can work in full scale images. For high dimensional samples x, the proposed kernel is unlikely to provide a useful distance measure between points. There are no convincing experiments in the paper that show otherwise. Specifically:

- There is no experiment that compares between standard GAN and GAN + repulsion, using the same architecture. (please address this in the rebuttal)
- If the Stein variational idea is taken literally, the right thing to do would be to fully optimize the generator at every step, and then taking a single optimization step on the discriminator. Instead, each is updated in turn, and the learning rates of both steps are adjusted to keep the two ""in line"".
- The kernel used to fit the generator is defined in the auto-encoder space of the discriminator, and thus depends on the discriminator parameters. The objective that is used to fit the generator thus changes at every step, and the procedure can no longer be interpreted as stochastic gradient descent with respect to any single well defined objective.

The authors obtain good results: The generated images clearly look better than those generated by DCGAN. However, their approach has a number of changes compared to DCGAN, so it is not clear where the improvement comes from. In addition, by now the DCGAN is no longer a very strong baseline, as various other techniques have been proposed.

Note: The use of phi for both the ""particle gradient direction"" and energy function is confusing

---

This paper presents an idea with a sensible core (augmenting amortized inference with per-instance optimization) but with an overcomplicated and ad-hoc execution. The reviewers provided clear guidance for how this paper could be improved, and thus I invite the authors to submit this paper to the workshop track.

---

We highly appreciate the time and feedback from all the reviewers, all of which we will take into serious consideration in our revision. We will particularly strengthen and clarify the empirical experiments. Below we address some of the major points: 

[Testing Accuracy Score]
We agree with the reviewers' point on the ""testing accuracy"" score, but think that it still provides some valuable insight about the dataset. Its blindness to the background can be a good thing in that it captures more information about the ""effective amount"" of objects the dataset contains.  The problem is that it is very difficult to obtain a *perfect* score, and reporting more than one metrics (in an objective fashion) can help to gain more comprehensive understandings. 

[Repulsive Term in High Dimension]
Our repulsive force works due to two tricks: 1) scaling the bandwidth with the data diversity using the median trick, which alleviates the exponential decay of RBF kernel. 2) define kernel on the feature space instead of the raw pixels of the images, which allows us to respect the manifold structure of the images. The framework of SVGD allows us to use any positive definite kernels and change it adaptively during iterations, because the kernel only defines the ""tangent space"" for improvement. 

SteinGAN without kernel corresponds to Viterbi training of the energy model and we find it work well with careful tuning of parameters, but tend to converge to a small number of bad-looking images after running a large number of iterations; adding the kernel under the same setting helps prevent this problem. Our current results on CIFAR10 shows that SteinGAN without kernel gives an inception score of 6.34, while that SteinGAN with kernel gives 6.76. 

[Amortized is slower than non-amortized]
Although the amortized algorithm has the overhead of updating $\xi$, it stores the information in a generative network, and allows us to simulate as many images as we need. By using the one-step gradient update we proposed, the update of $\xi$ is the same as standard backpropagation except replacing the Dlogp with the SVGD gradient.

---

This paper proposes an amortized version of the Stein variational gradient descent (SVGD) method in which ""a neural network is trained to mimic the SVGD dynamics"". It applies the method to generative adversarial training to yield a training procedure where the discriminator is interpreted as an energy-based probabilistic model.

One criticism I have of the presentation is that a lot of time and energy is spent setting the table for a method which is claimed to be widely applicable, and the scope of the empirical evaluation is narrowed down to a single specific setting. In my view, either the paper falls short of its goal of showing how widely applicable the proposed method is, or it spends too much time setting the table for SteinGAN and not enough time evaluating it.

The consequence of this is that the empirical results are insufficient in justifying the approach proposed by the paper. As another reviewer pointed out, DCGAN is becoming outdated as a benchmark for comparison.

Qualitatively, SteinGAN samples don't look significantly better than DCGAN samples, except for the CelebA dataset. In that particular case, the DCGAN samples don't appear to be the ones presented in the original paper; where do they come from?

Quantitatively, DCGAN beats SteinGAN by a small margin for the ImageNet Inception Score and SteinGAN beats DCGAN by an even smaller margin for the CIFAR10 Inception Score. Also, in my opinion, the ""testing accuracy"" score is not a convincing evaluation metric: while it is true that it measures the amount of information captured in the simulated image sets, it is only sensitive to information useful for the discrimination task, not for the more general modeling task. For instance, this score is likely completely blind to information present in the background of the image.

Because of the reasons outlined above, I don't think the paper is ready for publication at ICLR.

---

The authors propose amortized SVGD, an amortized form of prior work on SVGD, which is a particle variational method that maximally decreases the KL divergence at each update. ""amortized SVGD"" is done by training a neural network to learn this dynamic. They then apply this idea to train energy-based models, which admit a tractable unnormalized density.

In SVGD, the main difference from just MAP is the addition of a ""repulsive force"" that prevents degeneracy by encouraging probability mass to be spread to locations outside the mode. How this is able to still act as a strong enough entropy-like term in high dimensions is curious. From my understanding of their previous work, this was not a problem as the only experiments were on toy and UCI data sets.

In the experimental results here, they apply the kernel on the hidden representation of an autoencoder, which seems key, similar to Li et al. (2015) where their kernel approach for MMD would not work as well otherwise. However, unlike Li et al. (2015) the autoencoder is part of the model itself and not fixed. This breaks much of the authors' proposed motivation and criticisms of prior work, if they must autoencode onto some low-dimensional space (putting most effort then on the autoencoder, which changes per iteration) before then applying their method.

Unlike previous literature which uses inference networks, their amortized SVGD approach seems in fact slower than the non-amortized approach. This is because they must make the actual update on xi before then regressing to perform the update on eta (in previous approaches, this would be like having to perform local inferences before then updating inference network parameters, or at least partially performing the local inference). This seems quite costly during training.

I recommend the paper be rejected, and that the authors provide more comprehensive experimental results, expecially around the influence of the autoencoder, the incremental updates versus full updates, and the training time of amortized vs non-amortized approaches. The current results are promising but unclear why given the many knobs that the authors are playing with.

References

Li, Y., Swersky, K., & Zemel, R. (2015). Generative Moment Matching Networks. Presented at the International Conference on Machine Learning.

---

This paper considers the energy-based model interpretation of GAN, where the discriminator is an unnormalized model for the likelihood of a generative model p(x|theta) and the generator is a directed model that approximates this distribution. The generator is used to draw approximate negative phase samples that are used in stochastic maximum likelihood / contrastive divergence learning of the EBM / discriminator.

The main idea in the paper is to fit the generator by following the Stein variational gradient. In practice this gradient consists of the usual gradient provided by the discriminator with an added term that provides a repulsive force between the sampled data points to increase sample diversity.

The idea of using a kernel to push apart the sampled points is interesting, and will work in low dimensions, but it is hard to see how it can work in full scale images. For high dimensional samples x, the proposed kernel is unlikely to provide a useful distance measure between points. There are no convincing experiments in the paper that show otherwise. Specifically:

- There is no experiment that compares between standard GAN and GAN + repulsion, using the same architecture. (please address this in the rebuttal)
- If the Stein variational idea is taken literally, the right thing to do would be to fully optimize the generator at every step, and then taking a single optimization step on the discriminator. Instead, each is updated in turn, and the learning rates of both steps are adjusted to keep the two ""in line"".
- The kernel used to fit the generator is defined in the auto-encoder space of the discriminator, and thus depends on the discriminator parameters. The objective that is used to fit the generator thus changes at every step, and the procedure can no longer be interpreted as stochastic gradient descent with respect to any single well defined objective.

The authors obtain good results: The generated images clearly look better than those generated by DCGAN. However, their approach has a number of changes compared to DCGAN, so it is not clear where the improvement comes from. In addition, by now the DCGAN is no longer a very strong baseline, as various other techniques have been proposed.

Note: The use of phi for both the ""particle gradient direction"" and energy function is confusing",,,,,,4.0,,,3.3333333333333335,,
554,"Authors: DEEP Q-NETWORKS, Jean Harb, Doina Precup
Source file: 554.pdf

ABSTRACT
Eligibility traces in reinforcement learning are used as a bias-variance trade-off and can often speed up training time by propagating knowledge back over timesteps in a single update. We investigate the use of eligibility traces in combination with recurrent networks in the Atari domain. We illustrate the benefits of both recurrent nets and eligibility traces in some Atari games, and highlight also the importance of the optimization used in the training.

Eligibility traces in reinforcement learning are used as a bias-variance trade-off and can often speed up training time by propagating knowledge back over timesteps in a single update. We investigate the use of eligibility traces in combination with recurrent networks in the Atari domain. We illustrate the benefits of both recurrent nets and eligibility traces in some Atari games, and highlight also the importance of the optimization used in the training.

1 INTRODUCTION
Deep reinforcement learning has had many practical successes in game playing (Mnih et al. (2015),Silver et al. (2016)) and robotics (Levine & Abbeel (2014)). Our interest is in further exploring these algorithms in the context of environments with sparse rewards and partial observability. To this end, we investigate the use of two methods that are known to mitigate these problems: recurrent networks, which provide a form of memory summarizing past experiences, and eligibility traces, which allow information to propagate over multiple time steps. Eligibility traces have been shown empirically to provide faster learning (Sutton & Barto (2017), in preparation) but their use with deep RL has been limited so far (van Seijen & Sutton (2014), Hausknecht & Stone (2015)). We provide experiments in the Atari domain showing that eligibility traces boost the performance of Deep RL. We also demonstrate a surprisingly strong effect of the optimization method on the performance of the recurrent networks.
The paper is structured as follows. In Sec. 2 we provide background and notation needed for the paper. Sec. 3 describes the algorithms we use. In sec. 4 we present and discuss our experimental results. In Sec. 5 we conclude and present avenues for future work.

2 BACKGROUND
A Markov Decision Process (MDP) consists of a tuple 〈S,A, r,P, γ〉, where S is the set of states, A is the set of actions, r : S × A 7→ R is the reward function, P(s′|s, a) is the transition function (giving the next state distribution, conditioned on the current state and action), and γ ∈ [0, 1) is the discount factor. Reinforcement learning (RL) (Sutton & Barto, 1998) is a framework for solving unknown MDPs, which means finding a good (or optimal) way of behaving, also called a policy. RL works by obtaining transitions from the environment and using them, in order to compute a policy that maximizes the expected return, given by E [∑∞ t=0 γ trt ] .
The state-value function for a policy π : S × A → [0, 1], V π(s), is defined as the expected return obtained by starting at state s and picking actions according to π. State-action values Q(s, a) are similar to state values, but conditioned also on the initial action a. A policy can be derived from the Q values by picking always the action with the best estimated value at any state.
Monte Carlo (MC) and Temporal Difference (TD) are two standard methods for updating the value function from data. In MC, an entire trajectory’s return is used as the target value of the current
state.
MC error = ∞∑ i=0 γirt+i − V (st) (1)
In TD, the estimate of the next state’s value is used to correct the current state’s estimate:
TD error = rt + γV (st+1)− V (st) (2)
Q-learning is an RL algorithm that allows an agent to learn by imagining that it will take the best possible action in the following step:
TD error = rt + γmax a′
Q(st+1, a ′)−Q(st, at) (3)
This is an instance of off-policy learning, in which the agent gathers data with an exploratory policy, which randomizes the choice of action, but updates its estimates by constructing targets according to a differnet policy (in this case, the policy that is greedy with respect to the current value estimates.

2.1 ELIGIBILITY TRACES
Eligibility traces are a fundamental reinforcement learning mechanism which allows a trade-off between TD and MC. MC methods suffer from high variance, as many trajectories can be taken from any given state and stochasticity is often present in the MDP. TD suffers from high bias, as it updates values based on its own estimates.
Using eligibility traces allows one to design algorithms that cover the middle-ground between MC and TD. The central notion for these are n-step returns, which provide a way of calculating the target by using the value estimate for the state which occurs n steps in the future (compared to the current state):
R (n) t = n−1∑ i=0 γirt+i + γ nV (st+n). (4)
When n is 1, the results is the TD target, and taking n→∞ yields the MC target. Eligibility traces use a geometric weighting of these n-step returns, where the weight of the k-step return is λ times the weight of the k − 1-step return. Using a λ = 0 reduces to using TD, as all n-steps for n > 1 have a weight of 0. One of the appealing effects of using eligibility traces is that a single update allows states many steps behind a reward signal to receive credit. This propagates knowledge back at a faster rate, allowing for accelerated learning. Especially in environments where rewards are sparse and/or delayed, eligibility traces can help assign credit to past states and actions. Without traces, seeing a sparse reward will only propagate the value back by one step, which in turn needs to be sampled to send the value back a second step, and so on.
Rλt = (1− λ) ∞∑ i=0 λiR (i) t = (1− λ) ∞∑ i=1 λi−1 i−1∑ j=0 γjrj + γ i+1V (st+i) (5)
This way of viewing eligibility traces is called the forward view, as states are looking ahead at the rewards received in the future. The forward view is rarely used, as it requires a state to wait for the future to unfold before calculating an update, and requires memory to store the experience. There is an equivalent view called the backward view, which allows us to calculate updates for every previous state as we take a single action. This requires no memory and lets us perform updates without having to wait for the future. However, this view has had limited success in the neural network setting as it requires using a trace on each neuron of the network, which tend to be dense and heavily used at each step resulting in noisy signals. For this reason, eligibility traces aren’t heavily used when using deep learning, despite their potential benefits.
2.1.1 Q(λ)
Q(λ) is a variant of Q-learning where eligibility traces are used to calculate the TD error. As mentioned previously, the backwards view of traces is traditionally used.
A few versions of Q(λ) exist, but the most used one is Watkins’s Q(λ). As Q-learning is off-policy, the sequence of actions used in the past trajectory used to calculate the trace might be different from the actions that the current policy might take. In that case, one should not be using the trajectory past the point where actions differ. To handle such a case, Watkins’s Q(λ) sets the trace to 0 if the action that the current policy would select is different from the one used in the past.

2.2 DEEP Q-NETWORKS
Mnih et al. (2015) introduced deep Q-networks (DQN), one of the first successful reinforcement learning algorithms that use deep learning for function approximation in a way general enough which is applicable to a variety of environments. Applying it to a set of Atari games, they used a convolutional neural network (CNN) which took as input the last four frames of the game, and output Q-values for each possible action.
Equation 6 shows the DQN cost function, where we are optimizing the θ parameters. The θ− parameters represent frozen Q-value weights which are update at a chosen frequency.
L(st, at|θ) = (rt + γmax a′ Q(st+1, a ′|θ−)−Q(st, at|θ))2 (6)

2.2.1 DEEP RECURRENT Q-NETWORKS
As introduced in Hausknecht & Stone (2015), deep recurrent Q-networks (DRQN) are a modification on DQN, where single frames are passed through a CNN, which generates a feature vector that is then fed to an RNN which finally outputs Q-values. This architecture gives the agent a memory, allowing it to learn long-term temporal effects and handle partial observability, which is the case in many environments. The authors showed that randomly blanking out frames was difficult to overcome for DQN, but that DRQN learned to handle without issue.
To train DRQN, they proposed two variants of experience replay. The first was to sample entire trajectories and run the RNN from end to end. However this is very computationally demanding as some trajectories can be over 10000 steps long. The second alternative was to sample subtrajectories instead of single transitions. This is required as the RNN needs to fill its hidden state and to allow it to understand the temporal aspect of the data.

2.3 OPTIMIZERS
Stochastic gradient descent (SGD) is generally the algorithm used to optimize neural networks. However, some information is lost during the process as past gradients might signal that a weight drastically needs to change, or that it is oscillating, requiring a decrease in learning rate. Adaptive SGD algorithms have been built to use this information.
RMSprop (Tieleman & Hinton (2012)), uses a geometric averaging over gradients squared, and divides the current gradient by its square root. To perform RMSprop, first we calculate the averaging as g = βg + (1− β)∇θ2 and then update the parameters θ ← θ + α ∇θ√
g+ .
DQN (Graves (2013)) introduced a variant of RMSprop where the gradient is instead divided by the standard deviation of the running average. First we calculate the running averages m = βm+ (1− β)∇θ and g = βg + (1 − β)∇θ2, and then update the parameters using θ ← θ + α ∇θ√
g−m2+ . In
the rest of the paper, when mentioning RMSprop, we’ll be referring to this version.
Finally, Kingma & Ba (2014) introduced Adam, which is essentially RMSprop coupled with Nesterov momentum, along with the running averages being corrected for bias. We have a term for the rate of momentum of each of the running averages. To calculate the update with Adam, we start with the updating the averages m = β1m+ (1− β1)∇θ, v = β2v + (1− β2)∇θ2, the correct their biases m̂ = m/(1−βt1), v̂ = v/(1−βt2) and finally calculate the gradient update θ ← θ+α m̂√v̂+ .

3 EXPERIMENTAL SETUP
As explained, the forward view of eligibility traces can be useful, but is computationally demanding in terms of memory and time. One must store all transitions and apply the neural network to each state in the trajectory. By using DRQN, experience replay is already part of the algorithm, which removes the memory requirement of the traces. Then, by training on sub-trajectories of data, the states must be run through the RNN with all state values as the output, which eliminates the computational cost. Finally, all that’s left to use eligibility traces is simply to calculate the weighted sum of the targets, which is very cheap to do.
In this section we analyze the use of eligibility traces when training DRQN and try both RMSprop and Adam as optimizers. We only tested the algorithms on fully observable games as to compare the learning capacities without the unfair advantage of having a memory, as would be the case on partially observable environments.

3.1 ARCHITECTURE
We tested the algorithms on two Atari 2600 games, part of the Arcade Learning Environment (Bellemare et al. (2012)), Pong and Tennis. The architecture used is similar to the one used in Hausknecht & Stone (2015). The frames are converted to gray-scale and re-sized to 84x84. These are then fed to a CNN with the first layer being 32 8x8 filters and a stride of 4, followed by 64 4x4 filters with a stride of 2, then by 64 3x3 filters with a stride of 1. The output of the CNN is then flattened before being fed to a single dense layer of 512 output neurons, which is finally fed to an LSTM (Hochreiter & Schmidhuber (1997)) with 512 cells. We then have a last linear layer that takes the output of the recurrent layer to output the Q-values. All layers before the LSTM are activated using rectified linear units (ReLU).
As mentioned in subsection 2.2.1, we also altered experience replay to sample sub-trajectories. We use backprop through time (BPTT) to train the RNN, but only train on a sub-trajectory of experience. In runtime, the RNN will have had a large sequence of inputs in its hidden state, which can be problematic if always trained with an empty hidden state. Like in Lample & Singh Chaplot (2016), we therefore sample a slightly longer length of trajectory and use the first m states to fill the hidden state. In our experiments, we selected trajectory lengths of 32, where the first 10 states are used as filler and the remaining 22 are used for the traces and TD costs. We used a batch size of 4.
All experiments using eligibility traces use λ = 0.8. Furthermore, we use Watkins’s Q(λ). To limit computation costs of using traces, we cut the trace off once it becomes too small. In our experiments, we choose the limit of 0.01, which allows the traces to affect 21 states ahead (when λ = 0.8). We
calculate the trace for every state in the trajectory, except for a few in the beginning, use to fill in the hidden state of the RNN.
When using RMSprop, we used a momentum of 0.95, an epsilon of 0.01 and a learning rate of 0.00025. When using Adam, we used a momentum of gradients of 0.9, a momentum of squared gradients of 0.999, an epsilon of 0.001 and a learning rate of 0.00025.
Testing phases are consistent across all models, with the score being the average over each game played during 125000 frames. We also use an of 0.05 for action selection.
Choose k as number of trace steps and m as RNN-filler steps Initialize weights θ, experience replay D θ− ← θ s← s0 repeat
Initialize RNN hidden state to 0. repeat
Choose a according to −greedy policy on Q(s, a|θ) Take action a in s, observe s′, r Store s, a, r, s′ in Experience Replay Sample 4 sub-trajectories of m+ k sequential transitions (s, a, r, s′) from D
ŷ = { r s’ is terminal, r + γmax
ā Q(s′, ā|θ−) otherwise
foreach transition sampled do
λt = { λ at = arg maxā(st, ā|θ), 0 otherwise
end for l from 0 to k − 1 do
R̂λt+l = [∑k s=l (∏s i=l λt+i ) R (s−l+1) t+s ] / [∑k s=l (∏s i=l λt+i )] end Perform gradient descent on∂(R̂
λ−Q(s,a|θ))2 ∂θ
Every 10000 steps θ− ← θ s← s′
until s′ is terminal until training complete Algorithm 1: Deep Recurrent Q-Networks with forward view eligibility traces on Atari. The eligibility traces are calculated using the n-step return function R(n)t for time-step t was described in section 2.1.

4 EXPERIMENTAL RESULTS
We describe experiments in two Atari games: Pong and Tennis. We chose Pong because it permits quick experimentation, and Tennis because it is one of the games that has proven difficult in all published results on Atari.

4.1 PONG
First, we tested an RNN model both with λ = 0 and λ = 0.8, trained with RMSprop. Figure 2 shows that the model without a trace (λ = 0) learned at the same rate as DQN, while the model with traces (λ = 0.8) learned substantially faster and with more stability, without exhibiting any epochs with depressed performance. This is probably due to the eligibility traces propagating rewards back by many steps in a single update. In Pong, when the agent hits the ball, it must wait several time-steps before the ball gets either to or past the opponent. Once this happens, the agent must assign the credit of the event back to the time when it hit the ball, and not to the actions performed after the ball had already left. The traces clearly help send this signal back faster.
We then tested the same models but using Adam as the optimizer instead of RMSprop. All models learn much faster with this setting. However, the model with no trace gains significantly more than the model with the trace. Our current intuition is that some hyper-parameters, such as the frozen network’s update frequency, are limiting the rate at which the model can learn. Note also that the DQN model also learns faster with Adam as the optimizer, but remains quite unstable, in comparison with the recurrent net models.
Finally, the results in Table 1 show that both using eligibility traces and Adam provide performance improvements. While training with RMSProp, the model with traces gets to near optimal performance more than twice as quickly as the other models. With Adam, the model learns to be optimal in just 6 epochs.

4.2 TENNIS
The second Atari 2600 game we tested was Tennis. A match consists of only one set, which is won by the player who is the first to win 6 ”games” (as in regular tennis). The score ranges from 24 to -24, given as the difference between the number of balls won by the two players.
As in Pong, we first tried an RNN trained with RMSprop and the standard learning rate of 0.00025, both with and without eligibility traces (using again λ = 0.8 and λ = 0). Figure 3 shows that both RNN models learned to get optimal scores after about 50 epochs. This is in contrast with DQN, which never seems to be able to pass the 0 threshold, with large fluctuations ranging from -24 to 0. After visually inspecting the games played in the testing phase, we noticed that the DQN agent gets stuck in a loop, where it exchanges the ball with the opponent until the timer runs out. In such a case, the agent minimizes the number of points scored against, but never learns to beat the opponent. The score fluctuations depend on how few points the agent allows before entering the loop. We suspect that the agent gets stuck in this policy because the reward for trying to defeat the opponent is delayed, waiting for the ball to reach the opponent and get past it. Furthermore, the experiences of getting a point are relatively sparse. Together, it makes it difficult to propagate the reward back to the action of hitting the ball correctly.
We also notice that both the RNN with and without eligibility traces manage to learn a near-optimal policy without getting stuck in the bad policy. The RNN has the capacity of sending the signal back to past states with BPTT, allowing it to do credit assignment implicitly, which might explain their ability to escape the bad policy. Remarkably, this is the only algorithm that succeeds in getting near-optimal scores on Tennis, out of all variants of DQN (Mnih et al. (2015), Munos et al. (2016), Wang et al. (2015), Mnih et al. (2016), Schaul et al. (2015)), which tend to get stuck in the policy of delaying. The model without traces learned at a faster pace than the one with traces, arriving to a score of 20 in 45 epochs as opposed to 62 for its counterpart. It’s possible that the updates for model with traces were smaller, due to the weighting of target values, indirectly leading to a lower learning rate. We also trained the models with RMSprop and a higher learning rate of 0.001. This led to the model with traces getting to 20 points in just 27 epochs, while the model without traces lost its ability to get optimal scores and never passed the 0 threshold.
We then tried using Adam as the optimizer, with the original learning rate of 0.00025. Both RNN models learned substantially faster than with RMSprop, with the RNN with traces getting to nearoptimal performance in just 13 epochs. With Adam, the gradient for the positive TD is stored in the momentum part of the equation for quite some time. Once in momentum, the gradient is part of many updates, which makes it enough to overtake the safe strategy. We also notice that the model with traces was much more stable than its counterpart. The model without traces fell back to the policy of delaying the game on two occasions, after having learned to beat the opponent. Finally, we trained DQN with Adam, but the model acted the same way as DQN trained with RMSprop.

5 DISCUSSION AND CONCLUSION
In this paper, we analyzed the effects of using eligibility traces and different optimization functions. We showed that eligibility traces can improve and stabilize learning and using Adam can strongly accelerate learning.
As shown in the Pong results, the model using eligibility traces didn’t gain much performance from using Adam. One possible cause is the frozen network. While it has a stabilizing effect in DQN, by stopping policies from drastically changing from a single update, it also stops newly learned values from being propagated back. Double DQN seems to partially go around this issue, allowing
the policy of the next state to change, while keeping the values frozen. In future experiments, we must consider eliminating or increasing the frozen network’s update frequency. It would also be interesting to reduce the size of experience replay, as with increased learning speed, old observations can become too off-policy and barely be used in eligibility traces.
","Eligibility traces in reinforcement learning are used as a bias-variance trade-off and can often speed up training time by propagating knowledge back over timesteps in a single update. We investigate the use of eligibility traces in combination with recurrent networks in the Atari domain. We illustrate the benefits of both recurrent nets and eligibility traces in some Atari games, and highlight also the importance of the optimization used in the training.",ICLR 2017 conference submission,False,,"This paper investigates the use of eligibility traces with recurrent DQN agents. As in other recent work on deep RL, the forward view of Sutton and Barto is used to make eligibility traces practical to use with neural networks. Experiments on the Atari games Pong and Tennis show that traces work better than standard Q-learning.

The paper is well written and the use of traces in deep RL is indeed underexplored, but the experiments in the paper are too limited and do not answer the most interesting questions.

As pointed out in the questions, n-step returns have been shown to work better than 1-step returns both in the classical RL literature and more recently with deep networks. [1] shows that using n-step returns in the forward view with neural networks leads to big improvements on both Atari and TORCS. Their n-step Q-learning method also combines returns of different length in expectation, while traces do this explicitly. This paper does not compare traces with n-step returns and simply shows that traces used in the forward view help on two Atari games. This is not a very significant result. It would be much more interesting to see whether traces improve on what is already known to work well with neural networks.

The other claimed contribution of the paper is showing the strong effect of optimization. As with traces, I find it hard to make any conclusions from experiments on two games with fixed hyperparameter settings. This has already been demonstrated with much more thorough experiments in other papers. One could argue that these experiments show that importance of hyperparameter values and not of the optimization algorithm itself. Without tuning the optimization hyperparameters it's hard to claim anything about the relative merits of the methods.

[1] ""Asynchronous Methods for Deep Reinforcement Learning"", ICML 2016.

---

The reviewers agree that the paper is clear and well-written, but all reviewers raised significant concerns about the novelty of the work, since the proposed algorithm is a combination of well-known techniques in reinforcement learning. It is worth noting that the use of eligibility traces is not very heavily explored in the deep reinforcement learning literature, but since the contribution is primarily empirical rather than conceptual and algorithmic, there is a high bar for the rigorousness of the experiments. The reviewers generally did not find the evaluation to be compelling enough in this regard. Based on this evaluation, the paper is not ready for publication.

---

This paper combines DRQN with eligibility traces, and also experiment with the Adam optimizer for optimizing the q-network. This direction is worth exploring, and the experiments demonstrate the benefit from using eligibility traces and Adam on two Atari games. The methods themselves are not novel. Thus, the primary contributions are (1) applying eligibility traces and Adam to DRQN and (2) the experimental evaluation. The paper is well-written and easy to understand.

The experiments provide quantitative results and detailed qualitative intuition for how and why the methods perform as they do. However, with only two Atari games in the results, it is difficult to tell how well it the method would perform more generally. Showing results on several more games and/or other domains would significantly improve the paper. Showing error bars from multiple random seeds would also improve the paper.

---

The paper presents a deep RL with eligibility traces. The authors combine DRQN with eligibility traces for improved training. The new algorithm is evaluated on a two problems, with a single set of hyper-parameters, and compared with DQN.

The topic is very interesting. Adding eligibility traces to RL updates is not novel, but this family of the algorithms have not been explored for deep RL. The paper is written clearly, and the related literature is well-covered. More experiments would make this promising paper much stronger. As this is an investigative, experimental paper, it is crucial for it to contain a wider range of problems, different hyper-parameter settings, and comparison with vanilla DRQN, Deepmind's DQN implementation, as well as other state of the art methods.

---

This paper investigates the use of eligibility traces with recurrent DQN agents. As in other recent work on deep RL, the forward view of Sutton and Barto is used to make eligibility traces practical to use with neural networks. Experiments on the Atari games Pong and Tennis show that traces work better than standard Q-learning.

The paper is well written and the use of traces in deep RL is indeed underexplored, but the experiments in the paper are too limited and do not answer the most interesting questions.

As pointed out in the questions, n-step returns have been shown to work better than 1-step returns both in the classical RL literature and more recently with deep networks. [1] shows that using n-step returns in the forward view with neural networks leads to big improvements on both Atari and TORCS. Their n-step Q-learning method also combines returns of different length in expectation, while traces do this explicitly. This paper does not compare traces with n-step returns and simply shows that traces used in the forward view help on two Atari games. This is not a very significant result. It would be much more interesting to see whether traces improve on what is already known to work well with neural networks.

The other claimed contribution of the paper is showing the strong effect of optimization. As with traces, I find it hard to make any conclusions from experiments on two games with fixed hyperparameter settings. This has already been demonstrated with much more thorough experiments in other papers. One could argue that these experiments show that importance of hyperparameter values and not of the optimization algorithm itself. Without tuning the optimization hyperparameters it's hard to claim anything about the relative merits of the methods.

[1] ""Asynchronous Methods for Deep Reinforcement Learning"", ICML 2016.

---

This paper investigates the use of eligibility traces with recurrent DQN agents. As in other recent work on deep RL, the forward view of Sutton and Barto is used to make eligibility traces practical to use with neural networks. Experiments on the Atari games Pong and Tennis show that traces work better than standard Q-learning.

The paper is well written and the use of traces in deep RL is indeed underexplored, but the experiments in the paper are too limited and do not answer the most interesting questions.

As pointed out in the questions, n-step returns have been shown to work better than 1-step returns both in the classical RL literature and more recently with deep networks. [1] shows that using n-step returns in the forward view with neural networks leads to big improvements on both Atari and TORCS. Their n-step Q-learning method also combines returns of different length in expectation, while traces do this explicitly. This paper does not compare traces with n-step returns and simply shows that traces used in the forward view help on two Atari games. This is not a very significant result. It would be much more interesting to see whether traces improve on what is already known to work well with neural networks.

The other claimed contribution of the paper is showing the strong effect of optimization. As with traces, I find it hard to make any conclusions from experiments on two games with fixed hyperparameter settings. This has already been demonstrated with much more thorough experiments in other papers. One could argue that these experiments show that importance of hyperparameter values and not of the optimization algorithm itself. Without tuning the optimization hyperparameters it's hard to claim anything about the relative merits of the methods.

[1] ""Asynchronous Methods for Deep Reinforcement Learning"", ICML 2016.

---

The reviewers agree that the paper is clear and well-written, but all reviewers raised significant concerns about the novelty of the work, since the proposed algorithm is a combination of well-known techniques in reinforcement learning. It is worth noting that the use of eligibility traces is not very heavily explored in the deep reinforcement learning literature, but since the contribution is primarily empirical rather than conceptual and algorithmic, there is a high bar for the rigorousness of the experiments. The reviewers generally did not find the evaluation to be compelling enough in this regard. Based on this evaluation, the paper is not ready for publication.

---

This paper combines DRQN with eligibility traces, and also experiment with the Adam optimizer for optimizing the q-network. This direction is worth exploring, and the experiments demonstrate the benefit from using eligibility traces and Adam on two Atari games. The methods themselves are not novel. Thus, the primary contributions are (1) applying eligibility traces and Adam to DRQN and (2) the experimental evaluation. The paper is well-written and easy to understand.

The experiments provide quantitative results and detailed qualitative intuition for how and why the methods perform as they do. However, with only two Atari games in the results, it is difficult to tell how well it the method would perform more generally. Showing results on several more games and/or other domains would significantly improve the paper. Showing error bars from multiple random seeds would also improve the paper.

---

The paper presents a deep RL with eligibility traces. The authors combine DRQN with eligibility traces for improved training. The new algorithm is evaluated on a two problems, with a single set of hyper-parameters, and compared with DQN.

The topic is very interesting. Adding eligibility traces to RL updates is not novel, but this family of the algorithms have not been explored for deep RL. The paper is written clearly, and the related literature is well-covered. More experiments would make this promising paper much stronger. As this is an investigative, experimental paper, it is crucial for it to contain a wider range of problems, different hyper-parameter settings, and comparison with vanilla DRQN, Deepmind's DQN implementation, as well as other state of the art methods.

---

This paper investigates the use of eligibility traces with recurrent DQN agents. As in other recent work on deep RL, the forward view of Sutton and Barto is used to make eligibility traces practical to use with neural networks. Experiments on the Atari games Pong and Tennis show that traces work better than standard Q-learning.

The paper is well written and the use of traces in deep RL is indeed underexplored, but the experiments in the paper are too limited and do not answer the most interesting questions.

As pointed out in the questions, n-step returns have been shown to work better than 1-step returns both in the classical RL literature and more recently with deep networks. [1] shows that using n-step returns in the forward view with neural networks leads to big improvements on both Atari and TORCS. Their n-step Q-learning method also combines returns of different length in expectation, while traces do this explicitly. This paper does not compare traces with n-step returns and simply shows that traces used in the forward view help on two Atari games. This is not a very significant result. It would be much more interesting to see whether traces improve on what is already known to work well with neural networks.

The other claimed contribution of the paper is showing the strong effect of optimization. As with traces, I find it hard to make any conclusions from experiments on two games with fixed hyperparameter settings. This has already been demonstrated with much more thorough experiments in other papers. One could argue that these experiments show that importance of hyperparameter values and not of the optimization algorithm itself. Without tuning the optimization hyperparameters it's hard to claim anything about the relative merits of the methods.

[1] ""Asynchronous Methods for Deep Reinforcement Learning"", ICML 2016.",,,,,,3.6666666666666665,,,4.666666666666667,,
556,"AN EMPIRICAL ANALYSIS OF DEEP NETWORK LOSS SURFACES
Authors: Daniel Jiwoong Im, Michael Tao, Kristin Branson
Source file: 556.pdf

ABSTRACT
The training of deep neural networks is a high-dimension optimization problem with respect to the loss function of a model. Unfortunately, these functions are of high dimension and non-convex and hence difficult to characterize. In this paper, we empirically investigate the geometry of the loss functions for state-of-the-art networks with multiple stochastic optimization methods. We do this through several experiments that are visualized on polygons to understand how and when these stochastic optimization methods find local minima.

1 INTRODUCTION
Deep neural networks are trained by optimizing an extremely high-dimensional loss function with respect to the weights of the network’s linear layers. The objective function minimized is some measure of the error of the network’s predictions based on these weights compared to training data. This loss function is non-convex and has many local minima. These loss functions are usually minimized using first-order gradient descent (Robbins & Monro, 1951; Polyak, 1964) algorithms such as stochastic gradient descent (SGD) (Bottou, 1991). The success of deep learning critically depends on how well we can minimize this loss function, both in terms of the quality of the local minima found and the time to find them. Understanding the geometry of this loss function and how well optimization algorithms can find good local minima is thus of vital importance.
Several works have theoretically analyzed and characterized the geometry of deep network loss functions. However, to make these analyses tractible, they have relied on simplifications of the network structures, including that the networks are linear (Saxe et al., 2014), or assuming the path and variable independence of the neural networks (Choromanska et al., 2015). Orthogonally, the performance of various gradient descent algorithms has been theoretically characterized (Nesterov, 1983). Again, these analyses make simplifying assumptions, in particular that the loss function is strictly convex, i.e. there is only a single local minimum.
In this work, we empirically investigated the geometry of the real loss functions for state-of-the-art networks and data sets. In addition, we investigated how popular optimization algorithms interact with these real loss surfaces. To do this, we plotted low-dimensional projections of the loss function in subspaces chosen to investigate properties of the local minima selected by different algorithms. We chose these subspaces to address the following questions:
• What types of changes to the optimization procedure result in different local minima? • Do different optimization algorithms find qualitatively different types of local minima?

2 RELATED WORK

2.1 LOSS SURFACES
There have been several attempts to understand the loss surfaces of deep neural networks. Some have studied the critical points of the deep linear neural networks (Baldi, 1989; Baldi & Hornik,
∗Work done during an internship at Janelia Research Campus
1989; Baldi & Lu, 2012). Others further investigated the learning dynamics of the deep linear neural networks (Saxe et al., 2014). More recently, several others have attempted to study the loss surfaces of deep non-linear neural networks (Choromanska et al., 2015; Kawaguchi, 2016; Soudry & Carmon, 2016).
One approach is to analogize the states of neurons as the magnetics dipoles used in spherical spinglass Ising models from statistical physics (Parisi, 2016; Fyodorov & Williams, 2007; Bray & Dean, 2007). Choromanska et al. (2015) attempted to understand the loss function of neural networks through studying the random Gaussian error functions of Ising models. Recent results (Kawaguchi, 2016; Soudry & Carmon, 2016) have provided cursory evidence in agreement with the theory provided by Choromanska et al. (2015) in that they found that that there are no “poor” local minima in neural networks still with strong assumptions.
There is some potential disconnect between these theoretical results and what is found in practice due to several strong assumptions such as the activation of the hidden units and output being independent of the previous hidden units and input data. The work of Dauphin et al. (2014) empirically investigated properties of the critical points of neural network loss functions and demonstrated that their critical points behave similarly to the critical points of random Gaussian error functions in high dimensional space. We will expose further evidence along this trajectory.

2.2 OPTIMIZATION
In practice, the local minima of deep network loss functions are for the most part decent. This implies that we probably do not need to take many precautions to avoid bad local minima in practice. If all local minima are decent, then the task of finding a decent local minimum quickly is reduced to the task of finding any local minimum quickly. From an optimization perspective this implies that solely focusing on designing fast methods are of key importance for training deep networks.
In the literature the common method for measuring performance of optimization methods is to analyze them on nice convex quadratic functions (Polyak, 1964; Broyden, 1970; Nesterov, 1983; Martens, 2010; Erdogdu & Montanari, 2015) even though the problems are applied to non-convex problems. For non-convex problems, however, if two methods converge to different local minima their performance will be dictated on how those methods solve those two convex subproblems. It is challenging to show that one method will beat another without knowledge of the sort of convex subproblems, which is generally not known apriori. What we will explore is whether indeed are some characteristics that can found experimentally. If so, perhaps one could validate where these analytical results are valid or even improve methods for training neural networks.
2.2.1 LEARNING PHASES
One of the interesting empirical observation is that we often observe is that the incremental improvement of optimization methods decreases rapidly even in non-convex problems. This behavior has been discussed as a “transient” phase followed by a “minimization” phase (Sutskever et al., 2013)
where the former finds the neighborhood of a decent local minima and the latter finds the local minima within that neighborhood. The existence of these phases implies that if certain methods are better at different phases one could create novel methods that schedule when to apply each method.

3 EXPERIMENTAL SETUP AND TOOLS

3.1 NETWORK ARCHITECTURES AND DATA SETS
We conducted experiments on three state-of-the-art neural network architectures. Network-inNetwork (NIN) (Lin et al., 2014) and the VGG(Simonyan & Zisserman, 2015) network are feedforward convolutional networks developed for image classification, and have excellent performance on the Imagenet (Russakovsky et al., 2014) and CIFAR10 (Krizhevsky, 2009) data sets. The long short-term memory network (LSTM) (Hochreiter & Schmidhuber, 1997) is a recurrent neural network that has been successful in tasks that take variable-length sequences as input and/or produce variable-length sequences as output, such as speech recognition and image caption generation. These are large networks currently used in many machine vision and learning tasks, and the loss functions minimized by each are highly non-convex.
All results using the feed-forward convolutional networks (NIN and VGG) are on the CIFAR10 image classification data set, while the LSTM was tested on the Penn Treebank next-word prediction data set.

3.2 OPTIMIZATION METHODS
We analyzed the performance of five popular gradient-descent optimization methods for these learning frameworks: Stochastic gradient descent (SGD) (Robbins & Monro, 1951), stochastic gradient descent with momentum (SGDM), RMSprop (Tieleman & Hinton, 2012), Adadelta (Zeiler et al., 2011), and ADAM (Kingma & Ba, 2014). These are all first-order gradient descent algorithms that estimate the gradients based on randomly-grouped minibatches of training examples. One of the major differences between these algorithms is how they select the weight-update step-size at each iteration, with SGD and SGDM using fixed schedules, and RMSprop, Adadelta, and ADAM using adaptive, per-parameter step-sizes. Details are provided in Section A.2.
In addition to these five existing optimization methods, we compare to a new gradient descent method we developed based on the family of Runge Kutta integrators. In our experiments, we tested a second-order Runge-Kutta integrator in combination with SGD (RK2) and in combination with ADAM (ADAM&RK2). Details are provided in Section A.3).

3.3 ANALYSIS METHODS
Several of our empirical analyses are based on the technique of Goodfellow et al. (Goodfellow et al., 2015). They visualize the loss function by projecting it down to one carefully chosen dimension. They plot the value of the loss function along a set of samples along this dimension. The projection space is chosen based on important weight configurations, thus they plot the value of the loss function at linear interpolations between two weight configurations. They perform two such analyses: one in which they interpolate between the initialization weights and the final learned weights, and one in which they interpolate between two sets of final weights, each learned from different initializations.
In this work, we use a similar visualization technique, but choose different low-dimensional subspaces for the projection of the loss function. These subspaces are based on the initial weights as well as the final weights learned using the different optimization algorithms and combinations of them, and are chosen to answer a variety of questions about the loss function and how the different optimization algorithms interact with this loss function. In contrast, Goodfellow et al. only looked at SGDM. In addition, we explore the use of two-dimensional projections of the loss function, allowing us to better visualize the space between local minima. We do this via barycentric and bilinar interpolation for triplets and quartets of points respectively (details in Section A.1).
We refer to the critical points found using these variants of SGD, for which the gradient is approximately 0, as local minima. Our evidence that these are local minima as opposed to saddle points is
similar to that presented in Goodfellow et al. (Goodfellow et al., 2015). If we interpolate beyond the critical point, in this one-dimensional projection, the loss increases (Fig. 10).

3.4 TECHNICAL DETAILS
We used the VGG and NIN implementations from https://github.com/szagoruyko/cifar.torch.git.
The batch size was set to 128 and the number of epochs was set to 200. The learning rate was chosen from the discrete range between [0.2, 0.1, 0.05, 0.01] for SGD and [0.002, 0.001, 0.0005, 0.0001] for adaptive learning methods. We doubled the learning rates when we ran our augmented versions with Runge-Kutta because they required two stochastic gradient computations per epoch. We used batchnormalization and dropout to regularize our networks. All experiments were run on a 6-core Intel(R) Xeon(R) CPU @ 2.40GHz with a TITAN X.

4 EXPERIMENTAL RESULTS

4.1 DIFFERENT OPTIMIZATION METHODS FIND DIFFERENT LOCAL MINIMA
We trained the neural networks described above using each optimization method starting from the same initial weights and with the same minibatching. We computed the value of the loss function for weight vectors interpolated between the initial weights, the final weights for one algorithm, and the final weights for a second algorithm for several pairings of algorithms. The results are shown in the lower triangle of Table 1.
For every pair of optimization algorithms, we observe that the training loss between the final weights for different algorithms shows a sharp increase along the interpolated path. This suggests that each optimization algorithm found a different critical point, despite starting at the same initialization. We investigated the space between other triplets and quadruples of weight vectors (Figure 2 and 3), and even in these projections of the loss function, we still see that the local minima returned by different algorithms are separated by high loss weight parameters.
Deep networks are overparameterized. For example, if we switch all corresponding weights for a pair of nodes in our network, we will obtain effectively the same network, with both the original and permuted networks outputting the same prediction for a given input. To ensure that the weight vectors returned by the different algorithms were functionally different, we compared the outpts of the networks on each example in a validation data set:
dist(θ1, θ2) = √√√√ 1 Ntest Ntest∑ i=1 ‖F (xi, θ1)− F (xi, θ2)‖2,
where θ1 and θ2 are the weights learned by two different optimization algorithms, xi is the input for a validation example, and F (x, θ) is the output of the network for weights θ on input x.
We found that, for all pairs of algorithms, the average distance between the outputs of the networks (Equation 4.1) was approximately 0.16, corresponding to a label disagreement of about 8% (upper triangle of Table 1). Given the generalization error of these networks (approximately 11%, Figure 4), the maximum disagreement we could see was 22%. Thus, these networks disagreed on a large fraction of these test examples – over 13 rd. Thus, the local minima found by different algorithms correspond to effectively different networks, not trivial reparameterizations of the same one.

4.2 DIFFERENT OPTIMIZATION ALGORITHMS FIND DIFFERENT TYPES OF LOCAL MINIMA
Next, we investigated whether the local minima found by the different optimization algorithms had distinguishing properties. To do this, we trained the networks with each optimization algorithm using different initial parameters. We then compared differences between runs of the same algorithm but different initializations to differences between different algorithms.
As shown in Figure 4(a), in terms of training accuracy, we do see some stereotypy for the optima found by different algorithms, with SGD finding local minima with the lowest training accuracy and ADAM, Rmsprop, and Adadelta finding local minima with the highest training accuracy. However, this could be attributed to SGD’s asymtotically slow convergence near local minima due to the gradient diminishing near extrema. Despite this limitation, Figure 4(b) shows that the generalization accuracy of these different local minima on validation data was not significantly different between algorithms. We also did not see a relationship between the weight initialization and the validation accuracy. Thus, while these algorithms fall into different local minima, they are not different in terms of their final quality.
We visualized the loss surface around each of the local minima for the multiple runs. To do this, we plotted the value of the loss function between the initial and final weights for each algorithm (Figure 5(a,c)) for each run of the algorithm from a different initialization. In addition, we plotted
the value of the loss function between the final weights for selected pairs of algorithms for each run (Figure 5(b,d)). We see that the surfaces look strikingly similar for different runs of the same algorithm, but characteristically different for different algorithms. Thus, we found evidence that the different algorithms land in qualitatively different types of local minima.
In particular, we see in Figure 5(a,c) that the size of the basins around the local minima found by ADAM and ADAM&RK2 are larger than those found by SGD and RK2, i.e. that the training loss is small for a wider range of α values. This is a relative measure, and the magnitude of the change in the weight vector is ∆α‖θ1 − θ0‖ for a change of size ∆α, where θ0 is the initial weight vector θ1 is the result found by a given optimization algorithm. In Figure 6, we repeat this analysis, instead showing the loss as a function of the absolute distance in parameter space:
θ(λ) = θ1 + λ θ0 − θ1 ‖θ0 − θ1‖
(1)
We again see that the size of the basin around the local minima varies by optimization algorithm. Note that we evaluate the loss for weight vectors beyond the initial configuration, which had a loss of 2.4.

4.3 ANALYZING LEARNING AFTER “TRANSIENT PERIOD”
Recall that, during optimization, it has been observed that there is a short “transient” phase when the loss decreases rapidly and a “minimization” phase in which the loss decreases slowly (Section 2.2.1 and Figure 1). In this set of experiments, we investigated the effects of switching from one type of optimization method to another at various points during training, in particular at late stages of training when it is thought that a local minimum has been chosen and is only being localized. We switched from one optimization method to another 25%, 50%, and 75% of the way through training. The results are plotted in Figure 7d. We emphasize that we are not switching methods to improve performance, but rather to investigate the shape of the loss function in regions explored during the “minimization” phase of optimization.
We found that, regardless of how late we switch optimization algorithms, as shown in the rightmost column of Figure 7, the local minima found were all different. This directly disagrees with the notion that the local minimum has effectively been chosen before the “minimization” phase, but instead that which local minimum is found is still in flux this late in optimization. It appears that this switch from one local minimum to another happens almost immediately after the optimization method switches, with the training accuracy jumping to the characteristic accuracy for the given method within a few epochs (Figure 7, left column). Interestingly, we also see the distance between the initial and current weight vectors changes drastically after switching from one optimization
method to another, and that this distance is characteristic per algorithm (Figure 7, middle column). While distance increases with training epoch for any single optimization method, it actually starts to decrease when switching from ADAM to SGD.

4.4 EFFECTS OF BATCH-NORMALIZATION
To understand how batch normalization affects the types of local minima found, we performed a set of experiments comparing loss surfaces near local minima found with and without batch normal-
ization for each of the optimization methods. We visualized the surface near these local minima by interpolating between the initial weights and the final weights as well as between pairs of final weights found with different algorithms.
We observed clear qualitative differences between optimization with (Figure 5) and without (Figure 8) batch normalization. We see that, without batch normalization, the quality of local minimum found by a given algorithm is much more dependent on the initialization. In addition, the surfaces between different local minima are more complex in appearance: with batch normalization we see sharp unimodal jumps in performance but without batch normalization we obtain wide bumpy shapes that aren’t necessarily unimodal.
The neural networks are typically initialized with very small parameter values (Glorot & Bengio, 2010; He et al., 2015). Instead, we trained NIN with exotic intializations such as initial parameters drawn from N (−10.0, 0.01) or N (−1.0, 1.0) and observe the loss surface behaviours. The details of results are discussed in Appendix A.5.

5 CONCLUSIONS
In this work, we performed a series of empirical analyses to understand the geometry of the loss functions corresponding to deep neural networks, and how different optimization methods minimize this loss to answer the two questions posed in the introduction.
What types of changes to the optimization procedure result in different local minima?
We found that every type of change to the optimization procedure we tested resulted in a different local minimum. Different local minima were found using the different optimization algorithms from the same initialization (Section 4.1). Even switching the optimization algorithm to another very late in optimization – during the slow “mimimization” portion of learning – resulted in a different local minimum (Section 4.3). The quality of the local minima found, in terms of training and generalization error, is similar. These different local minima were not equivalent, and made mistakes on different test examples (Section 4.1). Thus, they were not trivially different local minima, as would occur if nodes in internal layers of the network were permuted. We observed that the quality of these local minima was only consistently good when we used batch normalization for regularization. Without batch normalization, the quality of the critical points found depended on the initialization, and some solutions found were not as good as others. Our observations are in contrast to the conclusions of Goodfellow et al., i.e. that local minima are not a problem in deep learning because, in the region of the loss function explored by SGD algorithms, the loss function is well-behaved (Goodfellow et al., 2015). Instead, our observations are more consistent with the explanation that the local minima found by popular SGD optimization methods are almost all good (Choromanska et al., 2015; Kawaguchi, 2016; Soudry & Carmon, 2016).

Do different optimization algorithms find qualitatively different types of local minima?
Interestingly, we found that, while the local minima found by the same optimization algorithm from different initializations were different, the shape of the loss function around these local minima was strikingly similar, and was a characteristic of the optimization algorithm. In particular, we found that the size of the basin around ADAM-based optimization was larger than that around vanilla SGD (Section 4.2). A large basin is related to a large margin, as small changes in the weight vector will not affect the training error, and perhaps could have some implications for generalization error. In our experiments, however, we did not observe better generalization error for ADAM than SGD. Questions for potential future research are why the shapes of the loss functions around different local minima found by the same algorithm are so similar, and what the practical implications of this are.

A SUPPLEMANTARY MATERIALS
A.1 3D VISUALIZATION
Goodfellow et al. (2015) introduced the idea of visualizing 1D subspace of the loss surface between the parameters. Here, we propose to visualize loss surface in 3D space through interpolating over three and four vertices.
Linear Interpolation Given two parameters θ1 and θ2, θi = αθ1 + (1− α)θ2, ∀α ∈ [0, 1]. (2)
Bilinear Interpolation Given four parameters θ0, θ1, θ2, and θ3, φi = αθ1 + (1− α)θ2 (3) ϕi = αθ3 + (1− α)θ4 (4) θj = βφi + (1− β)ϕi (5)
for all α ∈ [0, 1] and β ∈ [0, 1].
Barycentric Interpolation Given four parameters θ0, θ1, and θ2, let d1 = θ1 − θ0 and d2 = θ2 − θ0. Then, the formulation of the interpolation is
φi = αd1 + θ0 (6) ϕi = αd2 + θ0 (7) θj = βφi + (1− β)ϕi (8)
for all α ∈ [0, 1] and β ∈ [0, 1].

A.2 OPTIMIZATION METHODS

A.2.1 STOCHASTIC GRADIENT DESCENT
In many deep learning applications both the number of parameters and quantity of input data points can be quite large. This makes the full evaluation of U(θ) be prohibitively expensive. A standard technique for aleviating computational loadis to apply an stochastic approximation to the gradient Robbins & Monro (1951). More precisely, one approximates U by a subset of n data points, denoted by {σj}Nj=1 at each timestep:
Un(θ) = 1
n n∑ j=1 `(θ,xσj ) ' 1 N N∑ i=1 `(θ,xi) = U(θ) (9)
Of course this approximation also carries over to the gradient, which is of vital importance to optimization techniques:
∇Un(θ) = 1 n n∑ j=1 ∇`(θ,xσj ) ' ∇U(θ) (10)
This method is what is commonly called Stochastic Gradient Descent or SGD. So long as the data is distributed nicely the approximation error of Un should be sufficiently small such that not only will SGD still behave like normal GD , but it’s wall clock time for to converge should be significantly lower as well.
Usually one uses the stochastic gradient rather than the true gradient, but the inherent noisiness must be kept in mind. In what follows we will always mean the stochastic gradient.

A.2.2 MOMENTUM
In order to aleviate both noise in the input data as well as noise from stochasticity used in computing quantities one often maintains history of previous evaluations. In order to only require one extra variable one usually stores variables of the form
E[F ]t = αFt + βE[F ]t−1. (11)
where Ft is some value changing over time and E[F ]t is the averaged quantity.
An easy scheme to apply this method to is to compute a rolling weighted average of gradients such as E[g]t = (1− α)gt + αE[g]t−1 but there will be other uses in the future.

A.2.3 PERTINENT METHODS
With the aforementioned tools there are a variety of methods that can be constructed. We choose to view these algorithms as implementations of Explicit Euler on a variety of different vector fields to remove the ambiguity between η and gt. We therefore can define a method by the vector field Xt that explicit Euler is applied to with a single η that is never changed.
SGD with Momentum (SGDM) By simply applying momentum to gt one obtains this stabilized stochastic version of gradient descent:
Xt = −E[g]t. (12) This is the most fundamental method that is used in practice and the basis for everything that follows.
Adagrad Adagrad rescalesXt by summing up the sqaures of all previous gradients in a coefficientwise fashion:
Xt = − gt√∑t
i=1 g 2 i +
. (13)
Here is simply set to some small positive value to prevent division-by-zero. In the future we will neglect this term in denominators because it is always necessary.
The concept is to accentuate variations in gt, but because the denominator is monotonically nondecreasing over time this method is doomed to retard its own progress over time. The denominator can also be seen as a form of momentum where α and β are both set to 1.
Rmsprop A simple generalization of ADAGrad is to simply allow for α and β to be changed from 1. In particular one usually chooses a β less than 1, and presumably α = 1− β. Thus one arrives at a method where the effects of the distance history are diminished:
Xt =− gt√ E[g2]t . (14)
Adadelta Adadelta adds another term to RMSprop in order to guarantee that the magnitude of X is balanced with gt Zeiler et al. (2011). More precisely it maintains
Xt√ E[X 2t ] = − gt√ E[g2t ]
(15)
which results in the following vector field: Xt =− √ E[X 2t ]√ E[g2t ] gt. (16)
and η is set to 1.
ADAM By applying momentum to both gt and g2t one arrives at what is called ADAM. This is often considered a combination of SGDM + RMSprop,
Xt = ct E[g]t√ E[g2]t . (17)
ct =
√ 1−βt2
1−βt1 is the initialization bias correction term with β1, β2 ∈ [0, 1) being the β parameters
used in momentum for g and g2 respectively. Initialization bias is caused by the history of the momentum variable being initially set to zero.

A.3 RUNGE KUTTA
Runge-Kutta methods Butcher (1963) are a broad class of numerical integrators categorized by their truncation error. Because the ordinary differential equations Runge-Kutta methods solve generalize gradient descent, our augmentation is quite straightforward. Although our method applies to all explicit Runge-Kutta methods we will only describe second order methods for simplicity.
The general form of second-order explicit Runge-Kutta on a time-independent vector field is
θt+1 = θt + (a1k1 + a2k2)h (18) k1 = X (θt) (19) k2 = X (θt + q1hk1) (20)
where a1, a2, and q1 are parameters that define a given Runge-Kutta method. Table 3 refers to the parameters used for the different Runge-Kutta variants we use in our experiments.

A.3.1 AUGMENTING OPTIMIZATION WITH RUNGE KUTTA
For a given timestep, explicit integrators can be seen as a morphism over vector fields X → X̄ h. For a gradient gt = ∇θU we can solve a modified RK2 gradient ḡt in the following fashion:
θt+1 = θt + ḡth = Advect rk2 g (θ, h) (21)
rearranged with respect to ḡt
ḡt = Advectrk2g (θ, h)− θt
h (22)
= θt + (a1k1 + a2k2)h− θt
h (23)
= (a1k1 + a2k2). (24) If we simply substitute the gradient gt with ḡt one obtains an RK2-augmented optimization technique.

A.4 EXPERIMENTS WITH RUNGE-KUTTA INTEGRATOR
The results in Figure 9 illustrates that, with the exception of the Midpoint method, stochastic RungeKutta methods outperform SGD. “SGD x2” is the stochastic gradient descent with twice of the learning rate of “SGD”. From the figure, we observe that the Runge-Kutta methods perform even better with half the number of gradient computed by SGD. The reason is because SGD has the accumulated truncated error of O(h) while second-order Runge-Kutta methods have the accumulated truncated error of O(h2).
Unfortunately, ADAM outperforms ADAM+RK2 methods. We speculate that this is because the way how ADAM’s renormalization of input gradients in conjunction with momentum eliminates the value added by using our RK-based descent directions.

A.5 EFFECTS OF BATCH-NORMALIZATION AND EXTREME INITIALIZATIONS
The neural networks are typically initialized with very small parameter values (Glorot & Bengio, 2010; He et al., 2015). Instead, we trained NIN with exotic intializations such as initial parameters drawn from N (−10.0, 0.01) or N (−1.0, 1.0) and observe the loss surface behaviours. The results are shown in Figure 11. We can see that NIN without BN does not train at all with any of these initializations. Swirszcz et al. (2016) mentioned that bad performance of neural networks trained with these initializations are due to finding a bad local minima. However, we see that loss surface region around these initializations are plateau 1 rather than a bad local minima as shown in Figure 11b. On
1We used same initializations as (Swirszcz et al., 2016) but we trained different neural networks with SGD on a different dataset. We used NIN and CIFAR10 and Swirszcz et al. (2016) used smaller neural network and MNIST.
the other hand, NIN with BN does train slowly over time but finds a local minima. This implies that BN redeems the ill-posed loss surface (plateau region). Nevertheless, the local minima it found was not good as when the parameters were initialized with small values. However, it is not totally clear whether this is due to difficulty of training or due to falling in a bad local minima.

A.6 SWITCHING OPTIMIZATION METHODS
","The training of deep neural networks is a high-dimension optimization problem with respect to the loss function of a model. Unfortunately, these functions are of high dimension and non-convex and hence difficult to characterize. In this paper, we empirically investigate the geometry of the loss functions for state-of-the-art networks with multiple stochastic optimization methods. We do this through several experiments that are visualized on polygons to understand how and when these stochastic optimization methods find local minima.",ICLR 2017 conference submission,False,,"This paper provides an extensive analysis of the error loss function for different optimization methods. The presentation is well done and informative. The experimental procedure is clarified sufficiently well. Theoretical evaluations like this are crucial for a wide range of applications and help to better understand and improve the convergence behavior for a given system.

Pros:

- Important analysis
- Good visualizations

Cons:

- The paper describes mostly the observation that the optima vary for different methods, however doesn't attempt to explain why it happens and how to solve it (aside from batch-norm)
- Some fonts are very small (e.g. Fig. 5)

---

The paper proposes an empirical investigation of the energy landscape of deep neural networks using several stochastic optimization algorithms.
 
 The extensive experiments conducted by the authors are interesting and inspiring. However, several reviewers expressed major concerns pointing out the limitations of a experimental investigation on real-world datasets. The paper would benefit from additional experiments on simulated datasets. This would allow to complement the experimental analysis. Furthermore, theoretical analysis and analytical derivations, consistent with the simulated datasets, could shed light on the experimental results and allow to make more precise claims. 
 
 A revised version, following the reviewers' suggestions, will result in a stronger submission to a future venue.

---

First of all, I would like to thank the authors for putting this much work into a necessary but somewhat tedious topic. While I think the paper is somewhat below the standard of a conference paper (see detailed comments below), I would definitely love to see a version of this paper published with some of the issues ironed out. I also agree with many of the points raised by other reviewers and will not repeat them here.

Major points:

-- ""As we saw in the previous section, the minima of deep network loss functions are for the most part decent.""

All you said in the previous section was that theory shows that there are no bad minima under ""strong assumptions"". There is no practical proof that minima do not vary in quality.

-- ""This implies that we probably do not need to take many precautions to avoid bad minima in practice. If all minima are decent, then the task of finding a ""decent minima quickly"" is reduced to the task of finding any minima quickly.""

First of all, as one of the reviewers pointed out, we are never guaranteed in practice to actually reach a local minimum. We could always hit a region of the objective function where the algorithm makes essentially no further progress. The final error level, in practice, actually does depend significantly on many factors such as (i) optimization algorithm (ii) learning rate schedule (iii) initialization of weights (iv) presence of unsupervised pretraining (v) whether neurons are added or eliminated during training etc. etc. Therefore, the task of optimizing neural networks is far from being ""reduced to finding any minima quickly"".

-- Figure 1
 
I don't like Figure 1, because it suggests to me that you diagnosed exactly where the transition between the two phases happened, which I don't think you did.
 
Also, the concept of having a fast-decaying error followed by a slow-decaying error is simple enough for readers to understand without a dedicated graph. Minor point on presentation: The red brace is positioned lower in the figure than the blue brace and the braces don't join up horizontally. Please be more careful.

-- Misuse of the transient phase / minimization phase concept

In section 4.3, you talk about the transient and minimization phase of optimization. However, you have no way of diagnosing when or if your algorithm reaches the minimization phase. You seem to think that the minimization phase is simply the part of the optimization process where the error decreases slowly. AFAIK, this is not the case. The minimization phase is where the optimization algorithm enters the vicinity of the local minimum that can be approximated by the second-order Taylor expansion. For this to even occur, one would have to verify, for example, that the learning rate is small enough. You change the algorithm after 25%, 50% and 75% of training, but these points seem arbitrary. What is the minimization phase was reached at 99%, or 10 epochs after you decided to stop training?

-- Only 1 dataset
 
You run most experiments on only 1 dataset (CIFAR). Please replicate with at least one more dataset.
 
-- Many figures are unclear
 
For each figure, the following information are relevant: network used; dataset used; learning rate used; batch norm yes / no; whether figure shows train, test, or validation error. It should be easy for the reader to ascertain this information for all figures, not just for some.
 
-- You say at the beginning of section 4.1 that each algorithm finds a different minimum as if this is a significant finding. However, this is obvious because the updates taken by these algorithms vary wildly. Keep in mind that there is an exponentially large number of minima. The probability of different algorithms choosing the same minimum is essentially zero because of their sheer number. The same would be true if you even shift the learning rate slightly or use a different random seed for minibatch generation etc. etc.
  
-- Lack of confidence intervals

The value of Figures 1, 2, 3 and 6 is limited is because it is unclear how these plots would change if the random seed were changed. We only get information for a single weight initialization and a single minibatch sequence. While figures 5 and 7 can be used to try and infer what confidence intervals around plots in figures 1, 2, 3 and 6 might look like, I think those confidence intervals should still be shown for at least a subset of the configurations presented.

-- Lack of information regarding learning rate

There is big question mark left open regarding how all your results would change if different learning rates were used. You don't even tell us how you chose the learning rates from the intervals you gave in section 3.4.

-- Lack of information regarding the absolute distance of interpolated points

In most figures, you interpolate between two or three trained weight configuration. However, you do not say how far the interpolated points are apart. This is highly significant, because if points are close together and there is a big ""hump"" between them, it means that those points are more ""brittle"" than if they are far apart and there is a big ""hump"" between them.
 
Minor points:
 
-- LSTM is not a fixed network architecture like NiN or VGG, but a layer type. LSTM would be equivalent to CNN. Also, the VGG paper has multiple versions of VGG. You should specify which one you used.
  
-- The font size for the legends in the upper triangle of Table 1 is too small. You can't just write ""best viewed in zoom"" in the table caption and pretend that somehow fixes the problem. Personally, I prefer no legend over an unreadable legend.

---

The paper is dedicated to better understanding the optimization landscape in deep learning, in particular when explored with different optimization algorithms, and thus it also characterizes the behavior of these algorithms. It heavily re-uses the approach of Goodfellow et al. (2015). I find it hard to understand the contributions of the paper, for example: is it surprising that different algorithms reach different solutions when starting from the same initialization? It would be useful if the authors build such basic intuition in the paper. I also did not receive a clear answer to the question I posed to reviewers regarding clarifying how does the findings of the paper can contribute to future works on optimization in deep learning. And this is what I find fundamentally missing. So for example, there are probably plenty of ways to modify approach of Goodfellow et al. (2015), and similar works, and come up with interesting visualization methods for deep learning - but the question is: how is this helpful in terms of designing better algorithms, gaining more intuition how the optimization surface looks like in general, etc.? This is an interesting paper, though I am fairly confident it is a better fit for the journal than this conference. 

It would be interesting and instructive, even for sanity check, to plot the eigenspectra of the solutions recovered by the algorithms to see the order of critical points recovered.

---

I appreciate the work but I do not think the paper is clear enough. 
Moreover, the authors say ""local minimia"" ~70 times but do not show (except for Figure 11?) that the solutions found are not necessarily local minima. 
The authors do not talk about that fact that slices of a non-convex problem can look like the ones they show. 
It is well-known that the first-order methods may just fail to deal with certain non-convex ill-conditioned problems even in low-dimensional noiseless cases, the place/solution where they fail to make progress is not necessarily a local minimum. 
Some sentences like the one given below suggest that the study is too superficial:  
""One of the interesting empirical observation is that we often observe is that the incremental improvement
of optimization methods decreases rapidly even in non-convex problems.""

---

This paper provides an extensive analysis of the error loss function for different optimization methods. The presentation is well done and informative. The experimental procedure is clarified sufficiently well. Theoretical evaluations like this are crucial for a wide range of applications and help to better understand and improve the convergence behavior for a given system.

Pros:

- Important analysis
- Good visualizations

Cons:

- The paper describes mostly the observation that the optima vary for different methods, however doesn't attempt to explain why it happens and how to solve it (aside from batch-norm)
- Some fonts are very small (e.g. Fig. 5)

---

This paper provides an extensive analysis of the error loss function for different optimization methods. The presentation is well done and informative. The experimental procedure is clarified sufficiently well. Theoretical evaluations like this are crucial for a wide range of applications and help to better understand and improve the convergence behavior for a given system.

Pros:

- Important analysis
- Good visualizations

Cons:

- The paper describes mostly the observation that the optima vary for different methods, however doesn't attempt to explain why it happens and how to solve it (aside from batch-norm)
- Some fonts are very small (e.g. Fig. 5)

---

The paper proposes an empirical investigation of the energy landscape of deep neural networks using several stochastic optimization algorithms.
 
 The extensive experiments conducted by the authors are interesting and inspiring. However, several reviewers expressed major concerns pointing out the limitations of a experimental investigation on real-world datasets. The paper would benefit from additional experiments on simulated datasets. This would allow to complement the experimental analysis. Furthermore, theoretical analysis and analytical derivations, consistent with the simulated datasets, could shed light on the experimental results and allow to make more precise claims. 
 
 A revised version, following the reviewers' suggestions, will result in a stronger submission to a future venue.

---

First of all, I would like to thank the authors for putting this much work into a necessary but somewhat tedious topic. While I think the paper is somewhat below the standard of a conference paper (see detailed comments below), I would definitely love to see a version of this paper published with some of the issues ironed out. I also agree with many of the points raised by other reviewers and will not repeat them here.

Major points:

-- ""As we saw in the previous section, the minima of deep network loss functions are for the most part decent.""

All you said in the previous section was that theory shows that there are no bad minima under ""strong assumptions"". There is no practical proof that minima do not vary in quality.

-- ""This implies that we probably do not need to take many precautions to avoid bad minima in practice. If all minima are decent, then the task of finding a ""decent minima quickly"" is reduced to the task of finding any minima quickly.""

First of all, as one of the reviewers pointed out, we are never guaranteed in practice to actually reach a local minimum. We could always hit a region of the objective function where the algorithm makes essentially no further progress. The final error level, in practice, actually does depend significantly on many factors such as (i) optimization algorithm (ii) learning rate schedule (iii) initialization of weights (iv) presence of unsupervised pretraining (v) whether neurons are added or eliminated during training etc. etc. Therefore, the task of optimizing neural networks is far from being ""reduced to finding any minima quickly"".

-- Figure 1
 
I don't like Figure 1, because it suggests to me that you diagnosed exactly where the transition between the two phases happened, which I don't think you did.
 
Also, the concept of having a fast-decaying error followed by a slow-decaying error is simple enough for readers to understand without a dedicated graph. Minor point on presentation: The red brace is positioned lower in the figure than the blue brace and the braces don't join up horizontally. Please be more careful.

-- Misuse of the transient phase / minimization phase concept

In section 4.3, you talk about the transient and minimization phase of optimization. However, you have no way of diagnosing when or if your algorithm reaches the minimization phase. You seem to think that the minimization phase is simply the part of the optimization process where the error decreases slowly. AFAIK, this is not the case. The minimization phase is where the optimization algorithm enters the vicinity of the local minimum that can be approximated by the second-order Taylor expansion. For this to even occur, one would have to verify, for example, that the learning rate is small enough. You change the algorithm after 25%, 50% and 75% of training, but these points seem arbitrary. What is the minimization phase was reached at 99%, or 10 epochs after you decided to stop training?

-- Only 1 dataset
 
You run most experiments on only 1 dataset (CIFAR). Please replicate with at least one more dataset.
 
-- Many figures are unclear
 
For each figure, the following information are relevant: network used; dataset used; learning rate used; batch norm yes / no; whether figure shows train, test, or validation error. It should be easy for the reader to ascertain this information for all figures, not just for some.
 
-- You say at the beginning of section 4.1 that each algorithm finds a different minimum as if this is a significant finding. However, this is obvious because the updates taken by these algorithms vary wildly. Keep in mind that there is an exponentially large number of minima. The probability of different algorithms choosing the same minimum is essentially zero because of their sheer number. The same would be true if you even shift the learning rate slightly or use a different random seed for minibatch generation etc. etc.
  
-- Lack of confidence intervals

The value of Figures 1, 2, 3 and 6 is limited is because it is unclear how these plots would change if the random seed were changed. We only get information for a single weight initialization and a single minibatch sequence. While figures 5 and 7 can be used to try and infer what confidence intervals around plots in figures 1, 2, 3 and 6 might look like, I think those confidence intervals should still be shown for at least a subset of the configurations presented.

-- Lack of information regarding learning rate

There is big question mark left open regarding how all your results would change if different learning rates were used. You don't even tell us how you chose the learning rates from the intervals you gave in section 3.4.

-- Lack of information regarding the absolute distance of interpolated points

In most figures, you interpolate between two or three trained weight configuration. However, you do not say how far the interpolated points are apart. This is highly significant, because if points are close together and there is a big ""hump"" between them, it means that those points are more ""brittle"" than if they are far apart and there is a big ""hump"" between them.
 
Minor points:
 
-- LSTM is not a fixed network architecture like NiN or VGG, but a layer type. LSTM would be equivalent to CNN. Also, the VGG paper has multiple versions of VGG. You should specify which one you used.
  
-- The font size for the legends in the upper triangle of Table 1 is too small. You can't just write ""best viewed in zoom"" in the table caption and pretend that somehow fixes the problem. Personally, I prefer no legend over an unreadable legend.

---

The paper is dedicated to better understanding the optimization landscape in deep learning, in particular when explored with different optimization algorithms, and thus it also characterizes the behavior of these algorithms. It heavily re-uses the approach of Goodfellow et al. (2015). I find it hard to understand the contributions of the paper, for example: is it surprising that different algorithms reach different solutions when starting from the same initialization? It would be useful if the authors build such basic intuition in the paper. I also did not receive a clear answer to the question I posed to reviewers regarding clarifying how does the findings of the paper can contribute to future works on optimization in deep learning. And this is what I find fundamentally missing. So for example, there are probably plenty of ways to modify approach of Goodfellow et al. (2015), and similar works, and come up with interesting visualization methods for deep learning - but the question is: how is this helpful in terms of designing better algorithms, gaining more intuition how the optimization surface looks like in general, etc.? This is an interesting paper, though I am fairly confident it is a better fit for the journal than this conference. 

It would be interesting and instructive, even for sanity check, to plot the eigenspectra of the solutions recovered by the algorithms to see the order of critical points recovered.

---

I appreciate the work but I do not think the paper is clear enough. 
Moreover, the authors say ""local minimia"" ~70 times but do not show (except for Figure 11?) that the solutions found are not necessarily local minima. 
The authors do not talk about that fact that slices of a non-convex problem can look like the ones they show. 
It is well-known that the first-order methods may just fail to deal with certain non-convex ill-conditioned problems even in low-dimensional noiseless cases, the place/solution where they fail to make progress is not necessarily a local minimum. 
Some sentences like the one given below suggest that the study is too superficial:  
""One of the interesting empirical observation is that we often observe is that the incremental improvement
of optimization methods decreases rapidly even in non-convex problems.""

---

This paper provides an extensive analysis of the error loss function for different optimization methods. The presentation is well done and informative. The experimental procedure is clarified sufficiently well. Theoretical evaluations like this are crucial for a wide range of applications and help to better understand and improve the convergence behavior for a given system.

Pros:

- Important analysis
- Good visualizations

Cons:

- The paper describes mostly the observation that the optima vary for different methods, however doesn't attempt to explain why it happens and how to solve it (aside from batch-norm)
- Some fonts are very small (e.g. Fig. 5)",,,,,,4.5,,,3.75,,
566,"Authors: Melanie Ducoffe, Frederic Precioso
Source file: 566.pdf

ABSTRACT
One main concern of the deep learning community is to increase the capacity of representation of deep networks by increasing their depth. This requires to scale up the size of the training database accordingly. Indeed a major intuition lies in the fact that the depth of the network and the size of the training set are strongly correlated. However recent works tend to show that deep learning may be handled with smaller dataset as long as the training samples are carefully selected (let us mention for instance curriculum learning). In this context we introduce a scalable and efficient active learning method that can be applied to most neural networks, especially Convolutional Neural Networks (CNN). To the best of our knowledge, this paper is the first of its kind to design an active learning selection scheme based on a variational inference for neural networks. We also deduced a formulation of the posterior and prior distributions of the weights using statistical knowledge on the Maximum Likelihood Estimator. We describe our strategy to come up with our active learning criterion. We assess its consistency by checking the accuracy obtained by successive active learning steps on two benchmark datasets MNIST and USPS. We also demonstrate its scalability towards increasing training set size.

1 INTRODUCTION
A daunting challenge in many contexts is to gather annotated data. This can be a long and tedious process, which often slows down the development of a framework and may jeopardize its economic prospects.
We refer to active learning Cohn (1994) as the field of machine learning which targets building iteratively the annotated training set with the help of an oracle.
In this setting and in a context of pool-based active learning1, a model is trained on a small amount of data (i.e. the initial training set) and a scoring function discriminates samples which should be labeled by the oracle from the ones which do not hold new information for the model. The queried samples are then submitted to an oracle (which can be another decision algorithm for instance in co-training context, or a human expert in interactive learning context) to be labeled. They are then added to the current training set. Finally the model is retrained from scratch. This process is repeated recursively to grow the training set.
Although active learning and deep learning represent two important pillars of machine learning, they have mainly coexisted into independent stream of works owing to the complexity of combining them. The main issues are the scalability and the adaptability of common active learning schemes when considering architectures with a huge number of parameters such as deep networks. Another issue lies in the overall number of training iterations since training a deep architecture remains a computationally expensive process despite all the optimizations through GPU processing. This specificity has prevented deep learning from being prevalent within active learning. Indeed seminal active learning frameworks Cohn (1994) have mainly focused on adding one sample at-a-time. When it comes to selecting a batch of queries, the most intuitive solution is to select top scoring samples.
1Other settings exist but are not considered here for the sake of clarity, we refer the reader to Settles (2012)
Such a solution is immediate in the process but fails to model the correlations between samples. Labeling one sample at-a-time may therefore lead to the labeling of another sample totally useless.
In our work, batches of actively selected samples are added at each training iteration. We propose a batch active learning framework designed for deep architectures, especially Deep Convolutional Neural Networks (CNN).
Batch active learning is highly suitable for deep networks which are trained on minibatches of data at each iterations. Indeed training with minibatches help the training of deep networks, and we empirically noticed that the size of the minibatch is a major hyperparameter. Thus it makes sense to query a batch of unlabelled data whose size would be proportionnal to the size of a minibatch. In our work, batches have the same size as the minibatches but they could be decorrelated by considering importance sampling techniques.
Our model focuses on log loss which is involved in the training process of most neural networks. To achieve the required scalability of active learning for deep architectures, we step away from traditional active learning methods and focus our attention on a more general setting: Maximum Likelihood Estimation (MLE) and Bayesian inference. Provided certain assumptions, our active selection relies on a criterion which is based on Fisher information and is obtained from the minimization of a stochastic method for variational inference. Our active selection relies on the Fisher matrices on the unlabeled data and on the data selected by the active learning step. An approximation of Fisher information, based on a diagonal Kronecker-block decomposition makes our criterion computationally affordable for an active greedy selection scheme.
Variational methods have been previously explored like in Graves (2011) as a tractable approximation to Bayesian inference for Artificial Neural Networks (ANNs). One advantage of such a representation is that it leads to a two-term active learning criterion: one related to the prediction accuracy on the observed data and the second term expressing the model complexity. Such a two-fold criterion is, to the best of our knowledge, the first of the kind scalable for active learning. Such an expression may help both to analyze and to optimize ANNs, not only in an active learning framework but also for curriculum learning.
We dedicate section Related works to the presentation of active learning literature. Section Covering presents the theoretical aspects of our active learning framework, while section Active learning as a greedy selection scheme details our greedy algorithm. We then assess our active learning method through experiments on MNIST and USPS benchmarks. We discuss possible improvements of our approach and connections with previous MLE-based active learning methods before concluding.

2 RELATED WORKS
Active learning is a framework to automatize the selection of instances to be labeled in a learning process. Active learning offers a variety of strategies where the learner actively selects which samples seem “optimal” to annotate, so as to reduce the size of the labeled training set required to achieve equivalent performance.
We consider the context of pool-based active learning where the learner selects its queries among a given unlabeled data set. For other variants (query synthesis, (stream-based) selective sampling) we refer the reader to Settles (2012).

2.1 POOL-BASED ACTIVE LEARNING
When it comes to pool-based active learning, the most intuitive approaches focus on minimizing some error on the target classifier. Uncertainty sampling minimizes the training error by querying unlabeled data on which the current classifier (i.e. from previous training iteration) is assigning labels with the weakest confidence. This method, uncertainty sampling, while being the least computational consuming among all active learning techniques has the main drawback of ignoring much of the output distribution classes, and is prone to querying outliers. Thanks to its low cost and easy setup, uncertainty has been adapted to deep architectures for sentiment classification Zhou et al. (2010). However, deep architectures are subject to adversarial examples, a type of noise we suspect uncertainty selection to be highly sensitive to Szegedy et al. (2014); Goodfellow et al. (2015). Other strategies (expected error reduction, expected output variance reduction) directly minimize the
error on a validation set after querying a new unlabeled sample. However they are computationally expensive especially when considering neural networks.
Traditional active learning techniques handle selection of one sample at-a-time. One of the main drawbacks of the aforementioned active learning techniques is that is does not pay attention to the information held in unlabeled data besides considering it as potential queries. Hence once the strategy for selecting samples to be labeled and added to the training set is defined, the question on the impact of the possible correlation between successive selected samples remains.
To that end, one recent class of methods deals with the selection of a batch of samples during the active process, batch mode active learning. Batch mode active learning selects a set of most informative unlabeled sample instead of a unique sample. Such a strategy is highly suitable when retraining the model is not immediate and require to restart the training from scratch at each iteration as it is the case for neural networks. A simple strategy (whose has also been used for previous deep active learning strategy Zhou et al. (2010)) is to select a batch of top scoring instances. However that strategy fails to consider the correlation among pairs of samples. The redundancy between the so-selected samples may therefore hinder the learning process.
In the context of a deep learning scenario, if several elements related to the same direction of the gradient are in the same minibatch, the gradient descent in the next learning step may lead at once too close to a local minimum, diverting the process away from the global minimum.
While one sample at-a-time can prevent from being misled that way, it gets prohibitive when considering big data because the number of iterations is equal to the number of learning samples, unlike the batch-based strategies.
Recently some solutions have been proposed for choosing an appropriate subset of samples so as to minimize any significant loss in performance. Those methods consider the minimization of the Kullback-Leibler (KL) divergence between the resampled distribution of any possible subset selected for the active process and the whole unlabeled data set distribution. A lower bound of the negative of this KL divergence is then defined and rewritten as a submodular function. The minimization of the initial KL divergence becomes then a problem of submodular maximization Hoi et al. (2006). In Wei et al. (2015), Wei et al have designed several submodular functions so as to answer at best the need of specific classifiers (Naive Bayes Classifiers, Logistic Regression Classifier, Nearest Neighbor Classifier). However, their approach is hardly scalable to handle all the information from non-shallow classifiers such as deep networks.
Another solution to minimize the correlation among a set of queries is to perform bayesian inference on the weights of a neural network. In a bayesian context, a neural network is considered as a parametric model which assigns a conditional probability on the observed labeled data A given a set of weights w. The weights follow some prior distribution P (α) depending on the parameter α and the posterior distribution of the weights P (w | A, α) is deduced. The goal is thus to maximize the posterior probability of the weights on the observed data A. Indeed bayesian inference expresses the uncertainty of the weights which consequently leads to a relevant exploration of the underlying distribution of the input data X. When it comes to active learning, the learner needs not only to estimate the posterior given the observed data A but also to consider the impact of new data on that posterior Golovin et al. (2010). In our context, bayesian inference is intractable, partially due to the high number of weights involved in a deep network. To solve this issue, Graves (2011) introduced a variational approximation to perform bayesian inference on neural networks. Specifically he approximates the posterior distribution P (w | A, α) with a tractable distribution of his choice Q(w | β) depending on a new parameter β. The quality of the approximation Q(w | β) compared to the true posterior P (w | A, α) is measured by the variational free energy F with respect to the parameters α and β. F has no upper bound but gets closer to zero as both distributions become more and more similar.
F(α, β) = −Ew∼Q(β)
( ln ( P (A | w)P (w | α)
Q(w | β)
)) (1)
Finally in Graves (2011), F is then expressed as a minimum description loss function on α and β: F(α, β) = Ew∼Q(β)(L(A;w)) +KL(Q(w) || P(α)) (2)
where KL is the Kullback Leibler divergence between Q(β) and P(α).
Under certain assumptions on the family distribution for the posterior and prior of the weights ( diagonal gaussian ...), Graves proposed a backpropagation compatible algorithm to train an ensemble of networks, whose weights are sampled from a shared probability distribution.
The primary purpose of the variational free energy is to propose a new training objectif for neural network by learning α and β. In an active learning context, the main drawback of variational free energy based method is that it requires to update by backpropagation the parameters for each unlabeled data submitted as a query. However we know from statistical assumption on the maximum likelihood the posterior and prior distribution of trained weights given the current labeled training set: if and only if we consider trained networks, we know how to build α and β in a unique iteration without backpropagation. This knowledge helps us to extend Graves first objectives to the use of variational free energy to measure how new observations affect the posterior.

3 COVERING

3.1 VARIATIONAL INFERENCE ON NEURAL NETWORK WITH GAUSSIAN POSTERIOR WEIGHT DISTRIBUTION
As done for the majority of neural networks, we measure the error of the weights w by the negative log likelihood on an observed set of annotated data A:
L(A;w) = − ∑
(x,y)∈A
ln(P (y | x,w)) (3)
We consider the Maximum Likelihood Estimator (MLE) W as the value which makes the data observed A as likely as possible for a fixed architecture. Note than even for a fixed A in the case of neural network,W may not be unique.
W = argminwL(A;w) (4)
When assuming that an arbitrary parameterW∗ is governing the data generation process, we know that the expected negative log likelihood is lower bounded by the expected negative log likelihood of the true parameterW∗ governing the data generation process. What it means is that no distribution describes the data as well as the true distribution that generated it. It turns out that, under certain assumptions, we can prove using the central limit theorem that the MLE is asymptotically normal with a mean equal to the true parameter value and variance equal to the inverse of the expected Fisher information evaluated at the true parameter.
If we denote by X the underlying data distribution,WX the MLE andW∗X the true parameter, we know thatWX is a sample from a multivariate gaussian distribution parametrized byW∗X . Note that in this context we assume the Fisher matrices are invertible.
WX ∼ N (W∗X , I−1X (W ∗ X)) (5)
However the expected Fisher information on the underlying distribution is intractable. Eventually, using the law of large numbers, we know that the observed Fisher information converges to the expected Fisher information as the sample size increases. Another theoretical limitation is that the true parameter is unknown but we can approximate its observed Fisher information with the observed Fisher information at the MLE because of the consistency of the MLE. For a sake of simplicity we keep the same notation for observed and expected Fisher matrix.
Let denote by Y the random variable after resampling the underlying distribution X using an active learning strategy. W∗X,W∗Y are the true parameters with respect to their respective data distributions and their respective MLE variablesWX,WY, then the following relations hold:
WX ∼ N (W∗X, I−1X (W ∗ X)) WY ∼ N (W∗Y, I−1Y (W ∗ Y))
(6)
We thus notice than in an active learning context, the learner is trained on data uniformly sampled from Y, while the optimal solution would be when training on data uniformly sampled from X.
The asymptotic behaviour provides us with a prior distribution of the weights based on the data distribution X. In our context of active learning, we approximate the posterior distribution with the MLE distribution induced by the resampling Y. Hence we define a prior and posterior distribution which did not require to be learnt by backpropagation directly but depend on the two data distribution X and Y.
P (α) ≡ P (αX) = N (W∗X, I−1X (W ∗ X)) Q(β) ≡ Q(βY) = N (W∗Y, I−1Y (W ∗ Y))
(7)
Our active learning scheme relies on the selection of input data whose induced MLE distribution Q(βY) is minimizing the variational free energy.
Y = argminY F(αX, βY) (8)
It consists in the minimization of the sum of two terms which we denote respectively by the training factor EW∼Q(β)(L(A;W)) and the generalization factor KL(Q(β) || P(α)). It is possible to analyze both terms independently and explain their role into the minimization:
• Training factor: Ideally, the Cramer Rao bound implies that the minimum on the training factor is reached whenQ(β) matches the asymptotically most efficient estimator of the optimal parameter on the error loss on the observed data. Hence the training factor corresponds to the minimization of the error on the observed data A.
• Generalization factor: Empirical results Choromanska et al. (2015) tend to show that the variance of the accuracy diminishes as the depth of the classifier increases. So our ultimate goal would be to converge to any set of parameters of the MLE distribution as their effectiveness is similar. The goal of the generalization factor is to converge to the asymptotic distribution on the whole input distribution X and to minimize the error of prediction of new input data.
If the expression of the variational free energy provides us a theoretical context to work on, the usage of Fisher matrices of deep networks renders it computationally unaffordable. Indeed the Fisher matrix is a quadratic matrix in terms of the number of parameters of the deep networks. Because of the huge number of parameters involved, such matrices takes a lot of memory and processing them costs a lot of ressources, especially if the operations may be repeated often in the framework (as it would be the case for every possible query processed by an active learning scheme.)
The next section 3.2 explains the different approximation proposed to deduce a more friendly user criterion.

3.2 APPROXIMATIONS

3.2.1 KRONECKER FACTORED APPROXIMATION OF THE FISHER INFORMATION
OF A CNN
Recently an approximation of the Fisher information for deep architectures has been proposed first for fully connected layer in Martens & Grosse (2015), and then for convolutional layer as well in Grosse & Martens (2016). The block kronecker decomposition content (ψ, τ ) is explained in Martens & Grosse (2015); Grosse & Martens (2016)
Based on their decomposition definition, we define the evaluation of blocks of the Fisher information at a certain point xi(ψxi,l, τxi,l) and an empirical estimation of the Fisher matrix on a set of data A. A sum up of their decomposition is presented in Eq. (9) while the exact content of the kronecker
blocks ψ and τ is left as undescribed in this paper for the sake of concision.
IA(W) = diag([ψA,l(W)⊗ τA,l(W)]Ll=1)
ψA,l(W) = 1 | A | ∑ xi∈A ψxi,l(W)
τA,l(W) = 1 | A | ∑ xi∈A τxi,l(W)
(9)
The strength of this decomposition lies in the properties of block diagonal combined with those of the kronecker product. ψ and τ are respectively related to the covariance matrix of the activation and the covariance of the derivative given the input of a layer. Recent deep architectures tend to prevail the depth over the width (i.e. the number of input and output neurons) so this expression becomes really suitable and tractable.

3.3 APPROXIMATION OF THE TRAINING FACTOR
Despite the block kronecker product approximation of the Fisher matrix, sampling on Q(β) requires to compute the inverse. Because the kronecker blocks may still have an important number of parameters involved (especially the first fully connected layer suceeding to a convolutional layer), the inverse of the blocks may be still too computationally expensive. To approximate the training factor, we opt for a second order approximation of the log likelihood for parametersW close to the mean parameterW∗Y of Q(β).
L(A;W) ≈ L(A;W∗Y) + ∂L(A;W∗Y)
∂W + (W −W∗Y)T ∂2L(A;W∗Y) ∂W ′∂W (W −W∗Y) (10)
Our first approximation consists in assuming that the MLE parameter ŵ of the currently trained network is a good approximator ofW∗Y. Because the network has converged on the current set of observed data A the first derivative of the log likelihood is also set to zero. Hence Eq. (10) thus becomes:
L(A;W) ≈ L(A; ŵ) + (W − ŵ)T I−1A (ŵ)(W − ŵ) (11)
To compute the expectation over the range of weights sampled fromQ(β) we need to upperbound the expectation of the dot product ofW given the Fisher matrix. Because we assume our Fisher matrices invertible, and because a covariance matrix is at least semi-definite, our Fisher matrices are positive definite matrix. Hence every eigenvalue is positive and the trace of the Fisher matrix is greater than its maximum eigenvalue. From basic properties of the variance covariance matrix, if we denote by N the number of parameters in the network we obtain the following upperbound for the training factor:
EW∼Q(β)(L(A;W)) ≤ L(A; ŵ) + N√ π Tr(I−1Y (ŵ) T I−1A (ŵ)I −1 Y (ŵ)) (12)
When it comes to the trace of the inverse, we approximate it by the closest lower bound with the inverse of the trace like in Wei et al. (2015).
EW∼Q(β)(L(A;W)) ∝∼ L(A; ŵ) + N√ π
N2
Tr(IY(ŵ)IA(ŵ)IY(ŵ)T ) (13)

3.4 APPROXIMATION OF THE GENERALIZATION FACTOR
Our generalization factor corresponds to the KL divergence between the approximation of our posterior Q(β) and the prior P(α). Because both distributions are multivariate gaussians, we have a direct formulation of the KL which is always definite since the Fisher matrices are invertible:
KL(Q(β) || P(α)) = 1 2
( ln ( det(I−1X (W∗X)) det(I−1Y (W∗Y)) ) −N+Tr(IX(W∗X)I−1Y (W ∗ Y))+(W∗X−W∗Y)T IX(W∗X)(W∗X−W∗Y) ) (14)
Our first approximation consists in assuming that the MLE parameter ŵ of the currently trained network is a good approximator of both optimal parametersW∗X,W∗Y like in Zhang & Oles (2000). We also upper bound the determinant with a function of the trace and the number N of parameters. When it comes to the trace of the inverse, we approximate it again by the closest lower bound with the inverse of the trace.
KL(Q(β) || P(α)) ∝∼ N
( ln ( Tr(IY(ŵ)I−1X (ŵ)) ) +
N
Tr(IY(ŵ)I−1X (ŵ))
) (15)

3.5 APPROXIMATION OF THE VARIATIONAL FREE ENERGY
In the previous subsections, we proposed independent approximations of both our sub-criteria: the training factor and the generalization factor. However the scale of our approximations may not be balanced so we sum up our criterion with an hyperparameter factor γ which counterparts the difference of scale between the factors:
F ∝∼ γ ( N√ π
N2
Tr(IY(ŵ)IA(ŵ)IY(ŵ)T )
) +N ( ln ( Tr ( IY(ŵ)I−1X (ŵ) )) +
N
Tr(IY(ŵ)I−1X (ŵ)) ) (16)
We approximate the expected Fisher matrices on the underlying distribution Y and X by the observed Fisher matrices on a set of data sampled from those distributions. This approximation is relevant due to the consistenty of the MLE.
As we are in a pool-based selection case, we dispose at first of two sets of data: A and U which denote respectively the annotated observed data and unlabeled data. Note that the derivatives in the Fisher matrix computation implies to know the label of the samples. Thus at each active learning step, an unknown label is approximated by its prediction from the current trained network. We denote by S the subset of data to be queried by an oracle. The size of S is fixed with | S |= K. S is the subset sampled from Y while U is sampled from X. Finally an approximation of F will be:
F ∝∼ γ ( N√ π
N2
Tr(IS(ŵ)IA(ŵ)IS(ŵ)T )
) +N ( ln ( Tr ( IS(ŵ)I−1U (ŵ) )) +
N
Tr(IS(ŵ)I−1U (ŵ)) ) (17)
Now we express the trace based on the approximation of the Fisher matrix: we consider that every Fisher matrix for CNN is a L diagonal block matrix, with L the number of layers of the CNN. Every block is made of a kronecker product of two terms ψ and τ . We rely on the properties involved by the choice of this specific matrix topology to obtain a more computationally compliant approximation of F in Eq. (18):
F ∝∼γ ( N√ π N2∑ l∈(1,L) Tr(ψS,l(ŵ)ψ −1 A,l(ŵ)ψS,l(ŵ) T )Tr(τS,l(ŵ)τ −1 A,l(ŵ)τS,l(ŵ) T ) )
+N ( ln ( ∑ l∈(1,L) Tr ( ψS,l(ŵ)ψ −1 U,l(ŵ) ) Tr(τS,l(ŵ)τ −1 U,l (ŵ)) )
+ N∑
l∈(1,L) Tr(ψS,l(ŵ)ψ −1 U,l(ŵ))Tr(τS,l(ŵ)τ −1 U,l (ŵ))
) (18)

4 ACTIVE LEARNING AS A GREEDY SELECTION SCHEME ON THE VARIATIATIONAL FREE ENERGY
The selected subset S selected at one step of active learning is only involved through the kronecker product of the Fisher matrix IS(ŵ). We express our approximation of the free energy by a criterion
on the subset S in Eq. (19): C(S;A,U) =γ ( N√ π N2∑ l∈(1,L) Tr(ψS,l(ŵ)ψA,l(ŵ)ψS,l(ŵ) T )Tr(τS,l(ŵ)τA,l(ŵ)τS,l(ŵ)T ) )
+N ( ln ( ∑ l∈(1,L) Tr(ψS,l(ŵ)ψ −1 U,l(ŵ))Tr(τS,l(ŵ)τ −1 U,l (ŵ)) )
+ N∑
l∈(1,L) Tr(ψS,l(ŵ)ψ −1 U,l(ŵ))Tr(τS,l(ŵ)τ −1 U,l (ŵ)) ) (19)
Finally we estimate our subset S by a greedy procedure: to be more robust to outliers and for reasons of computational efficiency, we select first a pool of samples D ⊂ U which we will use as the set of possible queries. We recursively build S ⊂ D by picking the next sample xi ∈ D which minimizes C(S ∪ {xi};A,U) among all remaining samples in D. When it comes to the training factor coefficient, we notice that it is a quadratic term in IS(ŵ) which increases the complexity in a greedy selection scheme. Our choice is to estimate the trace in the following way:
Tr(ψS∪{x},l(ŵ)ψA,l(ŵ)ψS∪{x},l(ŵ) T ) ≈ Tr(ψS,l(ŵ)ψA,l(ŵ)ψS,l(ŵ)T )+Tr(ψ{x},l(ŵ)ψA,l(ŵ)ψ{x},l(ŵ)T )
Pseudo-code and illustration of the algorithm are provided in table 1 in appendix.

5 EXPERIMENTS
We demonstrate the validity of our approach on two datasets: MNIST (28-by-28 pictures, 50.000 training samples, 10.0000 validation samples and 10.000 test samples) and USPS (16-by-16 pictures, 4185 training samples, 464 validation samples and 4649 testing samples) both gray scaled digits image datasets. We describe the CNN configuration and the hyperparameters settings in table 2 in appendix. Note that we do not optimize the hyperparameters specifically for the size of the current annotated training set A. We picked those two similar datasets to judge of the robustness of our method against different size of unlabeled datasets, as expected our method is efficient on both small and large databases.

5.1 TEST ERROR
We run 10 runs of experiments and average the error on the test set of the best validation error before a pass of active learning. We start from an annotated training set of the size of one minibatch selected randomly. We stop both set of experiments after 30% of the training set has been selected (15.000 image for MNIST, 1255 for USPS). We compare the lowest test error achieved so far by our MLE based method against naive baselines: uncertainty sampling, curriculum sampling and a random selection of a minibatch of examples. We measure both uncertainty and curriculum scores based on the log likelihood of a sample using as label its prediction on the full network. While uncertainty selects samples with the highest log likelihood, our version of curriculum does the exact contrary. We select randomly the set of possible queries D among the unlabeled training data. Its size is set to 30 times the minibatch size. We present the results in two phases for the sake of clarity in figure 1 for MNIST and figure 2 for USPS: the first rounds of active learning when the annotated training set is almost empty, and the second round which is more stable in the evolution of the error. In both phases and for both databases we observe a clear difference between the test error achieved by our MLE method with the test error obtained by selecting randomly the data to be queried. Moreover the error achieved by our method on 30 % is close (even equivalent in the case of USPS), to the error achieved using the standard full training sets defined for both datasets (this error rate is defined as yellow line groundtruth on the figures). The experiments made appear that curriculum learning is not a good active learning strategy for both tested datasets. As for the uncertainty selection, it works really well on MNIST while it fails on USPS. While MNIST is a pretty clean database, USPS contains more outliers and noisy samples rendering it more difficult in terms of accuracy even though both databases are designed to assess digit classification. As other works we mentioned in the related work section, we are led to explain uncertainty selection to select useless samples with the amount of outliers and noisy samples in USPS.

5.2 TIME COMPLEXITY
To validate our method in terms of scalability and time complexity, we measured in seconds, the current processor time for one pass of active learning. We repeated this evaluation for different size of query (8 to 128 unlabeled samples added to the currrent training set). For this experiments we used a laptop with a Titan-X (GTX 980 M) with 8 GB RAM GPU memory. Metrics were reported in figure 3. Our criterions takes few seconds to select a batch of query of hundreds of unlabeled data. Moreover the evolution of the time given the size of the query is less than linear.
Page 1
9

6 DISCUSSION
We believe our method is a proof of concept for the use of variational inference for active learning on deep neural networks. However our approximations are subject to improvements which may lead to faster convergence and lower generalization error.
The first point to raise is that our approximation of the posterior is an asymptotic distribution which may be unstable on a small subset of observed data, as it is the case for active learning. Such a distribution may be regularized by taking the probability provided by the central limit theorem about how well our data fits to the asymptotic gaussian distribution. When it comes to the KFAC approximation, it suffers from the same issue and could be regularized when evaluating on small subset. A refinement of the approximations, especially for the generalization factor, following the approaches of submodular functions may be investigated.
Finally, an interesting observation is that our formulation of the variational free energy finds similarities with other MLE based active learning criteria previously proposed in the litterature. Indeed, in Zhang & Oles (2000) the authors study active learning by looking among the possible resampling of the input distribution. They formulate their criterion as the minimization of the trace of the inverse Fisher of the resampled distribution multiplied by the Fisher matrix on the input distribution: minS Tr(I−1S (ŵ)IU (ŵ))

7 CONCLUSION
In a nutshell, we proposed a scalable batch active learning framework for deep networks relying on a variational approximation to perform bayesian inference. We deduced a formulation of the posterior and prior distributions of the weights using statistical knowledge on the Maximum Likelihood Estimator. Those assumptions combined with an existing approximation of the Fisher information for neural network, lead us to a backpropagation free active criterion. Eventually we used our own approximations to obtain a greedy active selection scheme.
Our criterion is the first of the kind to scale batch active learning to deep networks, especially Convolutional Neural Networks. On different databases, it achieves better test accuracy than random sampling, and is scalable with increasing size of queries. It achieves near optimal error on the test set using a limited percentage (30%) of the annotated training set on larger and more reduced dataset. Our works demonstrated the validity of batch mode active learning for deep networks and the promise of the KFAC approximations for deep Fisher matrices for the active learning community. Such a solution is also interesting as a new technique for curriculum learning approach.
","One main concern of the deep learning community is to increase the capacity of representation of deep networks by increasing their depth. This requires to scale up the size of the training database accordingly. Indeed a major intuition lies in the fact that the depth of the network and the size of the training set are strongly correlated. However recent works tend to show that deep learning may be handled with smaller dataset as long as the training samples are carefully selected (let us mention for instance curriculum learning). In this context we introduce a scalable and efficient active learning method that can be applied to most neural networks, especially Convolutional Neural Networks (CNN). To the best of our knowledge, this paper is the first of its kind to design an active learning selection scheme based on a variational inference for neural networks. We also deduced a formulation of the posterior and prior distributions of the weights using statistical knowledge on the Maximum Likelihood Estimator. We describe our strategy to come up with our active learning criterion. We assess its consistency by checking the accuracy obtained by successive active learning steps on two benchmark datasets MNIST and USPS. We also demonstrate its scalability towards increasing training set size.",ICLR 2017 conference submission,False,,"This paper introduces a mechanism for active learning with convolutional neural networks (CNNs). I would not go as far as the authors in calling these ""deep"", seeing that they seem to have only 2 hidden layers with only 20 filters each. The active learning criterion is a greedy selection scheme based on variational free energy and a series of approximations.

The paper is sometimes hard to read, due to (a) many grammatical errors and (b) sloppy notation in some places (e.g., on page 5, line 1, f is used but never introduced before). Overall, I give an accepting score, but a weak one because of the grammatical errors. If the paper is accepted, these should be fixed for the final version, optimally by a native speaker. 

The paper's topic is interesting, and the paper appears to succeed in its goal of showing a proof of concept for active learning in CNNs (if only on toy datasets). I'm surprised by the new results on uncertainty sampling and curriculum learning the authors added: why do these methods both break for USPS? In particular, uncertainty sampling did very well (in fact, better than the authors' new method) on MNIST, but apparently horribly on USPS; some explanation for this would be useful.

I have one more question: why is it necessary to first sample a larger subset D \subset U, from which we select using active learning? Is this merely done for reasons of computational efficiency, or can it actually somehow improve results? (If so, it would be instrumental to see the worse results when this is not done.)

---

The reviewers agree that the paper pursues an interesting direction to explore active example selection for CNN training, but have unanimously raised serious concerns with regards to overall presentation which needs further improvement (I still see spelling/grammatical errors/sloppy notation in the latest draft). Some sections in the paper are hard to follow. With regards to technical motivation, the link between depth and need for active example selection is alluded to, but not properly explained in the paper. The PCs think that this paper has too many areas in need of improvement to be accepted to the conference.

---

The paper proposes to perform active learning using pool selection of deep learning mini-batches using an approximation of the bayesian posterior. Several terms are in turn approximated.

The Maximum Likelihood Estimation (MLE) bayesian inference approach to active learning, the various approximations, and more generally the theoretical framework is very interesting but difficult to follow.

The paper is written in poor English and is sometimes a bit painful to read.

Alternative Active learning strategies and techniques do not need to be described with such detail. On the other hand, the proposed approach has a lot of complex approximations which would benefit from a more detailed/structured presentation. Another dataset would be a big plus (both datasets concern gray digits and USPS and are arguably somewhat similar).

---

Quality:
The paper initiates a framework to incorporate active learning into the deep learning framework, mainly addressing challenges such as scalability that accompanies the training of a deep neural network. 
However, I think the paper is not well polished; there are quite a lot of grammatical and typing errors.

Clarity:
The paper needs major improvements in terms of clarity. The motivations in the introduction, i.e., why it is difficult to do active learning in deep architectures, could be better explained, and tied to the explanation in Section 3 of the paper. For example, the authors motivated the need of (mini)batch label queries, but never mention it again in Section 3, when they describe their main methodology. 
The related work section, although appearing systematic and thorough, is a little detached from the main body of the paper (related work section should not be a survey of the literature, but help readers locate your work in the relevant literature, and highlight the pros and cons. In this perspective, maybe the authors could shorten some explanations over the related work that are not directly related, while spending more time on discussing/comparing with works that are most related to your current work, e.g., that of Graves '11.

Originality & Significance:
The authors proposed an active learning training framework. The idea is to treat the network parameter optimization problem as a Bayesian inference problem (which is proposed previously by Graves) and formulate the active learning problem as that of sampling the most informative data, where the informativeness is defined by the variational free energy, which depends on the Fisher information. To reconcile the computational burden of computing the inverse of Fisher Information matrix, the authors proposed techniques to approximate it (which seems to be novel)

I think that this paper initiates an interesting direction: one that adapts deep learning to label-expensive problems, via active learning. But the paper needs to be improved in terms of presentation.

---

This paper introduces a mechanism for active learning with convolutional neural networks (CNNs). I would not go as far as the authors in calling these ""deep"", seeing that they seem to have only 2 hidden layers with only 20 filters each. The active learning criterion is a greedy selection scheme based on variational free energy and a series of approximations.

The paper is sometimes hard to read, due to (a) many grammatical errors and (b) sloppy notation in some places (e.g., on page 5, line 1, f is used but never introduced before). Overall, I give an accepting score, but a weak one because of the grammatical errors. If the paper is accepted, these should be fixed for the final version, optimally by a native speaker. 

The paper's topic is interesting, and the paper appears to succeed in its goal of showing a proof of concept for active learning in CNNs (if only on toy datasets). I'm surprised by the new results on uncertainty sampling and curriculum learning the authors added: why do these methods both break for USPS? In particular, uncertainty sampling did very well (in fact, better than the authors' new method) on MNIST, but apparently horribly on USPS; some explanation for this would be useful.

I have one more question: why is it necessary to first sample a larger subset D \subset U, from which we select using active learning? Is this merely done for reasons of computational efficiency, or can it actually somehow improve results? (If so, it would be instrumental to see the worse results when this is not done.)

---

Some of the approximations are quite complex; is your code available?

---

In Figures 1 and 2, what do you mean by groundtruth?

---

Section 5.2 studies the time complexity of your approach -- up to 30s to select the elements of one minibatch. How does this compare to the time required for using that minibatch to update the model by backprop?

---

This paper introduces a mechanism for active learning with convolutional neural networks (CNNs). I would not go as far as the authors in calling these ""deep"", seeing that they seem to have only 2 hidden layers with only 20 filters each. The active learning criterion is a greedy selection scheme based on variational free energy and a series of approximations.

The paper is sometimes hard to read, due to (a) many grammatical errors and (b) sloppy notation in some places (e.g., on page 5, line 1, f is used but never introduced before). Overall, I give an accepting score, but a weak one because of the grammatical errors. If the paper is accepted, these should be fixed for the final version, optimally by a native speaker. 

The paper's topic is interesting, and the paper appears to succeed in its goal of showing a proof of concept for active learning in CNNs (if only on toy datasets). I'm surprised by the new results on uncertainty sampling and curriculum learning the authors added: why do these methods both break for USPS? In particular, uncertainty sampling did very well (in fact, better than the authors' new method) on MNIST, but apparently horribly on USPS; some explanation for this would be useful.

I have one more question: why is it necessary to first sample a larger subset D \subset U, from which we select using active learning? Is this merely done for reasons of computational efficiency, or can it actually somehow improve results? (If so, it would be instrumental to see the worse results when this is not done.)

---

The reviewers agree that the paper pursues an interesting direction to explore active example selection for CNN training, but have unanimously raised serious concerns with regards to overall presentation which needs further improvement (I still see spelling/grammatical errors/sloppy notation in the latest draft). Some sections in the paper are hard to follow. With regards to technical motivation, the link between depth and need for active example selection is alluded to, but not properly explained in the paper. The PCs think that this paper has too many areas in need of improvement to be accepted to the conference.

---

The paper proposes to perform active learning using pool selection of deep learning mini-batches using an approximation of the bayesian posterior. Several terms are in turn approximated.

The Maximum Likelihood Estimation (MLE) bayesian inference approach to active learning, the various approximations, and more generally the theoretical framework is very interesting but difficult to follow.

The paper is written in poor English and is sometimes a bit painful to read.

Alternative Active learning strategies and techniques do not need to be described with such detail. On the other hand, the proposed approach has a lot of complex approximations which would benefit from a more detailed/structured presentation. Another dataset would be a big plus (both datasets concern gray digits and USPS and are arguably somewhat similar).

---

Quality:
The paper initiates a framework to incorporate active learning into the deep learning framework, mainly addressing challenges such as scalability that accompanies the training of a deep neural network. 
However, I think the paper is not well polished; there are quite a lot of grammatical and typing errors.

Clarity:
The paper needs major improvements in terms of clarity. The motivations in the introduction, i.e., why it is difficult to do active learning in deep architectures, could be better explained, and tied to the explanation in Section 3 of the paper. For example, the authors motivated the need of (mini)batch label queries, but never mention it again in Section 3, when they describe their main methodology. 
The related work section, although appearing systematic and thorough, is a little detached from the main body of the paper (related work section should not be a survey of the literature, but help readers locate your work in the relevant literature, and highlight the pros and cons. In this perspective, maybe the authors could shorten some explanations over the related work that are not directly related, while spending more time on discussing/comparing with works that are most related to your current work, e.g., that of Graves '11.

Originality & Significance:
The authors proposed an active learning training framework. The idea is to treat the network parameter optimization problem as a Bayesian inference problem (which is proposed previously by Graves) and formulate the active learning problem as that of sampling the most informative data, where the informativeness is defined by the variational free energy, which depends on the Fisher information. To reconcile the computational burden of computing the inverse of Fisher Information matrix, the authors proposed techniques to approximate it (which seems to be novel)

I think that this paper initiates an interesting direction: one that adapts deep learning to label-expensive problems, via active learning. But the paper needs to be improved in terms of presentation.

---

This paper introduces a mechanism for active learning with convolutional neural networks (CNNs). I would not go as far as the authors in calling these ""deep"", seeing that they seem to have only 2 hidden layers with only 20 filters each. The active learning criterion is a greedy selection scheme based on variational free energy and a series of approximations.

The paper is sometimes hard to read, due to (a) many grammatical errors and (b) sloppy notation in some places (e.g., on page 5, line 1, f is used but never introduced before). Overall, I give an accepting score, but a weak one because of the grammatical errors. If the paper is accepted, these should be fixed for the final version, optimally by a native speaker. 

The paper's topic is interesting, and the paper appears to succeed in its goal of showing a proof of concept for active learning in CNNs (if only on toy datasets). I'm surprised by the new results on uncertainty sampling and curriculum learning the authors added: why do these methods both break for USPS? In particular, uncertainty sampling did very well (in fact, better than the authors' new method) on MNIST, but apparently horribly on USPS; some explanation for this would be useful.

I have one more question: why is it necessary to first sample a larger subset D \subset U, from which we select using active learning? Is this merely done for reasons of computational efficiency, or can it actually somehow improve results? (If so, it would be instrumental to see the worse results when this is not done.)

---

Some of the approximations are quite complex; is your code available?

---

In Figures 1 and 2, what do you mean by groundtruth?

---

Section 5.2 studies the time complexity of your approach -- up to 30s to select the elements of one minibatch. How does this compare to the time required for using that minibatch to update the model by backprop?",,,,,,6.0,,,1.6666666666666667,,
574,"Authors: FASHION SEARCH, Se-Yeoung Kim, Sang-Il Na, Ha-Yoon Kim, Moon-Ki Kim, Byoung-Ki Jeon, Taewan Kim
Source file: 574.pdf

ABSTRACT
We build a large-scale visual search system which finds similar product images given a fashion item. Defining similarity among arbitrary fashion-products is still remains a challenging problem, even there is no exact ground-truth. To resolve this problem, we define more than 90 fashion-related attributes, and combination of these attributes can represent thousands of unique fashion-styles. We then introduce to use the recurrent neural networks (RNNs) recognising multiple fashion-attributes with the end-to-end manner. To build our system at scale, these fashion-attributes are again used to build an inverted indexing scheme. In addition to these fashion-attributes for semantic similarity, we extract colour and appearance features in a region-of-interest (ROI) of a fashion item for visual similarity. By sharing our approach, we expect active discussion on that how to apply current deep learning researches into the e-commerce industry.

1 INTRODUCTION
Online commerce has been a great impact on our life over the past decade. We focus on an online market for fashion related items1. Finding similar fashion-product images for a given image query is a classical problem in an application to computer vision, however, still challenging due to the absence of an absolute definition of the similarity between arbitrary fashion items.
Deep learning technology has given great success in computer vision tasks such as efficient feature representation (Razavian et al., 2014; Babenko et al., 2014), classification (He et al., 2016a; Szegedy et al., 2016b), detection (Ren et al., 2015; Zhang et al., 2016), and segmentation (Long et al., 2015). Furthermore, image to caption generation (Vinyals et al., 2015; Xu et al., 2015) and visual question answering (VQA) (Antol et al., 2015) are emerging research fields combining vision, language (Mikolov et al., 2010), sequence to sequence (Sutskever et al., 2014), long-term memory (Xiong et al., 2016) based modelling technologies.
These computer vision researches mainly concern about general object recognition. However, in our fashion-product search domain, we need to build a very specialised model which can mimic human's perception of fashion-product similarity. To this end, we start by brainstorming about what makes two fashion items are similar or dissimilar. Fashion-specialist and merchandisers are also involved. We then compose fashion-attribute dataset for our fashion-product images. Table 1 explains a part of our fashion-attributes. Conventionally, each of the columns in Table 1 can be modelled as a multi-class classification. Therefore, our fashion-attributes naturally is modelled as a multi-label classification.
∗This work was done by the author at SK Planet. 1In our e-commerce platform, 11st (http://english.11st.co.kr/html/en/main.html), al-
most a half of user-queries are related to the fashion styles, and clothes.
Multi-label classification has a long history in the machine learning field. To address this problem, a straightforward idea is to split such multi-labels into a set of multi-class classification problems. In our fashion-attributes, there are more than 90 attributes. Consequently, we need to build more than 90 classifiers for each attribute. It is worth noting that, for example, collar attribute can represent the upper-garments, but it is absent to represent bottom-garments such as skirts or pants, which means some attributes are conditioned on other attributes. This is the reason that the learning tree structure of the attributes dependency can be more efficient (Zhang & Zhang, 2010; Fu et al., 2012; Gibaja & Ventura, 2015).
Recently, recurrent neural networks (RNN) are very commonly used in automatic speech recognition (ASR) (Graves et al., 2013; Graves & Jaitly, 2014), language modelling (Mikolov et al., 2010), word dependency parsing (Mirowski & Vlachos, 2015), machine translation (Cho et al., 2014), and dialog modelling (Henderson et al., 2014; Serban et al., 2016). To preserve long-term dependency in hidden context, Long-Short Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997) and its variants (Zaremba et al., 2014; Cooijmans et al., 2016) are breakthroughs in such fields. We use this LSTM to learn fashion-attribute dependency structure implicitly. By using the LSTM, our attribute recognition problem is regarded to as a sequence classification. There is a similar work in Wang et al. (2016), however, we do not use the VGG16 network (Simonyan & Zisserman, 2014) as an image encoder but use our own encoder. To the best of our knowledge, it is the first work applying LSTM into a multi-label classification task in the commercial fashion-product search domain.
The remaining of this paper is organized as follows. In Sec. 2, We describe details about our fashion-attribute dataset. Sec. 3 describes the proposed fashion-product search system in detail. Sec. 4 explains empirical results given image queries. Finally, we draw our conclusion in Sec. 5.

2 BUILDING THE FASHION-ATTRIBUTE DATASET
We start by building large-scale fashion-attribute dataset in the last year. We employ maximum 100 man-months and take almost one year for completion. There are 19 fashion-categories and more than 90 attributes for representing a specific fashion-style. For example, top garments have the Tshirts, blouse, bag etc. The T-shirts category has the collar, sleeve-length, gender, etc. The gender attribute has binary classes (i.e. female and male). Sleeve-length attribute has multiple classes (i.e. long, a half, sleeveless etc.). Theoretically, the combination of our attributes can represent thousands of unique fashion-styles. A part of our attributes are in Table 1. ROIs for each fashion item in an image are also included in this dataset. Finally, we collect 1 million images in total. This internal dataset is to be used for training our fashion-attribute recognition model and fashion-product ROI detector respectively.

3 FASHION-PRODUCT SEARCH SYSTEM
In this section, we describe the details of our system. The whole pipeline is illustrated in Fig. 3. As a conventional information retrieval system, our system has offline and online phase. In offline process, we take both an image and its textual meta-information as the inputs. The reason we take additional textual meta-information is that, for example, in Fig. 1a dominant fashion item in the image is a white dress however, our merchandiser enrolled it to sell the brown cardigan as described
in its meta-information. In Fig. 1b, there is no way of finding which fashion item is to be sold without referring the textual meta-information seller typed manually. Therefore, knowing intension (i.e. what to sell) for our merchandisers is very important in practice. To catch up with these intension, we extract fashion-category information from the textual meta. The extracted fashion-category information is fed to the fashion-attribute recognition model. The fashion-attribute recognition model predicts a set of fashion-attributes for the given image. (see Fig. 2) These fashion-attributes are used as keys in the inverted indexing scheme. On the next stage, our fashion-product ROI detector finds where the fashion-category item is in the image. (see Fig. 8) We extract colour and appearance features for the detected ROI. These visual features are stored in a postings list. In these processes, it is worth noting that, as shown in Fig. 8, our system can generate different results in the fashion-attribute recognition and the ROI detection for the same image by guiding the fashioncategory information. In online process, there is two options for processing a user-query. We can
take a guided information, what the user wants to find, or the fashion-attribute recognition model automatically finds what fashion-category item is the most likely to be queried. This is up to the user's choice. For the given image by the user, the fashion-attribute recognition model generates fashion-attributes, and the results are fed into the fashion-product ROI detector. We extract colour and appearance features in the ROI resulting from the detector. We access to the inverted index addressed by the generated a set of fashion-attributes, and then get a postings list for each fashionattribute. We perform nearest-neighbor retrieval in the postings lists so that the search complexity is reduced drastically while preserving the semantic similarity. To reduce memory capacity and speed up this nearest-neighbor retrieval process once more, our features are binarized and CPU depen-
dent intrinsic instruction (i.e. assembly popcnt instruction2) is used for computing the hamming distance.

3.1 VISION ENCODER NETWORK
We build our own vision encoder network (ResCeption) which is based on inception-v3 architecture (Szegedy et al., 2016b). To improve both speed of convergence and generalization, we introduce a shortcut path (He et al., 2016a;b) for each data-flow stream (except streams containing one convolutional layer at most) in all inception-v3 modules. Denote input of l-th layer , xl ∈ R , output of the l-th layer, xl+1, a l-th layer is a function, H : xl 7→ xl+1 and a loss function, L(θ;xL). Then forward and back(ward)propagation is derived such that
xl+1 = H(xl) + xl (1) ∂xl+1
∂xl = ∂H(xl) ∂xl + 1 (2)
Imposing gradients from the loss function to l-th layer to Eq. (2),
∂L ∂xl := ∂L ∂xL . . . ∂xl+2 ∂xl+1 ∂xl+1 ∂xl
= ∂L ∂xL
( 1+ · · ·+ ∂H(x L−2)
∂xl + ∂H(xL−1) ∂xl ) =
∂L ∂xL
( 1+ l∑ i=L−1 ∂H(xi) ∂xl ) . (3)
As in the Eq. (3), the error signal, ∂L ∂xL
, goes down to the l-th layer directly through the shortcut path, and then the gradient signals from (L − 1)-th layer to l-th layer are added consecutively (i.e.∑l i=L−1 ∂H(xi) ∂xl
). Consequently, all of terms in Eq. (3) are aggregated by the additive operation instead of the multiplicative operation except initial error from the loss (i.e. ∂L
∂xL ). It prevents
from vanishing or exploding gradient problem. Fig. 4 depicts network architecture for shortcut
2http://www.gregbugaj.com/?tag=assembly (accessed at Aug. 2016)
paths in an inception-v3 module. We use projection shortcuts throughout the original inception-v3 modules due to the dimension constraint.3 To demonstrate the effectiveness of the shortcut paths in the inception modules, we reproduce ILSVRC2012 classification benchmark (Russakovsky et al., 2015) for inception-v3 and our ResCeption network. As in Fig. 5a, we verify that residual shortcut paths are beneficial for fast training and slight better generalization.4 The whole of the training curve is shown in Fig. 5b. The best validation error is reached at 23.37% and 6.17% at top-1 and top-5, respectively. That is a competitive result.5 To demonstrate the representation power of our ResCeption, we employ the transfer learning strategy for applying the pre-trained ResCeption as an image encoder to generate captions. In this experiment, we verify our ResCeption encoder outperforms the existing VGG16 network6 on MS-COCO challenge benchmark (Chen et al., 2015). The best validation CIDEr-D score (Vedantam et al., 2015) for c5 is 0.923 (see Fig. 5c) and test CIDEr-D score for c40 is 0.937.7

3.2 MULTI-LABEL LEARNING AS SEQUENCE PREDICTION BY USING THE RNN
The traditional multi-class classification associates an instance x with a single label a from previously defined a finite set of labels A. The multi-label classification task associates several finite sets of labels An ⊂ A. The most well known method in the multi-label literature are the binary relevance method (BM) and the label combination method (CM). There are drawbacks in both BM
3If the input and output dimension of the main-branch is not the same, projection shortcut should be used instead of identity shortcut.
4This is almost the same finding from Szegedy et al. (2016a) but our work was done independently. 5http://image-net.org/challenges/LSVRC/2015/results 6https://github.com/torch/torch7/wiki/ModelZoo 7We submitted our final result with beam search on MS-COCO evaluation server and found out the beam
search improves final CIDEr-D for c40 score by 0.02.
and CM. The BM ignores label correlations that exist in the training data. The CM directly takes into account label correlations, however, a disadvantage is its worst-case time complexity (Read et al., 2009). To tackle these drawbacks, we introduce to use the RNN. Suppose we have random variables a ∈ An, An ⊂ A. The objective of the RNN is to maximise the joint probability, p(at, at−1, at−2, . . . a0), where t is a sequence (time) index. This joint probability is factorized as a product of conditional probabilities recursively,
p(at, at−1, . . . a0) = p(a0)p(a1|a0)︸ ︷︷ ︸ p(a0,a1) p(a2|a1, a0)
︸ ︷︷ ︸ p(a0,a1,a2)
· · ·
︸ ︷︷ ︸ p(a0,a1,a2,... )
(4)
= p(a0) ∏T t=1 p(at|at−1, . . . , a0).
Following the Eq. 4, we can handle multi-label classification as sequence classification which is illustrated in Fig. 6. There are many label dependencies among our fashion-attributes. Direct modelling of such label dependencies in the training data using the RNN is our key idea. We use the ResCeption as a vision encoder θI , LSTM and softmax regression as our sequence classifier θseq, and negative log-likelihood (NLL) as the loss function. We backpropagage gradient signal from the sequence classifier to vision encoder.8 Empirical results of our ResCeption-LSTM based attribute recognition are in Fig. 2. Many fashion-category dependent attributes such as sweetpants, fading, zipper-lock, mini, and tailored-collar are recognized quite well. Fashion-category independent attributes (e.g., male, female) are also recognizable. It is worth noting we do not model the fashionattribute dependance tree at all. We demonstrate the RNN learns attribute dependency structure implicitly. We evaluate our attribute recognition model on the fashion-attribute dataset. We split this dataset into 721544, 40000, and 40000 images for training, validating, and testing. We employ the early-stopping strategy to preventing over-fitting using the validation set. We measure precision and recall for a set of ground-truth attributes and a set of predicted attributes for each image. The quantitative results are in Table 2.
8Our attribute recognition model is parameterized as θ = [θI ; θseq]. In our case, updating θI as well as θseq in the gradient descent step helps for much better performance.
3.3 Guided ATTRIBUTE-SEQUENCE GENERATION
Our prediction model of the fashion-attribute recognition is based on the sequence generation process in the RNN (Graves, 2013). The attribute-sequence generation process is illustrated in Fig. 7. First, we predict a probability of the first attribute for a given internal representation of the image i.e. pθseq(a0|gθI (I)), and then sample from the estimated probability of the attribute, a0 ∼ pθseq(a0|gθI (I)). The sampled symbol is fed to as the next input to compute pθseq(a1|a0, gθI (I)). This sequential process is repeated recursively until a sampled result is reached at the special endof-sequence (EOS) symbol. In case that we generate a set of attributes for a guided fashion-category, we do not sample from the previously estimated probability, but select the guided fashion-category, and then we feed into it as the next input deterministically. It is the key to considering for each seller's intention. Results for the guided attribute-sequence generation is shown in Fig. 8.
3.4 Guided ROI DETECTION
Our fashion-product ROI detection is based on the Faster R-CNN (Ren et al., 2015). In the conventional multi-class Faster R-CNN detection pipeline, one takes an image and outputs a tuple of (ROI coordinate, object-class, class-score). In our ROI detection pipeline, we take additional information, guided fashion-category from the ResCeption-LSTM based attribute-sequence generator. Our fashion-product ROI detector finds where the guided fashion-category item is in a given image. Jing et al. (2015) also uses a similar idea, but they train several detectors for each category independently so that their works do not scale well. We train a detector for all fashion-categories jointly. Our detector produces ROIs for all of the fashion-categories at once. In post-processing, we reject ROIs that their object-classes are not matched to the guided fashion-category. We demonstrate that the guided fashion-category information contributes to higher performance in terms of mean average precision (mAP) on the fashion-attribute dataset. We measure the mAP for the intersection-of-union (IoU) between ground-truth ROIs and predicted ROIs. (see Table 3) That is due to the fact that our guided fashion-category information reduces the false positive rate. In our fashion-product search pipeline, the colour and appearance features are extracted in the detected ROIs.

3.5 VISUAL FEATURE EXTRACTION
To extract appearance feature for a given ROI, we use pre-trained GoogleNet (Szegedy et al., 2015). In this network, both inception4 and inception5 layer's activation maps are used. We evaluate this feature on two similar image retrieval benchmarks, i.e. Holidays (Jegou et al., 2008) and UK-benchmark (UKB) (Nistér & Stewénius, 2006). In this experiment, we do not use any postprocessing method or fine-tuning at all. The mAP on Holidays is 0.783, and the precision@4 and recall@4 on UKB is 0.907 and 0.908 respectively. These scores are competitive against several deep feature representation methods (Razavian et al., 2014; Babenko et al., 2014). Examples of queries and resulting nearest-neighbors are in Fig. 9. On the next step, we binarize this appearance feature by simply thresholding at 0. The reason we take this simple thresholding to generate the hash code is twofold. The neural activation feature map at a higher layer is a sparse and distributed code in nature. Furthermore, the bias term in a linear layer (e.g., convolutional layer) compensates for
aligning zero-centering of the output feature space weakly. Therefore, we believe that a code from a well-trained neural model, itself, can be a good feature even to be binarized. In our experiment, such simple thresholding degrades mAP by 0.02 on the Holidays dataset, but this method makes it possible to scaling up in the retrieval. In addition to the appearance feature, we extract colour feature using the simple (bins) colour histogram in HSV space, and distance between a query and a reference image is computed by using the weighted combination of the two distances from the colour and the appearance feature.

4 EMPIRICAL RESULTS
To evaluate empirical results of the proposed fashion-product search system, we select 3 million fashion-product images in our e-commerce platform at random. These images are mutually exclusive to the fashion-attribute dataset. We have again selected images from the web used for the queries. All of the reference images pass through the offline process as described in Sec. 3, and resulting inverted indexing database is loaded into main-memory (RAM) by our daemon system. We send the pre-selected queries to the daemon system with the RESTful API. The daemon system then performs the online process and returns nearest-neighbor images correspond to the queries. In this scenario, there are three options to get similar fashion-product images. Option 1 is that the fashion-attribute recognition model automatically selects fashion-category, the most likely to be queried in the given image. Option 2 is that a user manually selects a fashion-category given a query image. (see Fig. 10) Option 3 is that a user draw a rectangle to be queried by hand like Jing et al. (2015). (see Fig. 11) By the recognized fashion-attributes, the retrieved results reflect the user's main needs, e.g. gender, season, utility as well as the fashion-style, that could be lacking when using visual feature representation only.

5 CONCLUSIONS
Today's deep learning technology has given great impact on various research fields. Such a success story is about to be applied to many industries. Following this trend, we traced the start-of-the art computer vision and language modelling research and then, used these technologies to create value for our customers especially in the e-commerce platform. We expect active discussion on that how to apply many existing research works into the e-commerce industry.
","We build a large-scale visual search system which finds similar product images given a fashion item. Defining similarity among arbitrary fashion-products is still remains a challenging problem, even there is no exact ground-truth. To resolve this problem, we define more than 90 fashion-related attributes, and combination of these attributes can represent thousands of unique fashion-styles. We then introduce to use the recurrent neural networks (RNNs) recognising multiple fashion-attributes with the end-to-end manner. To build our system at scale, these fashion-attributes are again used to build an inverted indexing scheme. In addition to these fashion-attributes for semantic similarity, we extract colour and appearance features in a region-of-interest (ROI) of a fashion item for visual similarity. By sharing our approach, we expect active discussion on that how to apply current deep learning researches into the e-commerce industry.",ICLR 2017 conference submission,False,,"The manuscript is a bit scattered and hard to follow. There is technical depth but the paper doesn't do a good job explaining what shortcoming the proposed methods are overcoming and what baselines they are outperforming. 

The writing could be improved. There are numerous grammatical errors.

The experiments in 3.1 are interesting, but you need to be clearer about the relationship of your ResCeption method to the state-of-the-art. The use of extensive footnotes on page 5 is a bit odd. ""That is a competitive result"" is vague. A footnote links to ""

---

Three knowledgable reviewers recommend rejection. The main concern is missing related work on fashion product search, and thus also baselines. The authors did not post a rebuttal to address the concerns. The AC agrees with the reviewers' recommendation.

---

The paper presents a large-scale visual search system for finding product images given a fashion item. The exploration is interesting and the paper does a nice job of discussing the challenges of operating in this domain. The proposed approach addresses several of the challenges. 

However, there are several concerns.

1) The main concern is that there are no comparisons or even mentions of the work done by Tamara Berg’s group on fashion recognition and fashion attributes, e.g., 
-  “Automatic Attribute Discovery and Characterization from Noisy Web Data” ECCV 2010 
- “Where to Buy It: Matching Street Clothing Photos in Online Shops” ICCV 2015,
- “Retrieving Similar Styles to Parse Clothing, TPAMI 2014,
etc
It is difficult to show the contribution and novelty of this work without discussing and comparing with this extensive prior art.

2) There are not enough details about the attribute dataset and the collection process. What is the source of the images? Are these clean product images or real-world images? How is the annotation done? What instructions are the annotators given? What annotations are being collected? I understand data statistics for example may be proprietary, but these kinds of qualitative details are important to understand the contributions of the paper. How can others compare to this work?

3) There are some missing baselines. How do the results in Table 2 compare to simpler methods, e.g., the BM or CM methods described in the text?

While the paper presents an interesting exploration, all these concerns would need to be addressed before the paper can be ready for publication.

---

This paper introduces a pratical large-scale visual search system for a fashion site. It uses RNN to recognize multi-label attributes and uses state-of-art faster RCNN to extract features inside those region-of-interest (ROI). The technical contribution of this paper is not clear. Most of the approaches used are standard state-of-art methods and there are not a lot of novelties in applying those methods. For multi-label recognition task, there are other available methods, e.g. using binary models, changing cross-entropy loss function, etc. There aren't any comparison between the RNN method and other simple baselines. The order of the sequential RNN prediction is not clear either. It seems that the attributes form a tree hierarchy and that is used as the order of sequence.

The paper is not well written either. Most results are reported in the internal dataset and the authors won't release the dataset.

---

The manuscript is a bit scattered and hard to follow. There is technical depth but the paper doesn't do a good job explaining what shortcoming the proposed methods are overcoming and what baselines they are outperforming. 

The writing could be improved. There are numerous grammatical errors.

The experiments in 3.1 are interesting, but you need to be clearer about the relationship of your ResCeption method to the state-of-the-art. The use of extensive footnotes on page 5 is a bit odd. ""That is a competitive result"" is vague. A footnote links to ""

---

We fixed minor typographical error in author's name and Section. 4. etc.
Our policy restricts to reveal much more details about the internal dataset and results of the end-user satisfaction measure, however, we did our best to introduce how our idea is to be used for multi-label learning in an application to computer vision, especially e-commerce industry.

---

The manuscript is a bit scattered and hard to follow. There is technical depth but the paper doesn't do a good job explaining what shortcoming the proposed methods are overcoming and what baselines they are outperforming. 

The writing could be improved. There are numerous grammatical errors.

The experiments in 3.1 are interesting, but you need to be clearer about the relationship of your ResCeption method to the state-of-the-art. The use of extensive footnotes on page 5 is a bit odd. ""That is a competitive result"" is vague. A footnote links to ""

---

Three knowledgable reviewers recommend rejection. The main concern is missing related work on fashion product search, and thus also baselines. The authors did not post a rebuttal to address the concerns. The AC agrees with the reviewers' recommendation.

---

The paper presents a large-scale visual search system for finding product images given a fashion item. The exploration is interesting and the paper does a nice job of discussing the challenges of operating in this domain. The proposed approach addresses several of the challenges. 

However, there are several concerns.

1) The main concern is that there are no comparisons or even mentions of the work done by Tamara Berg’s group on fashion recognition and fashion attributes, e.g., 
-  “Automatic Attribute Discovery and Characterization from Noisy Web Data” ECCV 2010 
- “Where to Buy It: Matching Street Clothing Photos in Online Shops” ICCV 2015,
- “Retrieving Similar Styles to Parse Clothing, TPAMI 2014,
etc
It is difficult to show the contribution and novelty of this work without discussing and comparing with this extensive prior art.

2) There are not enough details about the attribute dataset and the collection process. What is the source of the images? Are these clean product images or real-world images? How is the annotation done? What instructions are the annotators given? What annotations are being collected? I understand data statistics for example may be proprietary, but these kinds of qualitative details are important to understand the contributions of the paper. How can others compare to this work?

3) There are some missing baselines. How do the results in Table 2 compare to simpler methods, e.g., the BM or CM methods described in the text?

While the paper presents an interesting exploration, all these concerns would need to be addressed before the paper can be ready for publication.

---

This paper introduces a pratical large-scale visual search system for a fashion site. It uses RNN to recognize multi-label attributes and uses state-of-art faster RCNN to extract features inside those region-of-interest (ROI). The technical contribution of this paper is not clear. Most of the approaches used are standard state-of-art methods and there are not a lot of novelties in applying those methods. For multi-label recognition task, there are other available methods, e.g. using binary models, changing cross-entropy loss function, etc. There aren't any comparison between the RNN method and other simple baselines. The order of the sequential RNN prediction is not clear either. It seems that the attributes form a tree hierarchy and that is used as the order of sequence.

The paper is not well written either. Most results are reported in the internal dataset and the authors won't release the dataset.

---

The manuscript is a bit scattered and hard to follow. There is technical depth but the paper doesn't do a good job explaining what shortcoming the proposed methods are overcoming and what baselines they are outperforming. 

The writing could be improved. There are numerous grammatical errors.

The experiments in 3.1 are interesting, but you need to be clearer about the relationship of your ResCeption method to the state-of-the-art. The use of extensive footnotes on page 5 is a bit odd. ""That is a competitive result"" is vague. A footnote links to ""

---

We fixed minor typographical error in author's name and Section. 4. etc.
Our policy restricts to reveal much more details about the internal dataset and results of the end-user satisfaction measure, however, we did our best to introduce how our idea is to be used for multi-label learning in an application to computer vision, especially e-commerce industry.",,,,,,3.3333333333333335,,,3.6666666666666665,,
597,"Authors: REINFORCEMENT LEARNING, Irwan Bello, Hieu Pham, Quoc V. Le, Mohammad Norouzi, Samy Bengio
Source file: 597.pdf

ABSTRACT
This paper presents a framework to tackle combinatorial optimization problems using neural networks and reinforcement learning. We focus on the traveling salesman problem (TSP) and train a recurrent neural network that, given a set of city coordinates, predicts a distribution over different city permutations. Using negative tour length as the reward signal, we optimize the parameters of the recurrent neural network using a policy gradient method. We compare learning the network parameters on a set of training graphs against learning them on individual test graphs. Without much engineering and heuristic designing, Neural Combinatorial Optimization achieves close to optimal results on 2D Euclidean graphs with up to 100 nodes. Applied to the KnapSack, another NP-hard problem, the same method obtains optimal solutions for instances with up to 200 items. These results, albeit still far from state-of-the-art, give insights into how neural networks can be used as a general tool for tackling combinatorial optimization problems.

1 INTRODUCTION
Combinatorial optimization is a fundamental problem in computer science. A canonical example is the traveling salesman problem (TSP), where given a graph, one needs to search the space of permutations to find an optimal sequence of nodes with minimal total edge weights (tour length). The TSP and its variants have myriad applications in planning, manufacturing, genetics, etc. (see (Applegate et al., 2011) for an overview).
Finding the optimal TSP solution is NP-hard, even in the two-dimensional Euclidean case (Papadimitriou, 1977), where the nodes are 2D points and edge weights are Euclidean distances between pairs of points. In practice, TSP solvers rely on handcrafted heuristics that guide their search procedures to find competitive (and in many cases optimal) tours efficiently. Even though these heuristics work well on TSP, once the problem statement changes slightly, they need to be revised. In contrast, machine learning methods have the potential to be applicable across many optimization tasks by automatically discovering their own heuristics based on the training data, thus requiring less handengineering than solvers that are optimized for one task only.
While most successful machine learning techniques fall into the family of supervised learning, where a mapping from training inputs to outputs is learned, supervised learning is not applicable to most combinatorial optimization problems because one does not have access to optimal labels. However, one can compare the quality of a set of solutions using a verifier, and provide some reward feedbacks to a learning algorithm. Hence, we follow the reinforcement learning (RL) paradigm to tackle combinatorial optimization. We empirically demonstrate that, even when using optimal solutions as labeled data to optimize a supervised mapping, the generalization is rather poor compared to an RL agent that explores different tours and observes their corresponding rewards.
We propose Neural Combinatorial Optimization, a framework to tackle combinatorial optimization problems using reinforcement learning and neural networks. We consider two approaches based on policy gradients (Williams, 1992). The first approach, called RL pretraining, uses a training set to optimize a recurrent neural network (RNN) that parameterizes a stochastic policy over solutions, using the expected reward as objective. At test time, the policy is fixed, and one performs inference ∗Equal contributions. Members of the Google Brain Residency program (g.co/brainresidency).
by greedy decoding or sampling. The second approach, called active search, involves no pretraining. It starts from a random policy and iteratively optimizes the RNN parameters on a single test instance, again using the expected reward objective, while keeping track of the best solution sampled during the search. We find that combining RL pretraining and active search works best in practice.
On 2D Euclidean graphs with up to 100 nodes, Neural Combinatorial Optimization significantly outperforms the supervised learning approach to the TSP (Vinyals et al., 2015b) and obtains close to optimal results when allowed more computation time (see Figure 1). We illustrate the flexibility of the method by also applying it to the KnapSack problem, for which we get optimal results for instances with up to 200 items. Our results, while still inferior to the state-of-the-art in many dimensions (such as speed, scale and performance), give insights into how neural networks can be used as a general tool for tackling combinatorial optimization problems, especially those that are difficult to design heuristics for.

2 PREVIOUS WORK
The Traveling Salesman Problem is a well studied combinatorial optimization problem and many exact or approximate algorithms have been proposed for both Euclidean and non-Euclidean graphs. Christofides (1976) proposes a heuristic algorithm that involves computing a minimum-spanning tree and a minimum-weight perfect matching. The algorithm has polynomial running time and returns solutions that are guaranteed to be within a factor of 1.5× to optimality in the metric instance of the TSP.
The best known exact dynamic programming algorithm for TSP has a complexity of Θ(2nn2), making it infeasible to scale up to large instances, say with 40 points. Nevertheless, state of the art TSP solvers, thanks to carefully handcrafted heuristics that describe how to navigate the space of feasible solutions in an efficient manner, can solve symmetric TSP instances with thousands of nodes. Concorde (Applegate et al., 2006), widely accepted as one of the best exact TSP solvers, makes use of cutting plane algorithms (Dantzig et al., 1954; Padberg & Rinaldi, 1990; Applegate et al., 2003), iteratively solving linear programming relaxations of the TSP, in conjunction with a branch-and-bound approach that prunes parts of the search space that provably will not contain an optimal solution. Similarly, the Lin-Kernighan-Helsgaun heuristic (Helsgaun, 2000), inspired from the Lin-Kernighan heuristic (Lin & Kernighan, 1973), is a state of the art approximate search heuristic for the symmetric TSP and has been shown to solve instances with hundreds of nodes to optimality.
More generic solvers, such as Google’s vehicle routing problem solver (Google, 2016) that tackles a superset of the TSP, typically rely on a combination of local search algorithms and metaheuristics. Local search algorithms apply a specified set of local move operators on candidate solutions, based
on hand-engineered heuristics such as 2-opt (Johnson, 1990), to navigate from solution to solution in the search space. A metaheuristic is then applied to propose uphill moves and escape local optima. A popular choice of metaheuristic for the TSP and its variants is guided local search (Voudouris & Tsang, 1999), which moves out of a local minimum by penalizing particular solution features that it considers should not occur in a good solution.
The difficulty in applying existing search heuristics to newly encountered problems - or even new instances of a similar problem - is a well-known challenge that stems from the No Free Lunch theorem (Wolpert & Macready, 1997). Because all search algorithms have the same performance when averaged over all problems, one must appropriately rely on a prior over problems when selecting a search algorithm to guarantee performance. This challenge has fostered interest in raising the level of generality at which optimization systems operate (Burke et al., 2003) and is the underlying motivation behind hyper-heuristics, defined as ”search method[s] or learning mechanism[s] for selecting or generating heuristics to solve computation search problems”. Hyper-heuristics aim to be easier to use than problem specific methods by partially abstracting away the knowledge intensive process of selecting heuristics given a combinatorial problem and have been shown to successfully combine human-defined heuristics in superior ways across many tasks (see (Burke et al., 2013) for a survey). However, hyper-heuristics operate on the search space of heuristics, rather than the search space of solutions, therefore still initially relying on human created heuristics.
The application of neural networks to combinatorial optimization has a distinguished history, where the majority of research focuses on the Traveling Salesman Problem (Smith, 1999). One of the earliest proposals is the use of Hopfield networks (Hopfield & Tank, 1985) for the TSP. The authors modify the network’s energy function to make it equivalent to TSP objective and use Lagrange multipliers to penalize the violations of the problem’s constraints. A limitation of this approach is that it is sensitive to hyperparameters and parameter initialization as analyzed by (Wilson & Pawley, 1988). Overcoming this limitation is central to the subsequent work in the field, especially by (Aiyer et al., 1990; Gee, 1993). Parallel to the development of Hopfield networks is the work on using deformable template models to solve TSP. Perhaps most prominent is the invention of Elastic Nets as a means to solve TSP (Durbin, 1987), and the application of Self Organizing Map to TSP (Fort, 1988; Angeniol et al., 1988; Kohonen, 1990). Addressing the limitations of deformable template models is central to the following work in this area (Burke, 1994; Favata & Walker, 1991; Vakhutinsky & Golden, 1995). Even though these neural networks have many appealing properties, they are still limited as research work. When being carefully benchmarked, they have not yielded satisfying results compared to algorithmic methods (Sarwar & Bhatti, 2012; La Maire & Mladenov, 2012). Perhaps due to the negative results, this research direction is largely overlooked since the turn of the century.
Motivated by the recent advancements in sequence-to-sequence learning (Sutskever et al., 2014), neural networks are again the subject of study for optimization in various domains (Yutian et al., 2016), including discrete ones (Zoph & Le, 2016). In particular, the TSP is revisited in the introduction of Pointer Networks (Vinyals et al., 2015b), where a recurrent network with non-parametric softmaxes is trained in a supervised manner to predict the sequence of visited cities. Despite architecural improvements, their models were trained using supervised signals given by an approximate solver.

3 NEURAL NETWORK ARCHITECTURE FOR TSP
We focus on the 2D Euclidean TSP in this paper. Given an input graph, represented as a sequence of n cities in a two dimensional space s = {xi}ni=1 where each xi ∈ R2, we are concerned with finding a permutation of the points π, termed a tour, that visits each city once and has the minimum total length. We define the length of a tour defined by a permutation π as
L(π | s) = ∥∥xπ(n) − xπ(1)∥∥2 + n−1∑
i=1 ∥∥xπ(i) − xπ(i+1)∥∥2 , (1) where ‖·‖2 denotes `2 norm. We aim to learn the parameters of a stochastic policy p(π | s) that given an input set of points s, assigns high probabilities to short tours and low probabilities to long tours. Our neural network
architecture uses the chain rule to factorize the probability of a tour as
p(π | s) = n∏ i=1 p (π(i) | π(< i), s) , (2)
and then uses individual softmax modules to represent each term on the RHS of (2).
We are inspired by previous work (Sutskever et al., 2014) that makes use of the same factorization based on the chain rule to address sequence to sequence problems like machine translation. One can use a vanilla sequence to sequence model to address the TSP where the output vocabulary is {1, 2, . . . , n}. However, there are two major issues with this approach: (1) networks trained in this fashion cannot generalize to inputs with more than n cities. (2) one needs to have access to groundtruth output permutations to optimize the parameters with conditional log-likelihood. We address both isssues in this paper.
For generalization beyond a pre-specified graph size, we follow the approach of (Vinyals et al., 2015b), which makes use of a set of non-parameteric softmax modules, resembling the attention mechanism from (Bahdanau et al., 2015). This approach, named pointer network, allows the model to effectively point to a specific position in the input sequence rather than predicting an index value from a fixed-size vocabulary. We employ the pointer network architecture, depicted in Figure 2, as our policy model to parameterize p(π | s).

3.1 ARCHITECTURE DETAILS
Our pointer network comprises two recurrent neural network (RNN) modules, encoder and decoder, both of which consist of Long Short-Term Memory (LSTM) cells (Hochreiter & Schmidhuber, 1997). The encoder network reads the input sequence s, one city at a time, and transforms it into a sequence of latent memory states {enci}ni=1 where enci ∈ Rd. The input to the encoder network at time step i is a d-dimensional embedding of a 2D point xi, which is obtained via a linear transformation of xi shared across all input steps. The decoder network also maintains its latent memory states {deci}ni=1 where deci ∈ Rd and, at each step i, uses a pointing mechanism to produce a distribution over the next city to visit in the tour. Once the next city is selected, it is passed as the input to the next decoder step. The input of the first decoder step (denoted by 〈g〉 in Figure 2) is a d-dimensional vector treated as a trainable parameter of our neural network.
Our attention function, formally defined in Appendix A.1, takes as input a query vector q = deci ∈ Rd and a set of reference vectors ref = {enc1, . . . , enck} where enci ∈ Rd, and predicts a distribution A(ref, q) over the set of k references. This probability distribution represents the degree to which the model is pointing to reference ri upon seeing query q.
Vinyals et al. (2015a) also suggest including some additional computation steps, named glimpses, to aggregate the contributions of different parts of the input sequence, very much like (Bahdanau et al., 2015). We discuss this approach in details in Appendix A.1. In our experiments, we find that utilizing one glimpse in the pointing mechanism yields performance gains at an insignificant cost latency.
Algorithm 1 Actor-critic training 1: procedure TRAIN(training set S, number of training steps T , batch size B) 2: Initialize pointer network params θ 3: Initialize critic network params θv 4: for t = 1 to T do 5: si ∼ SAMPLEINPUT(S) for i ∈ {1, . . . , B} 6: πi ∼ SAMPLESOLUTION(pθ(.|si)) for i ∈ {1, . . . , B} 7: bi ← bθv (si) for i ∈ {1, . . . , B} 8: gθ ← 1B ∑B i=1(L(πi|si)− bi)∇θ log pθ(πi|si)
9: Lv ← 1B ∑B i=1 ‖bi − L(πi)‖ 2 2
10: θ ← ADAM(θ, gθ) 11: θv ← ADAM(θv,∇θvLv) 12: end for 13: return θ 14: end procedure

4 OPTIMIZATION WITH POLICY GRADIENTS
Vinyals et al. (2015b) proposes training a pointer network using a supervised loss function comprising conditional log-likelihood, which factors into a cross entropy objective between the network’s output probabilities and the targets provided by a TSP solver. Learning from examples in such a way is undesirable for NP-hard problems because (1) the performance of the model is tied to the quality of the supervised labels, (2) getting high-quality labeled data is expensive and may be infeasible for new problem statements, (3) one cares about finding a competitive solution more than replicating the results of another algorithm.
By contrast, we believe Reinforcement Learning (RL) provides an appropriate paradigm for training neural networks for combinatorial optimization, especially because these problems have relatively simple reward mechanisms that could be even used at test time. We hence propose to use model-free policy-based Reinforcement Learning to optimize the parameters of a pointer network denoted θ. Our training objective is the expected tour length which, given an input graph s, is defined as
J(θ | s) = Eπ∼pθ(.|s) L(π | s) . (3) During training, our graphs are drawn from a distribution S, and the total training objective involves sampling from the distribution of graphs, i.e. J(θ) = Es∼S J(θ | s) . We resort to policy gradient methods and stochastic gradient descent to optimize the parameters. The gradient of (3) is formulated using the well-known REINFORCE algorithm (Williams, 1992):
∇θJ(θ | s) = Eπ∼pθ(.|s) [( L(π | s)− b(s) ) ∇θ log pθ(π | s) ] , (4)
where b(s) denotes a baseline function that does not depend on π and estimates the expected tour length to reduce the variance of the gradients.
By drawing B i.i.d. sample graphs s1, s2, . . . , sB ∼ S and sampling a single tour per graph, i.e. πi ∼ pθ(. | si), the gradient in (4) is approximated with Monte Carlo sampling as follows:
∇θJ(θ) ≈ 1
B B∑ i=1 ( L(πi|si)− b(si) ) ∇θ log pθ(πi | si) . (5)
A simple and popular choice of the baseline b(s) is an exponential moving average of the rewards obtained by the network over time to account for the fact that the policy improves with training. While this choice of baseline proved sufficient to improve upon the Christofides algorithm, it suffers from not being able to differentiate between different input graphs. In particular, the optimal tour π∗ for a difficult graph s may be still discouraged if L(π∗|s) > b because b is shared across all instances in the batch.
Using a parametric baseline to estimate the expected tour length Eπ∼pθ(.|s)L(π | s) typically improves learning. Therefore, we introduce an auxiliary network, called a critic and parameterized
Algorithm 2 Active Search 1: procedure ACTIVESEARCH(input s, θ, number of candidates K, B, α) 2: π ← RANDOMSOLUTION() 3: Lπ ← L(π | s) 4: n← dK
B e
5: for t = 1 . . . n do 6: πi ∼ SAMPLESOLUTION(pθ(. | s)) for i ∈ {1, . . . , B} 7: j ← ARGMIN(L(π1 | s) . . . L(πB | s)) 8: Lj ← L(πj | s) 9: if Lj < Lπ then
10: π ← πj 11: Lπ ← Lj 12: end if 13: gθ ← 1B ∑B i=1(L(πi | s)− b)∇θ log pθ(πi | s) 14: θ ← ADAM(θ, gθ) 15: b← α× b+ (1− α)× ( 1
B ∑B i=1 bi)
16: end for 17: return π 18: end procedure
by θv , to learn the expected tour length found by our current policy pθ given an input sequence s. The critic is trained with stochastic gradient descent on a mean squared error objective between its predictions bθv (s) and the actual tour lengths sampled by the most recent policy. The additional objective is formulated as
L(θv) = 1
B B∑ i=1 ∥∥ bθv (si)− L(πi | si)∥∥22 . (6) Critic’s architecture for TSP. We now explain how our critic maps an input sequence s into a baseline prediction bθv (s). Our critic comprises three neural network modules: 1) an LSTM encoder, 2) an LSTM process block and 3) a 2-layer ReLU neural network decoder. Its encoder has the same architecture as that of our pointer network’s encoder and encodes an input sequence s into a sequence of latent memory states and a hidden state h. The process block, similarly to (Vinyals et al., 2015a), then performs P steps of computation over the hidden state h. Each processing step updates this hidden state by glimpsing at the memory states as described in Appendix A.1 and feeds the output of the glimpse function as input to the next processing step. At the end of the process block, the obtained hidden state is then decoded into a baseline prediction (i.e a single scalar) by two fully connected layers with respectively d and 1 unit(s).
Our training algorithm, described in Algorithm 1, is closely related to the asynchronous advantage actor-critic (A3C) proposed in (Mnih et al., 2016), as the difference between the sampled tour lengths and the critic’s predictions is an unbiased estimate of the advantage function. We perform our updates asynchronously across multiple workers, but each worker also handles a mini-batch of graphs for better gradient estimates.

4.1 SEARCH STRATEGIES
As evaluating a tour length is inexpensive, our TSP agent can easily simulate a search procedure at inference time by considering multiple candidate solutions per graph and selecting the best. This inference process resembles how solvers search over a large set of feasible solutions. In this paper, we consider two search strategies detailed below, which we refer to as sampling and active search.
Sampling. Our first approach is simply to sample multiple candidate tours from our stochastic policy pθ(.|s) and select the shortest one. In contrast to heuristic solvers, we do not enforce our model to sample different tours during the process. However, we can control the diversity of the sampled tours with a temperature hyperparameter when sampling from our non-parametric softmax (see Appendix A.2). This sampling process yields significant improvements over greedy decoding, which always selects the index with the largest probability. We also considered perturbing the pointing
mechanism with random noise and greedily decoding from the obtained modified policy, similarly to (Cho, 2016), but this proves less effective than sampling in our experiments.
Active Search. Rather than sampling with a fixed model and ignoring the reward information obtained from the sampled solutions, one can refine the parameters of the stochastic policy pθ during inference to minimize Eπ∼pθ(.|s)L(π | s) on a single test input s. This approach proves especially competitive when starting from a trained model. Remarkably, it also produces satisfying solutions when starting from an untrained model. We refer to these two approaches as RL pretraining-Active Search and Active Search because the model actively updates its parameters while searching for candidate solutions on a single test instance.
Active Search applies policy gradients similarly to Algorithm 1 but draws Monte Carlo samples over candidate solutions π1 . . . πB ∼ pθ(·|s) for a single test input. It resorts to an exponential moving average baseline, rather than a critic, as there is no need to differentiate between inputs. Our Active Search training algorithm is presented in Algorithm 2. We note that while RL training does not require supervision, it still requires training data and hence generalization depends on the training data distribution. In contrast, Active Search is distribution independent. Finally, since we encode a set of cities as a sequence, we randomly shuffle the input sequence before feeding it to our pointer network. This increases the stochasticity of the sampling procedure and leads to large improvements in Active Search.

5 EXPERIMENTS
We conduct experiments to investigate the behavior of the proposed Neural Combinatorial Optimization methods. We consider three benchmark tasks, Euclidean TSP20, 50 and 100, for which we generate a test set of 1, 000 graphs. Points are drawn uniformly at random in the unit square [0, 1]2.

5.1 EXPERIMENTAL DETAILS
Across all experiments, we use mini-batches of 128 sequences, LSTM cells with 128 hidden units, and embed the two coordinates of each point in a 128-dimensional space. We train our models with the Adam optimizer (Kingma & Ba, 2014) and use an initial learning rate of 10−3 for TSP20 and TSP50 and 10−4 for TSP100 that we decay every 5000 steps by a factor of 0.96. We initialize our parameters uniformly at random within [−0.08, 0.08] and clip the L2 norm of our gradients to 1.0. We use up to one attention glimpse. When searching, the mini-batches either consist of replications of the test sequence or its permutations. The baseline decay is set to α = 0.99 in Active Search. Our model and training code in Tensorflow (Abadi et al., 2016) will be made availabe soon. Table 1 summarizes the configurations and different search strategies used in the experiments. The variations of our method, experimental procedure and results are as follows.
Supervised Learning. In addition to the described baselines, we implement and train a pointer network with supervised learning, similarly to (Vinyals et al., 2015b). While our supervised data consists of one million optimal tours, we find that our supervised learning results are not as good as those reported in by (Vinyals et al., 2015b). We suspect that learning from optimal tours is harder for supervised pointer networks due to subtle features that the model cannot figure out only by looking at given supervised targets. We thus refer to the results in (Vinyals et al., 2015b) for TSP20 and TSP50 and report our results on TSP100, all of which are suboptimal compared to other approaches.
RL pretraining. For the RL experiments, we generate training mini-batches of inputs on the fly and update the model parameters with the Actor Critic Algorithm 1. We use a validation set of 10, 000 randomly generated instances for hyper-parameters tuning. Our critic consists of an encoder network which has the same architecture as that of the policy network, but followed by 3 processing steps and 2 fully connected layers. We find that clipping the logits to [−10, 10] with a tanh(·) activation function, as described in Appendix A.2, helps with exploration and yields marginal performance gains. The simplest search strategy using an RL pretrained model is greedy decoding, i.e. selecting the city with the largest probability at each decoding step. We also experiment with decoding greedily from a set of 16 pretrained models at inference time. For each graph, the tour found by each individual model is collected and the shortest tour is chosen. We refer to those approaches as RL pretraining-greedy and RL pretraining-greedy@16.
RL pretraining-Sampling. For each test instance, we sample 1, 280, 000 candidate solutions from a pretrained model and keep track of the shortest tour. A grid search over the temperature hyperparameter found respective temperatures of 2.0, 2.2 and 1.5 to yield the best results for TSP20, TSP50 and TSP100. We refer to the tuned temperature hyperparameter as T ∗. Since sampling does not require parameter udpates and is entirely parallelizable, we use a larger batch size for speed purposes.
RL pretraining-Active Search. For each test instance, we initialize the model parameters from a pretrained RL model and run Active Search for up to 10, 000 training steps with a batch size of 128, sampling a total of 1, 280, 000 candidate solutions. We set the learning rate to a hundredth of the initial learning rate the TSP agent was trained on (i.e. 10−5 for TSP20/TSP50 and 10−6 for TSP100).
Active Search. We allow the model to train much longer to account for the fact that it starts from scratch. For each test graph, we run Active Search for 100, 000 training steps on TSP20/TSP50 and 200, 000 training steps on TSP100.

5.2 RESULTS AND ANALYSES
We compare our methods against 3 different baselines of increasing performance and complexity: 1) Christofides, 2) the vehicle routing solver from OR-Tools (Google, 2016) and 3) optimality. Christofides solutions are obtained in polynomial time and guaranteed to be within a 1.5 ratio of optimality. OR-Tools improves over Christofides’ solutions with simple local search operators, including 2-opt (Johnson, 1990) and a version of the Lin-Kernighan heuristic (Lin & Kernighan, 1973), stopping when it reaches a local minimum. In order to escape poor local optima, ORTools’ local search can also be run in conjunction with different metaheuristics, such as simulated annealing (Kirkpatrick et al., 1983), tabu search (Glover & Laguna, 2013) or guided local search (Voudouris & Tsang, 1999). OR-Tools’ vehicle routing solver can tackle a superset of the TSP and operates at a higher level of generality than solvers that are highly specific to the TSP. While not state-of-the art for the TSP, it is a common choice for general routing problems and provides a reasonable baseline between the simplicity of the most basic local search operators and the sophistication of the strongest solvers. Optimal solutions are obtained via Concorde (Applegate et al., 2006) and LK-H’s local search (Helsgaun, 2012; 2000). While only Concorde provably solves instances to optimality, we empirically find that LK-H also achieves optimal solutions on all of our test sets after 50 trials per graph (which is the default parameter setting).
We report the average tour lengths of our approaches on TSP20, TSP50, and TSP100 in Table 2. Notably, results demonstrate that training with RL significantly improves over supervised learning
(Vinyals et al., 2015b). All our methods comfortably surpass Christofides’ heuristic, including RL pretraining-Greedy which also does not rely on search. Table 3 compares the running times of our greedy methods to the aforementioned baselines, with our methods running on a single Nvidia Tesla K80 GPU, Concorde and LK-H running on an Intel Xeon CPU E5-1650 v3 3.50GHz CPU and ORTool on an Intel Haswell CPU. We find that both greedy approaches are time-efficient but still quite far from optimality.
Searching at inference time proves crucial to get closer to optimality but comes at the expense of longer running times. Fortunately, the search from RL pretraining-Sampling and RL pretrainingActive Search can be stopped early with a small performance tradeoff in terms of the final objective. This can be seen in Table 4, where we show their performances and corresponding running times as a function of how many solutions they consider.
We also find that many of our RL pretraining methods outperform OR-Tools’ local search, including RL pretraining-Greedy@16 which runs similarly fast. Table 6 in Appendix A.3 presents the performance of the metaheuristics as they consider more solutions and the corresponding running times. In our experiments, Neural Combinatorial proves superior than Simulated Annealing but is slightly less competitive that Tabu Search and much less so than Guided Local Search.
We present a more detailed comparison of our methods in Figure 3, where we sort the ratios to optimality of our different learning configurations. RL pretraining-Sampling and RL pretrainingActive Search are the most competitive Neural Combinatorial Optimization methods and recover the optimal solution in a significant number of our test cases. We find that for small solution spaces, RL pretraining-Sampling, with a finetuned softmax temperature, outperforms RL pretraining-Active Search with the latter sometimes orienting the search towards suboptimal regions of the solution space (see TSP50 results in Table 4 and Figure 3). Furthermore, RL pretraining-Sampling benefits from being fully parallelizable and runs faster than RL pretraining-Active Search. However, for larger solution spaces, RL-pretraining Active Search proves superior both when controlling for the number of sampled solutions or the running time. Interestingly, Active Search - which starts from an untrained model - also produces competitive tours but requires a considerable amount of time (respectively 7 and 25 hours per instance of TSP50/TSP100). Finally, we show randomly picked example tours found by our methods in Figure 4 in Appendix A.4.

6 GENERALIZATION TO OTHER PROBLEMS
In this section, we discuss how to apply Neural Combinatorial Optimization to other problems than the TSP. In Neural Combinatorial Optimization, the model architecture is tied to the given combinatorial optimization problem. Examples of useful networks include the pointer network, when the output is a permutation or a truncated permutation or a subset of the input, and the classical seq2seq model for other kinds of structured outputs. For combinatorial problems that require to assign labels to elements of the input, such as graph coloring, it is also possible to combine a pointer module and a softmax module to simultaneously point and assign at decoding time. Given a model that encodes an instance of a given combinatorial optimization task and repeatedly branches into subtrees to construct a solution, the training procedures described in Section 4 can then be applied by adapting the reward function depending on the optimization problem being considered.
Additionally, one also needs to ensure the feasibility of the obtained solutions. For certain combinatorial problems, it is straightforward to know exactly which branches do not lead to any feasible solutions at decoding time. We can then simply manually assign them a zero probability when decoding, similarly to how we enforce our model to not point at the same city twice in our pointing mechanism (see Appendix A.1). However, for many combinatorial problems, coming up with a feasible solution can be a challenge in itself. Consider, for example, the Travelling Salesman Problem with Time Windows, where the travelling salesman has the additional constraint of visiting each city during a specific time window. It might be that most branches being considered early in the tour do not lead to any solution that respects all time windows. In such cases, knowing exactly which branches are feasible requires searching their subtrees, a time-consuming process that is not much easier than directly searching for the optimal solution unless using problem-specific heuristics.
Rather than explicitly constraining the model to only sample feasible solutions, one can also let the model learn to respect the problem’s constraints. A simple approach, to be verified experimentally in future work, consists in augmenting the objective function with a term that penalizes solutions for violating the problem’s constraints, similarly to penalty methods in constrained optimization. While this does not guarantee that the model consistently samples feasible solutions at inference time, this is not necessarily problematic as we can simply ignore infeasible solutions and resample from the model (for RL pretraining-Sampling and RL-pretraining Active Search). It is also conceivable to combine both approaches by assigning zero probabilities to branches that are easily identifiable as infeasible while still penalizing infeasible solutions once they are entirely constructed.

6.1 KNAPSACK EXAMPLE
As an example of the flexibility of Neural Combinatorial Optimization, we consider the KnapSack problem, another intensively studied problem in computer science. Given a set of n items i = 1...n, each with weight wi and value vi and a maximum weight capacity ofW , the 0-1 KnapSack problem consists in maximizing the sum of the values of items present in the knapsack so that the sum of the
weights is less than or equal to the knapsack capacity:
max S⊆{1,2,...,n} ∑ i∈S vi
subject to ∑ i∈S wi ≤W (7)
With wi, vi and W taking real values, the problem is NP-hard (Kellerer et al., 2004). A naive heuristic is to take the items ordered by their weight-to-value ratios until they fill up the weight capacity. Two simple heuristics are ExpKnap, which employs branch-and-bound with Linear Programming bounds (Pisinger, 1995), and MinKnap, which uses dynamic programming with enumerative bounds (Pisinger, 1997). Exact solutions can also be obtained by quantizing the weights to high precisions and then performing dynamic programming with pseudo-polynomial complexity (Bertsimas & Demir, 2002).
We apply the pointer network and encode each KnapSack instance as a sequence of 2D vectors (wi, vi). At decoding time, the pointer network points to items to include in the knapsack and stops when the total weight of the items collected so far exceeds the weight capacity. We generate three datasets, KNAP50, KNAP100 and KNAP200, of a thousand instances with items’ weights and values drawn uniformly at random in [0, 1]. Without loss of generality (since we can scale the items’ weights), we set the capacities to 12.5 for KNAP50 and 25 for KNAP100 and KNAP200. We present the performances of RL pretraining-Greedy and Active Search (which we run for 5, 000 training steps) in Table 5 and compare them to the following baselines: 1) random search (which we let sample as many feasible solutions seen by Active Search), 2) the greedy value-to-weight ratio heuristic, 3) MinKnap, 4) ExpKnap, 5) OR-Tools’ KnapSack solver (Google, 2016) and 6) optimality (which we obtained by quantizing the weights to high precisions and using dynamic programming).

7 CONCLUSION
This paper presents Neural Combinatorial Optimization, a framework to tackle combinatorial optimization with reinforcement learning and neural networks. We focus on the traveling salesman problem (TSP) and present a set of results for each variation of the framework. Experiments demonstrate that Neural Combinatorial Optimization achieves close to optimal results on 2D Euclidean graphs with up to 100 nodes. Our results, while still far from the strongest solvers (especially those which are optimized for one problem), provide an interesting research avenue for using neural networks as a general tool for tackling combinatorial optimization problems.

ACKNOWLEDGMENTS
The authors would like to thank Vincent Furnon, Mustafa Ispir, Lukasz Kaiser, Oriol Vinyals, Barret Zoph, the Google Brain team and the anonymous ICLR reviewers for insightful comments and discussion.

A APPENDIX
A.1 POINTING AND ATTENDING
Pointing mechanism: Its computations are parameterized by two attention matrices Wref ,Wq ∈ Rd×d and an attention vector v ∈ Rd as follows:
ui = { v> · tanh (Wref · ri +Wq · q) if i 6= π(j) for all j < i −∞ otherwise for i = 1, 2, ..., k (8)
A(ref, q;Wref ,Wq, v) def = softmax(u). (9)
Our pointer network, at decoder step j, then assigns the probability of visiting the next point π(j) of the tour as follows:
p(π(j)|π(< j), s) def= A(enc1:n, decj). (10)
Setting the logits of cities that already appeared in the tour to −∞, as shown in Equation 8, ensures that our model only points at cities that have yet to be visited and hence outputs valid TSP tours.
Attending mechanism: Specifically, our glimpse function G(ref, q) takes the same inputs as the attention function A and is parameterized by W gref ,W g q ∈ Rd×d and vg ∈ Rd. It performs the following computations:
p = A(ref, q;W gref ,W g q , v g) (11)
G(ref, q;W gref ,W g q , v g) def = k∑ i=1 ripi. (12)
The glimpse functionG essentially computes a linear combination of the reference vectors weighted by the attention probabilities. It can also be applied multiple times on the same reference set ref :
g0 def = q (13)
gl def = G(ref, gl−1;W g ref ,W g q , v g) (14)
Finally, the ultimate gl vector is passed to the attention function A(ref, gl;Wref ,Wq, v) to produce the probabilities of the pointing mechanism. We observed empirically that glimpsing more than once with the same parameters made the model less likely to learn and barely improved the results.
A.2 IMPROVING EXPLORATION
Softmax temperature: We modify Equation 9 as follows:
A(ref, q, T ;Wref ,Wq, v) def = softmax(u/T ), (15)
where T is a temperature hyperparameter set to T = 1 during training. When T > 1, the distribution represented by A(ref, q) becomes less steep, hence preventing the model from being overconfident.
Logit clipping: We modify Equation 9 as follows:
A(ref, q;Wref ,Wq, v) def = softmax(C tanh(u)), (16)
whereC is a hyperparameter that controls the range of the logits and hence the entropy ofA(ref, q).
A.3 OR TOOL’S METAHEURISTICS BASELINES FOR TSP
A.4 SAMPLE TOURS
(5.934)
RL pretraining -Greedy
(5.734)
RL pretraining -Sampling
(5.688)
RL pretraining -Active Search
(5.827)
Active Search
(5.688)
Optimal
RL pretraining -Greedy
RL pretraining -Sampling
RL pretraining -Active Search
Active Search
Optimal
","This paper presents a framework to tackle combinatorial optimization problems using neural networks and reinforcement learning. We focus on the traveling salesman problem (TSP) and train a recurrent neural network that, given a set of city coordinates, predicts a distribution over different city permutations. Using negative tour length as the reward signal, we optimize the parameters of the recurrent neural network using a policy gradient method. We compare learning the network parameters on a set of training graphs against learning them on individual test graphs. Without much engineering and heuristic designing, Neural Combinatorial Optimization achieves close to optimal results on 2D Euclidean graphs with up to 100 nodes. Applied to the KnapSack, another NP-hard problem, the same method obtains optimal solutions for instances with up to 200 items. These results, albeit still far from state-of-the-art, give insights into how neural networks can be used as a general tool for tackling combinatorial optimization problems.",ICLR 2017 conference submission,False,,"This paper proposes to use RNN and reinforcement learning for solving combinatorial optimization problems. The use of pointer network is interesting as it enables generalization to arbitrary input size. The proposed method also ""fintunes"" on test examples with active search to achieve better performance.

The proposed method is theoretically interesting as it shows that RNN and RL can be combined to solve combinatorial optimization problems and achieve comparable performance to traditional heuristic based algorithms.

However, the lack of complexity comparison against baselines make it impossible to tell whether the proposed method has any practical value. The matter is further complicated by the fact that the proposed method runs on GPU while baselines run on CPU: it is hard to even come up with a meaningful unit of complexity. Money spent on hardware and electricity per instance may be a viable option.

Further more, the performance comparisons should be taken with a grain of salt as traditional heuristic based algorithms can often give better performance if allowed more computation, which is not controlled across algorithms.

---

This was one of the more controversial submissions to this area, and there was extensive discussion over the merits and contributions of the work. The paper also benefitted from ICLRs open review system as additional researchers chimed in on the paper and the authors resubmitted a draft. The authors did a great job responding and updating the work and responding to criticisms. In the end though, even after these consideration, none of the reviewers strongly supported the work and all of them expressed some reservations. 
 
 Pros:
 - All agree that the work is extremely clear, going as far as saying the work is ""very well written"" and ""easy to understand"". 
 - Generally there was a predisposition to support the work for its originality particularly due to its ""methodological contributions"", and even going so far as a saying it would generally be a natural accept.
 
 Cons:
 - There was a very uncommonly strong backlash to the claims made by the paper, particularly the first draft, but even upon revisions. One reviewer even saying this was an ""excellent example of hype-generation far before having state-of-the-art results"" and that it was ""doing a disservice to our community since it builds up an expectation that the field cannot live up to"" . This does not seem to be an isolated reviewer, but a general feeling across the reviews. Another faulting ""the toy-ness of the evaluation metric"" and the way the comparisons were carried out.
 - A related concern was a feeling that the body of work in operations research was not fully taken account in this work, noting ""operations research literature is replete with a large number of benchmark problems that have become standard to compare solver quality"". The authors did fix some of these issues, but not to the point that any reviewer stood up for the work.

---

We ask reviewers to have a look at the new version of the paper again given the changes outlined below:

- We state clearly in the abstract, introduction, and conclusion that our results are still far from the state-of-the-art (this includes adding an updated version of Figure 1 back into the introduction).

- We include the original KnapSack baselines back into the paper.

- We explain in details how the running time of the LKH baseline is obtained.

- We modify the statement on the performance of greedy approaches: instead of stating that they are “just a few percents from optimality”, we express that they are “still quite far from optimality”.

We thank reviewers for their help in improving the quality of the paper.

---

I posted this question in a response below, but it seems to be getting ignored so I thought I'd bring it to the top, with some additional points.

Thanks for the update. The natural question to ask, then is - do there exist many (or any) problems that are both interesting and have not been, and cannot be, addressed by the existing combinatorial optimization community? You knock existing algorithms for being ""highly optimized"" to particular problems, but if every worthwhile problem has ""highly optimized"" solutions, what good is your work? 

Also, please stop calling existing TSP solvers such as concorde a heuristic. Concorde produces solutions which are provably correct. Your approach does not, nor is it remotely close. From a practical perspective, this is an important distinction; I don't see why anyone would choose the latter when given the choice. The second paragraphs of the related work and introduction are guilty of this. Also in the related work - you say it solves cities with ""thousands of cities"" when it has solved a 85k problem. 

I'd also echo concerns about the toy-ness of the evaluation metrics here - 100 cities is 800x smaller than existing SOTA of 85k from TSPLib - a gap made exponentially larger by the combinatorial nature of the problem.

---

We thank reviewers for their valuable feedback that helped us improve the paper. We appreciate their interest in the method and its novelty. We have made several changes to the paper which are summarized below. We ask reviewers to evaluate the new version of the paper and adjust their reviews if necessary.

1) Previous Figure 1, which was problematic due to different possible interpretations of “local search” was removed.

2) We added precise running time evaluations for all of the methods in the paper. Table 3 presents running time of the RL pretraining-greedy method and the solvers we compare against. Table 4 presents the performance and corresponding running time of RL pretraining-Sampling and RL pretraining-Active Search as a function of the number solutions considered. It shows how they can be stopped early at the cost of a small performance degradation. Table 6 contains the same information for the metaheuristics from OR-Tools vehicle routing library solver. We controlled the complexity of these approaches by letting all of them evaluate 1,280,000 solutions. Section 5.2 was rewritten in light of the new results.

3) We experimented with a new approach, called RL pretraining-Greedy@16, that decodes greedily from 16 different pretrained models at inference time and selects the shortest tour. It runs as fast as the solvers while only suffering from a small performance cost.

4) We added a discussion in Section 6 (Generalization to other problems) explaining how one may apply Neural Combinatorial Optimization to problems for which coming up with a feasible solution is challenging by itself.

5) We added a more detailed description of the critic network (see Section 4 - Critic’s architecture for TSP).

Please take a look and let us know your thoughts.

---

This paper applies the pointer network architecture—wherein an attention mechanism is fashioned to point to elements of an input sequence, allowing a decoder to output said elements—in order to solve simple combinatorial optimization problems such as the well-known travelling salesman problem. The network is trained by reinforcement learning using an actor-critic method, with the actor trained using the REINFORCE method, and the critic used to estimate the reward baseline within the REINFORCE objective.

The paper is well written and easy to understand. Its use of a reinforcement learning and attention model framework to learn the structure of the space in which combinatorial problems of variable size can be tackled appears novel. Importantly, it provides an interesting research avenue for revisiting classical neural-based solutions to some combinatorial optimization problems, using recently-developed sequence-to-sequence approaches. As such, I think it merits consideration for the conference.

I have a few comments and some important reservations with the paper:

1) I take exception to the conclusion that the pointer network approach can handle general types of combinatorial optimization problems. The crux of combinatorial problems — for practical applications — lies in the complex constraints that define feasible solutions (e.g. simple generalizations of the TSP that involve time windows, or multiple salesmen). For these problems, it is no longer so simple to exclude possible solutions from the enumeration of the solution by just « striking off » previously-visited instances; in fact, for many of these problems, finding a single feasible solution might in general be a challenge. It would be relevant to include a discussion of whether the Neural Combinatorial Optimization approach could scale to these important classes of problems, and if so, how. My understanding is that this approach, as presented, would be mostly suitable for assignment problems with a very simple constraint structure.

2) The operations research literature is replete with a large number of benchmark problems that have become standard to compare solver quality. For instance, TSPLIB contains a large number of TSP instances (

---

This paper is methodologically very interesting, and just based on the methodological contribution I would vote for acceptance. However, the paper's sweeping claims of clearly beating existing baselines for TSP have been shown to not hold, with the local search method LK-H solving all the authors' instances to optimality -- in seconds on a CPU, compared to clearly suboptimal results by the authors' method in 25h on a GPU. 

Seeing this clear dominance of the local search method LK-H, I find it irresponsible by the authors that they left Figure 1 as it is -- with the line for ""local search"" referring to an obviously poor implementation by Google rather than the LK-H local search method that everyone uses. For example, at NIPS, I saw this Figure 1 being used in a talk (I am not sure anymore by whom, but I don't think it was by the authors), the narrative being ""RNNs now also clearly perform better than local search"". Of course, people would use a figure like that for that purpose, and it is clearly up to the authors to avoid such misconceptions. 

The right course of action upon realizing the real strength of local search with LK-H would've been to make ""local search"" the same line as ""Optimal"", showing that the authors' method is still far worse than proper local search. But the authors chose to leave the figure as it was, still suggesting that their method is far better than local search. Probably the authors didn't even think about this, but this of course will mislead the many superficial readers. To people outside of deep learning, this must look like a sensational yet obviously wrong claim. I thus vote for rejection despite the interesting method. 

------------------------

Update after rebuttal and changes:

I'm torn about this paper. 

On the one hand, the paper is very well written and I do think the method is very interesting and promising. I'd even like to try it and improve it in the future. So, from that point of view a clear accept.

On the other hand, the paper was using extremely poor baselines, making the authors' method appear sensationally strong in comparison, and over the course of many iterations of reviewer questions and anonymous feedback, this has come down to the authors' methods being far inferior to the state of the art. That's fine (I expected that all along), but the problem is that the authors don't seem to want this to be true... E.g., they make statements, such as ""We find that both greedy approaches are time-efficient and just a few percents worse than optimality.""
That statement may be true, but it is very well known in the TSP community that it is typically quite trivial to get to a few percent worse than optimality. What's hard and interesting is to push those last few percent. 
(As a side note: the authors probably don't stop LK-H once it has found the optimal solution, like they do with their own method after finding a local optimum. LK-H is an anytime algorithm, so even if it ran for a day that doesn't mean that it didn't find the optimal solution after milliseconds -- and a solution a few percent suboptimal even faster).

Nevertheless, since the claims have been toned down over the course of the many iterations, I was starting to feel more positive about this paper when just re-reading it. That is, until I got to the section on Knapsack solving. The version of the paper I reviewed was not bad here, as it at least stated two simple heuristics that yield optimal solutions:

""Two simple heuristics are ExpKnap, which employs brand-and-bound with Linear Programming bounds (Pisinger, 1995), and MinKnap, which employs dynamic programming with enumerative bounds (Pisinger, 1997). Exact solutions can also be optained by quantizing the weights to high precisions and then performing dynamic programming with a pseudo-polynomial complexity (Bertsimas & Demir, 2002)."" That version then went on to show that these simple heuristics were already optimal, just like their own method.

In a revision between December 11 and 14, however, that paragraph, along with the optimal results of ExpKnap and MinKnap seems to have been dropped, and the authors instead introduced two new poor baseline methods (random search and greedy). This was likely in an effort to find some methods that are not optimal on these very easy instances. I personally find it pointless to present results for random search here, as nobody would use that for TSP. It's like comparing results on MNIST against a decision stump (yes, you'll do better than that, but that is not surprising). The results for greedy are interesting to see. However, dropping the strong results of the simple heuristics ExpKnap and MinKnap (and their entire discussion) appears unresponsible, since the resulting table in the new version of the paper now suggests that the authors' method is better than all baselines. Of course, if all that one is after is a column of bold numbers for ones own approach that's what one can do, but I don't find it responsible to hide the better baselines. Also, why don't the authors try at least the same OR-tools solver from Google that they tried for TSP? It seems to support Knapsack directly:

---

This paper proposes to use RNN and reinforcement learning for solving combinatorial optimization problems. The use of pointer network is interesting as it enables generalization to arbitrary input size. The proposed method also ""fintunes"" on test examples with active search to achieve better performance.

The proposed method is theoretically interesting as it shows that RNN and RL can be combined to solve combinatorial optimization problems and achieve comparable performance to traditional heuristic based algorithms.

However, the lack of complexity comparison against baselines make it impossible to tell whether the proposed method has any practical value. The matter is further complicated by the fact that the proposed method runs on GPU while baselines run on CPU: it is hard to even come up with a meaningful unit of complexity. Money spent on hardware and electricity per instance may be a viable option.

Further more, the performance comparisons should be taken with a grain of salt as traditional heuristic based algorithms can often give better performance if allowed more computation, which is not controlled across algorithms.

---

This is very interesting to me! Thank you for this.

After reading this paper, I tested the Concorde. I think the Concorde allows only integer distances(if use Euclidean distance, they round off), so cannot provide optimal solution of Euclidean TSP.
But error can be small if multiply the distance by a large constant.

I want to know that, if I correct, does 'optimal' means a solution which is very closed to optimal?

---

I am very glad to read ""Our model and training code will be made available soon."" Thanks for that! My question is: how soon is soon? During the review period? In time for the conference?

---

In Table 3, what is the performance for the missing values of RL pretraining with 10.000 batches for Sampling T=1 and T=T*? 

Since performance improved much more from 100 to 1.000 batches for RL pretraining Sampling T=T* than it did for RL pretraining AS (e.g., 5.79->5.71 vs 5.74->5.71 for TSP50), I would expect RL pretraining Sampling T=T* to do better than RL pretraining AS when you use 10.000 samples. This would also change your qualitative conclusion in Table 2 and the overall result of the paper. You seem to glance over this in the text by saying ""we sample 1000 batches from a pretrained model, afer which we do not see significant improvement"", but seeing the much larger ""gradient"" from 50, 100, and 1000 batches than for RL pretraining AS, and seeing how key the result is to the final take-away from the paper, I would be far more convinced by just seeing the numbers for 10.000 batches.

Also, what is actually the difference between RL pretraining Sampling T=1 and T=T*? (Maybe I just missed this in the text.)

---

There is a large body of work on solving TSP instances that this paper ignores. In particular, the concorde algorithm has produced provably optimal solutions to problems as large as 85,900 cities, and can solve 100+ city problems in a few seconds on a single 500MHz core. Thus, the claims made that this is even close to being a useful tool for solving TSP problems are demonstrably untrue.

---

This paper proposes to use RNN and reinforcement learning for solving combinatorial optimization problems. The use of pointer network is interesting as it enables generalization to arbitrary input size. The proposed method also ""fintunes"" on test examples with active search to achieve better performance.

The proposed method is theoretically interesting as it shows that RNN and RL can be combined to solve combinatorial optimization problems and achieve comparable performance to traditional heuristic based algorithms.

However, the lack of complexity comparison against baselines make it impossible to tell whether the proposed method has any practical value. The matter is further complicated by the fact that the proposed method runs on GPU while baselines run on CPU: it is hard to even come up with a meaningful unit of complexity. Money spent on hardware and electricity per instance may be a viable option.

Further more, the performance comparisons should be taken with a grain of salt as traditional heuristic based algorithms can often give better performance if allowed more computation, which is not controlled across algorithms.

---

This was one of the more controversial submissions to this area, and there was extensive discussion over the merits and contributions of the work. The paper also benefitted from ICLRs open review system as additional researchers chimed in on the paper and the authors resubmitted a draft. The authors did a great job responding and updating the work and responding to criticisms. In the end though, even after these consideration, none of the reviewers strongly supported the work and all of them expressed some reservations. 
 
 Pros:
 - All agree that the work is extremely clear, going as far as saying the work is ""very well written"" and ""easy to understand"". 
 - Generally there was a predisposition to support the work for its originality particularly due to its ""methodological contributions"", and even going so far as a saying it would generally be a natural accept.
 
 Cons:
 - There was a very uncommonly strong backlash to the claims made by the paper, particularly the first draft, but even upon revisions. One reviewer even saying this was an ""excellent example of hype-generation far before having state-of-the-art results"" and that it was ""doing a disservice to our community since it builds up an expectation that the field cannot live up to"" . This does not seem to be an isolated reviewer, but a general feeling across the reviews. Another faulting ""the toy-ness of the evaluation metric"" and the way the comparisons were carried out.
 - A related concern was a feeling that the body of work in operations research was not fully taken account in this work, noting ""operations research literature is replete with a large number of benchmark problems that have become standard to compare solver quality"". The authors did fix some of these issues, but not to the point that any reviewer stood up for the work.

---

We ask reviewers to have a look at the new version of the paper again given the changes outlined below:

- We state clearly in the abstract, introduction, and conclusion that our results are still far from the state-of-the-art (this includes adding an updated version of Figure 1 back into the introduction).

- We include the original KnapSack baselines back into the paper.

- We explain in details how the running time of the LKH baseline is obtained.

- We modify the statement on the performance of greedy approaches: instead of stating that they are “just a few percents from optimality”, we express that they are “still quite far from optimality”.

We thank reviewers for their help in improving the quality of the paper.

---

I posted this question in a response below, but it seems to be getting ignored so I thought I'd bring it to the top, with some additional points.

Thanks for the update. The natural question to ask, then is - do there exist many (or any) problems that are both interesting and have not been, and cannot be, addressed by the existing combinatorial optimization community? You knock existing algorithms for being ""highly optimized"" to particular problems, but if every worthwhile problem has ""highly optimized"" solutions, what good is your work? 

Also, please stop calling existing TSP solvers such as concorde a heuristic. Concorde produces solutions which are provably correct. Your approach does not, nor is it remotely close. From a practical perspective, this is an important distinction; I don't see why anyone would choose the latter when given the choice. The second paragraphs of the related work and introduction are guilty of this. Also in the related work - you say it solves cities with ""thousands of cities"" when it has solved a 85k problem. 

I'd also echo concerns about the toy-ness of the evaluation metrics here - 100 cities is 800x smaller than existing SOTA of 85k from TSPLib - a gap made exponentially larger by the combinatorial nature of the problem.

---

We thank reviewers for their valuable feedback that helped us improve the paper. We appreciate their interest in the method and its novelty. We have made several changes to the paper which are summarized below. We ask reviewers to evaluate the new version of the paper and adjust their reviews if necessary.

1) Previous Figure 1, which was problematic due to different possible interpretations of “local search” was removed.

2) We added precise running time evaluations for all of the methods in the paper. Table 3 presents running time of the RL pretraining-greedy method and the solvers we compare against. Table 4 presents the performance and corresponding running time of RL pretraining-Sampling and RL pretraining-Active Search as a function of the number solutions considered. It shows how they can be stopped early at the cost of a small performance degradation. Table 6 contains the same information for the metaheuristics from OR-Tools vehicle routing library solver. We controlled the complexity of these approaches by letting all of them evaluate 1,280,000 solutions. Section 5.2 was rewritten in light of the new results.

3) We experimented with a new approach, called RL pretraining-Greedy@16, that decodes greedily from 16 different pretrained models at inference time and selects the shortest tour. It runs as fast as the solvers while only suffering from a small performance cost.

4) We added a discussion in Section 6 (Generalization to other problems) explaining how one may apply Neural Combinatorial Optimization to problems for which coming up with a feasible solution is challenging by itself.

5) We added a more detailed description of the critic network (see Section 4 - Critic’s architecture for TSP).

Please take a look and let us know your thoughts.

---

This paper applies the pointer network architecture—wherein an attention mechanism is fashioned to point to elements of an input sequence, allowing a decoder to output said elements—in order to solve simple combinatorial optimization problems such as the well-known travelling salesman problem. The network is trained by reinforcement learning using an actor-critic method, with the actor trained using the REINFORCE method, and the critic used to estimate the reward baseline within the REINFORCE objective.

The paper is well written and easy to understand. Its use of a reinforcement learning and attention model framework to learn the structure of the space in which combinatorial problems of variable size can be tackled appears novel. Importantly, it provides an interesting research avenue for revisiting classical neural-based solutions to some combinatorial optimization problems, using recently-developed sequence-to-sequence approaches. As such, I think it merits consideration for the conference.

I have a few comments and some important reservations with the paper:

1) I take exception to the conclusion that the pointer network approach can handle general types of combinatorial optimization problems. The crux of combinatorial problems — for practical applications — lies in the complex constraints that define feasible solutions (e.g. simple generalizations of the TSP that involve time windows, or multiple salesmen). For these problems, it is no longer so simple to exclude possible solutions from the enumeration of the solution by just « striking off » previously-visited instances; in fact, for many of these problems, finding a single feasible solution might in general be a challenge. It would be relevant to include a discussion of whether the Neural Combinatorial Optimization approach could scale to these important classes of problems, and if so, how. My understanding is that this approach, as presented, would be mostly suitable for assignment problems with a very simple constraint structure.

2) The operations research literature is replete with a large number of benchmark problems that have become standard to compare solver quality. For instance, TSPLIB contains a large number of TSP instances (

---

This paper is methodologically very interesting, and just based on the methodological contribution I would vote for acceptance. However, the paper's sweeping claims of clearly beating existing baselines for TSP have been shown to not hold, with the local search method LK-H solving all the authors' instances to optimality -- in seconds on a CPU, compared to clearly suboptimal results by the authors' method in 25h on a GPU. 

Seeing this clear dominance of the local search method LK-H, I find it irresponsible by the authors that they left Figure 1 as it is -- with the line for ""local search"" referring to an obviously poor implementation by Google rather than the LK-H local search method that everyone uses. For example, at NIPS, I saw this Figure 1 being used in a talk (I am not sure anymore by whom, but I don't think it was by the authors), the narrative being ""RNNs now also clearly perform better than local search"". Of course, people would use a figure like that for that purpose, and it is clearly up to the authors to avoid such misconceptions. 

The right course of action upon realizing the real strength of local search with LK-H would've been to make ""local search"" the same line as ""Optimal"", showing that the authors' method is still far worse than proper local search. But the authors chose to leave the figure as it was, still suggesting that their method is far better than local search. Probably the authors didn't even think about this, but this of course will mislead the many superficial readers. To people outside of deep learning, this must look like a sensational yet obviously wrong claim. I thus vote for rejection despite the interesting method. 

------------------------

Update after rebuttal and changes:

I'm torn about this paper. 

On the one hand, the paper is very well written and I do think the method is very interesting and promising. I'd even like to try it and improve it in the future. So, from that point of view a clear accept.

On the other hand, the paper was using extremely poor baselines, making the authors' method appear sensationally strong in comparison, and over the course of many iterations of reviewer questions and anonymous feedback, this has come down to the authors' methods being far inferior to the state of the art. That's fine (I expected that all along), but the problem is that the authors don't seem to want this to be true... E.g., they make statements, such as ""We find that both greedy approaches are time-efficient and just a few percents worse than optimality.""
That statement may be true, but it is very well known in the TSP community that it is typically quite trivial to get to a few percent worse than optimality. What's hard and interesting is to push those last few percent. 
(As a side note: the authors probably don't stop LK-H once it has found the optimal solution, like they do with their own method after finding a local optimum. LK-H is an anytime algorithm, so even if it ran for a day that doesn't mean that it didn't find the optimal solution after milliseconds -- and a solution a few percent suboptimal even faster).

Nevertheless, since the claims have been toned down over the course of the many iterations, I was starting to feel more positive about this paper when just re-reading it. That is, until I got to the section on Knapsack solving. The version of the paper I reviewed was not bad here, as it at least stated two simple heuristics that yield optimal solutions:

""Two simple heuristics are ExpKnap, which employs brand-and-bound with Linear Programming bounds (Pisinger, 1995), and MinKnap, which employs dynamic programming with enumerative bounds (Pisinger, 1997). Exact solutions can also be optained by quantizing the weights to high precisions and then performing dynamic programming with a pseudo-polynomial complexity (Bertsimas & Demir, 2002)."" That version then went on to show that these simple heuristics were already optimal, just like their own method.

In a revision between December 11 and 14, however, that paragraph, along with the optimal results of ExpKnap and MinKnap seems to have been dropped, and the authors instead introduced two new poor baseline methods (random search and greedy). This was likely in an effort to find some methods that are not optimal on these very easy instances. I personally find it pointless to present results for random search here, as nobody would use that for TSP. It's like comparing results on MNIST against a decision stump (yes, you'll do better than that, but that is not surprising). The results for greedy are interesting to see. However, dropping the strong results of the simple heuristics ExpKnap and MinKnap (and their entire discussion) appears unresponsible, since the resulting table in the new version of the paper now suggests that the authors' method is better than all baselines. Of course, if all that one is after is a column of bold numbers for ones own approach that's what one can do, but I don't find it responsible to hide the better baselines. Also, why don't the authors try at least the same OR-tools solver from Google that they tried for TSP? It seems to support Knapsack directly:

---

This paper proposes to use RNN and reinforcement learning for solving combinatorial optimization problems. The use of pointer network is interesting as it enables generalization to arbitrary input size. The proposed method also ""fintunes"" on test examples with active search to achieve better performance.

The proposed method is theoretically interesting as it shows that RNN and RL can be combined to solve combinatorial optimization problems and achieve comparable performance to traditional heuristic based algorithms.

However, the lack of complexity comparison against baselines make it impossible to tell whether the proposed method has any practical value. The matter is further complicated by the fact that the proposed method runs on GPU while baselines run on CPU: it is hard to even come up with a meaningful unit of complexity. Money spent on hardware and electricity per instance may be a viable option.

Further more, the performance comparisons should be taken with a grain of salt as traditional heuristic based algorithms can often give better performance if allowed more computation, which is not controlled across algorithms.

---

This is very interesting to me! Thank you for this.

After reading this paper, I tested the Concorde. I think the Concorde allows only integer distances(if use Euclidean distance, they round off), so cannot provide optimal solution of Euclidean TSP.
But error can be small if multiply the distance by a large constant.

I want to know that, if I correct, does 'optimal' means a solution which is very closed to optimal?

---

I am very glad to read ""Our model and training code will be made available soon."" Thanks for that! My question is: how soon is soon? During the review period? In time for the conference?

---

In Table 3, what is the performance for the missing values of RL pretraining with 10.000 batches for Sampling T=1 and T=T*? 

Since performance improved much more from 100 to 1.000 batches for RL pretraining Sampling T=T* than it did for RL pretraining AS (e.g., 5.79->5.71 vs 5.74->5.71 for TSP50), I would expect RL pretraining Sampling T=T* to do better than RL pretraining AS when you use 10.000 samples. This would also change your qualitative conclusion in Table 2 and the overall result of the paper. You seem to glance over this in the text by saying ""we sample 1000 batches from a pretrained model, afer which we do not see significant improvement"", but seeing the much larger ""gradient"" from 50, 100, and 1000 batches than for RL pretraining AS, and seeing how key the result is to the final take-away from the paper, I would be far more convinced by just seeing the numbers for 10.000 batches.

Also, what is actually the difference between RL pretraining Sampling T=1 and T=T*? (Maybe I just missed this in the text.)

---

There is a large body of work on solving TSP instances that this paper ignores. In particular, the concorde algorithm has produced provably optimal solutions to problems as large as 85,900 cities, and can solve 100+ city problems in a few seconds on a single 500MHz core. Thus, the claims made that this is even close to being a useful tool for solving TSP problems are demonstrably untrue.",,,,,,6.0,,,4.0,,
611,"Authors: DUAL NETWORKS, Yilei Xiong, Dahua Lin, Haoying Niu, Jiefeng Cheng, Zhenguo Li
Source file: 611.pdf

ABSTRACT
Despite the long history of research on recommender systems, current approaches still face a number of challenges in practice, e.g. the difficulties in handling new items, the high diversity of user interests, and the noisiness and sparsity of observations. Many of such difficulties stem from the lack of expressive power to capture the complex relations between items and users. This paper presents a new method to tackle this problem, called Collaborative Deep Embedding. In this method, a pair of dual networks, one for encoding items and the other for users, are jointly trained in a collaborative fashion. Particularly, both networks produce embeddings at multiple aligned levels, which, when combined together, can accurately predict the matching between items and users. Compared to existing methods, the proposed one not only provides greater expressive power to capture complex matching relations, but also generalizes better to unseen items or users. On multiple real-world datasets, this method outperforms the state of the art.

Despite the long history of research on recommender systems, current approaches still face a number of challenges in practice, e.g. the difficulties in handling new items, the high diversity of user interests, and the noisiness and sparsity of observations. Many of such difficulties stem from the lack of expressive power to capture the complex relations between items and users. This paper presents a new method to tackle this problem, called Collaborative Deep Embedding. In this method, a pair of dual networks, one for encoding items and the other for users, are jointly trained in a collaborative fashion. Particularly, both networks produce embeddings at multiple aligned levels, which, when combined together, can accurately predict the matching between items and users. Compared to existing methods, the proposed one not only provides greater expressive power to capture complex matching relations, but also generalizes better to unseen items or users. On multiple real-world datasets, this method outperforms the state of the art.

1 INTRODUCTION
What do consumers really want? – this is a question to which everyone wishes to have an answer. Over the past decade, the unprecedented growth of web services and online commercial platforms such as Amazon, Netflix, and Spotify, gives rise to a vast amount of business data, which contain valuable information about the customers. However, “data don’t speak for themselves”. To accurately predict what the customers want, one needs not only the data, but also an effective means to extract useful messages therefrom.
There has been extensive study on recommender systems. Existing methods roughly fall into two categories, namely content-based filtering (Pazzani & Billsus, 2007) and collaborative filtering (Mnih & Salakhutdinov, 2008; Hu et al., 2008; Yu et al., 2009). The former focuses on extracting relevant features from the content, while the latter attempts to exploit the common interest among groups of users. In recent efforts, hybrid methods (Agarwal & Chen, 2009; Van den Oord et al., 2013) that combine both aspects have also been developed.
Whereas remarkable progress has been made on this topic, the state of the art remains far from satisfactory. The key challenges lie in several aspects. First, there is a large semantic gap between the true cause of a matching and what we observe from the data. For example, what usually attracts a book consumer is the implied emotion that one has to feel between the lines instead of the occurrences of certain words. It is difficult for classical techniques to extract such deep meanings from the observations. Second, the cold-start issue, namely making predictions for unseen items or users, has not been well addressed. Many collaborative filtering methods rely on the factorization of the matching matrix. Such methods implicitly assume that all the users and items are known in advance, and thus are difficult to be applied in real-world applications, especially online services.
The success of deep learning brings new inspiration to this task. In a number of areas, including image classification (Krizhevsky et al., 2012), speech recognition (Hinton et al., 2012), and natural language understanding (Socher et al., 2011), deep learning techniques have substantially pushed forward the state of the art. The power of deep networks in capturing complex variations and bridging semantic gaps has been repeatedly shown in previous study. However, deep models were primarily used for classification or regression, e.g. translating images to sentences. How deep networks can be used to model cross-domain relations remains an open question.
In this work, we aim to explore deep neural networks for learning the matching relations across two domains, with our focus placed on the matching between items and users. Specifically, we propose a new framework called Collaborative Deep Embedding, which comprises a pair of dual networks, one for encoding items and the other for users. Each network contains multiple embedding layers that are aligned with their dual counterparts of the other network. Predictions can then be made by coupling these embeddings. Note that unlike a conventional network, the dual networks are trained on two streams of data. In this paper, we devise an algorithm that can jointly train both networks using dual mini-batches. Compared to previous methods, this method not only narrows the semantic gap through a deep modeling architecture, but also provides a natural way to generalize – new items and new users can be encoded by the trained networks, just like those present in the training stage.
On a number of real world tasks, the proposed method yields significant improvement over the current state-of-the-art. It is worth stressing that whereas our focus is on the matching between items and users, Collaborative Deep Embedding is a generic methodology, which can be readily extended to model other kinds of cross-domain relations.

2 RELATED WORK
Existing methods for recommendation roughly fall into two categories: content-based methods (Pazzani & Billsus, 2007) and collaborative filtering (CF) (Mnih & Salakhutdinov, 2008; Hu et al., 2008; Yu et al., 2009). Specifically, content-based methods rely primarily on feature representation of the content, in which recommendations are often made based on feature similarity (Slaney et al., 2008). Following this, there are also attempts to incorporate additional information, such as meta-data of users, to further improve the performance (McFee et al., 2012). Instead, collaborative filtering exploits the interaction between users and items. A common approach to CF is to derive latent factors of both users and items through matrix factorization, and measure the degree of matching by their inner products. Previous study (Ricci et al., 2011) showed that CF methods tend to have higher recommendation accuracy than content-based methods, as they directly target the recommendation task. However, practical use of CF is often limited by the cold start problem. It is difficult to recommend items without a sufficient amount of use history. Issues like this motivated hybrid methods (Agarwal & Chen, 2009; Van den Oord et al., 2013) that combine both aspects of information, which have showed encouraging improvement. Our exploration is also along this line.
Despite the progress on both family of methods, the practical performance of state-of-the-art still leaves a lot to be desired. This, to a large extent, is due to the lack of capability of capturing complex variations in interaction patterns. Recently, deep learning (Bengio, 2009) emerges as an important technique in machine learning. In a number of successful stories (Krizhevsky et al., 2012; Hinton et al., 2012; Socher et al., 2011), deep models have demonstrated remarkable representation power in capturing complex patterns. This power has been exploited by some recent work for recommendation. Van den Oord et al. (2013) applies deep learning for music recommendation. It uses the latent item vector learned by CF as ground truth to train a deep network for extracting content features, obtaining considerable performance gain. However the latent vectors for known users and items are not improved. Wang & Wang (2014) proposed an extension to this method, which concatenates both the CF features and the deep features, resulting in slight improvement.
Wang & Blei (2011) showed that CF and topic modeling, when combined, can benefit each other. Inspired by this, Wang et al. (2015) proposed Collaborative Deep Learning (CDL), which incorporates CF and deep feature learning with a combined objective function. This work represents the latest advances in recommendation methods. Yet, its performance is still limited by several issues, e.g. the difficulties in balancing diversified objectives and the lack of effective methods for user encoding. An important aspect that distinguishes our work from CDL and other previous methods is that it encodes both items and users through a pair of deep networks that are jointly trained, which substantially
enhance the representation power on both sides. Moreover, the objective function of our learning framework directly targets the recommendation accuracy, which also leads to better performance.

3 COLLABORATIVE DEEP EMBEDDING
At the heart of a recommender system is matching model, namely, a model that can predict whether a given item matches the interest of a given user. Generally, this can be formalized as below. Suppose there are m users and n items, respectively indexed by i and j. Items are usually associated with inherent features, e.g. the descriptions or contents. Here, we use xj to denote the observed features of the j-th item. However, inherent information for users is generally very limited and often irrelevant. Hence, in most cases, users are primarily characterized by their history, i.e. the items they have purchased or rated. Specifically, the user history can be partly captured by a matching matrix R ∈ {0, 1}m×n, where R(i, j) = 1 indicates that the i-th user purchased the j-th item and gave a positive rating. Note that R is often an incomplete reflection of the user interest – it is not uncommon that a user does not purchase or rate an item that he/she likes.

3.1 DUAL EMBEDDING
To motivate our approach, we begin with a brief revisit of collaborative filtering (CF), which is widely adopted in practical recommender systems. The basic idea of CF is to derive vector representations for both users and items by factorizing the matching matrix R. A representative formulation in this family is the Weighted Matrix Factorization (WMF) (Hu et al., 2008), which adopts an objective function as below: ∑
i ∑ j cij(Rij − uTi vj)2 + λu ∑ i ‖ui‖22 + λv ∑ j ‖vj‖22. (1)
Here, ui and vj denote the vector representations of the i-th user and the j-th item, cij the confidence coefficient of an observed entry, and λu, λv the regularization coefficients. Underlying such methods lies a common assumption, namely, all users and items must be known a priori. As a result, they will face fundamental difficulties when handling new items and new users.
Encoding Networks. In this work, we aim to move beyond this limitation by exploring an alternative approach. Instead of pursuing the embeddings of a given set of items and users, our approach jointly learns a pair of encoding networks, respectively for items and users. Compared to CF, the key advantage of this approach is that it is generalizable by nature. When new items or new users come, their vector embeddings can be readily derived using the learned encoders.
Generally, the items can be encoded based on their own inherent features, using, for example, an auto-encoder. The key question here, however, is how to encode users, which, as mentioned, have no inherent features. Again, we revisit conventional CF methods such as WMF and find that in these methods, the user representations can be expressed as:
ui = argmin u ∑ j cij‖Rij − uTi vj‖2 + λu ∑ i ‖ui‖2 = ( VCiV T + λuI )−1 Vri. (2)
Here, V = [v1, . . . ,vn] is a matrix comprised of all item embeddings, each column for one; ri is the i-th row of R treated as a column vector, which represents the history of the i-th user; and Ci = diag(ci1, . . . , cin) captures the confidence weights.
The analysis above reveals that ui is a linear transform of ri as ui = Wuri, where the transform matrix Wu depends on the item embeddings V. This motivates our idea of user encoding, that is, to use a deep neural network instead the linear transform above, as
ui = g(ri;Wu), (3)
where g denotes a nonlinear transform based on a deep network with parameters Wu. As we will show in our experiments, by drawing on the expressive power of deep neural networks, the proposed way of user encoding can substantially improve the prediction accuracy.
Overall Formulation. By coupling an item-network denoted by f(xj ;Wv) and a user-network g as introduced above, we can predict the matching of any given pair of user and item based on the inner product of their embeddings, as 〈f(x;Wv), g(r;Wu)〉. The inputs to these networks include x, the inherent feature of the given item, and r, the history of the given user on a set of reference items. With both encoding networks, we formulate the learning objective as follows:
min Wu,Wv ∑ i ∑ j cij‖Rij − 〈f(xj ;Wv), g(ri;Wu)〉‖2. (4)
Here, X = [x1, . . . ,xn] denotes the input features of all reference items. This formulation differs from previous ones in two key aspects: (1) Both users and items are encoded using deep neural networks. The learning objective above encourages the cooperation of both networks such that the coupling of both sides yield the highest accuracy. Hence, the user-network parameters Wu depends on the item embeddings V, and likewise for the item-network. (2) The learning task is to estimate the parameters of the encoding networks. Once the encoding networks are learned, they encode users and items in a uniform way, no matter whether they are seen during training. In other words, new users and new items are no longer second-class citizens – they are encoded in exactly the same way as those in the training set.
Comparison with CDL. The Collaborative Deep Learning (CDL) recently proposed by Wang et al. (2015) was another attempt to tackle the cold-start issue. This method leverages the item features by aligning the item encoder with the embeddings resulted from matrix factorization. In particular, the objective function is given as follows:∑ ij cij(Rij−uTi vj)2+λv ∑ j ‖vj−fe(x̃j ,θ)‖2+λn ∑ j ‖x̃j−fr(x̃j ,θ)‖2+λu ∑ i ‖ui‖2+r(θ). (5) Here, a Stacked Denoising Autoencoder (SDAE) (Vincent et al., 2010) with parameter θ is used to encode the items, based on {x̃j}, noisy versions of their features. Compared to our formulation, CDL has several limitations: (1) The objective is to balance the SDAE reconstruction error and the matching accuracy, which does not necessarily lead to improved recommendation. Tuning this balance also turns out to be tricky. (2) Only items are encoded, while the representations of the users are still obtained by matrix factorization. As a result, its expressive power in capturing user interest remains limited. (3) There are inconsistencies between known items and new ones – the embedding of known items is resulted from a tradeoff between the matching accuracy and the fidelity to SDAE features, while the embedding of new items are purely based on SDAE encoding.

3.2 NETWORK ARCHITECTURE DESIGNS
Our model consists of two networks, namely the item-network f and the user-network g. We went through a progressive procedure in designing their architectures, obtaining three different designs, from basic design, multi-level design, to multi-level branching design. Each new design was motivated by the observation of certain limitations in the previous version.
The basic design, as shown in Figure 1 (a) adopts the multilayer perceptron as the basic architecture, using tanh as the nonlinear activation function between layers1. The top layer of the item-network produces a vector f(xj ;Wv) for each item; while that of the user-network produces a dual vector g(ri;Wu) for each user. During training, the loss layer takes their inner products and compares them with the ground-truth R(i, j).
Each layer in these networks generates a vector representation. We observe that representations from different layers are complementary. Representations from lower layers tend to be closer to the inputs and preserve more information; while those from higher layers focus on deeper semantics. The representations from these levels have their respective values, as different users tend to focus on different aspects of an item. Following this intuition, we reach a multi-level design, as shown in Figure 1 (b). In this design, dot products between dual embeddings at corresponding levels are aggregated to produce the final prediction.
There is an issue of the multi-level design – the output of each intermediate layer actually plays two roles. On one hand, it is the input to the next layer for further abstraction; on the other hand, it also serves as a facet to be matched with the other side. These two roles require different properties of the representations. Particularly, for the former role, the representation needs to preserve more information for higher-level abstraction; while for the latter, those parts related to the current level of matching need to be emphasized. To address this issue, we design a multi-level branching architecture, as shown in Figure 1 (c). In this design, a matching branch is introduced to transform the representation at each level to a form that is more suitable for matching. This can also be considered as learning an alternative metric to measure the matchness between the embeddings. As we will show in our experiments, this design can considerably improve the prediction accuracy.

4 TRAINING WITH DUAL MINI-BATCHES
A distinctive aspect of our training algorithm is the use of dual mini-batches. Specifically, in each iteration, Bv items and Bu users are selected. In addition to the item features and user histories, the corresponding part of the matching matrix R will also be loaded and fed to the network. Here, the two batch sizes Bv and Bu can be different, and they should be chosen according to the sparsity of the matching matrix R, such that each dual mini-batch can cover both positive and zero ratings.
During the backward pass, the loss layer that compares the predictions with the ground-truth matchings will produce two sets of gradients, respectively for items and users. These gradients are then back-propagated along respective networks. Note that when the multi-level designs (both with and without branching) are used, each intermediate layer will receive gradients from two sources – those from the upper layers and those from the dual network (via the dot-product layer). Hence, the training of one network would impact that of the other.
The entire training procedure consists of two stages: pre-training and optimization. In the pre-training stage, we initialize the item-network with unsupervised training (Vincent et al., 2010) and the usernetwork randomly. The unsupervised training of the item-network allows it to capture the feature statistics. Then both networks will be jointly refined in a layer-by-layer fashion. Particularly, we first tune the one-level networks, taking the dot products of their outputs as the predictions. Subsequently, we stack the second layers on top and refine them in a similar way. Empirically, we found that this layer-wise refinement scheme provides better initialization. In the optimization stage, we adopt the SGD algorithm with momentum and use the dual mini-batch scheme presented above. In this stage, the training is conducted in epochs. Each epoch, through multiple iterations, traverses the whole matching matrix R without repetition. The order of choosing mini-batches is arbitrary and will be shuffled at the beginning of each epoch. Additional tricks such as dropout and batch normalization are employed to further improve the performance.
1The choice of tanh as the activation function is based on empirical comparison.

5 EXPERIMENTS
We tested our method on three real-world datasets with different kinds of items and matching relations:
1. CiteULike, constructed by Wang & Blei (2011), provides a list of researchers and the papers that they interested. Each paper comes with a text document that comprises both the title and the abstract. In total, it contains 5, 551 researchers (as users) and 16, 980 papers (as items) with 0.22% density. The task is to predict the papers that a researcher would like.
2. MovieLens+Posters is constructed based on the MovieLens 20M Dataset (Harper & Konstan, 2016), which provides about 20M user ratings on movies. For each movie, we collect a movie poster from TMDb and extract a visual feature therefrom using a convolutional neural network (Szegedy et al., 2016) as the item feature. Removing all those movies without posters and the users with fewer than 10 ratings, we obtain a dataset that contains 76, 531 users and 14, 101 items with 0.24% density. In this dataset, all 5 ratings are considered as positive matchings.
3. Ciao is organized by Tang et al. (2012) from a product review site, where each product comes with a series of reviews. The reviews for each product are concatenated to serve as the item content. We removed those items with less than 5 rated users and the users with less than 10 ratings. This results in a dataset with 4, 663 users and 12, 083 items with 0.25% density. All ratings with 40 or above (the rating ranges from 0 to 50) are regarded as positive matchings.

5.1 EVALUATION
The performance of a recommender system can be assessed from different perspective. In this paper, we follow Wang & Blei (2011) and perform the evaluation from the retrieval perspective. Specifically, a fraction of rating entries are omitted in the training phase, and the algorithms being tested will be used to predict those entries. As pointed out by Wang & Blei (2011), as the ratings are implicit feedback (Hu et al., 2008) – some positive matchings are not reflected in the ratings, recall is more suitable than precision in measuring the performance. In particular, we use Recall@M averaged over all users as the performance metric. Here, for a certain user, Recall@M is defined as follows:
recall@M = the number of items a user likes in top M recommendations
the total number of items the user likes .
In our experiments, the value of M varies from 50 to 200.
Following Wang & Blei (2011), we consider two tasks, in-matrix prediction and out-matrix prediction. Specifically, we divide all users into two disjoint parts, known and unknown, by the ratio of 9 to 1. The in-matrix prediction task only considers known items. For this task, all rating entries are split into three disjoint sets: training, validation and testing, by the ratio 3 : 1 : 1. It is ensured that all items in the validation and testing sets have appeared in the training stage (just that part of their ratings were omitted). The out-matrix prediction task is to make predictions for the items that are completely unseen in the training phase. This task is to test the performance of generalization and the capability of handling the cold-start issue.

5.2 COMPARISON WITH OTHER METHODS
We compared our method, which we refer to as DualNet with two representative methods in previous work: (1) Weighted Matrix Factorization (WMF) (Hu et al., 2008), a representative method for for collaborative filtering (CF), and (2) Collaborative deep learning (CDL) (Wang et al., 2015), a hybrid method that combines deep encoding of the items and CF, which represents the latest advances in recommendation techniques.
On each dataset, we chose the design parameters for each method via grid search. The parameter combinations that attain best performance on the validation set are used. For our DualNet method, we adopt a three-level branching configuration, where the embedding dimensions of each network, from bottom to top, are set to 200, 200, 50. For WMF, the latent dimension is set to 300 on CDL and 450 on other datasets. For CDL, the best performance is attained when the structure of SDAE is configured to be (2000, 1000, 300), with drop out ratio 0.1. Other design parameters of CDL are set as a = 1.0, b = 0.01, lu = 1, lv = 10, ln = 1000, lw = 0.0005.
Note that on CiteULike, there are two ways to split the data. One is the scheme in (Wang et al., 2015), and the other is the scheme in (Wang & Blei, 2011), which is the one presented in the previous section. Note that in the former scheme, a fixed number of ratings from each user are selected for training. This may result in some testing items being missed in the training set. To provide a complete comparison with prior work, we use both schemes in our experiments, which are respectively denoted as CiteULike1 and CiteULike2.
Table 1 compares the performance of WML, CDL, and DualNet on all three datasets (four data splitting settings). From the results, we observed: (1) Our proposed DualNet method outperforms both WML and CDL on all datasets. On certain data sets, the performance gains are substantial. For example, on MovieLens, we obtained average recalls at 44.95%, 59.15%, and 72.56% respectively when M = 50, 100, 200. Comparing what CDL achieves (38.11%, 49, 73%, and 61.00%), the relative gains are around 18%. On other data sets, the gains are also considerable. (2) The performance gains vary significantly across different datasets, as they are closely related to the relevance of the item features. Particularly, when the item features are pertinent to the user interest, we may see remarkable improvement when those features are incorporated; otherwise, the performance gains would be relatively smaller.

5.3 DETAILED STUDY
We conducted additional experiments on CiteULike to further study the proposed algorithm. In this study, we investigate the performance of out-matrix prediction, the impact of various modeling choices, e.g. multi-level branching, as well as the influence of training tactics.
Out-matrix prediction. As mentioned, the out-matrix prediction task is to examine an algorithm’s capability of handling new items, i.e. those unseen in the training stage. For this task, we compared CDL and DualNet on the CiteULike dataset. WML is not included here as it is not able to handle new items. Table 2 shows the results. It can be clearly seen that DualNet outperforms CDL by a notable margin. For example, Recall@50 increases from 32.18% to 47.51% – the relative gain is 47.6%, a very remarkable improvement. The strong generalization performance as demonstrated here is, to a large extent, ascribed to our basic formulation, where the encoding networks uniformly encode both known and new items.
Multi-level branching. We compared three different designs presented in Section 3: basic design, multi-level design, and multi-level branching design. From the results shown in Table 3, we can observe limited improvement of the multi-level design over the basic one. More significant performance
gains are observed when the branching design is introduced. This shows that the branches contribute a lot to the overall performance.
Noise injection. Sometimes we noticed overfitting during training i.e. the validation performance gets worse while the training loss is decreasing. To tackle this issue, we inject noises to the inputs, i.e. setting a fraction of input entries to zeros. Generally, we observed that noise injection has little effect for Recall@M on in-matrix predictions when M < 30. However, it can considerably increase the recall for largeM value or out-matrix predictions. Particularly, on CiteULike, it increases in-matrix Recall@300 from 67.3% to 71.2%, and out-matrix Recall@50 from 38.6% to 47.5%.
Unsuccessful Tactics. Finally, we show some tactics that we have tried and found to be not working. (1) Replacing the weighted Euclidean loss with logistic loss would lead to substantial degradation of the performance (sometimes by up to 20%). Also, when using logistic loss, we observed severe overfitting. Rendle et al. (2009) proposed Bayesian Personalized Recommendation (BPR) which directly targets on ranking. We tested this on CiteULike with parameters tuned to obtain the optimal performance. Our experimental results showed that its performance is similar to WMF. Particularly, the Recall@50, 100, 200 for BPR are respectively 39.11%, 49.16%, 59.96%, while those for WMF are 40.45%, 50.25%, 59.95%.
(2) Motivated by the observation that positive ratings are sparse, we tried a scheme that ignores a fraction of dual mini-batches that correspond to all zero ratings, with an aim to speed up the training. Whereas this can reduces the time needed to run an epoch, it takes significantly more epochs to reach the same level of performance. As a result, the overall runtime is even longer.

6 CONCLUSIONS AND FUTURE WORK
This paper presented a new method for predicting the interactions between users and items, called Collaborative Deep Embedding. This method uses dual networks to encode users and items respectively. The user-network and item-network are trained jointly, in a collaborative manner, based on two streams of data. We obtained considerable performance gains over the state-of-the-art consistently on three large datasets. The proposed method also demonstrated superior generalization performance (on out-matrix predictions). This improvement, from our perspective, is ascribed to three important reasons: (1) the expressive power of deep models for capturing the rich variations in user interests, (2) the collaborative training process that encourages closely coupled embeddings, and (3) an objective function that directly targets the prediction accuracy.
We consider this work as a significant step that brings the power of deep models to relational modeling. However, the space of deep relational modeling remains wide open – lots of questions remain yet to be answered. In future, we plan to investigate more sophisticated network architectures, and extend the proposed methodology to applications that involve more than two domains.
","Despite the long history of research on recommender systems, current approaches still face a number of challenges in practice, e.g. the difficulties in handling new items, the high diversity of user interests, and the noisiness and sparsity of observations. Many of such difficulties stem from the lack of expressive power to capture the complex relations between items and users. This paper presents a new method to tackle this problem, called Collaborative Deep Embedding. In this method, a pair of dual networks, one for encoding items and the other for users, are jointly trained in a collaborative fashion. Particularly, both networks produce embeddings at multiple aligned levels, which, when combined together, can accurately predict the matching between items and users. Compared to existing methods, the proposed one not only provides greater expressive power to capture complex matching relations, but also generalizes better to unseen items or users. On multiple real-world datasets, this method outperforms the state of the art.",ICLR 2017 conference submission,False,,"The responses to the pre-review questions are not strong; especially w.r.t. the question about dataset density and why the dataset had to be subsampled, the authors responded that subsampling is common in recommender systems work, including the papers cited. This isn't a particularly strong justification of why subsampling is a good idea, and in particular doesn't answer the question of ""how would the results look without subsampling,"" which I think is a question that could easily have been answered directly.

Especially given that the goal of dealing with the cold-start issue is so heavily emphasized in the paper, in seems odd to sample the data to reduce sparsity.

Other than that, the pre-review questions seem to have been answered satisfactorily.

The contribution of the paper is to propose user and item embedding methods, as a means of learning complex non-linear interactions between users and items. This is fairly similar to recent work on deep RS, though the network formulation has some differences.

Overall this is an reasonably put together paper that makes a contribution in an important area, though there are still some shortcomings that should be addressed, namely:
1) The evaluation is unusual. Recall@M is the only result reported, though this is not usually an evaluation seen in recommender systems research. At the very least other performance measures (rmse or AUC) should be reported for completeness, even if the results are not strong
2) Given that the contribution is fairly simple (i.e., the ""standard"" recommender systems task, but with a new model) it's a shame that unusual data samples have to be taken. This should be a case where it's possible to report results against competing methods using *exactly* the same data they used, and exactly the same error measure, for the fairest comparison possible.

Without the above it's hard to tell how much the performance improvements are really due to the method being better, versus the choice of datasets and the choice of loss functions.

---

The paper presents a collaborative filtering method, using dual deep nets for users and items. The nets can take advantage of content in addition to ratings. This contribution is just below the bar, in that its novelty relative to existing methods is limited and the results are good but not sufficiently impressive, especially since they focus exclusively on Recall@N. In the response, the authors do present results on other metrics but the results are mixed.

---

The authors proposed to learn embeddings of users and items by using deep neural network for a recommendation task. The resulting method has only minor differences from the previous CDL, in which neural networks were also used for recommendation tasks. In the experiments, since the proposed method, DualNets have use more item features than WMF and CDL, the comparisons are unfair.

---

This paper provides a minor improvement paper of DeepRS. The major improvement comes from the coupling of user-item factors in prediction. While the motivation is clear, the improvement of the model architecture is minor. 
I think the author should improve the paper to discuss more on the impact of introduction of coupling, which might make this paper stronger. Specifically, conduct isolate experiment to change loss, architecture gradually, from a non-coupled network to a final proposed coupled network to demonstrate the importance of coupling.
Another important missing part of the paper seems to be time complexity. Since coupled net would be much more costly to generate recommendations, a discussion on how it would impact real world usages should be added.
Overall, I think this is a paper that should be improved before accepted.

---

The responses to the pre-review questions are not strong; especially w.r.t. the question about dataset density and why the dataset had to be subsampled, the authors responded that subsampling is common in recommender systems work, including the papers cited. This isn't a particularly strong justification of why subsampling is a good idea, and in particular doesn't answer the question of ""how would the results look without subsampling,"" which I think is a question that could easily have been answered directly.

Especially given that the goal of dealing with the cold-start issue is so heavily emphasized in the paper, in seems odd to sample the data to reduce sparsity.

Other than that, the pre-review questions seem to have been answered satisfactorily.

The contribution of the paper is to propose user and item embedding methods, as a means of learning complex non-linear interactions between users and items. This is fairly similar to recent work on deep RS, though the network formulation has some differences.

Overall this is an reasonably put together paper that makes a contribution in an important area, though there are still some shortcomings that should be addressed, namely:
1) The evaluation is unusual. Recall@M is the only result reported, though this is not usually an evaluation seen in recommender systems research. At the very least other performance measures (rmse or AUC) should be reported for completeness, even if the results are not strong
2) Given that the contribution is fairly simple (i.e., the ""standard"" recommender systems task, but with a new model) it's a shame that unusual data samples have to be taken. This should be a case where it's possible to report results against competing methods using *exactly* the same data they used, and exactly the same error measure, for the fairest comparison possible.

Without the above it's hard to tell how much the performance improvements are really due to the method being better, versus the choice of datasets and the choice of loss functions.

---

The responses to the pre-review questions are not strong; especially w.r.t. the question about dataset density and why the dataset had to be subsampled, the authors responded that subsampling is common in recommender systems work, including the papers cited. This isn't a particularly strong justification of why subsampling is a good idea, and in particular doesn't answer the question of ""how would the results look without subsampling,"" which I think is a question that could easily have been answered directly.

Especially given that the goal of dealing with the cold-start issue is so heavily emphasized in the paper, in seems odd to sample the data to reduce sparsity.

Other than that, the pre-review questions seem to have been answered satisfactorily.

The contribution of the paper is to propose user and item embedding methods, as a means of learning complex non-linear interactions between users and items. This is fairly similar to recent work on deep RS, though the network formulation has some differences.

Overall this is an reasonably put together paper that makes a contribution in an important area, though there are still some shortcomings that should be addressed, namely:
1) The evaluation is unusual. Recall@M is the only result reported, though this is not usually an evaluation seen in recommender systems research. At the very least other performance measures (rmse or AUC) should be reported for completeness, even if the results are not strong
2) Given that the contribution is fairly simple (i.e., the ""standard"" recommender systems task, but with a new model) it's a shame that unusual data samples have to be taken. This should be a case where it's possible to report results against competing methods using *exactly* the same data they used, and exactly the same error measure, for the fairest comparison possible.

Without the above it's hard to tell how much the performance improvements are really due to the method being better, versus the choice of datasets and the choice of loss functions.

---

The paper presents a collaborative filtering method, using dual deep nets for users and items. The nets can take advantage of content in addition to ratings. This contribution is just below the bar, in that its novelty relative to existing methods is limited and the results are good but not sufficiently impressive, especially since they focus exclusively on Recall@N. In the response, the authors do present results on other metrics but the results are mixed.

---

The authors proposed to learn embeddings of users and items by using deep neural network for a recommendation task. The resulting method has only minor differences from the previous CDL, in which neural networks were also used for recommendation tasks. In the experiments, since the proposed method, DualNets have use more item features than WMF and CDL, the comparisons are unfair.

---

This paper provides a minor improvement paper of DeepRS. The major improvement comes from the coupling of user-item factors in prediction. While the motivation is clear, the improvement of the model architecture is minor. 
I think the author should improve the paper to discuss more on the impact of introduction of coupling, which might make this paper stronger. Specifically, conduct isolate experiment to change loss, architecture gradually, from a non-coupled network to a final proposed coupled network to demonstrate the importance of coupling.
Another important missing part of the paper seems to be time complexity. Since coupled net would be much more costly to generate recommendations, a discussion on how it would impact real world usages should be added.
Overall, I think this is a paper that should be improved before accepted.

---

The responses to the pre-review questions are not strong; especially w.r.t. the question about dataset density and why the dataset had to be subsampled, the authors responded that subsampling is common in recommender systems work, including the papers cited. This isn't a particularly strong justification of why subsampling is a good idea, and in particular doesn't answer the question of ""how would the results look without subsampling,"" which I think is a question that could easily have been answered directly.

Especially given that the goal of dealing with the cold-start issue is so heavily emphasized in the paper, in seems odd to sample the data to reduce sparsity.

Other than that, the pre-review questions seem to have been answered satisfactorily.

The contribution of the paper is to propose user and item embedding methods, as a means of learning complex non-linear interactions between users and items. This is fairly similar to recent work on deep RS, though the network formulation has some differences.

Overall this is an reasonably put together paper that makes a contribution in an important area, though there are still some shortcomings that should be addressed, namely:
1) The evaluation is unusual. Recall@M is the only result reported, though this is not usually an evaluation seen in recommender systems research. At the very least other performance measures (rmse or AUC) should be reported for completeness, even if the results are not strong
2) Given that the contribution is fairly simple (i.e., the ""standard"" recommender systems task, but with a new model) it's a shame that unusual data samples have to be taken. This should be a case where it's possible to report results against competing methods using *exactly* the same data they used, and exactly the same error measure, for the fairest comparison possible.

Without the above it's hard to tell how much the performance improvements are really due to the method being better, versus the choice of datasets and the choice of loss functions.",,,,,,4.666666666666667,,,3.6666666666666665,,
612,"Authors: Joost van Amersfoort, Anitha Kannan, Marc’Aurelio Ranzato, Arthur Szlam, Du Tran
Source file: 612.pdf

ABSTRACT
In this work we propose a simple unsupervised approach for next frame prediction in video. Instead of directly predicting the pixels in a frame given past frames, we predict the transformations needed for generating the next frame in a sequence, given the transformations of the past frames. This leads to sharper results, while using a smaller prediction model. In order to enable a fair comparison between different video frame prediction models, we also propose a new evaluation protocol. We use generated frames as input to a classifier trained with ground truth sequences. This criterion guarantees that models scoring high are those producing sequences which preserve discriminative features, as opposed to merely penalizing any deviation, plausible or not, from the ground truth. Our proposed approach compares favourably against more sophisticated ones on the UCF-101 data set, while also being more efficient in terms of the number of parameters and computational cost.

1 INTRODUCTION
There has been an increased interest in unsupervised learning of representations from video sequences (Mathieu et al., 2016; Srivastava et al., 2015; Vondrick et al., 2016). A popular formulation of the task is to learn to predict a small number of future frames given the previous K frames; the motivation being that predicting future frames requires understanding how objects interact and what plausible sequences of motion are. These methods directly aim to predict pixel values, with either MSE loss or adversarial loss.
In this paper, we take a different approach to the problem of next frame prediction. In particular, our model operates in the space of transformations between frames, directly modeling the source of variability. We exploit the assumption that the transformations of objects from frame to frame should be smooth, even when the pixel values are not. Instead of predicting pixel values, we directly predict how objects transform. The key insight is that while there are many possible outputs, predicting one such transformation will yield motion that may not correspond to ground truth, yet will be realistic; see fig. 1. We therefore propose a transformation-based model that operates in the space of affine transforms. Given the affine transforms of a few previous frames, the model learns to predict the local affine transforms that can be deterministically applied on the image patches of the previous frame to generate the next frame. The intuition is that estimation errors will lead to a slightly different yet plausible motion. Note that this allows us to keep using the MSE criterion, which is easy to optimize, as long as it is in transformation space. No blur in the pixel space will be introduced since the output of the transformation model is directly applied to the pixels, keeping sharp edges intact. Refer to fig. 5 and our online material 1 for examples.
The other contribution of this work is the evaluation protocol. Typically, generative models of video sequences are evaluated in terms of MSE in pixel space (Srivastava et al., 2015), which is not a good choice since this metric favors blurry predictions over other more realistic looking options that just happen to differ from the ground truth. Instead, we propose to feed the generated frames to a video
∗Work done as part of internship with FAIR 1see: http://joo.st/ICLR/GenerationBenchmark
classifier trained on ground truth sequences. The idea is that the less the classifier’s performance is affected by the generates frames the more the model has preserved distinctive features and the more the generated sequences are plausible. Regardless of whether they resemble the actual ground truth or not. This protocol treats the classifier as a black box to measure how well the generated sequences can serve as surrogate for the truth sequence for the classification task. In this paper we will validate our assumption that motion can be modelled by local affine transforms, after which we will compare our method with networks trained using adversarial training and simple regression on the output frame, using both this new evaluation protocol and by providing samples for qualitative inspection.
Our experiments show that our simple and efficient model outperforms other baselines, including much more sophisticated models, on benchmarks on the UCF-101 data set (Soomro et al., 2012). We also provide qualitative comparisons to the moving MNIST digit data set (Srivastava et al., 2015).

1.1 RELATED WORK
Early work on video modeling focused on predicting small patches (Michalski et al., 2014; Srivastava et al., 2015); unfortunately, these models have not shown to scale to the complexity of highresolution videos. Also these models require a significant amount of parameters and computational power for even relatively simple data.
In Ranzato et al. (2014), the authors circumvented this problem by quantizing the space of image patches. While they were able to predict a few high-resolution frames in the future, it seems dissatisfying to impose such a drastic assumption to simplify the prediction task.
Mathieu et al. (2016) recently proposed to replace MSE in pixel space with a MSE on image gradients, leveraging prior domain knowledge, and further improved using a multi-scale architecture with adversarial training (Goodfellow et al., 2014). While producing better results than earlier methods, the models used require a very large amount of computational power. We make an explicit comparison to this paper in the experiments section 3.
In Oh et al. (2015), frames of a video game are predicted given an action (transformation) taken by the player. While the paper shows great results, the movement in a natural video cannot be described by a simple action and is therefore not widely applicable. Finally, our work is also related to optical flow estimation (Brox et al., 2004). Instead of estimating the flow of pixels, here we estimate the flow of patches and separately predict how these patches transform in future frames.
Prior work relating to the evaluation protocol can be found in Yan et al. (2015). The authors generate images using a set of predefined attributes and later show that they can recover these using a pretrained neural network. Our proposal extends this to videos, which is more complicated since both appearance and motion are needed for correct classification.

2 MODEL
The model we propose is based on three key assumptions: 1) just estimating object motion yields sequences that are plausible and relatively sharp, 2) global motion can be estimated by tiling highresolution video frames into patches and estimating motion “convolutionally” at the patch level, and 3) patches at the same spatial location over two consecutive time steps undergo a deformation which can be well described by an affine transformation.
The first assumption is at the core of the proposed method: by considering uncertainty in the space of transformations we produce sequences that may still look plausible. The other two assumptions state that a video sequence can be composed by patches undergoing affine transformations. We agree that these are simplistic assumptions, which ignore how object identity affects motion and do not account for out of plane rotations and more general forms of deformation. However, our qualitative and quantitative evaluation shows the efficacy of these assumptions to real video sequence as can be seen in section 3 and from visualizations in the supplementary material2.
Our approach consists of three steps. First, we estimate affine transforms of every video sequence to build a training set for our model. Second, we train a model that takes the past N affine transforms and predicts the next M affine transforms. Finally, at test time, the model uses the predicted affine transforms to reconstruct pixel values of the generated sequence. We describe the details of each phase in the following sections.

2.1 AFFINE TRANSFORM EXTRACTOR
Given a frame x and the subsequent frame y, the goal of the affine transform extractor is to learn mappings that can warp x into y. Since different parts of the scene may undergo different transforms, we tile x into overlapping patches and infer a transformation for each patch. The estimation process couples the transformations at different spatial locations because we minimize the reconstruction error of the entire frame y, as opposed to treating each patch independently.
2see: http://joo.st/ICLR/ReconstructionsFromGroundTruth and http://joo.st/ ICLR/GenerationBenchmark
Let x and y have size Dr ×Dc. Let image x be decomposed into a set of overlapping patches, each containing pixels from patches of size dr×dc with dr ≤ Dr and dc ≤ Dc. These patches are laid out on a regular grid with stride sr and sc pixels over rows and columns, respectively. Therefore, every pixel participates in drsr dc sc
overlapping patches, not taking into account for the sake of simplicity border effects and non-integer divisions. We denote the whole set of overlapping patches by {Xk}, where index k runs over the whole set of patches. Similarly and using the same coordinate system, we denote by {Yk} the set of overlapping patches of y. We assume that there is an affine mapping Ak that maps Xk to Yk, for all values of k. Ak is a 2 × 3 matrix of free parameters representing a generic affine transform (translation, rotation and scaling) between the coordinates of output and input frame. Let Ỹk be the transformed patches obtained when Ak is applied to Xk. Since coordinates overlap between patches, we reconstruct y by averaging all predictions at the same location, yielding the estimate ỹ. The joint set of Ak is then jointly determined by minimizing the mean squared reconstruction error between y and ỹ.
Notice that our approach and aim differs from spatial transformer networks (Jaderberg et al., 2015) since we perform this estimation off-line only for the input frames, computing one transform per patch.
In our experiments, we extracted 16 × 16 pixel patches from the input and we used stride 4 over rows and columns. The input patches are then matched at the output against smaller patches of size 8× 8 pixels, to account for objects moving in and out of the patch region.

2.2 AFFINE TRANSFORM PREDICTOR
The affine transform predictor is used to predict the affine transforms between the last input frame and the next frame in the sequence. A schematic illustration of the system is shown in fig. 2. It receives as input the affine transforms between pairs of adjacent frames, as produced by the affine transform extractor described in the previous section. Each transform is arranged in a grid of size 6 × n × n, where n is the number of patches in a row/column and 6 is the number of parameters of each affine transform. Therefore, if four frames are used to initialize the model, the actual input consists of 18 maps of size n × n, which are the concatenation of At−2, At−1, At, where At is the collection of patch affine transforms between frame at time t− 1 and t. The model consists of a multi-layer convolutional network without any pooling. The network is the composition of convolutional layers with ReLU non-linearity, computing a component-wise thresholding as in v = max(0, u). We learn the parameters in the filters of the convolutional layers by minimizing the mean squared error between the output of the network and the target transforms.
Notice that we do not add any regularization to the model. In particular, we rely on the convolutional structure of the model to smooth out predictions at nearby spatial locations.

2.3 MULTI-STEP PREDICTION
In the previous section, we described how to predict the set of affine transforms at the next time step. In practice, we would like to predict several time steps in the future.
A greedy approach would: a) train as described above to minimize the prediction error for the affine transforms at the next time step, and b) at test time, predict one step ahead and then re-circulate the model prediction back to the input to predict the affine transform two steps ahead, etc. Unfortunately, errors may accumulate throughout this process because the model was never exposed to its own predictions at training time.
The approach we propose replicates the model over time, also during training as shown in fig. 3. If we wish to predict M steps in the future, we replicate the CNN M times and pass the output of the CNN at time step t as input to the same CNN at time step t + 1, as we do at test time. Since predictions live in a continuous space, the whole system is differentiable and amenable to standard back-propagation of the error. Since parameters of the CNN are shared across time, the overall system is equivalent to a peculiar recurrent neural network, where affine transforms play the role of recurrent states. The experiments in section 3 demonstrate that this method is more accurate and robust than the greedy approach.

2.4 TESTING
At test time, we wish to predict M frames in the future given the past N frames. After extracting the N − 1 affine transforms from the frames we condition upon, we replicate the model M times and feed its own prediction back to the input, as explained in the previous section.
Once the affine transforms are predicted, we can reconstruct the actual pixel values. We use the last frame of the sequence and apply the first set of affine transforms to each patch in that frame. Each pixel in the output frame is predicted multiple times, depending on the stride used. We average these predictions and reconstruct the whole frame. As required, we can repeat this process for as many frames as necessary, using the last reconstructed frame and the next affine transform.
In order to evaluate the generation, we propose to feed the generated frames to a trained classifier for a task of interest. For instance, we can condition the generation using frames taken from video clips which have been labeled with the corresponding action. The classifier has been trained on ground truth data but it is evaluated using frames fantasized by the generative model. The performance of the classifier on ground truth data is an upper bound on the performance of any generative model. This evaluation protocol does not penalize any generation that deviates from the ground truth, as standard MSE would. It instead check that discriminative features and the overall semantics of the generated sequence is correct, which is ultimately what we are interested in.

3 EXPERIMENTS
In this section, we validate the key assumptions made by our model and compare against state-ofthe-art generative models on two data sets. We strongly encourage the reader to watch the short video clips in the Supplementary Material to better understand the quality of our generations.
In section 2, we discussed the three key assumptions at the foundations of our model: 1) errors in the transformation space look still plausible, 2) a frame can be decomposed into patches, and 3) each patch motion is well modeled by an affine transform. The results in the Supplementary Material 3 validate assumption 2 and 3 qualitatively. Every row shows a sequence from the UCF101 dataset (Soomro et al., 2012). The column on the left shows the original video frames and the one on the right the reconstructions from the estimated affine transforms, as described in section 2.1. As you can see there is barely any noticeable difference between these video sequences, suggesting that video sequences can be very well represented as tiled affine transforms. For a quantitative comparison and for an assessment of how well the first assumption holds, please refer to section 3.2.
3see: http://joo.st/ICLR/ReconstructionsFromGroundTruth
In the next section, we will first report some results using the toy data set of “moving MNIST digits” (Srivastava et al., 2015). We then discuss generations of natural high-resolution videos using the UCF-101 dataset and compare to current state-of-the-art methods.

3.1 MOVING MNIST
For our first experiment, we used the dataset of moving MNIST digits (Srivastava et al., 2015) and perform qualitative analysis4. It consists of one or two MNIST digits, placed at random locations and moving at constant speed inside a 64 × 64 frame. When a digit hits a boundary, it bounces, meaning that velocity in that direction is reversed. Digits can occlude each other and bounce off walls, making the data set challenging.
Using scripts provided by Srivastava et al. (2015), we generated a fixed dataset of 128,000 sequences and used 80% for training, 10% for validation and 10% for testing. Next, we estimated the affine transforms between every pair of adjacent frames to a total of 4 frames, and trained a small CNN in the space of affine transforms. The CNN has 3 convolutional layers and the following number of feature maps: 18, 32, 32, 6. All filters have size 3× 3. Fig. 4 shows some representative test sequences and the model outputs. Each subfigure corresponds to a sequence from the test set; the top row corresponds to the ground truth sequence while the bottom row shows the generations. The input to the CNN are three sets of affine transforms corresponding to the first four consecutive frames. The network predicts the next six sets of affine transforms from which we reconstruct the corresponding frames. These results should be compared to fig. 5 in Srivastava et al. (2015). The generations in fig. 4 show that the model has potential to represent and generate video sequences, it learns to move digits in the right direction, to bounce them, and it handles multiple digits well except when occluion makes inputs too ambiguous. The model’s performance is analyzed quantitatively in the next section using high resolution natural videos.

3.2 UCF 101 DATA SET
The UCF-101 dataset (Soomro et al., 2012) is a collection of 13320 videos of 101 action categories. Frames have size 240× 320 pixels. We train a CNN on patches of size 64× 64 pixels; the CNN has 6 convolutional layers and the following number of feature maps: 18, 128, 128, 128, 64, 32, 16, 6. All filters have size 3 × 3. The optimal number of filters has been found using cross-validation in order to minimize the estimation error of the affine transform parameters. Unless otherwise stated, we condition generation on 4 ground truth frames and we predict the following 8 frames.
We evaluate several models5: a) a baseline which merely copies the last frame used for conditioning, b) a baseline method which estimates optical flow (Brox et al., 2004) from two consecutive frames
4A quantitative analysis would be difficult for this data set because metrics reported in the literature like MSE (Srivastava et al., 2015) are not appropriate for measuring generation quality, and it would be difficult to use the metric we propose because we do not have labels at the sequence level and the design of a classifier is not trivial.
5Unfortunately, we could not compare against the LSTM-based method in Srivastava et al. (2015) because it does not scale to high-resolution videos, but only to small patches.
and extrapolates flow in subsequent frames under the assumption of constant flow speed, c) an adversarially trained multi-scale CNN (Mathieu et al., 2016) and several variants of our proposed approach.
Qualitative comparisons can be seen in the fig. 5 and in the supplementary material6. The first column on the page shows the input, the second the ground truth, followed by results from our model, Mathieu et al. (2016) and optical flow (Brox et al., 2004). Note especially the severe deformations in the last two columns, while our model keeps the frame recognizable. It produces fairly sharp reconstructions validating our first hypothesis that errors in the space of transformations still yield plausible reconstructions (see section 2). However it is also apparent that our approach underestimates movement, which follows directly from using the MSE criterion. As discussed before, MSE in pixel space leads to blurry results, however using MSE in transformation space also has some drawbacks. In practice, the model will predict the average of several likely transformations, which could lead to an understimation of the true movement.
6see: http://joo.st/ICLR/GenerationBenchmark
In order to quantify the generation quality we use the metric described in section 2.4. We use C3D network (Tran et al., 2015) as the video action classifier: C3D uses both appearance and temporal information jointly, and is pre-trained with Sports1M (Karpathy et al., 2014) and fine tuned on UCF 101. Due to the model constraints, we trained only two models, that takes 4 and 8 frames as input, respectively.
We evaluate the quality of generation using 4 (the first four predicted frames) and the whole set of 8 predicted frames, for the task of action classification. At test time, we generate frames from each model under consideration, and then use them as input to the corresponding C3D network.
Table 1 shows the accuracy of our approach and several baselines. The best performance is achieved by using ground truth frames, a result comparable to methods recently appeared in the literature (Karpathy et al., 2014; Tran et al., 2015). We see that for ground truth frames, the number of frames (4 or 8) doesn’t make a difference. There is not much additional temporal or spatial signal provided by having greater than four frames. Next, we evaluate how much we lose by representing frames as tiled affine transforms. As the second row shows there is negligible if any loss of accuracy when using frames reconstructed from the estimated affine transforms (using the method described in section 2.1), validating our assumptions at the beginning of section 2 on how video sequences can be represented. The next question is then whether these affine transforms are predictable at all. The last two rows of Table 1 show that this is indeed the case, to some extent. The longer the sequence of generated frames the poorer the performance, since the generation task gets more and more difficult.
Compared to other methods, our approach performs better than optical flow and even the more sophisticated multi-scale CNN proposed in Mathieu et al. (2016) while being computationally cheaper. For instance, our method has less than half a million parameters and requires about 2G floating point operations to generate a frame at test time, while the multi-scale CNN of Mathieu et al. (2016) has 25 times more parameters (not counting the discriminator used at training time) and it requires more than 100 times more floating point operations to generate a single frame.
Finally, we investigate the robustness of the system to its hyper-parameters: a) choice of patch size, b) number of input frames, and c) number of predicted frames. The results reported in Table 2 demonstrate that the model is overall pretty robust to these choices. Using patch sizes that are too big makes reconstructions blocky but within each block motion is coherent. Smaller patch sizes give more flexibility but make the prediction task harder as well. Mapping into patches of size smaller than 16 × 16 seems a good choice. Using only 2 input frames does not seem to provide enough context to the predictor, but anything above 3 works equally well. Training for prediction of the next frame works well, but better results can be achieved by training to predict several frames in the future, overall when evaluating longer sequences.

4 CONCLUSIONS
In this work, we proposed a new approach to generative modeling of video sequences. This model does not make any assumption about the spatio-temporal resolution of video sequences nor about object categories. The key insight of our approach is to model in the space of transformations as opposed to raw pixel space. A priori we lack a good metric to measure how well a frame is reconstructed under uncertainty due to objects motion in natural scenes. Uncertainty about object motion and occlusions causes blurry generations when using MSE in pixel space. Instead, by operating in the space of transformations we aim at predicting how objects move, and estimation errors only yield a different, and possibly still plausible, motion. With this motivation we proposed a simple CNN operating in the space of affine transforms and we showed that it can generate sensible sequences up to about 4 frames. This model produces sequences that are both visually and quantitatively better than previously proposed approaches.
The second contribution of this work is the metric to compare generative models of video sequences. A good metric should not penalize a generative model for producing a sequence which is plausible but different from the ground truth. With this goal in mind and assuming we have at our disposal labeled sequences, we can first train a classifier using ground truth sequences. Next, the classifier is fed with sequences produced by our generative model for evaluation. A good generative model should produce sequences that still retain discriminative features. In other words, plausibility of generation is assessed in terms of how well inherent information is preserved during generation as opposed to necessarily and merely reproducing the ground truth sequences.
The proposed model is relatively simple; straightforward extensions that could improve its prediction accuracy are the use of a multi-scale architecture and the addition of recurrent units. These would enable a better modeling of objects of different sizes moving at varying speeds and to better capture complex temporal dynamics (e.g., cyclical movements like walking). A larger extension would be the addition of an appearance model, which together with our explicit transformation model could lead to learning better feature representations for classification.
In our view, the proposed approach should be considered as a stronger baseline for future research into next frame prediction. Even though our analysis shows improved performance and better looking generations, there are also obvious limitations. The first such limitation is the underestimation of transformations due to usage of the MSE as a criterion. We consider two main avenues worth pursuing in this space. First, we consider modelling a distribution of transformations and sampling one from it. The challenge of this approach is to sample a consistent trajectory. One could model the distribution of an entire trajectory, but that is a complex optimization problem. A second option is to use adversarial training to force the model to pick a plausible action. This option does not guarantee that underestimation of movement will be avoided. This will depend on the discriminator model accepting this as a plausible option.
Another limitation is that the current model does not factor out the “what” from the “where”, appearance from motion. The representation of two distinct objects subject to the same motion, as well as the representation of the same object subject to two different motion patterns are intrinsically different. Instead, it would be more powerful to learn models that can discover such factorization and leverage it to produce more efficient and compact representations.

ACKNOWLEDGMENTS
Authors thank Camille Couprie and Michael Mathieu for discussions and helping with evaluation of their models.
","In this work we propose a simple unsupervised approach for next frame prediction in video. Instead of directly predicting the pixels in a frame given past frames, we predict the transformations needed for generating the next frame in a sequence, given the transformations of the past frames. This leads to sharper results, while using a smaller prediction model. In order to enable a fair comparison between different video frame prediction models, we also propose a new evaluation protocol. We use generated frames as input to a classifier trained with ground truth sequences. This criterion guarantees that models scoring high are those producing sequences which preserve discriminative features, as opposed to merely penalizing any deviation, plausible or not, from the ground truth. Our proposed approach compares favourably against more sophisticated ones on the UCF-101 data set, while also being more efficient in terms of the number of parameters and computational cost.",ICLR 2017 conference submission,False,,"This paper describes an approach to predict (unseen) future frames of a video given a set of known past frames. The approach is based on a CNN that, in contrast to most related papers, work in the space of affine transformations (instead of pixels or flow). Said another way, the network takes as input a set of affine transforms that describe the motion of patches in the past frames, and likewise, outputs a set of affine transforms that predict future patch motion.

To that aim, the authors make a few simplifying hypotheses, namely, that a sequence of frames can be modeled accurately enough in their patch-affine framework. This is not unreasonable. A lot of papers in the optical flow community are based on similar hypotheses, i.e. model the flow as a smoothly varying affine field (for instance see ""Locally affine sparse-to-dense matching for motion and occlusion estimation"" by Leordeanu et al., ""EpicFlow: Edge-Preserving Interpolation of Correspondences for Optical Flow"" by Revaud et al., ""Optical Flow With Semantic Segmentation and Localized Layers"" by Sevilla-Lara et al.). These methods are state of the art, which gives a hint about the validity of this kind of approach. 
In addition, it also seems very reasonable to reformulate the prediction task as predicting motion rather than predicting raw pixels. Indeed, the (patch-affine) motion space is considerably smaller than the image space, making the problem much more tractable and amenable to high-resolution videos.

While I agree with the authors on these points, I also find that the paper suffer from important flaws. Specifically:

  - the choice of not comparing with previous approaches in term of pixel prediction error seems very ""convenient"", to say the least. While it is clear that the evaluation metric is imperfect, it is not a reason to completely dismiss all quantitative comparisons with previous work. The frames output by the network on, e.g. the moving digits datasets (Figure 4), looks ok and can definitely be compared with other papers. Yet, the authors chose not to, which is suspicious.
  
  - The newly proposed metric poses several problems. First, action classification is evaluated with C3D, which is not a state-of-the-art approach at all for this task. Second, this metric actually *does not* evaluate what the network is claimed to do, that is, next frame prediction. Instead, it evaluates if another network, which was never trained to distinguish between real or synthetic frames by the way, can accurately classify an action from the predicted frames. I find that this proxy metric is only weakly related to what is supposed to be measured. In adition, it does not really make sense to train a network for something else that the final task it is evaluated for.
  
  - how is the affine motion of patches estimated? It is only explained that the problem is solved globally (not treating each patch independently) in a pretty vague manner. Estimating the motion of all patches is akin to solving the optical flow, which is still an active subject of research. Therefore, an important flaw of the paper lies in the potentially erroneous etimation of the motion input to the network. In the videos made available, it is clear that the motion is wrongly estimated sometimes. Since the entire approach depends on this input, I find it important to discuss this aspect. How do motion estimation failures impact the network? Also, the patch-affine hypothesis does not hold when patches are large enough that they cover several objects with contradictory motion. Which appears to be the case on UCF101 videos.
  
  - Even ignoring the weird proxy-evaluation part, the network is still not trained end-to-end. That is, the network is trained to minimize the difference between (noisy) ground-truth and output affine transforms, instead of minimizing a loss in the actual output space (frame pixels) for which an (exact) ground-truth is available. It is true that the MSE loss on raw pixels leads to blurry results, but other types of losses do exist, for instance the gradient loss introduced by Mathieu et al. was shown to solve this issue. As noted by the authors themselves, minimizing a loss in the transformation space, where affine parameters are harder to intepret, introduces unexpected artifacts. The motion is often largely underestimated, as is obvious in Figure 5 where it is hard to tell the difference between the input and output frames. 
  
  - The proposed approach is not sufficiently compared to previous work. In particular, the approach is closely related to ""SPATIO-TEMPORAL VIDEO AUTOENCODER WITH DIFFERENTIABLE MEMORY"" of Taraucean et al, ICLR'15. This paper also output prediction in the motion space. Experimental results should compare against it.

  - The comparison with optical flow is unfair. First, the approach of Brox et al. is more than 10 years old. Second, it is not really fair to assume a constant flow for all frames. At least some basic extrapolation could be done to take into account the flow of all pairs of input frames and not just the last one. Overall, the approach is not compared to very challenging baselines.

  - I disagree with the answer that the authors gave to a reviewer's question. Denote ground-truth frames as {X_0, X_1 ...} and predicted frames as {Y_1, Y_2, ...}. When asked if the videos at

---

There were serious concerns raised about the originality of the work in regard to prior methods. The authors' responses to numerous reviewer questions on this matter were also unsatisfactory. In the end, it is difficult to tell what the contribution in regard to the video prediction model is. The proposed evaluation metric is interesting, but also raised serious concerns. Finally, several reviewers raised concerns about the quality of the results.

---

The paper proposes a method for future frame prediction based on transformation of previous frame rather than direct pixel prediction.

Many previous works have proposed similar methods. The authors in their responses state that previous work is deterministic, yet the proposed model also does not handle multimodality.

Further, i asked if they could test their method using 2 RGB frames as input and predicting the transformation as output, to be able to quantify the importance of using transformations both as input and output, since this is the first work that uses transformations as input also. The authors dismissed the suggestion by saying ""if we were to use RGB frames as input and ask the model to output future frames it would produce very blurry results"", that is, misunderstanding what the suggestion was. So, currently, it does not seem to be a valid novel contribution in this work compared to previous works.

---

Paper Summary
This paper makes two contributions -
(1) A model for next step prediction, where the inputs and outputs are in the
space of affine transforms between adjacent frames.
(2) An evaluation method in which the quality of the generated data is assessed
by measuring the reduction in performance of another model (such as a
classifier) when tested on the generated data.

The authors show that according to this metric, the proposed model works better
than other baseline models (including the recent work of Mathieu et al. which
uses adversarial training).

Strengths
- This paper attempts to solve a major problem in unsupervised learning
  with videos, which is evaluating them.
- The results show that using MSE in transform space does prevent the blurring
  problem to a large extent (which is one of the main aims of this paper).
- The results show that the generated data reduces the performance of the C3D
  model on UCF-101 to a much less extent than other baselines.
- The paper validates the assumption that videos can be approximated to quite a
  few time steps by a sequence of affine transforms starting from an initial
frame.

Weaknesses
- The proposed metric makes sense only if we truly just care about the performance
  of a particular classifier on a given task. This significantly narrows the
scope of applicability of this metric because arguably, one the important
reasons for doing unsupervised learning is to come up a representation that is
widely applicable across a variety of tasks. The proposed metric would not help
evaluate generative models designed to achieve this objective.

- It is possible that one of the generative models being compared will interact
  with the idiosyncrasies of the chosen classifier in unintended ways.
Therefore, it would be hard to draw strong conclusions about the relative
merits of generative models from the results of such experiments. One way to
ameliorate this would be to use several different classifiers (C3D,
dual-stream network, other state-of-the-art methods) and show that the ranking
of different generative models is consistent across the choice of classifier.
Adding such experiments would help increase certainty in the conclusions drawn
in this paper.

- Using only 4 or 8 input frames sampled at 25fps seems like very little context
  if we really expect the model to extrapolate the kind of motion seen in
UCF-101. The idea of working in the space of affine transforms would be much
more appealing if the model can be shown to really generated non-trivial motion
patterns. Currently, the motion patterns seem to be almost linear
extrapolations.

- The model that predicts motion does not have access to content at all. It only
  gets access to previous motion. It seems that this might be a disadvantage
because the motion predictor cannot use any cues like object boundaries, or
decide what to do when two motion fields collide (it is probably easier to argue
about occlusions in content space).

Quality/Clarity
The paper is clearly written and easy to follow. The assumptions are clearly
specified and validated. Experimental details seem adequate.

Originality
The idea of generating videos by predicting motion has been used previously.
Several recent papers also use this idea. However the exact implementation in
this paper is new. The proposed evaluation protocol is novel.

Significance
The proposed evaluation method is an interesting alternative, especially if it
is extended to include multiple classifiers representative of different
state-of-the-art approaches. Given how hard it is to evaluate generative models
of videos, this paper could help start an effort to standardize on a benchmark
set.

Minor comments and suggestions

(1) In the caption for Table 1: ``Each column shows the accuracy on the test set
when taking a different number of input frames as input"" - ``input"" here refers
to the input to the classifier (Output of the next step prediction model). However
in the next sentence ``Our approach maps 16 \times 16 patches into 8 \times 8
with stride 4, and it takes 4 frames at the input"" - here ``input"" refers to
the input to the next step prediction model. It might be a good idea to rephrase
these sentences to make the distinction clear.

(2) In order to better understand the space of affine transform
parameters, it might help to include a histogram of these parameters in the
paper. This can help us see at a glance, what is the typical range of these
6 parameters, should we expect a lot of outliers, etc.

(3) In order to compare transforms A and B, instead of ||A - B||^2, one
could consider A^{-1}B being close to identity as the metric. Did the authors
try this ?

(4) ""The performance of the classifier on ground truth data is an upper bound on
the performance of any generative model."" This is not *strictly* true. It is
possible (though highly unlikely) that a generative model might make the data
look cleaner, sharper, or highlight some aspect of it which could improve the
performance of the classifier (even compared to ground truth). This is
especially true if the the generative model had access to the classifier, it
could then see what makes the classifier fire and highlight those discriminative
features in the generated output.

Overall
This paper proposes future prediction in affine transform space. This does
reduce blurriness and makes the videos look relatively realistic (at least to the
C3D classifier). However, the paper can be improved by showing that the model can
predict more non-trivial motion flows and the experiments can be strengthened by
adding more classifiers besides than C3D.

---

This paper describes an approach to predict (unseen) future frames of a video given a set of known past frames. The approach is based on a CNN that, in contrast to most related papers, work in the space of affine transformations (instead of pixels or flow). Said another way, the network takes as input a set of affine transforms that describe the motion of patches in the past frames, and likewise, outputs a set of affine transforms that predict future patch motion.

To that aim, the authors make a few simplifying hypotheses, namely, that a sequence of frames can be modeled accurately enough in their patch-affine framework. This is not unreasonable. A lot of papers in the optical flow community are based on similar hypotheses, i.e. model the flow as a smoothly varying affine field (for instance see ""Locally affine sparse-to-dense matching for motion and occlusion estimation"" by Leordeanu et al., ""EpicFlow: Edge-Preserving Interpolation of Correspondences for Optical Flow"" by Revaud et al., ""Optical Flow With Semantic Segmentation and Localized Layers"" by Sevilla-Lara et al.). These methods are state of the art, which gives a hint about the validity of this kind of approach. 
In addition, it also seems very reasonable to reformulate the prediction task as predicting motion rather than predicting raw pixels. Indeed, the (patch-affine) motion space is considerably smaller than the image space, making the problem much more tractable and amenable to high-resolution videos.

While I agree with the authors on these points, I also find that the paper suffer from important flaws. Specifically:

  - the choice of not comparing with previous approaches in term of pixel prediction error seems very ""convenient"", to say the least. While it is clear that the evaluation metric is imperfect, it is not a reason to completely dismiss all quantitative comparisons with previous work. The frames output by the network on, e.g. the moving digits datasets (Figure 4), looks ok and can definitely be compared with other papers. Yet, the authors chose not to, which is suspicious.
  
  - The newly proposed metric poses several problems. First, action classification is evaluated with C3D, which is not a state-of-the-art approach at all for this task. Second, this metric actually *does not* evaluate what the network is claimed to do, that is, next frame prediction. Instead, it evaluates if another network, which was never trained to distinguish between real or synthetic frames by the way, can accurately classify an action from the predicted frames. I find that this proxy metric is only weakly related to what is supposed to be measured. In adition, it does not really make sense to train a network for something else that the final task it is evaluated for.
  
  - how is the affine motion of patches estimated? It is only explained that the problem is solved globally (not treating each patch independently) in a pretty vague manner. Estimating the motion of all patches is akin to solving the optical flow, which is still an active subject of research. Therefore, an important flaw of the paper lies in the potentially erroneous etimation of the motion input to the network. In the videos made available, it is clear that the motion is wrongly estimated sometimes. Since the entire approach depends on this input, I find it important to discuss this aspect. How do motion estimation failures impact the network? Also, the patch-affine hypothesis does not hold when patches are large enough that they cover several objects with contradictory motion. Which appears to be the case on UCF101 videos.
  
  - Even ignoring the weird proxy-evaluation part, the network is still not trained end-to-end. That is, the network is trained to minimize the difference between (noisy) ground-truth and output affine transforms, instead of minimizing a loss in the actual output space (frame pixels) for which an (exact) ground-truth is available. It is true that the MSE loss on raw pixels leads to blurry results, but other types of losses do exist, for instance the gradient loss introduced by Mathieu et al. was shown to solve this issue. As noted by the authors themselves, minimizing a loss in the transformation space, where affine parameters are harder to intepret, introduces unexpected artifacts. The motion is often largely underestimated, as is obvious in Figure 5 where it is hard to tell the difference between the input and output frames. 
  
  - The proposed approach is not sufficiently compared to previous work. In particular, the approach is closely related to ""SPATIO-TEMPORAL VIDEO AUTOENCODER WITH DIFFERENTIABLE MEMORY"" of Taraucean et al, ICLR'15. This paper also output prediction in the motion space. Experimental results should compare against it.

  - The comparison with optical flow is unfair. First, the approach of Brox et al. is more than 10 years old. Second, it is not really fair to assume a constant flow for all frames. At least some basic extrapolation could be done to take into account the flow of all pairs of input frames and not just the last one. Overall, the approach is not compared to very challenging baselines.

  - I disagree with the answer that the authors gave to a reviewer's question. Denote ground-truth frames as {X_0, X_1 ...} and predicted frames as {Y_1, Y_2, ...}. When asked if the videos at

---

Could the authors comment on the relationship between their work and these previous works that appear to use a similar transformation-based video prediction technique?

Dynamic Filter Networks (NIPS 2016)
Unsupervised Learning for Physical Interaction through Video Prediction (NIPS 2016)
Visual Dynamics: Probabilistic Future Frame Synthesis via Cross Convolutional Networks (NIPS 2016)

---

The transformation-based approach for generating the next frame in a sequence was used in ICLRw2016

---

This paper describes an approach to predict (unseen) future frames of a video given a set of known past frames. The approach is based on a CNN that, in contrast to most related papers, work in the space of affine transformations (instead of pixels or flow). Said another way, the network takes as input a set of affine transforms that describe the motion of patches in the past frames, and likewise, outputs a set of affine transforms that predict future patch motion.

To that aim, the authors make a few simplifying hypotheses, namely, that a sequence of frames can be modeled accurately enough in their patch-affine framework. This is not unreasonable. A lot of papers in the optical flow community are based on similar hypotheses, i.e. model the flow as a smoothly varying affine field (for instance see ""Locally affine sparse-to-dense matching for motion and occlusion estimation"" by Leordeanu et al., ""EpicFlow: Edge-Preserving Interpolation of Correspondences for Optical Flow"" by Revaud et al., ""Optical Flow With Semantic Segmentation and Localized Layers"" by Sevilla-Lara et al.). These methods are state of the art, which gives a hint about the validity of this kind of approach. 
In addition, it also seems very reasonable to reformulate the prediction task as predicting motion rather than predicting raw pixels. Indeed, the (patch-affine) motion space is considerably smaller than the image space, making the problem much more tractable and amenable to high-resolution videos.

While I agree with the authors on these points, I also find that the paper suffer from important flaws. Specifically:

  - the choice of not comparing with previous approaches in term of pixel prediction error seems very ""convenient"", to say the least. While it is clear that the evaluation metric is imperfect, it is not a reason to completely dismiss all quantitative comparisons with previous work. The frames output by the network on, e.g. the moving digits datasets (Figure 4), looks ok and can definitely be compared with other papers. Yet, the authors chose not to, which is suspicious.
  
  - The newly proposed metric poses several problems. First, action classification is evaluated with C3D, which is not a state-of-the-art approach at all for this task. Second, this metric actually *does not* evaluate what the network is claimed to do, that is, next frame prediction. Instead, it evaluates if another network, which was never trained to distinguish between real or synthetic frames by the way, can accurately classify an action from the predicted frames. I find that this proxy metric is only weakly related to what is supposed to be measured. In adition, it does not really make sense to train a network for something else that the final task it is evaluated for.
  
  - how is the affine motion of patches estimated? It is only explained that the problem is solved globally (not treating each patch independently) in a pretty vague manner. Estimating the motion of all patches is akin to solving the optical flow, which is still an active subject of research. Therefore, an important flaw of the paper lies in the potentially erroneous etimation of the motion input to the network. In the videos made available, it is clear that the motion is wrongly estimated sometimes. Since the entire approach depends on this input, I find it important to discuss this aspect. How do motion estimation failures impact the network? Also, the patch-affine hypothesis does not hold when patches are large enough that they cover several objects with contradictory motion. Which appears to be the case on UCF101 videos.
  
  - Even ignoring the weird proxy-evaluation part, the network is still not trained end-to-end. That is, the network is trained to minimize the difference between (noisy) ground-truth and output affine transforms, instead of minimizing a loss in the actual output space (frame pixels) for which an (exact) ground-truth is available. It is true that the MSE loss on raw pixels leads to blurry results, but other types of losses do exist, for instance the gradient loss introduced by Mathieu et al. was shown to solve this issue. As noted by the authors themselves, minimizing a loss in the transformation space, where affine parameters are harder to intepret, introduces unexpected artifacts. The motion is often largely underestimated, as is obvious in Figure 5 where it is hard to tell the difference between the input and output frames. 
  
  - The proposed approach is not sufficiently compared to previous work. In particular, the approach is closely related to ""SPATIO-TEMPORAL VIDEO AUTOENCODER WITH DIFFERENTIABLE MEMORY"" of Taraucean et al, ICLR'15. This paper also output prediction in the motion space. Experimental results should compare against it.

  - The comparison with optical flow is unfair. First, the approach of Brox et al. is more than 10 years old. Second, it is not really fair to assume a constant flow for all frames. At least some basic extrapolation could be done to take into account the flow of all pairs of input frames and not just the last one. Overall, the approach is not compared to very challenging baselines.

  - I disagree with the answer that the authors gave to a reviewer's question. Denote ground-truth frames as {X_0, X_1 ...} and predicted frames as {Y_1, Y_2, ...}. When asked if the videos at

---

There were serious concerns raised about the originality of the work in regard to prior methods. The authors' responses to numerous reviewer questions on this matter were also unsatisfactory. In the end, it is difficult to tell what the contribution in regard to the video prediction model is. The proposed evaluation metric is interesting, but also raised serious concerns. Finally, several reviewers raised concerns about the quality of the results.

---

The paper proposes a method for future frame prediction based on transformation of previous frame rather than direct pixel prediction.

Many previous works have proposed similar methods. The authors in their responses state that previous work is deterministic, yet the proposed model also does not handle multimodality.

Further, i asked if they could test their method using 2 RGB frames as input and predicting the transformation as output, to be able to quantify the importance of using transformations both as input and output, since this is the first work that uses transformations as input also. The authors dismissed the suggestion by saying ""if we were to use RGB frames as input and ask the model to output future frames it would produce very blurry results"", that is, misunderstanding what the suggestion was. So, currently, it does not seem to be a valid novel contribution in this work compared to previous works.

---

Paper Summary
This paper makes two contributions -
(1) A model for next step prediction, where the inputs and outputs are in the
space of affine transforms between adjacent frames.
(2) An evaluation method in which the quality of the generated data is assessed
by measuring the reduction in performance of another model (such as a
classifier) when tested on the generated data.

The authors show that according to this metric, the proposed model works better
than other baseline models (including the recent work of Mathieu et al. which
uses adversarial training).

Strengths
- This paper attempts to solve a major problem in unsupervised learning
  with videos, which is evaluating them.
- The results show that using MSE in transform space does prevent the blurring
  problem to a large extent (which is one of the main aims of this paper).
- The results show that the generated data reduces the performance of the C3D
  model on UCF-101 to a much less extent than other baselines.
- The paper validates the assumption that videos can be approximated to quite a
  few time steps by a sequence of affine transforms starting from an initial
frame.

Weaknesses
- The proposed metric makes sense only if we truly just care about the performance
  of a particular classifier on a given task. This significantly narrows the
scope of applicability of this metric because arguably, one the important
reasons for doing unsupervised learning is to come up a representation that is
widely applicable across a variety of tasks. The proposed metric would not help
evaluate generative models designed to achieve this objective.

- It is possible that one of the generative models being compared will interact
  with the idiosyncrasies of the chosen classifier in unintended ways.
Therefore, it would be hard to draw strong conclusions about the relative
merits of generative models from the results of such experiments. One way to
ameliorate this would be to use several different classifiers (C3D,
dual-stream network, other state-of-the-art methods) and show that the ranking
of different generative models is consistent across the choice of classifier.
Adding such experiments would help increase certainty in the conclusions drawn
in this paper.

- Using only 4 or 8 input frames sampled at 25fps seems like very little context
  if we really expect the model to extrapolate the kind of motion seen in
UCF-101. The idea of working in the space of affine transforms would be much
more appealing if the model can be shown to really generated non-trivial motion
patterns. Currently, the motion patterns seem to be almost linear
extrapolations.

- The model that predicts motion does not have access to content at all. It only
  gets access to previous motion. It seems that this might be a disadvantage
because the motion predictor cannot use any cues like object boundaries, or
decide what to do when two motion fields collide (it is probably easier to argue
about occlusions in content space).

Quality/Clarity
The paper is clearly written and easy to follow. The assumptions are clearly
specified and validated. Experimental details seem adequate.

Originality
The idea of generating videos by predicting motion has been used previously.
Several recent papers also use this idea. However the exact implementation in
this paper is new. The proposed evaluation protocol is novel.

Significance
The proposed evaluation method is an interesting alternative, especially if it
is extended to include multiple classifiers representative of different
state-of-the-art approaches. Given how hard it is to evaluate generative models
of videos, this paper could help start an effort to standardize on a benchmark
set.

Minor comments and suggestions

(1) In the caption for Table 1: ``Each column shows the accuracy on the test set
when taking a different number of input frames as input"" - ``input"" here refers
to the input to the classifier (Output of the next step prediction model). However
in the next sentence ``Our approach maps 16 \times 16 patches into 8 \times 8
with stride 4, and it takes 4 frames at the input"" - here ``input"" refers to
the input to the next step prediction model. It might be a good idea to rephrase
these sentences to make the distinction clear.

(2) In order to better understand the space of affine transform
parameters, it might help to include a histogram of these parameters in the
paper. This can help us see at a glance, what is the typical range of these
6 parameters, should we expect a lot of outliers, etc.

(3) In order to compare transforms A and B, instead of ||A - B||^2, one
could consider A^{-1}B being close to identity as the metric. Did the authors
try this ?

(4) ""The performance of the classifier on ground truth data is an upper bound on
the performance of any generative model."" This is not *strictly* true. It is
possible (though highly unlikely) that a generative model might make the data
look cleaner, sharper, or highlight some aspect of it which could improve the
performance of the classifier (even compared to ground truth). This is
especially true if the the generative model had access to the classifier, it
could then see what makes the classifier fire and highlight those discriminative
features in the generated output.

Overall
This paper proposes future prediction in affine transform space. This does
reduce blurriness and makes the videos look relatively realistic (at least to the
C3D classifier). However, the paper can be improved by showing that the model can
predict more non-trivial motion flows and the experiments can be strengthened by
adding more classifiers besides than C3D.

---

This paper describes an approach to predict (unseen) future frames of a video given a set of known past frames. The approach is based on a CNN that, in contrast to most related papers, work in the space of affine transformations (instead of pixels or flow). Said another way, the network takes as input a set of affine transforms that describe the motion of patches in the past frames, and likewise, outputs a set of affine transforms that predict future patch motion.

To that aim, the authors make a few simplifying hypotheses, namely, that a sequence of frames can be modeled accurately enough in their patch-affine framework. This is not unreasonable. A lot of papers in the optical flow community are based on similar hypotheses, i.e. model the flow as a smoothly varying affine field (for instance see ""Locally affine sparse-to-dense matching for motion and occlusion estimation"" by Leordeanu et al., ""EpicFlow: Edge-Preserving Interpolation of Correspondences for Optical Flow"" by Revaud et al., ""Optical Flow With Semantic Segmentation and Localized Layers"" by Sevilla-Lara et al.). These methods are state of the art, which gives a hint about the validity of this kind of approach. 
In addition, it also seems very reasonable to reformulate the prediction task as predicting motion rather than predicting raw pixels. Indeed, the (patch-affine) motion space is considerably smaller than the image space, making the problem much more tractable and amenable to high-resolution videos.

While I agree with the authors on these points, I also find that the paper suffer from important flaws. Specifically:

  - the choice of not comparing with previous approaches in term of pixel prediction error seems very ""convenient"", to say the least. While it is clear that the evaluation metric is imperfect, it is not a reason to completely dismiss all quantitative comparisons with previous work. The frames output by the network on, e.g. the moving digits datasets (Figure 4), looks ok and can definitely be compared with other papers. Yet, the authors chose not to, which is suspicious.
  
  - The newly proposed metric poses several problems. First, action classification is evaluated with C3D, which is not a state-of-the-art approach at all for this task. Second, this metric actually *does not* evaluate what the network is claimed to do, that is, next frame prediction. Instead, it evaluates if another network, which was never trained to distinguish between real or synthetic frames by the way, can accurately classify an action from the predicted frames. I find that this proxy metric is only weakly related to what is supposed to be measured. In adition, it does not really make sense to train a network for something else that the final task it is evaluated for.
  
  - how is the affine motion of patches estimated? It is only explained that the problem is solved globally (not treating each patch independently) in a pretty vague manner. Estimating the motion of all patches is akin to solving the optical flow, which is still an active subject of research. Therefore, an important flaw of the paper lies in the potentially erroneous etimation of the motion input to the network. In the videos made available, it is clear that the motion is wrongly estimated sometimes. Since the entire approach depends on this input, I find it important to discuss this aspect. How do motion estimation failures impact the network? Also, the patch-affine hypothesis does not hold when patches are large enough that they cover several objects with contradictory motion. Which appears to be the case on UCF101 videos.
  
  - Even ignoring the weird proxy-evaluation part, the network is still not trained end-to-end. That is, the network is trained to minimize the difference between (noisy) ground-truth and output affine transforms, instead of minimizing a loss in the actual output space (frame pixels) for which an (exact) ground-truth is available. It is true that the MSE loss on raw pixels leads to blurry results, but other types of losses do exist, for instance the gradient loss introduced by Mathieu et al. was shown to solve this issue. As noted by the authors themselves, minimizing a loss in the transformation space, where affine parameters are harder to intepret, introduces unexpected artifacts. The motion is often largely underestimated, as is obvious in Figure 5 where it is hard to tell the difference between the input and output frames. 
  
  - The proposed approach is not sufficiently compared to previous work. In particular, the approach is closely related to ""SPATIO-TEMPORAL VIDEO AUTOENCODER WITH DIFFERENTIABLE MEMORY"" of Taraucean et al, ICLR'15. This paper also output prediction in the motion space. Experimental results should compare against it.

  - The comparison with optical flow is unfair. First, the approach of Brox et al. is more than 10 years old. Second, it is not really fair to assume a constant flow for all frames. At least some basic extrapolation could be done to take into account the flow of all pairs of input frames and not just the last one. Overall, the approach is not compared to very challenging baselines.

  - I disagree with the answer that the authors gave to a reviewer's question. Denote ground-truth frames as {X_0, X_1 ...} and predicted frames as {Y_1, Y_2, ...}. When asked if the videos at

---

Could the authors comment on the relationship between their work and these previous works that appear to use a similar transformation-based video prediction technique?

Dynamic Filter Networks (NIPS 2016)
Unsupervised Learning for Physical Interaction through Video Prediction (NIPS 2016)
Visual Dynamics: Probabilistic Future Frame Synthesis via Cross Convolutional Networks (NIPS 2016)

---

The transformation-based approach for generating the next frame in a sequence was used in ICLRw2016",,,,,,4.666666666666667,,,3.3333333333333335,,
632,"Authors: QUESTION ANSWERING, Liwen Zhang
Source file: 632.pdf

ABSTRACT
We propose the Gaussian attention model for content-based neural memory access. With the proposed attention model, a neural network has the additional degree of freedom to control the focus of its attention from a laser sharp attention to a broad attention. It is applicable whenever we can assume that the distance in the latent space reflects some notion of semantics. We use the proposed attention model as a scoring function for the embedding of a knowledge base into a continuous vector space and then train a model that performs question answering about the entities in the knowledge base. The proposed attention model can handle both the propagation of uncertainty when following a series of relations and also the conjunction of conditions in a natural way. On a dataset of soccer players who participated in the FIFA World Cup 2014, we demonstrate that our model can handle both path queries and conjunctive queries well.

We propose the Gaussian attention model for content-based neural memory access. With the proposed attention model, a neural network has the additional degree of freedom to control the focus of its attention from a laser sharp attention to a broad attention. It is applicable whenever we can assume that the distance in the latent space reflects some notion of semantics. We use the proposed attention model as a scoring function for the embedding of a knowledge base into a continuous vector space and then train a model that performs question answering about the entities in the knowledge base. The proposed attention model can handle both the propagation of uncertainty when following a series of relations and also the conjunction of conditions in a natural way. On a dataset of soccer players who participated in the FIFA World Cup 2014, we demonstrate that our model can handle both path queries and conjunctive queries well.

1 INTRODUCTION
There is a growing interest in incorporating external memory into neural networks. For example, memory networks (Weston et al., 2014; Sukhbaatar et al., 2015) are equipped with static memory slots that are content or location addressable. Neural Turing machines (Graves et al., 2014) implement memory slots that can be read and written as in Turing machines (Turing, 1938) but through differentiable attention mechanism.
Each memory slot in these models stores a vector corresponding to a continuous representation of the memory content. In order to recall a piece of information stored in memory, attention is typically employed. Attention mechanism introduced by Bahdanau et al. (2014) uses a network that outputs a discrete probability mass over memory items. A memory read can be implemented as a weighted sum of the memory vectors in which the weights are given by the attention network. Reading out a single item can be realized as a special case in which the output of the attention network is peaked at the desired item. The attention network may depend on the current context as well as the memory item itself. The attention model is called location-based and content-based, if it depends on the location in the memory and the stored memory vector, respectively.
Knowledge bases, such as WordNet and Freebase, can also be stored in memory either through an explicit knowledge base embedding (Bordes et al., 2011; Nickel et al., 2011; Socher et al., 2013) or through a feedforward network (Bordes et al., 2015).
When we embed entities from a knowledge base in a continuous vector space, if the capacity of the embedding model is appropriately controlled, we expect semantically similar entities to be close to each other, which will allow the model to generalize to unseen facts. However the notion of proximity may strongly depend on the type of a relation. For example, Benjamin Franklin was an engineer but also a politician. We would need different metrics to capture his proximity to other engineers and politicians of his time.
In this paper, we propose a new attention model for content-based addressing. Our model scores each item vitem in the memory by the (logarithm of) multivariate Gaussian likelihood as follows:
score(vitem) = log φ(vitem|µcontext,Σcontext)
= −1 2 (vitem − µcontext)Σ −1 context(vitem − µcontext) + const. (1)
where context denotes all the variables that the attention depends on. For example, “American engineers in the 18th century” or “American politicians in the 18th century” would be two contexts that include Benjamin Franklin but the two attentions would have very different shapes.
Compared to the (normalized) inner product used in previous work (Sukhbaatar et al., 2015; Graves et al., 2014) for content-based addressing, the Gaussian model has the additional control of the spread of the attention over items in the memory. As we show in Figure 1, we can view the conventional inner-product-based attention and the proposed Gaussian attention as addressing by an affine energy function and a quadratic energy function, respectively. By making the addressing mechanism more complex, we may represent many entities in a relatively low dimensional embedding space. Since knowledge bases are typically extremely sparse, it is more likely that we can afford to have a more complex attention model than a large embedding dimension.
We apply the proposed Gaussian attention model to question answering based on knowledge bases. At the high-level, the goal of the task is to learn the mapping from a question about objects in the knowledge base in natural language to a probability distribution over the entities. We use the scoring function (1) for both embedding the entities as vectors, and extracting the conditions mentioned in the question and taking a conjunction of them to score each candidate answer to the question.
The ability to compactly represent a set of objects makes the Gaussian attention model well suited for representing the uncertainty in a multiple-answer question (e.g., “who are the children of Abraham Lincoln?”). Moreover, traversal over the knowledge graph (see Guu et al., 2015) can be naturally handled by a series of Gaussian convolutions, which generalizes the addition of vectors. In fact, we model each relation as a Gaussian with mean and variance parameters. Thus a traversal on a relation corresponds to a translation in the mean and addition of the variances.
The proposed question answering model is able to handle not only the case where the answer to a question is associated with an atomic fact, which is called simple Q&A (Bordes et al., 2015), but also questions that require composition of relations (path queries in Guu et al. (2015)) and conjunction of queries. An example flow of how our model deals with a question “Who plays forward for Borussia Dortmund?” is shown in Figure 2 in Section 3.
This paper is structured as follows. In Section 2, we describe how the Gaussian scoring function (1) can be used to embed the entities in a knowledge base into a continuous vector space. We call our model TransGaussian because of its similarity to the TransE model proposed by Bordes et al. (2013). Then in Section 3, we describe our question answering model. In Section 4, we carry out experiments on WorldCup2014 dataset we collected. The dataset is relatively small but it allows us to evaluate not only simple questions but also path queries and conjunction of queries. The proposed TransGaussian embedding with the question answering model achieves significantly higher accuracy than the vanilla TransE embedding or TransE trained with compositional relations Guu et al. (2015) combined with the same question answering model.

2 KNOWLEDGE BASE EMBEDDING
In this section, we describe the proposed TransGaussian model based on the Gaussian attention model (1). While it is possible to train a network that computes the embedding in a single pass (Bordes et al., 2015) or over multiple passes (Li et al., 2015), it is more efficient to offload the embedding as a separate step for question answering based on a large static knowledge base.

2.1 THE TRANSGAUSSIAN MODEL
Let E be the set of entities and R be the set of relations. A knowledge base is a collection of triplets (s, r, o), where we call s ∈ E , r ∈ R, and o ∈ E , the subject, the relation, and the object of the triplet, respectively. Each triplet encodes a fact. For example, (Albert Einstein,has profession,theoretical physicist). All the triplets given in a knowledge base are assumed to be true. However generally speaking a triplet may be true or false. Thus knowledge base embedding aims at training a model that predict if a triplet is true or not given some parameterization of the entities and relations (Bordes et al., 2011; 2013; Nickel et al., 2011; Socher et al., 2013; Wang et al., 2014).
In this paper, we associate a vector vs ∈ Rd with each entity s ∈ E , and we associate each relation r ∈ R with two parameters, δr ∈ Rd and a positive definite symmetric matrix Σr ∈ Rd×d++ . Given subject s and relation r, we can compute the score of an object o to be in triplet (s, r, o) using the Gaussian attention model as (1) with
score(s, r, o) = log φ(vo|µcontext,Σcontext), (2) where µcontext = vs + δr, Σcontext = Σr. Note that if Σr is fixed to the identity matrix, we are modeling the relation of subject vs and object vo as a translation δr, which is equivalent to the TransE model (Bordes et al., 2013). We allow the covariance Σr to depend on the relation to handle one-to-many relations (e.g., profession has person relation) and capture the shape of the distribution of the set of objects that can be in the triplet. We call our model TransGaussian because of its similarity to TransE (Bordes et al., 2013).
Parameterization For computational efficiency, we will restrict the covariance matrix Σr to be diagonal in this paper. Furthermore, in order to ensure that Σr is strictly positive definite, we employ the exponential linear unit (ELU, Clevert et al., 2015) and parameterize Σr as follows:
Σr = diag
( ELU(mr,1)+1+
. . . ELU(mr,d)+1+ ) where mr,j (j = 1, . . . , d) are the unconstrained parameters that are optimized during training and is a small positive value that ensure the positivity of the variance during numerical computation. The ELU is defined as
ELU(x) = { x, x ≥ 0, exp (x)− 1, x < 0.
Ranking loss Suppose we have a set of triplets T = {(si, ri, oi)}Ni=1 from the knowledge base. Let N (s, r) be the set of incorrect objects to be in the triplet (s, r, ·). Our objective function uses the ranking loss to measure the margin between the scores of true answers and those of false answers and it can be written as follows:
min {ve:e∈E},
{δr,Mr,:r∈R̄}
1
N ∑ (s,r,o)∈T Et′∼N (s,r) [ [µ− score(s, r, o) + score(s, r, t′)]+ ]
+ λ ∑ e∈E ‖ve‖22 + ∑ r∈R̄ ( ‖δr‖22 + ‖M r‖2F ) , (3) where, N = |T |, µ is the margin parameter and M r denotes the diagonal matrix with mr,j , j = 1, . . . , d on the diagonal; the function [·]+ is defined as [x]+ = max(0, x). Here, we treat an inverse
relation as a separate relation and denote by R̄ = R∪R−1 the set of all the relations including both relations in R and their inverse relations; a relation r̃ is the inverse relation of r if (s, r̃, o) implies (o, r, s) and vice versa. Moreover, Et′∼N (s,r) denotes the expectation with respect to the uniform distribution over the set of incorrect objects, which we approximate with 10 random samples in the experiments. Finally, the last terms are `2 regularization terms for the embedding parameters.

2.2 COMPOSITIONAL RELATIONS
Guu et al. (2015) has recently shown that training TransE with compositional relations can make it competitive to more complex models, although TransE is much simpler compared to for example, neural tensor networks (NTN, Socher et al. (2013)) and TransH Wang et al. (2014). Here, a compositional relation is a relation that is composed as a series of relations in R, for example, grand father of can be composed as first applying the parent of relation and then the father of relation, which can be seen as a traversal over a path on the knowledge graph.
TransGaussian model can naturally handle and propagate the uncertainty over such a chain of relations by convolving the Gaussian distributions along the path. That is, the score of an entity o to be in the τ -step relation r1/r2/ · · · /rτ with subject s, which we denote by the triplet (s, r1/r2/ · · · /rτ , o), is given as
score(s, r1/r2/ · · · /rτ , o) = log φ(vo|µcontext,Σcontext), (4) with µcontext = vs + ∑τ t=1 δrt , Σcontext = ∑τ t=1 Σrt , where the covariance associated with each relation is parameterized in the same way as in the previous subsection. Training with compositional relations Let P = {( si, ri1/ri2/ · · · /rili , oi )}N ′ i=1
be a set of randomly sampled paths from the knowledge graph. Here relation rik in a path can be a relation in R or an inverse relation in R−1. With the scoring function (4), the generalized training objective for compositional relations can be written identically to (3) except for replacing T with T ∪ P and replacing N with N ′ = |T ∪ P|.

3 QUESTION ANSWERING
Given a set of question-answer pairs, in which the question is phrased in natural language and the answer is an entity in the knowledge base, our goal is to train a model that learns the mapping from the question to the correct entity. Our question answering model consists of three steps, entity recognition, relation composition, and conjunction. We first identify a list of entities mentioned in the question (which is assumed to be provided by an oracle in this paper). If the question is “Who plays Forward for Borussia Dortmund?” then the list would be [Forward, Borussia Dortmund]. The next step is to predict the path of relations on the knowledgegraph starting from each entity in the list extracted in the first step. In the above example, this will be (smooth versions of) /Forward/position played by/ and /Borussia Dortmund/has player/ predicted as series of Gaussian convolutions. In general, we can have multiple relations appearing in each path. Finally, we take a product of all the Gaussian attentions and renormalize it, which is equivalent to Bayes’ rule with independent observations (paths) and a noninformative prior.

3.1 ENTITY RECOGNITION
We assume that there is an oracle that provides a list containing all the entities mentioned in the question, because (1) a domain specific entity recognizer can be developed efficiently (Williams et al., 2015) and (2) generally entity recognition is a challenging task and it is beyond the scope of this paper to show whether there is any benefit in training our question answering model jointly with a entity recognizer. We assume that the number of extracted entities can be different for each question.

3.2 RELATION COMPOSITION
We train a long short-term memory (LSTM, Hochreiter & Schmidhuber, 1997) network that emits an output ht for each token in the input sequence. Then we compute the attention over the hidden
states for each recognized entity e as
pt,e = softmax (f(ve,ht)) (t = 1, . . . , T ),
where ve is the vector associated with the entity e. We use a two-layer perceptron for f in our experiments, which can be written as follows:
f(ve,ht) = u > f ReLU (W f,vve +W f,hht + b1) + b2,
whereW f,v∈RL×d,W f,h∈RL×H , b1 ∈ RL, uf ∈ RL, b2 ∈ R are parameters. Here ReLU(x)= max(0, x) is the rectified linear unit. Finally, softmax denotes softmax over the T tokens.
Next, we use the weights pt,e to compute the weighted sum over the hidden states ht as oe = ∑T
t=1 pt,eht. (5)
Then we compute the weights αr,e over all the relations as αr,e = ReLU ( w>r oe ) (∀r ∈ R ∪ R−1). Here the rectified linear unit is used to ensure the positivity of the weights. Note however that the weights should not be normalized, because we may want to use the same relation more than once in the same path. Making the weights positive also has the effect of making the attention sparse and interpretable because there is no cancellation.
For each extracted entity e, we view the extracted entity and the answer of the question to be the subject and the object in some triplet (e, p, o), respectively, where the path p is inferred from the question as the weights αr,e as we described above. Accordingly, the score for each candidate answer o can be expressed using (1) as:
scoree(vo) = log φ(vo|µe,α,KB,Σe,α,KB) (6) with µe,α,KB = ve + ∑ r∈R̄ αr,eδr, Σe,α,KB = ∑ r∈R̄ α 2 r,eΣr, where ve is the vector associated with entity e and R̄ = R∪R−1 denotes the set of relations including the inverse relations.

3.3 CONJUNCTION
Let E(q) be the set of entities recognized in the question q. The final step of our model is to take the conjunction of the Gaussian attentions derived in the previous step. This step is simply carried out by multiplying the Gaussian attentions as follows:
score(vo|E(q),Θ) = log ∏
e∈E(q)
φ(vo|µe,α,KB,Σe,α,KB)
= −1 2 ∑ e∈E(q) ( vo − µe,α,KB )> Σ−1e,α,KB(vo − µe,α,KB) + const., (7)
which is again a (logarithm of) Gaussian scoring function, where µe,α,KB and Σe,α,KB are the mean and the covariance of the Gaussian attention given in (6). Here Θ denotes all the parameters of the question-answering model.

3.4 TRAINING THE QUESTION ANSWERING MODEL
Suppose we have a knowledge base (E ,R, T ) and a trained TransGaussian model( {ve}e∈E , {(δr,Σr)}r∈R̄ ) , where R̄ is the set of all relations including the inverse relations. During training time, we assume the training set is a supervised question-answer pairs {(qi, E(qi), ai) : i = 1, 2, . . . ,m}. Here, qi is a question formulated in natural language, E(qi) ⊂ E is a set of knowledge base entities that appears in the question, and ai ∈ E is the answer to the question. For example, on a knowledge base of soccer players, a valid training sample could be
(“Who plays forward for Borussia Dortmund?”,[Forward,Borussia Dortmund], Marco Reus).
Note that the answer to a question is not necessarily unique and we allow ai to be any of the true answers in the knowledge base. During test time, our model is shown (qi, E(qi)) and the task is to find ai. We denote the set of answers to qi by A(qi).
To train our question-answering model, we minimize the objective function
1
m m∑ i=1 ( E t′∼N (qi) [ [µ− score(vai |E(qi),Θ) + score(vt′ |E(qi),Θ)]+ ] + ν ∑ e∈E(qi) ∑ r∈R̄ |αr,e| ) + λ‖Θ‖22
where Et′∼N (qi) is expectation with respect to a uniform distribution over of all incorrect answers to qi, which we approximate with 10 random samples. We assume that the number of relations implied in a question is small compared to the total number of relations in the knowledge base. Hence the coefficients αr,e computed for each question qi are regularized by their `1 norms.

4 EXPERIMENTS
As a demonstration of the proposed framework, we perform question and answering on a dataset of soccer players. In this work, we consider two types of questions. A path query is a question that contains only one named entity from the knowledge base and its answer can be found from the knowledge graph by walking down a path consisting of a few relations. A conjunctive query is a question that contains more than one entities and the answer is given as the conjunction of all path queries starting from each entity. Furthermore, we experimented on a knowledge base completion task with TransGaussian embeddings to test its capability of generalization to unseen fact. Since knowledge base completion is not the main focus of this work, we include the results in the Appendix.

4.1 WORLDCUP2014 DATASET
We build a knowledge base of football players that participated in FIFA World Cup 2014 1. The original dataset consists of players’ information such as nationality, positions on the field and ages etc. We picked a few attributes and constructed 1127 entities and 6 atomic relations. The entities include 736 players, 297 professional soccer clubs, 51 countries, 39 numbers and 4 positions. And the six atomic relations are
1The original dataset can be found at https://datahub.io/dataset/fifa-world-cup-2014-all-players.
plays in club: PLAYER→ CLUB, plays position: PLAYER→ POSITION, is aged: PLAYER→ NUMBER, wears number 2: PLAYER→ NUMBER, plays for country: PLAYER→ COUNTRY, is in country: CLUB→ COUNTRY,
where PLAYER, CLUB, NUMBER, etc, denote the type of entities that can appear as the left or right argument for each relation. Some relations share the same type as the right argument, e.g., plays for country and is in country.
Given the entities and relations, we transformed the dataset into a set of 3977 triplets. A list of sample triplets can be found in the Appendix. Based on these triplets, we created two sets of question answering tasks which we call path query and conjunctive query respectively. The answer of every question is always an entity in the knowledge base and a question can involve one or two triplets. The questions are generated as follows.
Path queries. Among the paths on the knowledge graph, there are some natural composition of relations, e.g., plays in country (PLAYER → COUNTRY) can be decomposed as the composition of plays in club (PLAYER→ CLUB) and is in country (CLUB→ COUNTRY). In addition to the atomic relations, we manually picked a few meaningful compositions of relations and formed query templates, which takes the form “find e ∈ E , such that (s, p, e) is true”, where s is the subject and p can be an atomic relation or a path of relations. To formulate a set of path-based question-answer pairs, we manually created one or more question templates for every query template (see Table 5) Then, for a particular instantiation of a query template with subject and object entities, we randomly select a question template to generate a question given the subject; the object entity becomes the answer of the question. See Table 6 for the list of composed relations, sample questions, and answers. Note that all atomic relations in this dataset are many-to-one while these composed relations can be one-to-many or many-to-many as well.
Conjunctive queries. To generate question-and-answer pairs of conjunctive queries, we first picked three pairs of relations and used them to create query templates of the form “Find e ∈ E , such that both (s1, r1, e) and (s2, r2, e) are true.” (see Table 5). For a pair of relations r1 and r2, we enumerated all pairs of entities s1, s2 that can be their subjects and formulated the corresponding query in natural language using question templates as in the same way as path queries. See Table 7 for a list of sample questions and answers.
As a result, we created 8003 question-and-answer pairs of path queries and 2208 pairs of conjunctive queries which are partitioned into train / validation / test subsets. We refer to Table 1 for more statistics about the dataset. Templates for generating the questions are list in Table 5.

4.2 EXPERIMENTAL SETUP
To perform question and answering under our proposed framework, we first train the TransGaussian model on WorldCup2014 dataset. In addition to the atomic triplets, we randomly sampled 50000 paths with length 1 or 2 from the knowledge graph and trained a TransGaussian model compositionally as described in Set 2.2. An inverse relation is treated as a separate relation. Following the naming convention from Guu et al. (2015), we denote this trained embedding by TransGaussian (COMP). We found that the learned embedding possess some interesting properties. Some dimensions of the embedding space dedicate to represent a particular relation. Players are clustered by their attributes when entities’ embeddings are projected to the corresponding lower dimensional subspaces. We elaborate and illustrate such properties in the Appendix.
Baseline methods We also trained a TransGaussian model only on the atomic triplets and denote such a model by TransGaussian (SINGLE). Since no inverse relation was involved when TransGaussian (SINGLE) was trained, to use this embedding in question answering tasks, we represent the inverse relations as follows: for each relation r with mean δr and variance Σr, we model its inverse r−1 as a Gaussian attention with mean −δr and variance equal to Σr. We also trained TransE models on WorldCup2014 dataset by using the code released by the authors of Guu et al. (2015). Likewise, we use TransE (SINGLE) to denote the model trained with atomic triplets only and use TransE (COMP) to denote the model trained with the union of triplets and paths. Note that TransE can be considered as a special case of TransGaussian where the variance matrix is the identity and hence, the scoring formula Eq. (7) is applicable to TransE as well.
Training configurations For all models, dimension of entity embeddings was set to 30. The hidden size of LSTM was set to 80. Word embeddings were trained jointly with the question answering model and dimension of word embedding was set to 40. We employed Adam (Kingma & Ba, 2014) as the optimizer. All parameters were tuned on the validation set. Under the same setting, we experimented with two cases: first, we trained models for path queries and conjunctive queries separately; Furthermore, we trained a single model that addresses both types queries. We present the results of the latter case in the next subsection while the results of the former are included in the Appendix.
Evaluation metrics During test time, our model receives a question in natural language and a list of knowledge base entities contained in the question. Then it predicts the mean and variance of a Gaussian attention formulated in Eq. (7) which is expected to capture the distribution of all positive answers. We rank all entities in the knowledge base by their scores under this Gaussian attention. Next, for each entity which is a correct answer, we check its rank relative to all incorrect answers and call this rank the filtered rank. For example, if a correct entity is ranked above all negative answers except for one, it has filtered rank two. We compute this rank for all true answers and report mean filtered rank and H@1 which is the percentage of true answers that have filtered rank 1.

4.3 EXPERIMENTAL RESULTS
We present the results of joint learning in Table 2. These results show that TransGaussian works better than TransE in general. In fact, TransGaussian (COMP) achieved the best performance in almost all aspects. Most notably, it achieved the highest H@1 rates on challenging questions such as “where is the club that edin dzeko plays for?” (#11, composition of two relations) and “who are the defenders on german national team?” (#14, conjunction of two queries).
The same table shows that TransGaussian benefits remarkably from compositional training. For example, compositional training improved TransGaussian’s H@1 rate by near 60% in queries on players from a given countries (#8) and queries on players who play a particular position (#9). It also boosted TransGaussian’s performance on all conjunctive quries (#13–#15) significantly.
To understand TransGaussian (COMP)’s weak performance on answering queries on the professional football club located in a given country (#10) and queries on professional football club that has players from a particular country (#12), we tested its capability of modeling the composed relation by feeding the correct relations and subjects during test time. It turns out that these two relations were not modeled well by TransGaussian (COMP) embedding, which limits its performance in question answering. (See Table 8 in the Appendix for quantitative evaluations.) The same limit was found in the other three embeddings as well.
Note that all the models compared in Table 2 uses the proposed Gaussian attention model because TransE is the special case of TransGaussian where the variance is fixed to one. Thus the main differences are whether the variance is learned and whether the embedding was trained compositionally. Finally, we refer to Table 9 and 10 in the Appendix for experimental results of models trained on path and conjunctive queries separately.

5 RELATED WORK
The work of Vilnis & McCallum (2014) is similar to our Gaussian attention model. They discuss many advantages of the Gaussian embedding; for example, it is arguably a better way of handling asymmetric relations and entailment. However the work was presented in the word2vec (Mikolov et al., 2013)-style word embedding setting and the Gaussian embedding was used to capture the diversity in the meaning of a word. Our Gaussian attention model extends their work to a more general setting in which any memory item can be addressed through a concept represented as a Gaussian distribution over the memory items.
Bordes et al. (2014; 2015) proposed a question-answering model that embeds both questions and their answers to a common continuous vector space. Their method in Bordes et al. (2015) can combine multiple knowledge bases and even generalize to a knowledge base that was not used during training. However their method is limited to the simple question answering setting in which the answer of each question associated with a triplet in the knowledge base. In contrast, our method can handle both composition of relations and conjunction of conditions, which are both naturally enabled by the proposed Gaussian attention model.
Neelakantan et al. (2015a) proposed a method that combines relations to deal with compositional relations for knowledge base completion. Their key technical contribution is to use recurrent neural networks (RNNs) to encode a chain of relations. When we restrict ourselves to path queries, question answering can be seen as a sequence transduction task (Graves, 2012; Sutskever et al., 2014) in which the input is text and the output is a series of relations. If we use RNNs as a decoder, our model would be able to handle non-commutative composition of relations, which the current weighted convolution cannot handle well. Another interesting connection to our work is that they take the maximum of the inner-product scores (see also Weston et al., 2013; Neelakantan et al., 2015b), which are computed along multiple paths connecting a pair of entities. Representing a set as a collection of vectors and taking the maximum over the inner-product scores is a natural way to represent a set of memory items. The Gaussian attention model we propose in this paper, however, has the advantage of differentiability and composability.

6 CONCLUSION
In this paper, we have proposed the Gaussian attention model which can be used in a variety of contexts where we can assume that the distance between the memory items in the latent space is compatible with some notion of semantics. We have shown that the proposed Gaussian scoring function can be used for knowledge base embedding achieving competitive accuracy. We have also shown that our embedding model can naturally propagate uncertainty when we compose relations together. Our embedding model also benefits from compositional training proposed by Guu et al. (2015). Furthermore, we have demonstrated the power of the Gaussian attention model in a challenging question answering problem which involves both composition of relations and conjunction of queries. Future work includes experiments on natural question answering datasets and end-to-end training including the entity extractor.

ACKNOWLEDGMENTS
The authors would like to thank Daniel Tarlow, Nate Kushman, and Kevin Gimpel for valuable discussions.

A WORDCUP2014 DATASET

B TRANSGAUSSIAN EMBEDDING OF WORLDCUP2014
We trained our TransGaussian model on triplets and paths from WorldCup2014 dataset and illustrated the embeddings in Fig 3 and 4. Recall that we modeled every relation as a Gaussian with diagonal covariance matrix. Fig 3 shows the learned variance parameters of different relations. Each row corresponds to the variances of one relation. Columns are permuted to reveal the block structure. From this figure, we can see that every relation has a small variance in two or more dimensions. This implies that the coordinates of the embedding space are partitioned into semantically coherent clusters each of which represent a particular attribute of a player (or a football club). To verify this further, we picked the two coordinates in which a relation (e.g. plays position) has the least variance and projected the embedding of all valid subjects and objects (e.g. players and positions) of the relation to this 2 dimensional subspace. See Fig. 4. The relation between the subjects and the objects are simply translation in the projection when the corresponding subspace is two dimensional (e.g., plays position relation in Fig. 4 (a)). The same is true for other relations that requires larger dimension but it is more challenging to visualize in two dimensions. For relations that have a large number of unique objects, we only plotted for the eight objects with the most subjects for clarity of illustration.
Furthermore, in order to elucidate whether we are limited by the capacity of the TransGaussian embedding or the ability to decode question expressed in natural language, we evaluated the test question-answer pairs using the TransGaussian embedding composed according to the ground-truth relations and entities. The results were evaluated with the same metrics as in Sec. 4.3. This estimation is conducted for TransE embeddings as well. See Table 8 for the results. Compared to Table 2, the accuracy of TransGaussian (COMP) is higher on the atomic relations and path queries but lower on conjunctive queries. This is natural because when the query is simple there is not much room for the question-answering network to improve upon just combining the relations according to the ground truth relations, whereas when the query is complex the network could combine the embedding in a more creative way to overcome its limitation. In fact, the two queries (#10 and #12) that TransGaussian (COMP) did not perform well in Table 2 pertain to a single relation is in country−1 (#10) and a composition of two relations plays for country−1 / plays in club (#12). The performance of the two queries were low even when the ground truth
relations were given, which indicates that the TransGaussian embedding rather than the questionanswering network is the limiting factor.

C KNOWLEDGE BASE COMPLETION
Knowledge base completion has been a common task for testing knowledge base models on their ability of generalizing to unseen facts. Here, we apply our TransGaussian model to a knowledge completion task and show that it has competitive performance.
We tested on the subset of WordNet released by Guu et al. (2015). The atomic triplets in this dataset was originally created by Socher et al. (2013) and Guu et al. (2015) added path queries that were randomly sampled from the knowledge graph. We build our TransGaussian model by training on these triplets and paths and tested our model on the same link prediction task as done by Socher et al. (2013); Guu et al. (2015).
As done by Guu et al. (2015), we trained TransGaussian (SINGLE) with atomic triplets only and trained TransGaussian (COMP) with the union of atomic triplets and paths. We did not incorporate
word embedding in this task and each entity is assigned its individual vector. Without getting parameters tuned too much, TransGaussian (COMP) obtained accuracy comparable to TransE (COMP). See Table 11.
","We propose the Gaussian attention model for content-based neural memory access. With the proposed attention model, a neural network has the additional degree of freedom to control the focus of its attention from a laser sharp attention to a broad attention. It is applicable whenever we can assume that the distance in the latent space reflects some notion of semantics. We use the proposed attention model as a scoring function for the embedding of a knowledge base into a continuous vector space and then train a model that performs question answering about the entities in the knowledge base. The proposed attention model can handle both the propagation of uncertainty when following a series of relations and also the conjunction of conditions in a natural way. On a dataset of soccer players who participated in the FIFA World Cup 2014, we demonstrate that our model can handle both path queries and conjunctive queries well.",ICLR 2017 conference submission,False,,"SUMMARY.

The paper propose a new scoring function for knowledge base embedding.
The scoring function called TransGaussian is an novel take on (or a generalization of) the well-known TransE scoring function.
The proposed function is tested on two tasks knowledge-base completion and question answering.

----------

OVERALL JUDGMENT
While I think this proposed work is very interesting and it is an idea worth to explore further, the presentation and the experimental section of the paper have some problems.
Regarding the presentation, as far as I understand this is not an attention model as intended standardly in the literature.
Plus, it has hardly anything to share with memory networks/neural Turing machines, the parallel that the authors try to make is not very convincing.
Regarding the experimental section, for a fair comparison the authors should test their model on standard benchmarks, reporting state-of-the-art models.
Finally, the paper lack of discussion of results and insights on the behavior of the proposed model.


----------

DETAILED COMMENTS


In section 2.2 when the authors calculate \mu_{context} do not they loose the order of relations? And if it is so, does it make any sense?

---

Three knowledgable reviewers recommend rejection. While they agree that the paper has interesting aspects, they suggest a more convincing evaluation. The authors did not address some of the reviewer's concerns. The AC strongly encourages the authors to improve their paper and resubmit it to a future conference.

---

The contribution of this paper can be summarized as:

1, A TransGaussian model (in a similar idea of TransE) which models the subject / object embeddings in a parameterization of Gaussian distribution.  The model can be naturally adapted to path queries like the formulation of (Guu et al, 2015).
2. Along with the entity / relation representations trained by TransGaussian, an LSTM + attention model is built on natural language questions, aiming at learning a distribution (not normalized though) over relations for question answering.
3. Experiments on a generated WorldCup2014 dataset, focusing on path queries and conjunctive queries.

Overall, I think the Gaussian parameterization exhibits some nice properties, and could be suitable to KB completion and question answering. However, some details and the main experimental results are not convincing enough to me.  The paper writing also needs to be improved. More comments below:

[Major comments]

- My main concern is that that evaluation results are NOT strong. Either knowledge base completion or KB-based question answering, there are many existing and competitive benchmarks (e.g., FB15k / WebQuestions). Experimenting with such a tiny WordCup2014 dataset is not convincing.  Moreover, the questions are just generated by a few templates, which is far from NL questions. I am not even not sure why we need to apply an LSTM in such scenario. The paper would be much stronger if you can demonstrate its effectiveness on the above benchmarks. 

- Conjunctive queries:  the current model assumes that all the detected entities in the question could be aligned to one or more relations and we can take conjunctions in the end. This assumption might be not always correct, so it is more necessary to justify this on real QA datasets.

- The model is named as  “Gaussian attention” and I kind of think it is not very closely related to well-known attention mechanism, but more related to KB embedding literature.

[Minor comments]
- I find Figure 2 a bit confusing. The first row of orange blocks denote KB relations, and the second row of those denote every single word of the NL question. Maybe make it clearer?

- Besides “entity recognition”, usually we still need an “entity linker” component which links the text mention to the KB entity.

---

This paper presents extensions to previous work using embeddings for modeling Knowledge Bases and performing Q&A on them, centered around the use of multivariate gaussian likelihood instead of inner products to score attention. This is supposed to allow more control on the attention by dealing with its spread.

This is a dense paper centered around a quite complicated model. With the supplementary material, this makes a 16p paper. It might be clearer to make 2 separate papers: one on KB completion and another one on Q&A.

I like the idea of controlling the spread of the attention. This makes sense. However, I do not feel that this paper is convincing enough to justify its use compared to usual inner products.

For several reasons:
- These should be more ablation experiments to separate the different pieces of the model and study their influence separately. The only interesting point in that sense is Table 8 in Appendix B. We need more of this. 
- In particular, a canonical experiments comparing Gaussian interaction vs inner product would be very useful. 
- Experiments on existing benchmarks (for KB completion, or QA) would help. I agree with the authors that it is difficult to find the perfect benchmark, so it is a good idea to propose a new one (WorldCup2014). But this should come in addition to experiments on existing data.
- Table 11 of Appendix C (page 16) that compares TransE and TransGaussian for the task of link prediction on WordNet can be seen as fixing the two points above (simple setting on existing benchmark). Unfortunately, TransGaussian does not perform well compared to simpler TransE. This, along with the poor results of TransGaussian (SINGLE) of Table 2, indicate that training TransGaussian seems pretty complex, and hence question the actual validity of this architecture.

---

SUMMARY.

The paper propose a new scoring function for knowledge base embedding.
The scoring function called TransGaussian is an novel take on (or a generalization of) the well-known TransE scoring function.
The proposed function is tested on two tasks knowledge-base completion and question answering.

----------

OVERALL JUDGMENT
While I think this proposed work is very interesting and it is an idea worth to explore further, the presentation and the experimental section of the paper have some problems.
Regarding the presentation, as far as I understand this is not an attention model as intended standardly in the literature.
Plus, it has hardly anything to share with memory networks/neural Turing machines, the parallel that the authors try to make is not very convincing.
Regarding the experimental section, for a fair comparison the authors should test their model on standard benchmarks, reporting state-of-the-art models.
Finally, the paper lack of discussion of results and insights on the behavior of the proposed model.


----------

DETAILED COMMENTS


In section 2.2 when the authors calculate \mu_{context} do not they loose the order of relations? And if it is so, does it make any sense?

---

SUMMARY.

The paper propose a new scoring function for knowledge base embedding.
The scoring function called TransGaussian is an novel take on (or a generalization of) the well-known TransE scoring function.
The proposed function is tested on two tasks knowledge-base completion and question answering.

----------

OVERALL JUDGMENT
While I think this proposed work is very interesting and it is an idea worth to explore further, the presentation and the experimental section of the paper have some problems.
Regarding the presentation, as far as I understand this is not an attention model as intended standardly in the literature.
Plus, it has hardly anything to share with memory networks/neural Turing machines, the parallel that the authors try to make is not very convincing.
Regarding the experimental section, for a fair comparison the authors should test their model on standard benchmarks, reporting state-of-the-art models.
Finally, the paper lack of discussion of results and insights on the behavior of the proposed model.


----------

DETAILED COMMENTS


In section 2.2 when the authors calculate \mu_{context} do not they loose the order of relations? And if it is so, does it make any sense?

---

Three knowledgable reviewers recommend rejection. While they agree that the paper has interesting aspects, they suggest a more convincing evaluation. The authors did not address some of the reviewer's concerns. The AC strongly encourages the authors to improve their paper and resubmit it to a future conference.

---

The contribution of this paper can be summarized as:

1, A TransGaussian model (in a similar idea of TransE) which models the subject / object embeddings in a parameterization of Gaussian distribution.  The model can be naturally adapted to path queries like the formulation of (Guu et al, 2015).
2. Along with the entity / relation representations trained by TransGaussian, an LSTM + attention model is built on natural language questions, aiming at learning a distribution (not normalized though) over relations for question answering.
3. Experiments on a generated WorldCup2014 dataset, focusing on path queries and conjunctive queries.

Overall, I think the Gaussian parameterization exhibits some nice properties, and could be suitable to KB completion and question answering. However, some details and the main experimental results are not convincing enough to me.  The paper writing also needs to be improved. More comments below:

[Major comments]

- My main concern is that that evaluation results are NOT strong. Either knowledge base completion or KB-based question answering, there are many existing and competitive benchmarks (e.g., FB15k / WebQuestions). Experimenting with such a tiny WordCup2014 dataset is not convincing.  Moreover, the questions are just generated by a few templates, which is far from NL questions. I am not even not sure why we need to apply an LSTM in such scenario. The paper would be much stronger if you can demonstrate its effectiveness on the above benchmarks. 

- Conjunctive queries:  the current model assumes that all the detected entities in the question could be aligned to one or more relations and we can take conjunctions in the end. This assumption might be not always correct, so it is more necessary to justify this on real QA datasets.

- The model is named as  “Gaussian attention” and I kind of think it is not very closely related to well-known attention mechanism, but more related to KB embedding literature.

[Minor comments]
- I find Figure 2 a bit confusing. The first row of orange blocks denote KB relations, and the second row of those denote every single word of the NL question. Maybe make it clearer?

- Besides “entity recognition”, usually we still need an “entity linker” component which links the text mention to the KB entity.

---

This paper presents extensions to previous work using embeddings for modeling Knowledge Bases and performing Q&A on them, centered around the use of multivariate gaussian likelihood instead of inner products to score attention. This is supposed to allow more control on the attention by dealing with its spread.

This is a dense paper centered around a quite complicated model. With the supplementary material, this makes a 16p paper. It might be clearer to make 2 separate papers: one on KB completion and another one on Q&A.

I like the idea of controlling the spread of the attention. This makes sense. However, I do not feel that this paper is convincing enough to justify its use compared to usual inner products.

For several reasons:
- These should be more ablation experiments to separate the different pieces of the model and study their influence separately. The only interesting point in that sense is Table 8 in Appendix B. We need more of this. 
- In particular, a canonical experiments comparing Gaussian interaction vs inner product would be very useful. 
- Experiments on existing benchmarks (for KB completion, or QA) would help. I agree with the authors that it is difficult to find the perfect benchmark, so it is a good idea to propose a new one (WorldCup2014). But this should come in addition to experiments on existing data.
- Table 11 of Appendix C (page 16) that compares TransE and TransGaussian for the task of link prediction on WordNet can be seen as fixing the two points above (simple setting on existing benchmark). Unfortunately, TransGaussian does not perform well compared to simpler TransE. This, along with the poor results of TransGaussian (SINGLE) of Table 2, indicate that training TransGaussian seems pretty complex, and hence question the actual validity of this architecture.

---

SUMMARY.

The paper propose a new scoring function for knowledge base embedding.
The scoring function called TransGaussian is an novel take on (or a generalization of) the well-known TransE scoring function.
The proposed function is tested on two tasks knowledge-base completion and question answering.

----------

OVERALL JUDGMENT
While I think this proposed work is very interesting and it is an idea worth to explore further, the presentation and the experimental section of the paper have some problems.
Regarding the presentation, as far as I understand this is not an attention model as intended standardly in the literature.
Plus, it has hardly anything to share with memory networks/neural Turing machines, the parallel that the authors try to make is not very convincing.
Regarding the experimental section, for a fair comparison the authors should test their model on standard benchmarks, reporting state-of-the-art models.
Finally, the paper lack of discussion of results and insights on the behavior of the proposed model.


----------

DETAILED COMMENTS


In section 2.2 when the authors calculate \mu_{context} do not they loose the order of relations? And if it is so, does it make any sense?",,,,,,4.333333333333333,,,3.6666666666666665,,
648,"CONTEXT-CONDITIONAL GENERATIVE ADVERSARIAL NETWORKS
Authors: Emily Denton
Source file: 648.pdf

ABSTRACT
We introduce a simple semi-supervised learning approach for images based on in-painting using an adversarial loss. Images with random patches removed are presented to a generator whose task is to fill in the hole, based on the surrounding pixels. The in-painted images are then presented to a discriminator network that judges if they are real (unaltered training images) or not. This task acts as a regularizer for standard supervised training of the discriminator. Using our approach we are able to directly train large VGG-style networks in a semi-supervised fashion. We evaluate on STL-10 and PASCAL datasets, where our approach obtains performance comparable or superior to existing methods.

1 INTRODUCTION
Deep neural networks have yielded dramatic performance gains in recent years on tasks such as object classification (Krizhevsky et al., 2012), text classification (Zhang et al., 2015) and machine translation (Sutskever et al., 2014; Bahdanau et al., 2015). These successes are heavily dependent on large training sets of manually annotated data. In many settings however, such large collections of labels may not be readily available, motivating the need for methods that can learn from data where labels are rare.
We propose a method for harnessing unlabeled image data based on image in-painting. A generative model is trained to generate pixels within a missing hole, based on the context provided by surrounding parts of the image. These in-painted images are then used in an adversarial setting (Goodfellow et al., 2014) to train a large discriminator model whose task is to determine if the image was real (from the unlabeled training set) or fake (an in-painted image). The realistic looking fake examples provided by the generative model cause the discriminator to learn features that generalize to the related task of classifying objects. Thus adversarial training for the in-painting task can be used to regularize large discriminative models during supervised training on a handful of labeled images.

1.1 RELATED WORK
Learning From Context: The closest work to ours is the independently developed context-encoder approach of Pathak et al. (2016). This introduces an encoder-decoder framework, shown in Fig. 1(a), that is used to in-paint images where a patch has been randomly removed. After using this as a pre-training task, a classifier is added to the encoder and the model is fine-tuned using the labeled examples. Although both approaches use the concept of in-painting, they differ in several important ways. First, the architectures are different (see Fig. 1): in Pathak et al. (2016), the features for the classifier are taken from the encoder, whereas ours come from the discriminator network. In practice this makes an important difference as we are able to directly train large models such as VGG (Simonyan & Zisserman, 2015) using adversarial loss alone. By contrast, Pathak et al. (2016) report difficulties in training an AlexNet encoder with this loss. This leads to the second difference, namely that on account of these issues, they instead employ an `2 loss when training models for classification and detection (however they do use a joint `2 and adversarial loss to achieve impressive in-painting results). Finally, the unsupervised learning task differs between the two models. The context-encoder learns a feature representation suitable for in-painting whereas our model learns a feature representation suitable for differentiating real/fake in-paintings. Notably, while we also use a neural network to generate the in-paintings, this model is only used as an adversary for the
discriminator, rather than as a feature extractor. In section 4, we compare the performance of our model to the context-encoder on the PASCAL dataset.
Other forms of spatial context within images have recently been utilized for representation learning. Doersch et al. (2015) propose training a CNN to predict the spatial location of one image patch relative to another. Noroozi & Favaro (2016) propose a model that learns by unscrambling image patches, essentially solving a jigsaw puzzle to learn visual representations. In the text domain, context has been successfully leveraged as an unsupervised criterion for training useful word and sentence level representations (Collobert et al., 2011; Mikolov et al., 2015; Kiros et al., 2015).
Deep unsupervised and semi-supervised learning: A popular method of utilizing unlabeled data is to layer-wise train a deep autoencoder or restricted Botlzmann machine (Hinton et al., 2006) and then fine tune with labels on a discriminative task. More recently, several autoencoding variants have been proposed for unsupervised and semi-supervised learning, such as the ladder network (Rasmus et al., 2015), stacked what-where autoencoders (Zhao et al., 2016) and variational autoencoders (Kingma & Welling, 2014; Kingma et al., 2014).
Dosovitskiy et al. (2014) achieved state-of-the-art results by training a CNN with a different class for each training example and introducing a set of transformations to provide multiple examples per class. The pseudo-label approach (Lee, 2013) is a simple semi-supervised method that trains using the maximumly predicted class as a label when labels are unavailable. Springenberg (2015) propose a categorical generative adversarial network (CatGAN) which can be used for unsupervised and semi-supervised learning. The discriminator in a CatGAN outputs a distribution over classes and is trained to minimize the predicted entropy for real data and maximize the predicted entropy for fake data. Similar to our model, CatGANs use the feature space learned by the discriminator for the final supervised learning task. Salimans et al. (2016) recently proposed a semi-supervised GAN model in which the discriminator outputs a softmax over classes rather than a probability of real vs. fake. An additional ‘generated’ class is used as the target for generated samples. This method differs from our work in that it does not utilize context information and has only been applied to datasets of small resolution. However, the discriminator loss is similar to the one we propose and could be combined with our context-conditional approach.
More traditional semi-supervised methods include graph-based approaches (Zhou et al., 2004; Zhu, 2006) that show impressive performance when good image representations are available. However, the focus of our work is on learning such representations.
Generative models of images: Restricted Boltzmann machines (Salakhutdinov, 2015), de-noising autoencoders (Vincent et al., 2008) and variational autoencoders (Kingma & Welling, 2014) optimize a maximum likelihood criterion and thus learn decoders that map from latent space to image space. More recently, generative adversarial networks (Goodfellow et al., 2014) and generative mo-
ment matching networks (Li et al., 2015; Dziugaite et al., 2015) have been proposed. These methods ignore data likelihoods and instead directly train a generative model to produce realistic samples. Several extensions to the generative adversarial network framework have been proposed to scale the approach to larger images (Denton et al., 2015; Radford et al., 2016; Salimans et al., 2016). Our work draws on the insights of Radford et al. (2016) regarding adversarial training practices and architecture for the generator network, as well as the notion that the discriminator can produce useful features for classification tasks.
Other models used recurrent approaches to generate images (Gregor et al., 2015; Theis & Bethge, 2015; Mansimov et al., 2016; van den Oord et al., 2016). Dosovitskiy et al. (2015) trained a CNN to generate objects with different shapes, viewpoints and color. Sohl-Dickstein et al. (2015) propose a generative model based on a reverse diffusion process. While our model does involve image generation, it differs from these approaches in that the main focus is on learning a good representation for classification tasks.
Predictive generative models of videos aim to extrapolate from current frames to future ones and in doing so learn a feature representation that is useful for other tasks. In this vein, Ranzato et al. (2014) used an `2-loss in pixel-space. Mathieu et al. (2015) combined an adversarial loss with `2, giving models that generate crisper images. While our model is also predictive, it only considers interpolation within an image, rather than extrapolation in time.

2 APPROACH
We present a semi-supervised learning framework built on generative adversarial networks (GANs) of Goodfellow et al. (2014). We first review the generative adversarial network framework and then introduce context conditional generative adversarial networks (CC-GANs). Finally, we show how combining a classification objective and a CC-GAN objective provides a unified framework for semi-supervised learning.

2.1 GENERATIVE ADVERSARIAL NETWORKS
The generative adversarial network approach (Goodfellow et al., 2014) is a framework for training generative models, which we briefly review. It consists of two networks pitted against one another in a two player game: A generative model, G, is trained to synthesize images resembling the data distribution and a discriminative model, D, is trained to distinguish between samples drawn from G and images drawn from the training data.
More formally, let X = {x1, ...,xn} be a dataset of images of dimensionality d. Let D denote a discriminative function that takes as input an image x ∈ Rd and outputs a scalar representing the probability of input x being a real sample. Let G denote the generative function that takes as input a random vector z ∈ Rz sampled from a prior noise distribution pNoise and outputs a synthesized image x̃ = G(z) ∈ Rd. Ideally, D(x) = 1 when x ∈ X and D(x) = 0 when x was generated from G. The GAN objective is given by:
min G max D Ex∼X [logD(x)] + Ez∼pNoise(z)[log(1−D(G(z)))] (1)
The conditional generative adversarial network (Mirza & Osindero, 2014) is an extension of the GAN in which bothD andG receive an additional vector of information y as input. The conditional GAN objective is given by:
min G max D Ex,y∼X [logD(x,y)] + Ez∼pNoise(z)[log(1−D(G(z,y),x))] (2)

2.2 CONTEXT-CONDITIONAL GENERATIVE ADVERSARIAL NETWORKS
We propose context-conditional generative adversarial networks (CC-GANs) which are conditional GANs where the generator is trained to fill in a missing image patch and the generator and discriminator are conditioned on the surrounding pixels.
In particular, the generator G receives as input an image with a randomly masked out patch. The generator outputs an entire image. We fill in the missing patch from the generated output and then pass the completed image into D. We pass the completed image into D rather than the context and the patch as two separate inputs so as to prevent D from simply learning to identify discontinuities along the edge of the missing patch.
More formally, let m ∈ Rd denote to a binary mask that will be used to drop out a specified portion of an image. The generator receives as input m x where denotes element-wise multiplication. The generator outputs xG = G(m x, z) ∈ Rd and the in-painted image xI is given by:
xI = (1−m) xG +m x (3)
The CC-GAN objective is given by:
min G max D
Ex∼X [logD(x)] + Ex∼X ,m∼M[log(1−D(xI))] (4)

2.3 COMBINED GAN AND CC-GAN
While the generator of the CC-GAN outputs a full image, only a portion of it (corresponding to the missing hole) is seen by the discriminator. In the combined model, which we denote by CC-GAN2, the fake examples for the discriminator include both the in-painted image xI and the full image xG produced by the generator (i.e. not just the missing patch). By combining the GAN and CC-GAN approaches, we introduce a wider array of negative examples to the discriminator. The CC-GAN2 objective given by:
min G max D
Ex∼X [logD(x)] (5)
+ Ex∼X ,m∼M[log(1−D(xI))] (6) + Ex∼X ,m∼M[log(1−D(xG))] (7)

2.4 SEMI-SUPERVISED LEARNING WITH CC-GANS
A common approach to semi-supervised learning is to combine a supervised and unsupervised objective during training. As a result unlabeled data can be leveraged to aid the supervised task.
Intuitively, a GAN discriminator must learn something about the structure of natural images in order to effectively distinguish real from generated images. Recently, Radford et al. (2016) showed that a GAN discriminator learns a hierarchical image representation that is useful for object classification. Such results suggest that combining an unsupervised GAN objective with a supervised classification objective would produce a simple and effective semi-supervised learning method. This approach, denoted by SSL-GAN, is illustrated in Fig. 1(b). The discriminator network receives a gradient from the real/fake loss for every real and generated image. The discriminator also receives a gradient from the classification loss on the subset of (real) images for which labels are available.
Generative adversarial networks have shown impressive performance on many diverse datasets. However, samples are most coherent when the set of images the network is trained on comes from a limited domain (eg. churches or faces). Additionally, it is difficult to train GANs on very large images. Both these issues suggest semi-supervised learning with vanilla GANs may not scale well to datasets of large diverse images. Rather than determining if a full image is real or fake, context conditional GANs address a different task: determining if a part of an image is real or fake given the surrounding context.
Formally, let XL = {(x1, y1), ..., (xn, yn)} denote a dataset of labeled images. Let Dc(x) denote the output of the classifier head on the discriminator (see Fig. 1(c) for details). Then the semisupervised CC-GAN objective is:
min G max D Ex∼X [logD(x)] + Ex∼X ,m∼M[log(1−D(xI))] + λcEx,y∼XL [log(Dc(y|x))] (8)
The hyperparameter λc balances the classification and adversarial losses. We only consider the CCGAN in the semi-supervised setting and thus drop the SSL notation when referring to this model.

2.5 MODEL ARCHITECTURE AND TRAINING DETAILS
The architecture of our generative model,G, is inspired by the generator architecture of the DCGAN (Radford et al., 2016). The model consists of a sequence of convolutional layers with subsampling (but no pooling) followed by a sequence of fractionally-strided convolutional layers. For the discriminator, D, we used the VGG-A network (Simonyan & Zisserman, 2015) without the fully connected layers (which we call the VGG-A’ architecture). Details of the generator and discriminator are given
in Fig. 2. The input to the generator is an image with a patch zeroed out. In preliminary experiments we also tried passing in a separate mask to the generator to make the missing area more explicit but found this did not effect performance.
Even with the context conditioning it is difficult to generate large image patches that look realistic, making it problematic to scale our approach to high resolution images. To address this, we propose conditioning the generator on both the high resolution image with a missing patch and a low resolution version of the whole image (with no missing region). In this setting, the generators task becomes one of super-resolution on a portion of an image. However, the discriminator does not receive the low resolution image and thus is still faced with the same problem of determining if a given in-painting is viable or not. Where indicated, we used this approach in our PASCAL VOC 2007 experiments, with the original image being downsampled by a factor of 4. This provided enough information for the generator to fill in larger holes but not so much that it made the task trivial. This optional low resolution image is illustrated in Fig. 2(left) with the dotted line.
We followed the training procedures of Radford et al. (2016). We used the Adam optimizer (Kingma & Ba, 2015) in all our experiments with learning rate of 0.0002, momentum term β1 of 0.5, and the remaining Adam hyperparameters set to their default values. We set λc = 1 for all experiments.

3 EXPERIMENTS

3.1 STL-10 CLASSIFICATION
STL-10 is a dataset of 96×96 color images with a 1:100 ratio of labeled to unlabeled examples, making it an ideal fit for our semi-supervised learning framework. The training set consists of 5000 labeled images, mapped to 10 pre-defined folds of 1000 images each, and 100,000 unlabeled images. The labeled images belong to 10 classes and were extracted from the ImageNet dataset and the unlabeled images come from a broader distribution of classes. We follow the standard testing protocol and train 10 different models on each of the 10 predefined folds of data. We then evaluate classification accuracy of each model on the test set and report the mean and standard deviation.
We trained our CC-GAN and CC-GAN2 models on 64×64 crops of the 96×96 image. The hole was 32×32 pixels and the location of the hole varied randomly (see Fig. 3(top)). We trained for 100 epochs and then fine-tuned the discriminator on the 96x96 labeled images, stopping when training accuracy reached 100%. As shown in Table 1, the CC-GAN model performs comparably to current state of the art (Dosovitskiy et al., 2014) and the CC-GAN2 model improves upon it.
We also trained two baseline models in an attempt to tease apart the contributions of adversarial training and context conditional adversarial training. The first is a purely supervised training of the VGG-A’ model (the same architecture as the discriminator in the CC-GAN framework). This was trained using a dropout of 0.5 on the final layer and weight decay of 0.001. The performance of this model is significantly worse than the CC-GAN model.
We also trained a semi-supervised GAN (SSL-GAN, see Fig. 1(b)) on STL-10. This consisted of the same discriminator as the CC-GAN (VGG-A’ architecture) and generator from the DCGAN model (Radford et al., 2016). The training setup in this case is identical to the CC-GAN model. The SSLGAN performs almost as well as the CC-GAN, confirming our hypothesis that the GAN objective is a useful unsupervised criterion.

3.2 PASCAL VOC CLASSIFICATION
In order to compare against other methods that utilize spatial context we ran the CC-GAN model on PASCAL VOC 2007 dataset. This dataset consists of natural images coming from 20 classes. The dataset contains a large amount of variability with objects varying in size, pose, and position. The training and validation sets combined contain 5,011 images, and the test set contains 4,952 images. The standard measure of performance is mean average precision (mAP).
We trained each model on the combined training and validation set for ∼5000 epochs and evaluated on the test set once1. Following Pathak et al. (2016), we train using random cropping, and then evaluate using the average prediction from 10 random crops.
Our best performing model was trained on images of resolution 128×128 with a hole size of 64×64 and a low resolution input of size 32×32. Table 2 compares our CC-GAN method to other feature learning approaches on the PASCAL test set. It outperforms them, beating the current state of the art (Wang & Gupta, 2015) by 3.8%. It is important to note that our feature extractor is the VGGA’ model which is larger than the AlexNet architecture (Krizhevsky et al., 2012) used by other approaches in Table 2. However, purely supervised training of the two models reveals that VGG-A’
1Hyperparameters were determined by initially training on the training set alone and measuring performance on the validation set.
is less than 2% better than AlexNet. Furthermore, our model outperforms the supervised VGG-A’ baseline by a 7% margin (62.2% vs. 55.2%). This suggests that our gains stem from the CC-GAN method rather than the use of a better architecture.
Table 3 shows the effect of training on different resolutions. The CC-GAN improves over the baseline CNN consistently regardless of image size. We found that conditioning on the low resolution image began to help when the hole size was largest (64×64). We hypothesize that the low resolution conditioning would be more important for larger images, potentially allowing the method to scale to larger image sizes than we explored in this work.

3.3 INPAINTING
We now show some sample in-paintings produced by our CC-GAN generators. In our semisupervised learning experiments on STL-10 we remove a single fixed size hole from the image. The top row of Fig. 3 shows in-paintings produced by this model. We can also explored different masking schemes as illustrated in the remaining rows of Fig. 3 (however these did not improve classification results). In all cases we see that training the generator with the adversarial loss produces sharp semantically plausible in-painting results.
Fig. 4 shows generated images and in-painted images from a model trained with the CC-GAN2 criterion. The output of a CC-GAN generator tends to be corrupted outside the patch used to inpaint the image (since gradients only flow back to the missing patch). However, in the CC-GAN2 model, we see that both the in-painted image and the generated image are coherent and semantically consistent with the masked input image.
Fig. 5 shows in-painted images from a generator trained on 128×128 PASCAL images. Fig. 6 shows the effect of adding a low resolution (32×32) image as input to the generator. For comparison we also show the result of in-painting by filling in with a bi-linearly upsampled image. Here we see the generator produces high-frequency structure rather than simply learning to copy the low resolution patch.

4 DISCUSSION
We have presented a simple semi-supervised learning framework based on in-painting with an adversarial loss. The generator in our CC-GAN model is capable of producing semantically meaningful
in-paintings and the discriminator performs comparable to or better than existing semi-supervised methods on two classification benchmarks.
Since discrimination of real/fake in-paintings is more closely related to the target task of object classification than extracting a feature representation suitable for in-filling, it is not surprising that we are able to exceed the performance of Pathak et al. (2016) on PASCAL classification. Furthermore, since our model operates on images half the resolution as those used by other approaches (128×128 vs. 224×244), there is potential for further gains if improvements in the generator resolution can be made. Our models and code are available at https://github.com/edenton/cc-gan.
Acknowledgements: Emily Denton is supported by a Google Fellowship. Rob Fergus is grateful for the support of CIFAR.
","We introduce a simple semi-supervised learning approach for images based on in-painting using an adversarial loss. Images with random patches removed are presented to a generator whose task is to fill in the hole, based on the surrounding pixels. The in-painted images are then presented to a discriminator network that judges if they are real (unaltered training images) or not. This task acts as a regularizer for standard supervised training of the discriminator. Using our approach we are able to directly train large VGG-style networks in a semi-supervised fashion. We evaluate on STL-10 and PASCAL datasets, where our approach obtains performance comparable or superior to existing methods.",ICLR 2017 conference submission,False,,"After rebuttal:

Thanks for reporting the AlexNet results. The fact that they are not great is not so bad by itself, and as the authors mention, it would be interesting to understand why this happens. But the fact that these results  were not in the paper (and in fact still are not there) is disturbing. Moreover, some claims in the paper look wrong in the light of these results, for example:
- ""This suggests that our gains stem from the CC-GAN method rather than the use of a better architecture.""
- ""Since discrimination of real/fake in-paintings is more closely related to the target task of object classification than extracting a feature representation suitable for in-filling, it is not surprising that we are able to exceed the performance of Pathak et al. (2016) on PASCAL classification.""

These statements, and possibly other parts of the paper, have to be updated. I think the paper cannot be published in its current form. Perhaps after a revision.

--------
Initial review:

The paper demonstrates an application of generative adversarial networks (GAN) to unsupervised feature learning. The authors show that the representation learned by the discriminator of a conditional GAN trained for image inpainting performs well on image classification. As a side-effect, fairly convincing inpaintings are produced.

The proposed method combines two existing ideas: using the discriminator of a GAN as a feature learner [Radford et al. 2015] and performing unsupervised feature learning with image inpainting [Pathak et al. 2016]. Therefore conceptual novelty of the paper is limited. On the plus side, the authors implement their idea well and demonstrate state-of-the-art results on STL-10 and good results on Pascal VOC (although Pascal experiments are incomplete, see below). Overall, I am in the borderline mode, and I will gladly raise the score if the authors address my concerns regarding the experiments.

1) Experimental evaluation on Pascal VOC is not quite satisfactory. Comparison with prior work is unfair because the network architecture used by the authors (VGG) is different from the architecture used by all existing methods (AlexNet). It is great that the authors do not try to hide this fact in the paper, but I do not understand why the authors are not willing to simply run their method with AlexNet architecture, although two commenters asked them to do so. Such an experiment would strongly support authors’ claims. Current reasoning that “we thought it reasonable to use more current models while making the difference clear” is not convincing. It is great that better architectures lead to better results, but it is also very important to properly compare to prior work. On a related topic, Doersch et al. also tried using VGG architecture, would it be possible to compare to that? Yet another question: why are you not comparing to [Noroozi&Favaro, ECCV 2016] ? I would also like the authors to address the comment by Richard Zhang.

2) Qualitative inpainting results are incomplete: comparison with previous methods (for instance, [Pathak et al 2016]) is missing, and it is impossible to compare different versions of the proposed method because different images are used for different variants. I realize there may be too little space in the main paper to show all the results, but many more results should be shown in the supplementary material. Quantitative results are missing. Currently the inpainting results are just interesting pictures to look at, but they do not add as much to the paper as they could.

---

There has been prior work on semi-supervised GAN, though this paper is the first context conditional variant. The novelty of the approach was questioned by two of the reviewers, as the approach seems more incremental. Furthermore, it would have been helpful if the issues one of the reviewer had with statements in the document were addressed.

---

This paper presents a semi-supervised algorithm for regularizing deep convolutional neural networks.
They propose an adversarial approach for image inpainting where the discriminator learns to identify whether an inpainted image comes from the data distribution or the generator, while at the same time it learns to recognize objects in an image from the data distribution. In experiments, they show the usefulness of their algorithm in which the features learned by the discriminator result in comparable or better object recognition performance to the reported state-of-the-art in two datasets.
Overall, the proposed idea seems a simple yet an effective way for regularize CNNs to improve the classification performance.

---

This paper proposes a method to incorporate super-resolution and inpainting in the GAN framework for semi-supervised learning using the GAN discriminative features on larger images.

The core idea of the paper is not very novel. The usefulness of the GAN discriminative features for semi-supervised learning is already established in previous works such as CatGAN, DCGAN and Salimans et al. However this paper does a good job in actually getting the semi-supervised GAN framework working on larger images such as STL-10 and Pascal datasets using the proposed context conditioning approach, and achieves the state-of-the-art on these datasets.

I think that the authors should provide the SSL-GAN baseline for the PASCAL dataset as it is very important to compare the contribution of the context conditioning idea with the standard way of using GAN for semi-supervised learning, i.e., SSL-GAN. I can't see why the SSL-GAN can not be applied to the 64*64 and 96*96 version of the Pascal dataset (Table 3). If they have trouble training the vanilla GAN on Pascal even on the 64*64 image size, this should be mentioned in the paper and be explained. I am concerned about this specially because CC-GAN almost matches the SSL-GAN baseline on STL-10, and CC-GAN2, to me, seems like a hacky way to improve upon the core CC-GAN idea. So it would be great to compare CC-GAN and SSL-GAN on some other dataset, even if it is a downsampled PASCAL dataset.

---

After rebuttal:

Thanks for reporting the AlexNet results. The fact that they are not great is not so bad by itself, and as the authors mention, it would be interesting to understand why this happens. But the fact that these results  were not in the paper (and in fact still are not there) is disturbing. Moreover, some claims in the paper look wrong in the light of these results, for example:
- ""This suggests that our gains stem from the CC-GAN method rather than the use of a better architecture.""
- ""Since discrimination of real/fake in-paintings is more closely related to the target task of object classification than extracting a feature representation suitable for in-filling, it is not surprising that we are able to exceed the performance of Pathak et al. (2016) on PASCAL classification.""

These statements, and possibly other parts of the paper, have to be updated. I think the paper cannot be published in its current form. Perhaps after a revision.

--------
Initial review:

The paper demonstrates an application of generative adversarial networks (GAN) to unsupervised feature learning. The authors show that the representation learned by the discriminator of a conditional GAN trained for image inpainting performs well on image classification. As a side-effect, fairly convincing inpaintings are produced.

The proposed method combines two existing ideas: using the discriminator of a GAN as a feature learner [Radford et al. 2015] and performing unsupervised feature learning with image inpainting [Pathak et al. 2016]. Therefore conceptual novelty of the paper is limited. On the plus side, the authors implement their idea well and demonstrate state-of-the-art results on STL-10 and good results on Pascal VOC (although Pascal experiments are incomplete, see below). Overall, I am in the borderline mode, and I will gladly raise the score if the authors address my concerns regarding the experiments.

1) Experimental evaluation on Pascal VOC is not quite satisfactory. Comparison with prior work is unfair because the network architecture used by the authors (VGG) is different from the architecture used by all existing methods (AlexNet). It is great that the authors do not try to hide this fact in the paper, but I do not understand why the authors are not willing to simply run their method with AlexNet architecture, although two commenters asked them to do so. Such an experiment would strongly support authors’ claims. Current reasoning that “we thought it reasonable to use more current models while making the difference clear” is not convincing. It is great that better architectures lead to better results, but it is also very important to properly compare to prior work. On a related topic, Doersch et al. also tried using VGG architecture, would it be possible to compare to that? Yet another question: why are you not comparing to [Noroozi&Favaro, ECCV 2016] ? I would also like the authors to address the comment by Richard Zhang.

2) Qualitative inpainting results are incomplete: comparison with previous methods (for instance, [Pathak et al 2016]) is missing, and it is impossible to compare different versions of the proposed method because different images are used for different variants. I realize there may be too little space in the main paper to show all the results, but many more results should be shown in the supplementary material. Quantitative results are missing. Currently the inpainting results are just interesting pictures to look at, but they do not add as much to the paper as they could.

---

Nice work! I am curious about the SSL experiments: since

---

After rebuttal:

Thanks for reporting the AlexNet results. The fact that they are not great is not so bad by itself, and as the authors mention, it would be interesting to understand why this happens. But the fact that these results  were not in the paper (and in fact still are not there) is disturbing. Moreover, some claims in the paper look wrong in the light of these results, for example:
- ""This suggests that our gains stem from the CC-GAN method rather than the use of a better architecture.""
- ""Since discrimination of real/fake in-paintings is more closely related to the target task of object classification than extracting a feature representation suitable for in-filling, it is not surprising that we are able to exceed the performance of Pathak et al. (2016) on PASCAL classification.""

These statements, and possibly other parts of the paper, have to be updated. I think the paper cannot be published in its current form. Perhaps after a revision.

--------
Initial review:

The paper demonstrates an application of generative adversarial networks (GAN) to unsupervised feature learning. The authors show that the representation learned by the discriminator of a conditional GAN trained for image inpainting performs well on image classification. As a side-effect, fairly convincing inpaintings are produced.

The proposed method combines two existing ideas: using the discriminator of a GAN as a feature learner [Radford et al. 2015] and performing unsupervised feature learning with image inpainting [Pathak et al. 2016]. Therefore conceptual novelty of the paper is limited. On the plus side, the authors implement their idea well and demonstrate state-of-the-art results on STL-10 and good results on Pascal VOC (although Pascal experiments are incomplete, see below). Overall, I am in the borderline mode, and I will gladly raise the score if the authors address my concerns regarding the experiments.

1) Experimental evaluation on Pascal VOC is not quite satisfactory. Comparison with prior work is unfair because the network architecture used by the authors (VGG) is different from the architecture used by all existing methods (AlexNet). It is great that the authors do not try to hide this fact in the paper, but I do not understand why the authors are not willing to simply run their method with AlexNet architecture, although two commenters asked them to do so. Such an experiment would strongly support authors’ claims. Current reasoning that “we thought it reasonable to use more current models while making the difference clear” is not convincing. It is great that better architectures lead to better results, but it is also very important to properly compare to prior work. On a related topic, Doersch et al. also tried using VGG architecture, would it be possible to compare to that? Yet another question: why are you not comparing to [Noroozi&Favaro, ECCV 2016] ? I would also like the authors to address the comment by Richard Zhang.

2) Qualitative inpainting results are incomplete: comparison with previous methods (for instance, [Pathak et al 2016]) is missing, and it is impossible to compare different versions of the proposed method because different images are used for different variants. I realize there may be too little space in the main paper to show all the results, but many more results should be shown in the supplementary material. Quantitative results are missing. Currently the inpainting results are just interesting pictures to look at, but they do not add as much to the paper as they could.

---

There has been prior work on semi-supervised GAN, though this paper is the first context conditional variant. The novelty of the approach was questioned by two of the reviewers, as the approach seems more incremental. Furthermore, it would have been helpful if the issues one of the reviewer had with statements in the document were addressed.

---

This paper presents a semi-supervised algorithm for regularizing deep convolutional neural networks.
They propose an adversarial approach for image inpainting where the discriminator learns to identify whether an inpainted image comes from the data distribution or the generator, while at the same time it learns to recognize objects in an image from the data distribution. In experiments, they show the usefulness of their algorithm in which the features learned by the discriminator result in comparable or better object recognition performance to the reported state-of-the-art in two datasets.
Overall, the proposed idea seems a simple yet an effective way for regularize CNNs to improve the classification performance.

---

This paper proposes a method to incorporate super-resolution and inpainting in the GAN framework for semi-supervised learning using the GAN discriminative features on larger images.

The core idea of the paper is not very novel. The usefulness of the GAN discriminative features for semi-supervised learning is already established in previous works such as CatGAN, DCGAN and Salimans et al. However this paper does a good job in actually getting the semi-supervised GAN framework working on larger images such as STL-10 and Pascal datasets using the proposed context conditioning approach, and achieves the state-of-the-art on these datasets.

I think that the authors should provide the SSL-GAN baseline for the PASCAL dataset as it is very important to compare the contribution of the context conditioning idea with the standard way of using GAN for semi-supervised learning, i.e., SSL-GAN. I can't see why the SSL-GAN can not be applied to the 64*64 and 96*96 version of the Pascal dataset (Table 3). If they have trouble training the vanilla GAN on Pascal even on the 64*64 image size, this should be mentioned in the paper and be explained. I am concerned about this specially because CC-GAN almost matches the SSL-GAN baseline on STL-10, and CC-GAN2, to me, seems like a hacky way to improve upon the core CC-GAN idea. So it would be great to compare CC-GAN and SSL-GAN on some other dataset, even if it is a downsampled PASCAL dataset.

---

After rebuttal:

Thanks for reporting the AlexNet results. The fact that they are not great is not so bad by itself, and as the authors mention, it would be interesting to understand why this happens. But the fact that these results  were not in the paper (and in fact still are not there) is disturbing. Moreover, some claims in the paper look wrong in the light of these results, for example:
- ""This suggests that our gains stem from the CC-GAN method rather than the use of a better architecture.""
- ""Since discrimination of real/fake in-paintings is more closely related to the target task of object classification than extracting a feature representation suitable for in-filling, it is not surprising that we are able to exceed the performance of Pathak et al. (2016) on PASCAL classification.""

These statements, and possibly other parts of the paper, have to be updated. I think the paper cannot be published in its current form. Perhaps after a revision.

--------
Initial review:

The paper demonstrates an application of generative adversarial networks (GAN) to unsupervised feature learning. The authors show that the representation learned by the discriminator of a conditional GAN trained for image inpainting performs well on image classification. As a side-effect, fairly convincing inpaintings are produced.

The proposed method combines two existing ideas: using the discriminator of a GAN as a feature learner [Radford et al. 2015] and performing unsupervised feature learning with image inpainting [Pathak et al. 2016]. Therefore conceptual novelty of the paper is limited. On the plus side, the authors implement their idea well and demonstrate state-of-the-art results on STL-10 and good results on Pascal VOC (although Pascal experiments are incomplete, see below). Overall, I am in the borderline mode, and I will gladly raise the score if the authors address my concerns regarding the experiments.

1) Experimental evaluation on Pascal VOC is not quite satisfactory. Comparison with prior work is unfair because the network architecture used by the authors (VGG) is different from the architecture used by all existing methods (AlexNet). It is great that the authors do not try to hide this fact in the paper, but I do not understand why the authors are not willing to simply run their method with AlexNet architecture, although two commenters asked them to do so. Such an experiment would strongly support authors’ claims. Current reasoning that “we thought it reasonable to use more current models while making the difference clear” is not convincing. It is great that better architectures lead to better results, but it is also very important to properly compare to prior work. On a related topic, Doersch et al. also tried using VGG architecture, would it be possible to compare to that? Yet another question: why are you not comparing to [Noroozi&Favaro, ECCV 2016] ? I would also like the authors to address the comment by Richard Zhang.

2) Qualitative inpainting results are incomplete: comparison with previous methods (for instance, [Pathak et al 2016]) is missing, and it is impossible to compare different versions of the proposed method because different images are used for different variants. I realize there may be too little space in the main paper to show all the results, but many more results should be shown in the supplementary material. Quantitative results are missing. Currently the inpainting results are just interesting pictures to look at, but they do not add as much to the paper as they could.

---

Nice work! I am curious about the SSL experiments: since",,,,,,5.666666666666667,,,4.0,,
670,"Authors: MULTIPLE TASKS, Antonin Raffin, Sebastian Höfer, Rico Jonschkowski
Source file: 670.pdf

ABSTRACT
We present an approach for learning state representations in multi-task reinforcement learning. Our method learns multiple low-dimensional state representations from raw observations in an unsupervised fashion, without any knowledge of which task is executed, nor of the number of tasks involved. The method is based on a gated neural network architecture, trained with an extension of the learning with robotic priors objective. In simulated experiments, we show that our method is able to learn better state representations for reinforcement learning, and we analyze why and when it manages to do so.

1 INTRODUCTION
(a) (b)
Figure 1: Slot car racing – the agent has learn how to drive any of the cars as far as possible (left), based on its raw observations (right).
In many reinforcement learning problems, the agent has to solve a variety of different tasks to fulfill its overall goal. A common approach to this problem is to learn a single policy for the whole problem, and leave the decomposition of the problem into subtasks to the learner. In many cases, this approach is successful (Mnih et al., 2015; Zahavy et al., 2016), but it comes at the expense of requiring large amounts of training data. Alternatively, multiple policies dedicated to different subtasks can be learned. This, however, requires prior knowledge about how the overal problem decomposes into subtasks. More-
over, it can run into the same issue of requiring large amounts of data, because the subtasks might overlap and thus afford shared computation to solve them.
A common approach to address overlapping problems is multi-task learning (Caruana, 1997): by learning a single policy with different subgoals, knowledge between the different tasks can be transferred. This not only allows to learn a compact representation more efficiently, but also improves the agent’s performance on all the individual subtasks (Rusu et al., 2016).
Multi-task learning, however, faces two problems: it requires the decomposition of the overall problem into subtasks to be given. Moreover, it is not applicable if the subtasks are unrelated, and are better solved without sharing computation. In this case, the single-policy approach results in an agent that does not perform well on any of the individual tasks (Stulp et al., 2014) or that unlearns
1The first two authors contributed equally to this work.
the successful strategy for one subtasks once it switches to another one, an issue known as catastrophic forgetting (McCloskey & Cohen, 1989).
In this work, we address the problem of identifying and isolating individual unrelated subtasks, and learning multiple separate policies in an unsupervised way. To that end, we present MT-LRP, an algorithm for learning state representations for multiple tasks by learning with robotic priors. MT-LRP is able to acquire different low-dimensional state representations for multiple tasks in an unsupervised fashion. Importantly, MT-LRP does not require knowledge about which task is executed at a given time or about the number of tasks involved. The representations learned with MT-LRP enable the use of standard reinforcement learning methods to compute effective policies from few data.
As explained before, our approach is orthogonal to the classical multi-task learning approach, and constitutes a problem of its own right due to the issues of underperformance and catastrophic forgetting. Therefore, we disregard the shared knowledge problem in this paper. However, any complete reinforcement learning system will need to combine both flavors of multi-task learning, for related and unrelated tasks, and future work will have to address the two problems together.
MT-LRP is implemented as two neural networks, coupled by a gating mechanism (Sigaud et al., 2015; Droniou et al., 2015) as illustrated in Figure 2. The first network, χ , detects which task is being executed and selects the corresponding state representation. The second network, ϕ , learns task-specific state representations. The networks are trained simultaneously using the robotic priors learning objective (Jonschkowski & Brock, 2015), exploiting physics-based prior knowledge about how states, actions, and rewards relate to each other. Both networks learn from raw sensor data, without supervision and solely based on the robot’s experiences.
In a simulated experimental scenario, we show that MT-LRP is able to learn multiple state representations and task detectors from raw observations and that these representations allow to learn better policies from fewer data when compared with other methods. Moreover, we analyze the contribution to this result of each the method’s individual components.

2 RELATED WORK
MT-LRP combines three ideas into a novel approach for task discovery and state representation learning: 1) extracting state representations for each task with robotic priors (Jonschkowski & Brock, 2015); 2) discovering discrete tasks and corresponding actions/policies in a RL context (Stulp et al., 2014; Höfer & Brock, 2016); 3) using gated networks to implement a “mixture of experts” (Jacobs et al., 1991; Droniou et al., 2015).
State Representation Learning: Learning from raw observations is considered a holy grail in reinforcement learning (RL). Deep RL has had major success in this, using model-free (Mnih et al., 2015) but also by combining model-free and model-based RL (Levine et al., 2015). These approaches apply end-to-end learning to get from raw input to value functions and policies. A different approach is to explicitly learn state representations using unsupervised learning, e.g. using auto-encoders (Lange et al., 2012). Recently, Watter et al. (2015) extended this idea to learn state representations jointly with dynamic models and apply optimal control to compute a policy. We use learning with robotic priors (Jonschkowski & Brock, 2015), a state representation learning method
that exploits information about temporal structure, actions, and rewards. We go beyond previous work by not only learning single state representations, but learning multiple state representations given raw data from multiple tasks.
Options and Parameterized Skills: A common approach to factorizing a RL problem into subtasks are macro-actions, often called options (Sutton et al., 1999; Hengst, 2002). The main difference with our approach is that options are used to hierarchically decompose one high-level task into subtasks (and learn sub-policies for these subtasks), whereas we learn task-specific state representations for different high-level tasks. However, options bear resemblance on a technical level, since they are often implemented by a high-level “selection” policy that parametrizes low-level policies (Daniel et al., 2012; Kupcsik et al., 2013; Stulp et al., 2014). Continuous versions of options, referred to as parametrized skills, have been proposed, too (Da Silva et al., 2012; Deisenroth et al., 2014; DoshiVelez & Konidaris, 2016). However, in all the work above, the state representation is given. To the best of our knowledge, state representation learning has not yet been considered in the context of RL with options or parameterized skills.
Gated Networks for Mixtures of Experts and Submanifold Learning: Gated networks are networks that contain gating connections, in which the outputs of at least two neurons are multiplied (Sigaud et al., 2015). This allows a gating neuron g to prohibit (or limit) the flow of information from one neuron x to another neuron y, similar to how transistors function. An early example of gated networks is the mixture of experts approach (Jacobs et al., 1991; Jacobs & Jordan, 1993; Haruno et al., 2001), where separate networks in a modular neural network specialize in predicting subsets of training examples from a database. Our contribution is to extend mixtures of experts by state representation learning (e.g. from raw images) and to the more difficult RL (rather than supervised learning) context. Our gated network architecture is similar to the one proposed by Droniou et al. (2015). Their network simultaneously learns discrete classes jointly with continuous class variations (called submanifolds) in an unsupervised way, e.g., discrete digit classes and shape variations within each class. We use a similar architecture, but in a different way: rather than learning discrete classes, we learn discrete tasks; class-specific submanifolds correspond to task-specific state representations; and finally, we consider a RL rather than an unsupervised learning context.
As mentioned in the introduction, our work is orthogonal to multi-task learning (Caruana, 1997) which has been extensively studied in recent reinforcement learning literature, too (Parisotto et al., 2016). Our approach can be trivially combined with multi-task learning by by prepending the gate and state extraction modules with a subnetwork that shares knowledge across tasks. Another interesting multi-task approach is policy distillation (Rusu et al., 2016). This method combines different policies for multiple tasks into a single network, which enables to share information between tasks and to learn a compact network that can even outperform the individual policies.

3 BACKGROUND: STATE REPRESENTATION LEARNING FOR REINFORCEMENT LEARNING
We formulate MT-LRP in a reinforcement learning (RL) setting using a Markov decision process (MDP) (S,A,T,R,γ): Based on the current state s ∈ S, the agent chooses and executes an action a ∈ A, obtains a new state s′ ∈ S (according to the transition function T ) and collects a reward r ∈ R. The agent’s goal is to learn a policy π : S→ A that maximizes the expected return IE(∑∞t=0 γ trt), with rt being the reward collected at time t and 0 < γ ≤ 1 the discount factor. We consider an episodic setting with episodes of finite length, a continuous state space S and a discrete action space A.
In this work, we assume that the agent cannot directly observe the state s but only has access to observations o ∈ O, which are usually high-dimensional and contain task-irrelevant distractors. This requires us to extract the state from the observations by learning an observation-state-mapping ϕ : O→ S, and use the resulting state representation S to solve the RL problem (assuming that a Markov state can be extracted from a single observation). To learn the state representation, we apply learning with robotic priors (Jonschkowski & Brock (2015), from now on referred to as LRP). This method learns ϕ from a set of temporally ordered experiences D = {(ot ,at ,rt)}dt=1 by optimizing the following loss:
LRP(D,ϕ) = ωtLtemp.(D,ϕ)+ωpLprop.(D,ϕ)+ωcLcaus.(D,ϕ)+ωrLrep.(D,ϕ). (1)
This loss consists of four terms, each expressing a different prior about suitable state representations for robot RL. We optimize it using gradient descent, assuming ϕ to be differentiable. We now explain the four robotic prior loss terms in Eq. (1).
Temporal Coherence enforces states to change gradually over time (Wiskott & Sejnowski, 2002): Ltemp.(D,ϕ) = IE [ ‖∆st‖2 ] ,
where ∆st = st+1− st denotes the state change. (To increase readability we replace ϕ(o) by s.) Proportionality expresses the prior that the same action should change the state by the same magnitude, irrespective of time and the location in the state space:
Lprop.(D,ϕ) = IE [ (‖∆st2‖−‖∆st1‖) 2 ∣∣∣ at1 = at2].
Causality enforces two states st1 ,st2 to be dissimilar if executing the same action in st1 generates a different reward than in st2 .
Lcaus.(D,ϕ) = IE [ e−‖st2−st1‖ 2 ∣∣∣ at1 = at2 ,rt1+1 6= rt2+1].
Repeatability requires actions to have repeatable effects by enforcing that the same action produces a similar state change in similar states:
Lrep.(D, ϕ̂) = IE [ e−‖st2−st1‖ 2‖∆st2 −∆st1‖ 2 ∣∣∣ at1 = at2].
Additionally, the method enforces simplicity by requiring s to be low-dimensional.
Note that learning with robotic priors only makes use of the actions a, rewards r, and temporal information t during optimization, but not at test time for computing ϕ(o) = s. Using a, r and t in this way is an instance of the learning with side information paradigm (Jonschkowski et al., 2015).

4 MULTI-TASK STATE REPRESENTATIONS: MT-LRP
Now consider a scenario in which an agent is learning multiple distinct tasks. For each task τ ∈ {1, . . . ,T}, the agent now requires a task-specific policy πτ : Sτ → A. We approach the problem by learning a task-specific state representation ϕτ : O→ Sτ for each policy, and a task detector χ which determines the task, given the current observation. We will consider a probabilistic task-detector χ : O→ [0,1]T that assigns a probability to each task being active. In order to solve the full multi-task RL problem, we must learn χ, {ϕτ}τ∈{1,...,T} and {πτ}τ∈{1,...,T}. We propose to address this problem by MT-LRP, a method that jointly learns χ and {ϕτ}τ∈{1,...,T} from raw observations, actions, and rewards. MT-LRP then uses the state representations {ϕτ} to learn task-specific policies {πτ}τ∈{1,...,T} (using standard RL methods), and switches between them using the task detector χ . To solve the joint learning problem, MT-LRP generalizes LRP (Jonschkowski & Brock, 2015) in the following regards: (i) we replace the linear observation-statemapping from the original method with a gated neural network, where the gates act as task detectors that switch between different task-specific observation-state-mappings; (ii) we extend the list of robotic priors by the prior of task coherence, which allows us to train multiple task-specific state representations without any specification (or labels) of tasks and states.

4.1 GATED NEURAL NETWORK ARCHITECTURE
We use a gated neural network architecture as shown schematically in Fig. 2. The key idea is that both the task detector χ as well as the state representation ϕ are computed from raw inputs. However, the output of the task detector gates the output of the state representation. Effectively, this means the output of χ(o) decides which task-specific state representation ϕτ is passed further to the policy, which is also gated by the output of χ(o).
Formally, χ(o) = σ(χpre(o)) is composed of a function χpre with T -dimensional output and a softmax σ(z) = e z j
∑k ezk . The softmax ensures that χ computes a proper probability distribution over tasks.
The probabilities are then used to gate ϕ . To do this, we decompose ϕ into a pre-gating function
ϕpre that extracts features shared across all tasks (i.e. ”multi-task” in the sense of Caruana (1997), unless set to the identity), and a T ×M×N gating tensor G that encodes the T (linear) observationstate mappings (M = dim(s) and N is the output dimension of ϕpre). The value of the state’s i-th dimension si computes as the expectation of the dot product of gating tensor and ϕpre(o) over the task probabilities χ(o):
si = ϕi(o) = T
∑ k=1 χk(o) 〈Gk,i,:,ϕpre(o)〉. (2)

4.2 LEARNING OBJECTIVE
To train the network, we extend the robotic prior loss LRP (Eq. 1), by a task-coherence prior Lτ :
L= LRP(D,ϕ)+ωτLτ(D,χ), (3)
where ωτ is a scalar weight balancing the influence of the additional loss term. Task coherence is the assumption that a task only changes between training episodes, not within the same episode. It does not presuppose any knowledge about the number of tasks or the task presented in an episode, but it exploits the fact that task switching weakly correlates with training episodes. Moreover, this assumption only needs to hold during training: since χ operates directly on the observation o, it can in principle switch the task at every point in time during execution. Task-coherence applies directly to the output of the task detector, χ(o), and consists of two terms:
Lcon+sepτ = Lconτ +L sep τ . (4)
The first term enforces task consistency during an episode: Lconτ = IE [ H(χ(ot1),χ(ot2)) ∣∣∣ episodet1 = episodet2], (5) where H denotes the cross-entropy H(p,q) = −∑x p(x) logq(x). It can be viewed as a measure of dissimilarity between probability distributions p and q. We use it to penalize χ if it assigns different task distributions to inputs ot1 , ot2 that belong to the same episode. Note that task-consistency can be viewed as a temporal coherence prior on the task level (Wiskott & Sejnowski, 2002).
The second term expresses task separation and encourages χ to assign tasks to different episodes: Lsepτ = IE [ e−H(χ(ot1 ),χ(ot2 )) ∣∣∣ episodet1 6= episodet2]. (6) This loss is complementary to task consistency, as it penalizes χ if it assigns similar task distributions to ot1 , ot2 from different episodes. Note that L sep τ will in general not become zero. The reason is that the number of episodes usually exceeds the number of tasks, and therefore two observations from different episodes sometimes do belong to the same task. We will evaluate the contribution of each of the two terms to learning success in Section 5.2.

5 EXPERIMENTS
We evaluate MT-LRP in two scenarios. In the multi-task slot-car racing scenario (inspired by Lange et al. (2012)), we apply MT-LRP to a linearly solvable problem, allowing us to easily inspect what and how MT-LRP learns. In slot-car racing, the agent controls one of multiple cars (Figure 1), with the goal of traversing the circuit as fast as possible without leaving the track due to speeding in curves. However, the agent does not know a priori which car it controls, and only receives the raw visual signal as input. Additionally, uncontrolled cars driving at random velocity, act as visual distractors. We turn this scenario into a multi-task problem in which the agent must learn to control each car, where controlling the different cars corresponds to separate tasks. We will now provide the technical details of our experimental set-up.

5.1 EXPERIMENTAL SET-UP: SLOT-CAR RACING
The agent controls the velocity of one car (see Fig. 1), receives a reward proportional to the car’s velocity, chosen from [0.01, 0.02, . . . , 0.1], and a negative reward of −10 if the car goes too fast
in curves. The velocity is subject to Gaussian noise (zero mean, standard deviation 10%) of the commanded velocity. All cars move on independent lanes and do not influence each other. The agent observes the scenario by getting a downscaled 16x16 RGB top-down view (dimension N = 16×16×3 = 768) of the car circuit (Fig. 1(b)). In our experiments, there are two or three cars on the track, and the agent controls a different one in every episode. To recognize the task, the agent must be able to extract a visual cue from the observation which correlates with the task. We study two types of visual cues: Static Visual Cue: The arrangement of cars stays the same in all episodes and a static visual cue (a picture of the controlled car) in the top-left image corner indicates which car is currently controlled. Dynamic Visual Cue: The agent always controls the same car (with a certain color), but in each task the car is located on a different lane (as in Fig. 1(b)).
Data Collection and Learning Procedure: The agent collects 40 episodes per task, each episode consisting of 100 steps. To select an action in each step, the agent performs ε-greedy exploration by picking a random action with probability ε = 0.3 and the best action according to its current policy otherwise. The agent computes a policy after every τ episodes, by first learning the observation-state mapping ϕ (state representation) and then computing policies π1, . . . ,πτ (based on the outcomes of the learned χ and ϕ). To monitor the agent’s learning progress, we measure the average reward the agent attains on T test episodes, i.e. one test episode of length 100 per task (using the greedy policy), amounting to 8000 experiences in total. To collect sufficient statistics, the whole experiment is repeated 10 times. Policy Learning: We consider the model-free setting with continuous states S, discrete actions A and solve it using nearest-neighbor Q-learning kNN-TD-RL (Martı́n H et al., 2009) with k = 10. More recent approaches to model-free RL would be equally applicable (Mnih et al., 2015). Learning Strategies and Baselines: We compare five strategies. We run a) MT-LRP with 5 gate units (two/three more than necessary), state dimensionality M = 2 and using Lcon+sepτ as taskcoherence prior. We compare MT-LRP to several state representation methods; for each method we evaluate different M and report only the best performing M: a) robotic priors without gated network, LRP (M = 4), b) principal components analysis (PCA) on the observations (M = 20) and c) raw observations (M = 768). Additionally, we evaluate d) a lower baseline in the form of a randomly moving agent and e) an upper baseline by applying RL on the known 2D-position of the slot car under control (M = 2). We use the same RL algorithm for all methods. To learn the state representations with robotic priors, we base our implementation on Theano and lasagne, using the Adam optimizer with learning rate 0.005, batch size 100, Glorot’s weight initialization and ωt = 1,ωp = 5,ωc = 1,ωr = 5,ωτ = 10. Moreover, we apply an L1 regularization of 0.001 on ϕ . Additionally, we analyze the contribution of task coherence priors by applying MT-LRP to the full set of 8000 experiences a) without task-coherence, b) with task consistency Lconτ only c) with task separation Lconτ only) and d) without task consistency and separation L con+sep τ .

5.2 RESULTS
We will now present the three main results of our experiments: (i) we show that MT-LRP enables the agent to extract better representations for RL; (ii) we provide insight in how the learner detects the task and encodes the state representations; and finally, (iii) we show the contribution of each of the task-coherence loss terms.
MT-LRP Extracts Better State Representations for RL Figure 3 shows the learning curves for RL based on state representations learned by the different methods in the two-slot-car scenario (static visual cue on the left, dynamic on the right). No method reaches the performance of the upper baseline, mainly due to aliasing errors resulting from the low image resolution.
The random baseline ranges around an average reward of −84.9 with standard error 0.72 and was omitted from the Figure. The state representation learning baselines without robotic priors perform poorly because they are unable to identify the taskirrelevant distractions. MT-LRP gets very close to the performance of the upper baseline, especially for very low amounts of training data (d < 2500), whereas LRP does not even attain this level of performance for the full training set d = 8000 in the static task. The gap between MT-LRP and LRP increases even more if we add another car (Figure 5) because LRP can only learn one state representation for all three tasks. Including the three slot cars in this representation results in distractions for the RL method. However, in the dynamic-visual-cue scenario LRP-4 performs on par with MT-LRP. Surprisingly, running LRP with only two dimensions suffices to achieve the performance of MT-LRP. We
will explain this phenomenon below. To conclude, MT-LRP allows to learn as good or better policies than the baselines in all slot-car scenarios.
MT-LRP Detects All Tasks and Learns Good State Representations To gain more insight into what is learned, we analyze the state representations extracted by MT-LRP and LRP. Figure 4 shows the state representation learned by MT-LRP for the static-visual-cue scenario. Each point in the figure corresponds to one observation, markers indicate the task and colors the most active gate unit. We see that the first gate unit (blue) is always active for task 1 (circle), and the second gate unit for task 2. This shows that the task is detected with high accuracy. The task detector χ is also highly cer-
tain which is reflected in the fact that its entropy evaluated on the data is close to zero. Moreover, the states reflect the circular structure of the slot car racing track. We thus conclude that MT-LRP has learned to identify the tasks and to represent the position of each car on the track.
The RL experiments raised the question why LRP manages to solve the dynamic, but not the static-visual-cue scenario as well as MT-LRP. We hypothesize that, for the dynamic cue, LRP is able to extract the position of the car on regardless of which lane it is in using a single linear mapping. Figure 6 confirms this hypothesis: LRP filters for the car’s color (blue) along the track and assigns increasing weights to these pixels which results in the extraction of its position. It also assigns constant weights along the track in the red channel using the lane change of the two cars as an offset. This results in a mapping to two circles similar to Fig. 4, where the state encodes both the position and the task. Such a mapping can be expressed by a linear
function precisely because the features that are relevant for one task do not reappear in another task (e.g. a blue slot car in track 1 does not appear in the task where the blue car is in track 2).
However, there exists no equivalent linear mapping for the static-visual-cue variant of the slotcar problem, because cars that are relevant for one task are also present in every other task.
We can generalize from this insight as follows. A single linear observation-state-mapping is sufficient for multiple tasks if the state representation for every task can be extracted by a linear function using only features that stay constant for all other tasks. If this is the case, than there is no need for decoupling the extraction of task and state.
Task-Consistency is Critical for Learning Performance To understand the influence of
the different task-coherence prior variants, we compared their performance in Figure 7. We see that relying solely on the robotic priors gives poor results, mainly because the gate units are not used properly: more than one gate unit is activated per task (χ has high entropy). Adding the taskseparation prior forces the network to use as many gates as possible (5 in our case), leading to bad state representations. Interestingly, using task consistency only gives roughly the same result as using task consistency and task separation.
Discussion The experiments showed that MT-LRP is able to solve the representation and reinforcement learning tasks better than the baselines. Important questions for future work concern: the necessity and influence of the task-separation loss, in particular for short episode lengths and if the number of expected tasks exceeds the number of actual tasks; and transferring knowledge by adding a shared neural network layers before gating.

6 CONCLUSION
We have presented MT-LRP, a method for multi-task state representation learning with robotic priors. The method learns in an unsupervised fashion, solely based on the robots own observations, actions, and rewards. Our experiments confirmed that MT-LRP is effective in simultaneously identifying tasks and learning task-specific state representations. This capability is beneficial for scaling reinforcement learning to realistic scenarios that require dedicated skills for different tasks.

ACKNOWLEDGMENTS
We gratefully acknowledge the funding provided by the German Research Foundation (DFG, Exploration Challenge, BR 2248/3-1), the Alexander von Humboldt foundation through an Alexander von Humboldt professorship (funded by the German Federal Ministry of Education and Research). Additionally, Antonin Raffin was supported by an Erasmus+ grant.
","We present an approach for learning state representations in multi-task reinforcement learning. Our method learns multiple low-dimensional state representations from raw observations in an unsupervised fashion, without any knowledge of which task is executed, nor of the number of tasks involved. The method is based on a gated neural network architecture, trained with an extension of the learning with robotic priors objective. In simulated experiments, we show that our method is able to learn better state representations for reinforcement learning, and we analyze why and when it manages to do so.",ICLR 2017 conference submission,False,,"The paper presents a method to learn a low-dimensional state representations from raw obervation for multi-task setting. In contrast to classic multi-task learning setting where a joint representation is usually learned by exploring the transferable information among different tasks, the method aims to identify individual task and solve them separately. To this end, the authors extend the learning with robotic priors approach by extending the loss function with additional term for task coherence, i.e., a task only changes representation between training episodes. The method has been evaluated on two tasks, multi-task slot-car racing and mobile navigation to prove its efficacy.

there were several unclear issues:

1. The first question is that if the method is only appealing on the scenario like the slot-car racing, otherwise it should be benchmarked with mutli-task learning. While the author made the argument in the related work, the proposed method is orthogonal to multi-task learning they did admit both explore shared knowledge between tasks. What's the advantage and disadvantage for the proposed method for general mutiple task setting, in particular over the multi-task learning?
The reply of the authors was not fully satisfactory. The argument did not support the lack of comparison to multi-task joint-learning. It seems they don't plan to include any comparison neither. I think it's important for the fundamental motivation for the work, without such comparison, the method seems to be purely an alternative to multi-task joint-learning without any(or much) practical advantage.

2.Following up to the previous question, please clarify the results on the mobile navigation scenario. It's not clear how the plot on the right indicates MT-LRP identifies all tasks as the author claimed and and seems very weak to support the method, in particular compared to the multi-slot car-racing driving experiment, there is too little results to make sound argument (almost no comparison to alternative methods, i.e. no baseline method, is that true for the problem).
The explanation of the authors did provide more details and more explicit information. 

3. The proposed gated neural network architecture seems to be a soft gated structure(correct me if I am wrong), a possible baseline would be a hard gated unit, how would this affect the conclusion. This is particularly interesting as the authors reflect on the constraint that the representation should stay consistent during the training.
The author simply stated again what they did for the modeling without counter the comparison to hard-gating, but it's probably less an issue compared to Question 1.

In summary, while there are remaining concerns about lacking comparisons, the is a weak tendency towards accepting the submission.

---

The authors propose to explore an important problem -- learning state representations for reinforcement learning. However, the experimental evaluation raised a number of concerns among the reviewers. The method is tested on only a single relatively easy domain, with some arbitrarily choices justified in unconvincing ways (e.g. computational limitations as a reason to not fix aliasing). The evaluation also compares to extremely weak baselines. In the end, there is insufficient evidence to convincingly show that the method actually works, and since the contribution is entirely empirical (as noted by the reviewers in regard to some arbitrary parameter choices), the unconvincing evaluation makes this method unsuitable for publication at this time. The authors would be encouraged to evaluate the method rigorously on a wide range of realistic tasks.

---

We would like to thank all reviewers for their thorough and helpful comments!

1) Before we turn to the individual questions raised the reviewers, we would like to address the main issue that all reviewers raised, namely the relationship of our method to multi-task learning:

“The authors state that the proposed method is orthogonal to multi-task learning though the end goal of learning to solve multiple tasks is the same.” (AnonReviewer1)
“The argument did not support the lack of comparison to multi-task joint-learning.” (AnonReviewer2)
“Limiting the multi-task learning to be different to individual tasks rather than sharing and transferring knowledge between tasks” (AnonReviewer 3)

The intro has been rewritten to clarify our motivation and how our work compares to multi-task learning. We completely agree that successful RL will require multi-task learning to share knowledge that generalizes over multiple tasks. 

But there are sets of tasks that require multiple dedicated skills without sharing knowledge. For instance, in a video game, an agent have to achieve several subgoals (fight an enemy, avoid obstacles,...), each of these can be seen as individual task. Learning multiple, (sub-)policies dedicated to *different* tasks is a problem of its own right, as it faces significant theoretical issues, such as “catastrophic forgetting”. We have elaborated on this argument in the introduction.

Since there is few work approaching this problem in RL, our paper studies the question of how to learn fully independent policies for different tasks. We fully agree that future work will need to combine learning shared and separate representations but we regard our work on the  independent-policy multi-task RL problem as a contribution in itself. 

We now reply to the individual comments raised by the reviewers.

----

AnonReviewer1

1) “References to other multi-task learning works, e.g. policy distillation and actor-mimic (both ICLR ’16), may be appropriate as well.”

Thank you for the pointers, we have integrated the two suggested papers in the related work of the paper.

2) “The approach does not necessarily share information across tasks for better learning, and requires learning a different policy for each task”

This is the very idea of the method proposed, we updated the introduction to clarify the reasons we focused on this approach.

3) “In the second navigation scenario, only the state representation is qualitatively shown, not the resulting control policy nor any other learned state representations for comparison. Since the multi-task state representation learning approach is only useful if you can also learn control better, the paper should also evaluate on control, with the same comparisons as in the first experiment. Without this evaluation, the experiment is incomplete.”

We agree and as mentioned before this was a preliminary and incomplete experiment, and we decided to remove it from the paper. 

4) “Lastly, to be on par with publications at a venue like ICLR, the method should be evaluated more thoroughly, on a wider range of set-ups [...]”

We agree that it is beneficial to apply a method to a wider range of tasks. Yet, we chose to invest into rigorously evaluating the performance of the method on the chosen task, and provide a thorough argument why and how the method works. We believe that it will scale to a wider range of tasks, but we will have to address this in future work.

5) “Could this approach be combined with other state representation learning approaches? e.g. approaches that use an autoencoder.”

Yes, in principle it would be possible to use other state representation learning objectives.  Note, however, that in the slot car racing scenario a PCA/auto-encoder loss will not perform as well as LRP, as it has will try to explain all variations in the observation, in particular the second slot car. This has been shown in our previous work (Jonschkowski & Brock 2015) and is also reflected in the performance of PCA in the slot-car experiment.

6) ”One additional useful comparison would be to evaluate performance in the single-task setting (e.g. only controlling the red car), as an upper bound on how well the policy should be able to perform. Does the learned multi-task policy reach the same level of performance? This upper bound will be tighter than the “known car position” baseline (which is also useful in its own right).”

Thank you for this suggestion; in our experiment, however, the performance of the car in the single-task setting is identical to the performance we see in the multi-task setting. The reason is that the task detector module has a very high accuracy (greater than 99%) for the slot car tasks, and in consequence, a separate policy for each slot car is learned.

7) “Does the “observations” baseline eventually reach the performance of the LRP approach? It would be useful to know if this approach simply speeds up learning (significantly) or if it enables better performance.”

Our experiments and previous work (Jonschkowski & Brock 2015) suggest that it will eventually reach the same performance with enough data, but for now, even in our largest experiments, we did not see it happening.

8) “If there are aliasing issues with the images, why not just use higher resolution images?”

Mainly computational reasons: we wanted to evaluate a wide variety of parameter settings and study their influence on our algorithm, yet we did not have the computational power to do this exhaustively on higher resolutions.

---

AnonReviewer2

1) “The author simply stated again what they did for the modeling without counter the comparison to hard-gating, but it's probably less an issue compared to Question 1.”

We are sorry that our answer in the pre-review phase did not address your question. We were trying to explain that our method is technically a soft gating, but effectively learns to perform hard gating. We are not sure how whether and how using a hard would influence the conclusion of the paper, and we are not aware of a way to implement a differentiable hard gate (if it is not differentiable, we cannot train it using backpropagation).

---

AnonReviewer3

2) “Parameters choice is arbitrary (w parameters)”
The weights w for the different methods are chosen as described in Jonschkowski & Brock 2015, by monitoring the gradient on a small part of the training set. The goal is to have gradients of the same magnitude for the different terms in the loss, so only relative weighting matters. Small changes to the parameters do not affect the method and there is no need for careful tuning.

3) “The experiments could have been conducted using a standardized simulation tool such as OpenAI Gym to make it easy to compare.”
We agree that evaluating experiments on a standardized tool such as OpenAI gym is a great idea. We want to point out, though, that the slot car racing task considered in the paper is a well-known task that has been evaluated in previous work, too, e.g. (Lange et al., 2012). Moreover, it is the simplest task that has the properties we are interested in this paper (non-overlapping tasks).
But we agree that that open simulation tools such as OpenAI gym are great and we will apply our method to these tasks in future work.

---

This paper is about learning unsupervised state representations using multi-task reinforcement learning.  The authors propose a novel approach combining gated neural networks with multitask learning with robotics priors. They evaluated their approach on two simulated datasets and showed promising results. The paper is clearly written and is theoretically sound.

Positives:
+ Gating to enable learning a joint representation
+ Multi-task learning extended from a single task in prior work
+ Combining multiple types of losses to learn a strong representation (Coherence, Proportionality, Causality, Repeatability, Consistency and Separation)

Negatives:
- Parameters choice is arbitrary (w parameters)
- Limiting the multi-task learning to be different to individual tasks rather than sharing and transferring knowledge between tasks
- The experiments could have been conducted using a standardized simulation tool such as OpenAI Gym to make it easy to compare.

I would recommend that the authors consider a more standardized way of picking the model parameters and evaluate on a more standard and high-dimensional datasets.

---

This paper builds upon the method of Jonschkowski & Brock to learn state representations for multiple tasks, rather than a single task. The research direction of learning representations for multiple tasks is an interesting one, and largely unexplored. The approach in the paper is to learn a different representation for each task, and a different policy for each task, where the task is detected automatically and built into the neural network.

The authors state that the proposed method is orthogonal to multi-task learning, though the end goal of learning to solve multiple tasks is the same. It would be interesting and helpful to see more discussion on this point in the paper, as discussed in the pre-review question phase. References to other multi-task learning works, e.g. policy distillation and actor-mimic (both ICLR ’16), may be appropriate as well.

The method proposes to jointly learn a task classifier with a state representation learner, by using a differentiable gating mechanism to control the flow of information. The paper proposes a task coherence prior for this gating mechanism to ensure that the learned task classifier is temporally coherent. Introducing this structure is what enables the method to improve performance over the standard, non-multitask approach.

The evaluation involves two toy experimental scenarios. The first involves controlling one of two cars to drive around a track. In this task, detecting the “task” is very easy, and the learned state representation is linear in the observation. The paper evaluates the performance of the policies learned with the proposed approach, and shows sufficient comparisons to demonstrate the usefulness of the approach over a standard non-multitask set-up.

In the second navigation scenario, only the state representation is qualitatively shown, not the resulting control policy nor any other learned state representations for comparison. Since the multi-task state representation learning approach is only useful if you can also learn control better, the paper should also evaluate on control, with the same comparisons as in the first experiment. Without this evaluation, the experiment is incomplete.

Lastly, to be on par with publications at a venue like ICLR, the method should be evaluated more thoroughly, on a wider range of set-ups, to demonstrate the generality of the approach and show that the method applies to more complex tasks. While in theory, the method should scale, the experiments do not demonstrate that it can handle more realistic scenarios, such as scaling beyond MNIST-level images, to 3D or real images, or higher-dimensional control tasks. Evaluating the method in this more complex scenario is important, because unexpected issues can come up when trying to scale. If scaling-up is straight-forward, then running this experiment (and including it in the paper) should be straight-forward.

In summary, here are the pros and cons of this paper:
Cons
- The approach does not necessarily share information across tasks for better learning, and requires learning a different policy for each task
- Only one experimental set-up that evaluates learned policy with multi-task state representation
- No experiments on more realistic scenarios, such 3D environments or high-dimensional control problems
Pros: 
- This approach enables using the same network for multiple tasks, which is often not true for transfer and multi-task learning approaches
- Novel way to learn a single policy for multiple tasks, including a task coherence prior which ensures that the task classification is meaningful
- Experimentally validated on two toy tasks. One task shows improvement over baseline approaches

Thus, my rating would be higher if the paper included an evaluation of the control policy for navigation and included another more challenging and compelling scenario.


Lastly, here are some minor comments/questions on how I think the paper could be improved, but are not as important as the above:

Approach:
Could this approach be combined with other state representation learning approaches? e.g. approaches that use an autoencoder.

Experiments:
One additional useful comparison would be to evaluate performance in the single-task setting (e.g. only controlling the red car), as an upper bound on how well the policy should be able to perform. Does the learned multi-task policy reach the same level of performance? This upper bound will be tighter than the “known car position” baseline (which is also useful in its own right).

Does the “observations” baseline eventually reach the performance of the LRP approach? It would be useful to know if this approach simply speeds up learning (significantly) or if it enables better performance.

If there are aliasing issues with the images, why not just use higher resolution images?

---

The paper presents a method to learn a low-dimensional state representations from raw obervation for multi-task setting. In contrast to classic multi-task learning setting where a joint representation is usually learned by exploring the transferable information among different tasks, the method aims to identify individual task and solve them separately. To this end, the authors extend the learning with robotic priors approach by extending the loss function with additional term for task coherence, i.e., a task only changes representation between training episodes. The method has been evaluated on two tasks, multi-task slot-car racing and mobile navigation to prove its efficacy.

there were several unclear issues:

1. The first question is that if the method is only appealing on the scenario like the slot-car racing, otherwise it should be benchmarked with mutli-task learning. While the author made the argument in the related work, the proposed method is orthogonal to multi-task learning they did admit both explore shared knowledge between tasks. What's the advantage and disadvantage for the proposed method for general mutiple task setting, in particular over the multi-task learning?
The reply of the authors was not fully satisfactory. The argument did not support the lack of comparison to multi-task joint-learning. It seems they don't plan to include any comparison neither. I think it's important for the fundamental motivation for the work, without such comparison, the method seems to be purely an alternative to multi-task joint-learning without any(or much) practical advantage.

2.Following up to the previous question, please clarify the results on the mobile navigation scenario. It's not clear how the plot on the right indicates MT-LRP identifies all tasks as the author claimed and and seems very weak to support the method, in particular compared to the multi-slot car-racing driving experiment, there is too little results to make sound argument (almost no comparison to alternative methods, i.e. no baseline method, is that true for the problem).
The explanation of the authors did provide more details and more explicit information. 

3. The proposed gated neural network architecture seems to be a soft gated structure(correct me if I am wrong), a possible baseline would be a hard gated unit, how would this affect the conclusion. This is particularly interesting as the authors reflect on the constraint that the representation should stay consistent during the training.
The author simply stated again what they did for the modeling without counter the comparison to hard-gating, but it's probably less an issue compared to Question 1.

In summary, while there are remaining concerns about lacking comparisons, the is a weak tendency towards accepting the submission.

---

The paper presents a method to learn a low-dimensional state representations from raw obervation for multi-task setting. In contrast to classic multi-task learning setting where a joint representation is usually learned by exploring the transferable information among different tasks, the method aims to identify individual task and solve them separately. To this end, the authors extend the learning with robotic priors approach by extending the loss function with additional term for task coherence, i.e., a task only changes representation between training episodes. The method has been evaluated on two tasks, multi-task slot-car racing and mobile navigation to prove its efficacy.

there were several unclear issues:

1. The first question is that if the method is only appealing on the scenario like the slot-car racing, otherwise it should be benchmarked with mutli-task learning. While the author made the argument in the related work, the proposed method is orthogonal to multi-task learning they did admit both explore shared knowledge between tasks. What's the advantage and disadvantage for the proposed method for general mutiple task setting, in particular over the multi-task learning?
The reply of the authors was not fully satisfactory. The argument did not support the lack of comparison to multi-task joint-learning. It seems they don't plan to include any comparison neither. I think it's important for the fundamental motivation for the work, without such comparison, the method seems to be purely an alternative to multi-task joint-learning without any(or much) practical advantage.

2.Following up to the previous question, please clarify the results on the mobile navigation scenario. It's not clear how the plot on the right indicates MT-LRP identifies all tasks as the author claimed and and seems very weak to support the method, in particular compared to the multi-slot car-racing driving experiment, there is too little results to make sound argument (almost no comparison to alternative methods, i.e. no baseline method, is that true for the problem).
The explanation of the authors did provide more details and more explicit information. 

3. The proposed gated neural network architecture seems to be a soft gated structure(correct me if I am wrong), a possible baseline would be a hard gated unit, how would this affect the conclusion. This is particularly interesting as the authors reflect on the constraint that the representation should stay consistent during the training.
The author simply stated again what they did for the modeling without counter the comparison to hard-gating, but it's probably less an issue compared to Question 1.

In summary, while there are remaining concerns about lacking comparisons, the is a weak tendency towards accepting the submission.

---

The authors propose to explore an important problem -- learning state representations for reinforcement learning. However, the experimental evaluation raised a number of concerns among the reviewers. The method is tested on only a single relatively easy domain, with some arbitrarily choices justified in unconvincing ways (e.g. computational limitations as a reason to not fix aliasing). The evaluation also compares to extremely weak baselines. In the end, there is insufficient evidence to convincingly show that the method actually works, and since the contribution is entirely empirical (as noted by the reviewers in regard to some arbitrary parameter choices), the unconvincing evaluation makes this method unsuitable for publication at this time. The authors would be encouraged to evaluate the method rigorously on a wide range of realistic tasks.

---

We would like to thank all reviewers for their thorough and helpful comments!

1) Before we turn to the individual questions raised the reviewers, we would like to address the main issue that all reviewers raised, namely the relationship of our method to multi-task learning:

“The authors state that the proposed method is orthogonal to multi-task learning though the end goal of learning to solve multiple tasks is the same.” (AnonReviewer1)
“The argument did not support the lack of comparison to multi-task joint-learning.” (AnonReviewer2)
“Limiting the multi-task learning to be different to individual tasks rather than sharing and transferring knowledge between tasks” (AnonReviewer 3)

The intro has been rewritten to clarify our motivation and how our work compares to multi-task learning. We completely agree that successful RL will require multi-task learning to share knowledge that generalizes over multiple tasks. 

But there are sets of tasks that require multiple dedicated skills without sharing knowledge. For instance, in a video game, an agent have to achieve several subgoals (fight an enemy, avoid obstacles,...), each of these can be seen as individual task. Learning multiple, (sub-)policies dedicated to *different* tasks is a problem of its own right, as it faces significant theoretical issues, such as “catastrophic forgetting”. We have elaborated on this argument in the introduction.

Since there is few work approaching this problem in RL, our paper studies the question of how to learn fully independent policies for different tasks. We fully agree that future work will need to combine learning shared and separate representations but we regard our work on the  independent-policy multi-task RL problem as a contribution in itself. 

We now reply to the individual comments raised by the reviewers.

----

AnonReviewer1

1) “References to other multi-task learning works, e.g. policy distillation and actor-mimic (both ICLR ’16), may be appropriate as well.”

Thank you for the pointers, we have integrated the two suggested papers in the related work of the paper.

2) “The approach does not necessarily share information across tasks for better learning, and requires learning a different policy for each task”

This is the very idea of the method proposed, we updated the introduction to clarify the reasons we focused on this approach.

3) “In the second navigation scenario, only the state representation is qualitatively shown, not the resulting control policy nor any other learned state representations for comparison. Since the multi-task state representation learning approach is only useful if you can also learn control better, the paper should also evaluate on control, with the same comparisons as in the first experiment. Without this evaluation, the experiment is incomplete.”

We agree and as mentioned before this was a preliminary and incomplete experiment, and we decided to remove it from the paper. 

4) “Lastly, to be on par with publications at a venue like ICLR, the method should be evaluated more thoroughly, on a wider range of set-ups [...]”

We agree that it is beneficial to apply a method to a wider range of tasks. Yet, we chose to invest into rigorously evaluating the performance of the method on the chosen task, and provide a thorough argument why and how the method works. We believe that it will scale to a wider range of tasks, but we will have to address this in future work.

5) “Could this approach be combined with other state representation learning approaches? e.g. approaches that use an autoencoder.”

Yes, in principle it would be possible to use other state representation learning objectives.  Note, however, that in the slot car racing scenario a PCA/auto-encoder loss will not perform as well as LRP, as it has will try to explain all variations in the observation, in particular the second slot car. This has been shown in our previous work (Jonschkowski & Brock 2015) and is also reflected in the performance of PCA in the slot-car experiment.

6) ”One additional useful comparison would be to evaluate performance in the single-task setting (e.g. only controlling the red car), as an upper bound on how well the policy should be able to perform. Does the learned multi-task policy reach the same level of performance? This upper bound will be tighter than the “known car position” baseline (which is also useful in its own right).”

Thank you for this suggestion; in our experiment, however, the performance of the car in the single-task setting is identical to the performance we see in the multi-task setting. The reason is that the task detector module has a very high accuracy (greater than 99%) for the slot car tasks, and in consequence, a separate policy for each slot car is learned.

7) “Does the “observations” baseline eventually reach the performance of the LRP approach? It would be useful to know if this approach simply speeds up learning (significantly) or if it enables better performance.”

Our experiments and previous work (Jonschkowski & Brock 2015) suggest that it will eventually reach the same performance with enough data, but for now, even in our largest experiments, we did not see it happening.

8) “If there are aliasing issues with the images, why not just use higher resolution images?”

Mainly computational reasons: we wanted to evaluate a wide variety of parameter settings and study their influence on our algorithm, yet we did not have the computational power to do this exhaustively on higher resolutions.

---

AnonReviewer2

1) “The author simply stated again what they did for the modeling without counter the comparison to hard-gating, but it's probably less an issue compared to Question 1.”

We are sorry that our answer in the pre-review phase did not address your question. We were trying to explain that our method is technically a soft gating, but effectively learns to perform hard gating. We are not sure how whether and how using a hard would influence the conclusion of the paper, and we are not aware of a way to implement a differentiable hard gate (if it is not differentiable, we cannot train it using backpropagation).

---

AnonReviewer3

2) “Parameters choice is arbitrary (w parameters)”
The weights w for the different methods are chosen as described in Jonschkowski & Brock 2015, by monitoring the gradient on a small part of the training set. The goal is to have gradients of the same magnitude for the different terms in the loss, so only relative weighting matters. Small changes to the parameters do not affect the method and there is no need for careful tuning.

3) “The experiments could have been conducted using a standardized simulation tool such as OpenAI Gym to make it easy to compare.”
We agree that evaluating experiments on a standardized tool such as OpenAI gym is a great idea. We want to point out, though, that the slot car racing task considered in the paper is a well-known task that has been evaluated in previous work, too, e.g. (Lange et al., 2012). Moreover, it is the simplest task that has the properties we are interested in this paper (non-overlapping tasks).
But we agree that that open simulation tools such as OpenAI gym are great and we will apply our method to these tasks in future work.

---

This paper is about learning unsupervised state representations using multi-task reinforcement learning.  The authors propose a novel approach combining gated neural networks with multitask learning with robotics priors. They evaluated their approach on two simulated datasets and showed promising results. The paper is clearly written and is theoretically sound.

Positives:
+ Gating to enable learning a joint representation
+ Multi-task learning extended from a single task in prior work
+ Combining multiple types of losses to learn a strong representation (Coherence, Proportionality, Causality, Repeatability, Consistency and Separation)

Negatives:
- Parameters choice is arbitrary (w parameters)
- Limiting the multi-task learning to be different to individual tasks rather than sharing and transferring knowledge between tasks
- The experiments could have been conducted using a standardized simulation tool such as OpenAI Gym to make it easy to compare.

I would recommend that the authors consider a more standardized way of picking the model parameters and evaluate on a more standard and high-dimensional datasets.

---

This paper builds upon the method of Jonschkowski & Brock to learn state representations for multiple tasks, rather than a single task. The research direction of learning representations for multiple tasks is an interesting one, and largely unexplored. The approach in the paper is to learn a different representation for each task, and a different policy for each task, where the task is detected automatically and built into the neural network.

The authors state that the proposed method is orthogonal to multi-task learning, though the end goal of learning to solve multiple tasks is the same. It would be interesting and helpful to see more discussion on this point in the paper, as discussed in the pre-review question phase. References to other multi-task learning works, e.g. policy distillation and actor-mimic (both ICLR ’16), may be appropriate as well.

The method proposes to jointly learn a task classifier with a state representation learner, by using a differentiable gating mechanism to control the flow of information. The paper proposes a task coherence prior for this gating mechanism to ensure that the learned task classifier is temporally coherent. Introducing this structure is what enables the method to improve performance over the standard, non-multitask approach.

The evaluation involves two toy experimental scenarios. The first involves controlling one of two cars to drive around a track. In this task, detecting the “task” is very easy, and the learned state representation is linear in the observation. The paper evaluates the performance of the policies learned with the proposed approach, and shows sufficient comparisons to demonstrate the usefulness of the approach over a standard non-multitask set-up.

In the second navigation scenario, only the state representation is qualitatively shown, not the resulting control policy nor any other learned state representations for comparison. Since the multi-task state representation learning approach is only useful if you can also learn control better, the paper should also evaluate on control, with the same comparisons as in the first experiment. Without this evaluation, the experiment is incomplete.

Lastly, to be on par with publications at a venue like ICLR, the method should be evaluated more thoroughly, on a wider range of set-ups, to demonstrate the generality of the approach and show that the method applies to more complex tasks. While in theory, the method should scale, the experiments do not demonstrate that it can handle more realistic scenarios, such as scaling beyond MNIST-level images, to 3D or real images, or higher-dimensional control tasks. Evaluating the method in this more complex scenario is important, because unexpected issues can come up when trying to scale. If scaling-up is straight-forward, then running this experiment (and including it in the paper) should be straight-forward.

In summary, here are the pros and cons of this paper:
Cons
- The approach does not necessarily share information across tasks for better learning, and requires learning a different policy for each task
- Only one experimental set-up that evaluates learned policy with multi-task state representation
- No experiments on more realistic scenarios, such 3D environments or high-dimensional control problems
Pros: 
- This approach enables using the same network for multiple tasks, which is often not true for transfer and multi-task learning approaches
- Novel way to learn a single policy for multiple tasks, including a task coherence prior which ensures that the task classification is meaningful
- Experimentally validated on two toy tasks. One task shows improvement over baseline approaches

Thus, my rating would be higher if the paper included an evaluation of the control policy for navigation and included another more challenging and compelling scenario.


Lastly, here are some minor comments/questions on how I think the paper could be improved, but are not as important as the above:

Approach:
Could this approach be combined with other state representation learning approaches? e.g. approaches that use an autoencoder.

Experiments:
One additional useful comparison would be to evaluate performance in the single-task setting (e.g. only controlling the red car), as an upper bound on how well the policy should be able to perform. Does the learned multi-task policy reach the same level of performance? This upper bound will be tighter than the “known car position” baseline (which is also useful in its own right).

Does the “observations” baseline eventually reach the performance of the LRP approach? It would be useful to know if this approach simply speeds up learning (significantly) or if it enables better performance.

If there are aliasing issues with the images, why not just use higher resolution images?

---

The paper presents a method to learn a low-dimensional state representations from raw obervation for multi-task setting. In contrast to classic multi-task learning setting where a joint representation is usually learned by exploring the transferable information among different tasks, the method aims to identify individual task and solve them separately. To this end, the authors extend the learning with robotic priors approach by extending the loss function with additional term for task coherence, i.e., a task only changes representation between training episodes. The method has been evaluated on two tasks, multi-task slot-car racing and mobile navigation to prove its efficacy.

there were several unclear issues:

1. The first question is that if the method is only appealing on the scenario like the slot-car racing, otherwise it should be benchmarked with mutli-task learning. While the author made the argument in the related work, the proposed method is orthogonal to multi-task learning they did admit both explore shared knowledge between tasks. What's the advantage and disadvantage for the proposed method for general mutiple task setting, in particular over the multi-task learning?
The reply of the authors was not fully satisfactory. The argument did not support the lack of comparison to multi-task joint-learning. It seems they don't plan to include any comparison neither. I think it's important for the fundamental motivation for the work, without such comparison, the method seems to be purely an alternative to multi-task joint-learning without any(or much) practical advantage.

2.Following up to the previous question, please clarify the results on the mobile navigation scenario. It's not clear how the plot on the right indicates MT-LRP identifies all tasks as the author claimed and and seems very weak to support the method, in particular compared to the multi-slot car-racing driving experiment, there is too little results to make sound argument (almost no comparison to alternative methods, i.e. no baseline method, is that true for the problem).
The explanation of the authors did provide more details and more explicit information. 

3. The proposed gated neural network architecture seems to be a soft gated structure(correct me if I am wrong), a possible baseline would be a hard gated unit, how would this affect the conclusion. This is particularly interesting as the authors reflect on the constraint that the representation should stay consistent during the training.
The author simply stated again what they did for the modeling without counter the comparison to hard-gating, but it's probably less an issue compared to Question 1.

In summary, while there are remaining concerns about lacking comparisons, the is a weak tendency towards accepting the submission.",,,,,,5.666666666666667,,,3.6666666666666665,,
678,"Authors: READING COMPREHENSION, Rudolf Kadlec, Ondrej Bajgar, Peter Hrincar, Jan Kleindienst
Source file: 678.pdf

ABSTRACT
Deep learning has proven useful on many NLP tasks including reading comprehension. However, it requires large amounts of training data which are not available in some domains of application. Hence we examine the possibility of using data-rich domains to pre-train models and then apply them in domains where training data are harder to get. Specifically, we train a neural-network-based model on two context-question-answer datasets, the BookTest and CNN/Daily Mail, and we monitor transfer to subsets of bAbI, a set of artificial tasks designed to test specific reasoning abilities, and of SQuAD, a question-answering dataset which is much closer to real-world applications. Our experiments show very limited transfer if the model is not shown any training examples from the target domain however the results are encouraging if the model is shown at least a few target-domain examples. Furthermore we show that the effect of pre-training is not limited to word embeddings.

1 INTRODUCTION
Machine intelligence has had some notable successes, however often in narrow domains which are sometimes of little practical use to humans – for instance games like chess (Campbell et al., 2002) or Go (Silver et al., 2016). If we aimed to build a general AI that would be able to efficiently assist humans in a wide range of settings, we would want it to have a much larger set of skills – among them would be an ability to understand human language, to perform common-sense reasoning and to be able to generalize its abilities to new situations like humans do.
If we want to achieve this goal through Machine Learning, we need data to learn from. A lot of data if the task at hand is complex – which is the case for many useful tasks. One way to achieve wide applicability would be to provide training data for each specific task we would like the machine to perform. However it is unrealistic to obtain a sufficient amount of training data for some domains – it may for instance require expensive human annotation or all domains of application may be difficult to predict in advance – while the amount of training data in other domains is practically unlimited, (e.g. in language modelling or Cloze-style question answering).
The way to bridge this gap – and to achieve the aforementioned adaptability – is transfer learning (Pan & Yang, 2010) and closely related semi-supervised learning (Zhu & Goldberg, 2009) which allow the system to acquire a set of skills on domains where data are abundant and then use these skills to succeed on previously unseen domains. Despite how important generalization is for general AI, a lot of research keeps focusing on solving narrow tasks.
In this paper we would like to examine transfer of learnt skills and knowledge within the domain of text comprehension, a field that has lately attracted a lot of attention within the NLP community (Hermann et al., 2015; Hill et al., 2015; Kobayashi et al., 2016; Kadlec et al., 2016b; Chen et al., 2016; Sordoni et al., 2016; Dhingra et al., 2016; Trischler et al., 2016; Weissenborn, 2016; Cui et al., 2016b;a;
∗These authors contributed equally to this work.
Li et al., 2016; Shen et al., 2016). Specifically, we would like to address the following research questions:
1. Whether we could train models on natural-language tasks where data are abundant and transfer the learnt skills to tasks where in-domain training data may be difficult to obtain. We will first look into what reasoning abilities a model learns from two large-scale readingcomprehension datasets using artificial tasks, and then check whether it can transfer its skills to real world tasks. Spoiler: both these transfers are very poor if we allow no training at all on the target task.
2. Whether pre-training on large-scale datasets does help if we allow the model to train on a small sample of examples from the target tasks. Here the results are much more positive.
3. Finally we examine whether the benefits of pre-training are concentrated in any particular part of the model - namely the word-embedding part or the context encoder (the reasoning part). It turns out that pre-training is useful for both components.
Although our results do not improve current state of the art in any of the studied tasks, they show a clear positive effect of large-dataset pre-training on the performance of our baseline machine-learning model. Previous studies of transfer learning and semi-supervised learning in NLP focused on text classification (Dai & Le, 2015; Mou et al., 2016) and various parsing tasks (Collobert et al., 2011; Hashimoto et al., 2016). To our knowledge this work is the first study of transfer learning in reading comprehension, and we hope it will stimulate further work in this important area.
We will first briefly introduce the datasets we will be using on the pre-training and target sides, then our baseline model and afterwards in turn describe the method and results of each of the three experiments.

2 DATASETS

2.1 PRE-TRAINING DATASETS
We have mentioned that for the model pre-training we would want to use a task where training data are abundant. An example of such task is context-dependent cloze-style-question answering since the training data for this task can be generated automatically from a suitable corpus. We will use two such pre-training datasets in our experiments: the BookTest (Bajgar et al., 2016) and the CNN/Daily Mail (CNN/DM) news dataset (Hermann et al., 2015).
The task associated with both datasets is to answer a cloze-style question (i.e. fill in a blank in a sentence) the answer to which needs to be inferred from a context document provided with the question.

2.1.1 BOOKTEST
In the BookTest dataset, the context document is formed from 20 consecutive sentences from a book. The question is then formed by omitting a common noun or a named entity from the subsequent 21st sentence. Among datasets of this kind, the BookTest is among the largest with more than 14 million training examples coming from 3555 copyright-free books avalable thanks to Project Gutenberg.

2.1.2 CNN/DAILY MAIL
In the CNN/DM dataset the context document is formed from a news article while the cloze-style question is formed by removing a named entity from one of the short summary sentences which often appear at the top of the article.
To stop the model from using world knowledge from outside the context article (and hence truly test the comprehension of the article), all named entities were replaced by anonymous tags, which are further shuffled for each example. This may make the comprehension more difficult; however, since the answer is always one of the anonymized entities, it also reduces the number of possible answers making guessing easier.

2.2 TARGET DATASETS

2.2.1 BABI
The first target dataset are the bAbI tasks (Weston et al., 2016) – a set of artificial tasks each of which is designed to test a specific kind of reasoning. This toy dataset will allow us to observe what particular skills the model may be learning from each of the three training datasets.
For our experiments we will be using an architecture designed to select one word from the context document as the answer. Hence we have selected Tasks 1,2,3,4,5,11,12,13,14 and 16 which fulfill this requirement and added task 15 which required a slight modification. Furthermore because both pre-training datasets are cloze-style we converted also the bAbI task questions into cloze style (e.g. ”Where is John?” to ”John is in the XXXXX.”).
For the models pre-trained on CNN/DM we also anonymized the tasks in a way similar to the pre-training dataset - i.e. we replaced all names of characters and also all words that can appear as answers for the given task by anonymous tags in the style of CNN/DM. This gives even models that have not seen any training examples from the target domain a chance to answer the questions.
Full details about these alterations can be found in Appendix A.

2.2.2 SQUAD
Secondly, we will look on transfer to the SQuAD dataset (Rajpurkar et al., 2016); here the associated task may be already useful in the real world. Although cloze-style questions have the huge advantage in the possibility of being automatically generated from a suitable corpus – the path taken by CNN/DM and the BookTest – in practice humans would use a proper question, not its cloze-style substitute. This brings us to the need of transfer from the data-rich cloze-style training to the domain of proper questions where data are much scarcer due to the necessary human annotation.
The SQuAD dataset is a great target dataset to use for this. As opposed to the bAbI tasks, the goal of this dataset is actually a problem whose solving would be useful to humans - answering natural questions based on an natural language encyclopedic knowledge base.
For our experiments we selected only a subset of the SQuAD training and development examples where the answer is only a single word, since this is an inherent assumption of our machine learning model. This way we extracted 28,346 training examples out of the original 100,000 examples and 3,233 development examples out of 10,570.

3 MACHINE LEARNING MODEL: AS READER
We perform our experiments using the Attention Sum Reader (AS Reader) (Kadlec et al., 2016b) model. The AS Reader is simple to implement while it achieves strong performance on several text comprehension tasks (Kadlec et al., 2016b; Bajgar et al., 2016; Chu et al., 2016). Since the AS Reader is a building block of many recent text-comprehension models (Trischler et al., 2016; Sordoni et al., 2016; Dhingra et al., 2016; Cui et al., 2016b;a; Shen et al., 2016; Munkhdalai & Yu, 2016) it is a good representative of current research in this field.
A high level structure of the AS Reader is shown in Figure 1. The words from the document and the question are first converted into vector embeddings using a look-up matrix. The document is then read by a bidirectional Gated Recurrent Unit (GRU) network (Cho et al., 2014). A concatenation of the hidden states of the forward and backward GRUs at each word is then used as a contextual embedding of this word, intuitively representing the context in which the word is appearing. We can also understand it as representing the set of questions to which this word may be an answer.
Similarly the question is read by a bidirectional GRU but in this case only the final hidden states are concatenated to form the question embedding.
The attention over each word in the context is then calculated as the dot product of its contextual embedding with the question embedding. This attention is then normalized by the softmax function and summed across all occurrences of each answer candidate. The candidate with most accumulated attention is selected as the final answer.
For a more detailed description of the model including equations check Kadlec et al. (2016b).

4 EXPERIMENTS: TRANSFER LEARNING IN TEXT COMPREHENSION
Now let us turn in more detail to the three kinds of experiments that we performed.

4.1 PRE-TRAINED WITHOUT TARGET ADJUSTMENT
In the first experiment we tested how a model trained on one of the large-scale pre-training datasets performs on the bAbI tasks without any opportunity to train on bAbI. Since the BookTest and CNN/DM tasks involve only cloze-style questions, we can’t expect a model trained on them to answer natural ?-style questions. Hence we did not study the transfer to SQuAD in this case, only the transfer to the (cloze-converted) bAbI tasks.

4.1.1 METHOD
First we tested how the AS Reader architecture (Kadlec et al., 2016b) can handle the tasks if trained directly on the bAbI training data for each task. Then we tested the degree of transfer from the BookTest and CNN/DM data to the 11 selected bAbI tasks.
In the first part of the experiment we trained a separate instance of the AS Reader on the 10,000- example version of the bAbI training data for each of the 11 tasks (for more details see Appendix B.1). On 8 of them the architecture was able to learn the task with accuracy at least 95% 1 (results for each task can be found in Table 4 in Appendix C). Hence if given appropriate training the AS Reader is capable of the reasoning needed to solve most of the selected bAbI tasks. Now when we know that the AS Reader is powerful enough to learn the target tasks we can turn to transfer from the two large-scale datasets.
The main part of this first experiment was then straightforward: we pre-trained multiple models on the BookTest and CNN/DM datasets and then simply evaluated them on the test datasets of the 11 selected bAbI tasks.

4.1.2 RESULTS
Table 1 summarizes the results of this experiment. Both the models trained on the BookTest and those trained on the CNN/DM dataset perform quite poorly on bAbI and achieve much lower accuracy than
1It should be noted that there are several machine learning models that perform better than the AS Reader in the 10k weakly supervised setting, e.g. (Sukhbaatar et al., 2015; Xiong et al., 2016; Graves et al., 2016), however they often need significant fine-tuning. On the other hand we trained plain AS Reader model without any modifications. Hyperparameter and feature fine-tuning could probably further increase its performance on individual tasks however it goes directly against the idea of generality that is at the heart of this work. For comparison with state of the art we include results of DMN+ (Xiong et al., 2016) in Table 1 which had the best average performance over the original 20 tasks.
the models trained directly on each individual bAbI task. However there is some transfer between the tasks since the AS Reader trained on either the BookTest or CNN/DM outperforms a random baseline2 and even an improved baseline which selects the most frequent word from the context that also appears as an answer in the training data for this task.
The results also show that the models trained on CNN/DM perform somewhat better on most tasks than the BookTest models. This may be due to the fact that bAbI tasks generally require the model to summarize information from the context document, which is also what the CNN/DM dataset is testing. On the other hand, the BookTest requires prediction of a possible continuation of a story, where the required kind of reasoning is much less clear but certainly different from pure summarization. Another explanation for better performance of CNN/DM models might be that they solve slightly simpler task since the candidate answers were already pre-selected in the entity anonymization step.
Readers interested in how the training-dataset size affects this kind of transfer can check (Kadlec et al., 2016a) where we show that the target-task performance is a bit better if we use the large BookTest as opposed to its smaller subset, the Children’s Book Test (CBT) (Hill et al., 2015).
Conclusions from this experiment are that the skills learned from two large-scale datasets generalize surprisingly poorly to even simple toy tasks. This may make us ask whether most teams’ focus on solving narrow tasks is truly beneficial if the skills learnt on these tasks are hard to apply elsewhere. However it also brings us to our next experiment, where we try to provide some help to the struggling pre-trained models.

4.2 PRE-TRAINED WITH TARGET ADJUSTMENT
After showing that the skills learnt from the BookTest and CNN/DM datasets are by themselves insufficient for solving the toy tasks, the next natural question is whether they are useful if helped by training on a small sample of examples from the target task. We call this additional phase of training target adjustment. For this experiment we again use the bAbI tasks, however we also test transfer to a subset of the SQuAD dataset, which is much closer to real-world natural-language question answering.
The results presented in this and the following section are based on training 3701 model instances.

4.2.1 METHOD
Common to bAbI and SQuAD datasets. In this experiment we started with a pre-trained model which we used in the previous experiment. However, after it finished training on one of the large pre-training datasets, we allowed it to train on a subset of training examples from the target dataset. We tried subsets of various sizes ranging from a single example to thousands. We tried training four different pre-trained models and also, for comparison, four randomly-initialized models with the same hyperparameters (see Appendix B.2 for details). The experiment with each task-model couple was run on 4 different data samples of each size which were randomly drawn from the training dataset
2The random baseline selects randomly uniformly between all unique words contained in the context document.
of the task to account for variations between these random samples – which may be substantial given the small sample size.3
bAbI. For each of these models we observed the test accuracy at the best-validation epoch and compared this number between the randomly initialized and pre-trained models. Validation was done using 100 examples which were set aside from the task’s original 10k training data.4 We perform the experiment with models pre-trained on the BookTest and also on CNN/DM.
SQuAD subset. In the SQuAD experiment, we trained the model on a subset of the original training dataset where answers were only single words and its sub-subsets. We report the best-validation accuracy on a development set filtered in the same way. This experiment was performed only with the models pre-trained on BookTest.

4.2.2 RESULTS
The results of these experiments are summarized in Figures 2 and 3.
3We are planning to release the split training datasets soon. 4The other models trained on the full 10k dataset usually use 1000 validation examples (Sukhbaatar et al., 2015; Xiong et al., 2016), however we wanted to focus on low data regime thus we used 10 times less examples.
bAbI. Sub-figure 2a shows mean test accuracy of the models that achieved the best validation result for each single task. The results for both BookTest and CNN/DM experiments confirm positive effect of pre-training compared to randomly initialized baseline. Figure 3 shows performance on selected bAbI tasks where pre-training has clearly positive effect, such plot for each of the target tasks is provided in Appendix C.2 (Figure 4).
Note that the CNN/DM models cannot be directly compared to BookTest results due to entity anonymization that seems to simplify the task when the model is trained on smaller datasets.
Since our evaluation methodology with different training set sizes is novel, we can compare our result only to MemN2N (Sukhbaatar et al., 2015) trained on a 1k dataset. MemN2N is the only weakly supervised model that reports accuracy when trained on less than 10k examples. MemN2N achieves average accuracy 93.2%5 on the eleven selected tasks. This is substantially better than both our random baseline (78.0%) and the BookTest-pre-trained model (79.5%), however our model is not tuned in any way towards this particular task. One important conceptual difference is that the AS Reader processes the whole context as one sequence of words, whereas MemN2N receives the context split into single sentences, which simplifies the task for the network.
SQuAD subset. The results of SQuAD experiment also confirm positive effect of pre-training, see Sub-figure 2b, for now compare just lines showing performance of the fully pre-trained model and the randomly initialized model – the meaning of the remaining two lines shall become clear in the next section.
More detailed statistics about the results of this experiment can be found in Appendix D.
We should note that performance of our model is not competitive with the state of the art models on this dataset. For instance the DCR model (Yu et al., 2016) trained on our SQuAD subset achieves validation accuracy 74.9% in this task which is better than our randomly initialized (35.4%) and pre-trained (51.6%) models6. However, the DCR model is designed specifically for the SQuAD task, for instance it utilizes features that are not used by our model.

4.3 PARTIALLY PRE-TRAINED MODEL
Since our previous experiment confirmed positive effect of pre-training if followed by target-domain adjustment, we wondered which part of the model contains the knowledge transferable to new domains. To examine this we performed the following experiment.

4.3.1 METHOD
Our machine learning model, the AS Reader, consists of two main parts: the word-embedding look-up and the bidirectional GRUs used to encode the document and question (see Figure 1). Therefore a natural question was what the contribution of each of these parts is.
To test this we created two models out of each pre-trained model used in the previous experiment. The first model variant uses the pre-trained word embeddings from the original model while the GRU encoders are randomly initialized. We say that this model has pre-trained embeddings. The second model variant uses the opposite setting where the word embeddings are randomly initialized while the encoders are taken form a pre-trained model. We call this pre-trained encoders.
bAbI. For this experiment we selected only a subset of tasks with training set of 100 examples where there was significant difference in accuracy between randomly-initialized and pre-trained models. For evaluation we use the same methodology as in the previous experiment, that is, we report accuracy of the best-validation model averaged over 4 training splits.
SQuAD subset. We evaluated both model variants on all training sets from the previous SQuAD experiment using the same methodology.
5MemN2N trained on each single task with PE LS RN features, see (Sukhbaatar et al., 2015) for details. 6We would like to thank Yu et al. (2016) for training their system on our dataset.

4.3.2 RESULTS
bAbI. Table 2 shows improvement of pre-trained models over a randomly initialized baseline. In most cases (all except Task 5) the fully pre-trained model achieved the best accuracy.
SQuAD subset. The accuracies of the four model variants are plotted in Figure 2b together with results of the previous SQuAD experiment. The graph shows that both pre-trained embeddings and pre-trained encoders alone improve performance over the randomly initialized baseline, however the fully pre-trained model is always the best.
The overall result of this experiment is that both pre-training of the word embeddings and pre-training of the encoder parameters are important since the fully pre-trained model outperforms both partially pre-trained variants.

5 CONCLUSION
Our experiments show that transfer from two large cloze-style question-answering datasets to our two target tasks is suprisingly poor, if the models aren’t provided with any examples from the target domain. However we show that models that pre-trained models perform significantly better than a randomly initialized model if they are shown at least a few training examples from the target domain. The usefulness of pre-trained word embeddings is well known in the NLP community however we show that the power of our pre-trained model does not lie just in the embeddings. This suggests that once the text-comprehension community agrees on sufficiently versatile model, much larger parts of the model could start being reused than just the word-embeddings.
The generalization of skills from a training domain to new tasks is an important ingredient of any system we would want to call intelligent. This work is an early step to explore this direction.

A CLOZE STYLE BABI DATASET
Since our AS Reader architecture is designed to select a single word from the context document as an answer (the task of CBT and BookTest), we selected 10 bAbI tasks that fulfill this requirement out of the original 20. These tasks are: 1. single supporting fact, 2. two supporting facts, 3. three supporting facts, 4. two argument relations, 5. three argument relations, 11. basic coreference, 12. conjunction, 13. compound coreference, 14. time reasoning and 16. basic induction.
Task 15 needed a slight modification to satisfy this requirement: we converted the answers into plural (e.g. ”Q: What is Gertrude afraid of? A: wolf.” was converted into ”A: wolves” which also seems to be the more natural way to formulate the answer to such a question.).
Also since CBT and BookTest train the model for Cloze-style question answering, we modify the original bAbI dataset by reformulating the questions into Cloze-style. For example we translate a question ”Where is John ?” to ”John is in the XXXXX .”
For the models pre-trained on CNN/DM we also replace two kinds of words by anonymized tags (e.g. ”@entity56”) in a style similar to the pre-training dataset. Specifically we replace two (largely overlapping) categories of words:
1. Proper names of story characters (e.g. John, Sandra) 2. Any word that can appear as an answer for the particular task (e.g. kitchen, garden if the
task is asking about locations).

B METHOD DETAILS
B.1 DIRECT TRAINING ON BABI – METHOD
Here we give a more detailed description of the method we used to arrive to our results. We highlight only facts particular to this experiment. A more detailed general description of training the AS Reader is given in (Kadlec et al., 2016b).
The results given for AS Reader trained on bAbI are each for a single model with 64 hidden units in each direction of the GRU context encoder and embedding dimension 32 trained on the 10k training data provided with that particular task.
The results for AS Reader trained on the BookTest and the CNN/DM are for a greedy ensemble consisting of 4 models whose predictions were simply averaged. The models and ensemble were all validated on the validation set corresponding to the training dataset. The performance on the bAbI tasks oscillated notably during training however the ensemble averaging does somewhat mitigate this to get more representative numbers.
B.2 HYPERPARAMETERS FOR THE TARGET-ADJUSTMENT EXPERIMENTS
Table 3 lists hyperparameters of the pre-trained AS Reader instances used in our experiments with target adjustment.

C DETAILED RESULTS
C.1 EXPERIMENTS WITHOUT TARGET ADJUSTMENT
Table 4 shows detailed results for the experiments on models which were just pre-trained on one of the pre-training datasets without any target-adjustment. It also shows several baselines and results of a state-of-the-art model.
C.2 TARGET-ADJUSTMENT EXPERIMENTS
C.2.1 RESULTS FOR ALL BABI TASKS
Figure 4 shows the test accuracies of all models that we trained in the target-adjustment experiments as well as lines joining the accuracies of the best-validation models.
Ta bl
e 4:
Pe rf
or m
an ce
of th
e A
S R
ea de
r w
he n
tr ai
ne d
on th
e bA
bI 10
k, B
oo kT
es ta
nd C
N N
/D M
da ta
se ts
an d
th en
ev al
ua te
d on
bA bI
te st
da ta
. T
he D
yn am ic M em or y N et w or k (D M N +) is th e st at eof -t he -a rt m od el in a w ea kl y su pe rv is ed se tti ng on th e bA bI 10 k da ta se t. It s re su lts ar e ta ke n fr om (X io ng et al ., 20 16 ). M em N 2N (S uk hb aa ta re ta l., 20 15 )i s th e st at eof -t he -a rt m od el on th e 1k tr ai ni ng da ta se t; fo rc om pl et en es s w e al so in cl ud e its re su lts w ith th e 10 k tr ai ni ng .
M od
el :
R an
do m
R nd
ca nd
. M
em N 2N (s in gl e) (P E L S R
N )
M em
N 2N
(s in
gl e)
(P E
L S
LW R
N )
D M
N +
(s in
gl e)
A SR
ea de
r
a a
a a
a a a
a a a
Te st
da ta
se t Tr ai
n da
ta se
t no
t tr ai ne
d bA
bI 10
k bA
bI 1k
bA bI 10 k
bA bI 10 k
bA bI 10 k
B oo
kT es t 14
M D
M +C
N N
1. 2M
1 Si
ng le
su pp
or tin
g fa
ct 7.
80 31
.2 0
10 0.
00 10
0. 00
10 0.
00 10
0. 00
37 .3
0 51
.5 0
2 Tw
o su
pp or
tin g
fa ct
s 4.
40 26
.9 6
91 .7
0 99
.7 0
99 .7
0 91
.9 0
25 .8
0 28
.9 0
3 T
hr ee
su pp
or tin
g fa
ct s
3. 40
19 .1
4 59
.7 0
97 .9
0 98
.9 0
86 .0
0 22
.2 0
27 .4 0 4 Tw oar gu m en tr el at io ns 10 .5 0 33 .5 8 97 .2 0 10 0. 00 10 0. 00 10 0. 00 50 .3 0 54 .9 0 5 T hr ee -a rg um en tr el at io ns 4. 40 21 .4 2 86 .9 0 99 .2 0 99 .5 0 99 .8 0 67 .6 0 68 .1 0 11 B as ic co re fe re nc e 6. 20 30 .4 2 99 .1 0 99 .9 0 10 0. 00 10 0. 00 33 .0 0 20 .8 0 12 C on ju nc tio n 6. 70 27 .2 5 99 .8 0 10 0. 00 10 0. 00 10 0. 00 30 .4 0 37 .7 0 13 C om po un d co re fe re nc e 5. 60 27 .7 3 99 .6 0 10 0. 00 10 0. 00 10 0. 00 33 .8 0 14 .0 0 14 Ti m e re as on in g 5. 00 27 .8 2 98 .3 0 99 .9 0 99 .8 0 95 .0 0 27 .6 0 50 .5 0 15 B as ic de du ct io n 5. 20 37 .2 0 10 0. 00 10 0. 00 10 0. 00 96 .7 0 39 .9 0 17 .6 0 16 B as ic in du ct io n 7. 50 45 .6 5 98 .7 0 48 .2 0 54 .7 0 50 .3 0 15 .1 0 48 .0 0 bA bI m ea n (1 1 ta sk s) 6. 06 29 .8 5 93 .7 3 94 .9 8 95 .6 9 92 .7 0 34 .8 2 38 .1 3
C.2.2 AVERAGE OVER ALL MODELS TRAINED ON BABI TASKS
Figure 5 plots mean accuracy of all models trained in our experiments. This suggests that pre-training helped all models, not only the top performing ones selected by validation as already shown in Figure 2a.

D MEANS, STANDARD DEVIATIONS AND P-VALUES BY EXPERIMENT
Table 5 shows the mean accuracy across all models trained for each combination of task, pre-training dataset and target-adjustment dataset size. Table 6 shows the corresponding standard deviations.
Table 7 then shows the p-value that whether the expected accuracy of pre-trained models is greater than the expected accuracy of randomly initialized models. This shows that the pre-trained models are statistically significantly better for all target-adjustment set sizes on the SQuAD dataset. On bAbI the BookTest pre-trained models perform convincingly better especially for target-adjustment dataset sizes 100, 500 and 1000, with Task 16 being the main exception to this because the AS Reader struggles to learn it in any setting. For the CNN+DM pre-training the results are not conclusive.
Ta sk
Pr et
ra in
in g
Ta rg
et -a
dj us
tm en
ts et
si ze
0 1
10 10
0 50
0 10
00 50
00 10
00 0
28 17 4 SQ uA D B oo kT es t 1. 01 e45 4. 07 e05 7. 40 e05 7. 82 e08 N A 5. 17 e08 N A 3. 93 e08 8.
52 e03 Ta sk 1 B oo kT es t 3. 34 e83 1. 81 e03 1. 33 e01 2. 35 e19 9. 41 e04 1. 67 e02 1. 32 e01 N A N A Ta sk 2 B oo kT es t 1. 24 e34 3. 86 e07 7. 29 e03 2. 59 e01 1. 39 e08 2. 63 e06 7. 54 e09 2. 04 e01 N A Ta sk 3 B oo kT es t 9. 84 e55 1. 27 e05 7. 66 e03 1. 48 e03 3. 18 e04 2. 18 e03 2. 16 e04 1. 03 e01 N A Ta sk 4 B oo kT es t 7. 25 e78 9. 50 e01 9. 71 e01 1. 04 e05 6. 38 e03 1. 70 e02 1. 81 e02 N A N A Ta sk 5 B oo kT es t 6. 55 e11 5 9. 88 e22 8. 87 e19 5. 25 e05 3. 66 e03 8. 61 e02 5. 65 e03 N A N A Ta sk 11 B oo kT es t 6. 78 e15 2 1. 00 e+ 00 9. 94 e01 4. 07 e09 2. 50 e04 2. 28 e02 6. 37 e02 N A N A Ta sk 12 B oo kT es t 2. 27 e90 9. 10 e01 6. 46 e01 1. 89 e05 2. 78 e04 1. 43 e02 2. 36 e02 N A N A Ta sk 13 B oo kT es t 5. 30 e91 9. 75 e01 9. 99 e01 2. 88 e02 2. 74 e02 1. 03 e01 7. 06 e02 N A N A Ta sk 14 B oo kT es t 1. 97 e20 0 1. 01 e03 6. 79 e01 2. 22 e14 3. 40 e05 2. 93 e03 3. 66 e06 3. 97 e01 N A Ta sk 15 B oo kT es t 3. 64 e09 4. 75 e01 4. 12 e01 6. 70 e01 1. 68 e03 3. 70 e03 1. 03 e05 4. 54 e01 N A Ta sk 16 B oo kT es t 1. 81 e05 8. 28 e04 4. 38 e01 2. 72 e01 4. 89 e01 5. 71 e01 7. 40 e03 N A N A Ta sk 1 C N N +D M 9. 43 e09 2. 99 e01 1. 11 e01 1. 05 e01 9. 54 e02 1. 45 e01 3. 97 e03 N A N A Ta sk 2 C N N +D M 9. 38 e17 6. 93 e01 9. 02 e01 9. 15 e01 1. 05 e03 4. 20 e01 2. 64 e03 8. 49 e02 N A Ta sk 3 C N N +D M 2. 42 e16 4. 95 e02 6. 30 e01 1. 75 e01 2. 13 e03 6. 59 e04 4. 68 e02 1. 24 e01 N A Ta sk 4 C N N +D M 5. 84 e03 9. 70 e01 1. 37 e01 4. 83 e03 3. 33 e01 8. 84 e01 1. 08 e01 N A N A Ta sk 5 C N N +D M 1. 17 e10 7. 00 e03 7. 93 e04 5. 20 e01 9. 70 e01 5. 66 e01 1. 83 e01 N A N A Ta sk 11 C N N +D M 1. 00 e+ 00 9. 84 e01 9. 73 e01 2. 58 e01 7. 17 e01 1. 45 e01 6. 95 e01 N A N A Ta sk 12 C N N +D M 1. 93 e14 9. 32 e01 9. 92 e01 2. 57 e02 4. 06 e01 6. 65 e02 2. 09 e01 N A N A Ta sk 13 C N N +D M 8. 69 e02 9. 61 e01 9. 72 e01 9. 89 e01 6. 22 e01 9. 44 e01 2. 83 e01 N A N A Ta sk 14 C N N +D M 2. 17 e12 6. 64 e02 1. 11 e01 2. 05 e02 3. 66 e02 4. 52 e01 9. 10 e01 8. 24 e01 N A Ta sk 15 C N N +D M 1. 36 e52 5. 30 e03 3. 48 e02 7. 21 e02 8. 36 e01 3. 09 e01 8. 47 e01 9. 84 e01 N A Ta sk 16 C N N +D M 6. 39 e35 4. 56 e02 9. 66 e01 5. 95 e01 7. 19 e01 4. 09 e02 2. 51 e02 2. 22 e03 N A
Ta bl
e 7:
O ne
-s id
ed p-
va lu
e w
he th
er th
e m
ea n
ac cu
ra cy
of pr
etr
ai ne
d m
od el
s is
gr ea
te rt
ha n
th e
ac cu
ra cy
of th
e ra
nd om
ly in
iti al
iz ed
on es
fo re
ac h
co m
bi na
tio n of ta sk pr etr ai ni ng da ta se t. pva lu es be lo w 0. 05 ar e m ar ke d in gr ee n.
","Deep learning has proven useful on many NLP tasks including reading comprehension. However, it requires large amounts of training data which are not available in some domains of application. Hence we examine the possibility of using data-rich domains to pre-train models and then apply them in domains where training data are harder to get. Specifically, we train a neural-network-based model on two context-question-answer datasets, the BookTest and CNN/Daily Mail, and we monitor transfer to subsets of bAbI, a set of artificial tasks designed to test specific reasoning abilities, and of SQuAD, a question-answering dataset which is much closer to real-world applications. Our experiments show very limited transfer if the model is not shown any training examples from the target domain however the results are encouraging if the model is shown at least a few target-domain examples. Furthermore we show that the effect of pre-training is not limited to word embeddings.",ICLR 2017 conference submission,False,,"This work investigates the performance of transfer learning from resource-rich setup (BookTest, CNN/Daily Mail corpora) to low-resource (bAbI, SQuAD benchmarks) settings. Experiments show poor improvements in 0-shot learning. However, when the model is exposed to few training instances some improvements are observed.

The claims made here require a more comprehensive analysis. I criticize the use of bAbI as a low-resource real-world scenario. bAbI is designed as a unit test and is far from representing many natural language phenomena. Thus, the claims related to bAbI can only be weak evidence for questioning transfer learning high-resource to low-resource in real-world scenarios. I highly recommend using recently proposed real-world scenarios [1,2].

More importantly, the work does not explain why and how do we get improvement using transfer learning. They remotely address this by hypothesizing the knowledge of transfer is not just encoded in embeddings but also in the model. Considering the related work [3], these claims bring a marginal novelty and still ""how and why"" should be central in this work.  

[1]

---

The area chair agrees with reviewers 1 and 3 that the paper does not meet the bar for ICLR. Reviewer 3 in particular points out how the paper can be strengthened for future revisions.

---

Dear authors and reviewers, this paper is currently very close to the decision boundary for acceptance and would benefit from a bit more discussion.

---

First I would like to apologize for the delay in reviewing.

summary : This work explores several experiments to transfer training a specific model of reading comprehension ( AS Reader), in an artificial and well populated dataset in order to perform in another target dataset. 

Here is what I understand are their several experiments to transfer learning, but I am not 100% sure.
1. The model is trained on the big artificial dataset and tested on the small target datasets (section 4.1)
2. The model is pre-trained on the big artificial dataset like before, then fine-tuned on a few examples from the target dataset and tested on the remaining target examples. Several such models are trained using different sub-sets of fine-tuning examples. The results are tested against the performance of randomly intialized then fine-tuned models (section 4.2).
3. The model is pre-trained on the big artificial dataset like before. The model is made of an embedding component and an encoder  component. Alternatively, each component is reset to a random initialization, to test the importance of the pre-training in each component. Then the model is fine-tuned on a few examples from the target dataset and tested on the remaining target examples. (section 4.3)

I think what makes things difficult to follow is the fact that the test set is composed by several sub tasks, and sometimes what is reported is the mean performance across the tasks, sometimes the performance on a few tasks. Sometimes what we see is the mean performance of several models? You should report standard deviations also. Could you better explain what you mean by best validation ?

Interesting and unpretentious work. The clarity of the presentation could be improved maybe by simplifying the experimental setup? The interesting conclusion I think is reported at the end of the section 4.1, when the nuanced difference between the datasets are exposed.

Minor: unexplained acronyms: GRU, BT, CBT.
benfits p. 2
subsubset p. 6

---

This paper proposes a study of transfer learning in the context of QA from stories. A system is presented with a a short story and has to answer a question about it. This paper studies how a system trained to answer questions on a dataset can eventually be used to answer questions from another dataset. The results are mostly negative: transfer seems almost non-existant.

This paper is centered around presenting negative results. Indeed the main hypothesis of transferring between QA datasets with the attention sum reader turns out impossible and one needs a small portion of labeled data from the target dataset to get meaningful performance.

Having only negative results could be fine if the paper was bringing some value with a sharp analysis of the failure modes and of the reasons behind it. Because this might indicate some research directions to follow. However, there is not much of that. 

The answers to the pre-review questions actually start to give some insights: typing seems to be transferred for instance. How about the impact of syntax (very different between bAbI, Gutenberg books, and CNN news articles)? And the word/entity/ngrams distributions overlap between the 3 datasets?

Unfortunately, there is not much to take-away from this paper.

---

This work investigates the performance of transfer learning from resource-rich setup (BookTest, CNN/Daily Mail corpora) to low-resource (bAbI, SQuAD benchmarks) settings. Experiments show poor improvements in 0-shot learning. However, when the model is exposed to few training instances some improvements are observed.

The claims made here require a more comprehensive analysis. I criticize the use of bAbI as a low-resource real-world scenario. bAbI is designed as a unit test and is far from representing many natural language phenomena. Thus, the claims related to bAbI can only be weak evidence for questioning transfer learning high-resource to low-resource in real-world scenarios. I highly recommend using recently proposed real-world scenarios [1,2].

More importantly, the work does not explain why and how do we get improvement using transfer learning. They remotely address this by hypothesizing the knowledge of transfer is not just encoded in embeddings but also in the model. Considering the related work [3], these claims bring a marginal novelty and still ""how and why"" should be central in this work.  

[1]

---

Dear Authors,

Please resubmit your paper in the ICLR 2017 format with the correct font for your submission to be considered. Thank you!

---

This work investigates the performance of transfer learning from resource-rich setup (BookTest, CNN/Daily Mail corpora) to low-resource (bAbI, SQuAD benchmarks) settings. Experiments show poor improvements in 0-shot learning. However, when the model is exposed to few training instances some improvements are observed.

The claims made here require a more comprehensive analysis. I criticize the use of bAbI as a low-resource real-world scenario. bAbI is designed as a unit test and is far from representing many natural language phenomena. Thus, the claims related to bAbI can only be weak evidence for questioning transfer learning high-resource to low-resource in real-world scenarios. I highly recommend using recently proposed real-world scenarios [1,2].

More importantly, the work does not explain why and how do we get improvement using transfer learning. They remotely address this by hypothesizing the knowledge of transfer is not just encoded in embeddings but also in the model. Considering the related work [3], these claims bring a marginal novelty and still ""how and why"" should be central in this work.  

[1]

---

The area chair agrees with reviewers 1 and 3 that the paper does not meet the bar for ICLR. Reviewer 3 in particular points out how the paper can be strengthened for future revisions.

---

Dear authors and reviewers, this paper is currently very close to the decision boundary for acceptance and would benefit from a bit more discussion.

---

First I would like to apologize for the delay in reviewing.

summary : This work explores several experiments to transfer training a specific model of reading comprehension ( AS Reader), in an artificial and well populated dataset in order to perform in another target dataset. 

Here is what I understand are their several experiments to transfer learning, but I am not 100% sure.
1. The model is trained on the big artificial dataset and tested on the small target datasets (section 4.1)
2. The model is pre-trained on the big artificial dataset like before, then fine-tuned on a few examples from the target dataset and tested on the remaining target examples. Several such models are trained using different sub-sets of fine-tuning examples. The results are tested against the performance of randomly intialized then fine-tuned models (section 4.2).
3. The model is pre-trained on the big artificial dataset like before. The model is made of an embedding component and an encoder  component. Alternatively, each component is reset to a random initialization, to test the importance of the pre-training in each component. Then the model is fine-tuned on a few examples from the target dataset and tested on the remaining target examples. (section 4.3)

I think what makes things difficult to follow is the fact that the test set is composed by several sub tasks, and sometimes what is reported is the mean performance across the tasks, sometimes the performance on a few tasks. Sometimes what we see is the mean performance of several models? You should report standard deviations also. Could you better explain what you mean by best validation ?

Interesting and unpretentious work. The clarity of the presentation could be improved maybe by simplifying the experimental setup? The interesting conclusion I think is reported at the end of the section 4.1, when the nuanced difference between the datasets are exposed.

Minor: unexplained acronyms: GRU, BT, CBT.
benfits p. 2
subsubset p. 6

---

This paper proposes a study of transfer learning in the context of QA from stories. A system is presented with a a short story and has to answer a question about it. This paper studies how a system trained to answer questions on a dataset can eventually be used to answer questions from another dataset. The results are mostly negative: transfer seems almost non-existant.

This paper is centered around presenting negative results. Indeed the main hypothesis of transferring between QA datasets with the attention sum reader turns out impossible and one needs a small portion of labeled data from the target dataset to get meaningful performance.

Having only negative results could be fine if the paper was bringing some value with a sharp analysis of the failure modes and of the reasons behind it. Because this might indicate some research directions to follow. However, there is not much of that. 

The answers to the pre-review questions actually start to give some insights: typing seems to be transferred for instance. How about the impact of syntax (very different between bAbI, Gutenberg books, and CNN news articles)? And the word/entity/ngrams distributions overlap between the 3 datasets?

Unfortunately, there is not much to take-away from this paper.

---

This work investigates the performance of transfer learning from resource-rich setup (BookTest, CNN/Daily Mail corpora) to low-resource (bAbI, SQuAD benchmarks) settings. Experiments show poor improvements in 0-shot learning. However, when the model is exposed to few training instances some improvements are observed.

The claims made here require a more comprehensive analysis. I criticize the use of bAbI as a low-resource real-world scenario. bAbI is designed as a unit test and is far from representing many natural language phenomena. Thus, the claims related to bAbI can only be weak evidence for questioning transfer learning high-resource to low-resource in real-world scenarios. I highly recommend using recently proposed real-world scenarios [1,2].

More importantly, the work does not explain why and how do we get improvement using transfer learning. They remotely address this by hypothesizing the knowledge of transfer is not just encoded in embeddings but also in the model. Considering the related work [3], these claims bring a marginal novelty and still ""how and why"" should be central in this work.  

[1]

---

Dear Authors,

Please resubmit your paper in the ICLR 2017 format with the correct font for your submission to be considered. Thank you!",,,,,,4.333333333333333,,,4.0,,
687,"THE INCREDIBLE SHRINKING NEURAL NETWORK: NEW PERSPECTIVES
Authors: Nikolas Wolfe, Aditya Sharma
Source file: 687.pdf

ABSTRACT
How much can pruning algorithms teach us about the fundamentals of learning representations in neural networks? A lot, it turns out. Neural network model compression has become a topic of great interest in recent years, and many different techniques have been proposed to address this problem. In general, this is motivated by the idea that smaller models typically lead to better generalization. At the same time, the decision of what to prune and when to prune necessarily forces us to confront our assumptions about how neural networks actually learn to represent patterns in data. In this work we set out to test several long-held hypotheses about neural network learning representations and numerical approaches to pruning. To accomplish this we first reviewed the historical literature and derived a novel algorithm to prune whole neurons (as opposed to the traditional method of pruning weights) from optimally trained networks using a second-order Taylor method. We then set about testing the performance of our algorithm and analyzing the quality of the decisions it made. As a baseline for comparison we used a first-order Taylor method based on the Skeletonization algorithm and an exhaustive brute-force serial pruning algorithm. Our proposed algorithm worked well compared to a first-order method, but not nearly as well as the brute-force method. Our error analysis led us to question the validity of many widely-held assumptions behind pruning algorithms in general and the trade-offs we often make in the interest of reducing computational complexity. We discovered that there is a straightforward way, however expensive, to serially prune 40-70% of the neurons in a trained network with minimal effect on the learning representation and without any re-training.

1 INTRODUCTION
In this work we propose and evaluate a novel algorithm for pruning whole neurons from a trained neural network without any re-training and examine its performance compared to two simpler methods. We then analyze the kinds of errors made by our algorithm and use this as a stepping off point to launch an investigation into the fundamental nature of learning representations in neural networks. Our results corroborate an insightful though largely forgotten observation by Mozer & Smolensky (1989a) concerning the nature of neural network learning. This observation is best summarized in a quotation from Segee & Carter (1991) on the notion of fault-tolerance in multilayer perceptron networks:
Contrary to the belief widely held, multilayer networks are not inherently fault tolerant. In fact, the loss of a single weight is frequently sufficient to completely
disrupt a learned function approximation. Furthermore, having a large number of weights does not seem to improve fault tolerance. [Emphasis added]
Essentially, Mozer & Smolensky (1989b) observed that during training neural networks do not distribute the learning representation evenly or equitably across hidden units. What actually happens is that a few, elite neurons learn an approximation of the input-output function, and the remaining units must learn a complex interdependence function which cancels out their respective influence on the network output. Furthermore, assuming enough units exist to learn the function in question, increasing the number of parameters does not increase the richness or robustness of the learned approximation, but rather simply increases the likelihood of overfitting and the number of noisy parameters to be canceled during training. This is evinced by the fact that in many cases, multiple neurons can be removed from a network with no re-training and with negligible impact on the quality of the output approximation. In other words, there are few bipartisan units in a trained network. A unit is typically either part of the (possibly overfit) input-output function approximation, or it is part of an elaborate noise cancellation task force. Assuming this is the case, most of the compute-time spent training a neural network is likely occupied by this arguably wasteful procedure of silencing superfluous parameters, and pruning can be viewed as a necessary procedure to “trim the fat.”
We observed copious evidence of this phenomenon in our experiments, and this is the motivation behind our decision to evaluate the pruning algorithms in this study on the simple criteria of their ability to trim neurons without any re-training. If we were to employ re-training as part of our evaluation criteria, we would arguably not be evaluating the quality of our algorithm’s pruning decisions per se but rather the ability of back-propagation trained networks to recover from faults caused by non-ideal pruning decisions, as suggested by the conclusions of Segee & Carter (1991) and Mozer & Smolensky (1989a). Moreover, as Fahlman & Lebiere (1989) discuss, due to the “herd effect” and “moving target” phenomena in back-propagation learning, the remaining units in a network will simply shift course to account for whatever error signal is re-introduced as a result of a bad pruning decision or network fault. So long as there are enough critical parameters to learn the function in question, a network can typically recover faults with additional training. This limits the conclusions we can draw about the quality of our pruning criteria when we employ re-training.
In terms of removing units without re-training, what we discovered is that predicting the behavior of a network when a unit is to be pruned is very difficult, and most of the approximation techniques put forth in existing pruning algorithms do not fare well at all when compared to a brute-force search. To begin our discussion of how we arrived at our algorithm and set up our experiments, we review of the existing literature.

2 LITERATURE REVIEW
Pruning algorithms, as comprehensively surveyed by Reed (1993), are a useful set of heuristics designed to identify and remove elements from a neural network which are either redundant or do not significantly contribute to the output of the network. This is motivated by the observed tendency of neural networks to overfit to the idiosyncrasies of their training data given too many trainable parameters or too few input patterns from which to generalize, as stated by Chauvin (1990).
Network architecture design and hyperparameter selection are inherently difficult tasks typically approached using a few well-known rules of thumb, e.g. various weight initialization procedures, choosing the width and number of layers, different activation functions, learning rates, momentum, etc. Some of this “black art” appears unavoidable. For problems which cannot be solved using linear threshold units alone, Baum & Haussler (1989) demonstrate that there is no way to precisely determine the appropriate size of a neural network a priori given any random set of training instances. Using too few neurons seems to inhibit learning, and so in practice it is common to attempt to overparameterize networks initially using a large number of hidden units and weights, and then prune or compress them afterwards if necessary. Of course, as the old saying goes, there’s more than one way to skin a neural network.

2.1 NON-PRUNING BASED GENERALIZATION & COMPRESSION TECHNIQUES
The generalization behavior of neural networks has been well studied, and apart from pruning algorithms many heuristics have been used to avoid overfitting, such as dropout (Srivastava et al.
(2014)), maxout (Goodfellow et al. (2013)), and cascade correlation (Fahlman & Lebiere (1989)), among others. Of course, while cascade correlation specifically tries to construct of minimal networks, many techniques to improve network generalization do not explicitly attempt to reduce the total number of parameters or the memory footprint of a trained network per se.
Model compression often has benefits with respect to generalization performance and the portability of neural networks to operate in memory-constrained or embedded environments. Without explicitly removing parameters from the network, weight quantization allows for a reduction in the number of bytes used to represent each weight parameter, as investigated by Balzer et al. (1991), Dundar & Rose (1994), and Hoehfeld & Fahlman (1992).
A recently proposed method for compressing recurrent neural networks (Prabhavalkar et al. (2016)) uses the singular values of a trained weight matrix as basis vectors from which to derive a compressed hidden layer. Øland & Raj (2015) successfully implemented network compression through weight quantization with an encoding step while others such as Han et al. (2016) have tried to expand on this by adding weight-pruning as a preceding step to quantization and encoding.
In summary, we can say that there are many different ways to improve network generalization by altering the training procedure, the objective error function, or by using compressed representations of the network parameters. But these are not, strictly speaking, examples of techniques to reduce the number of parameters in a network. For this we must employ some form of pruning criteria.

2.2 PRUNING TECHNIQUES
If we wanted to continually shrink a neural network down to minimum size, the most straightforward brute-force way to do it is to individually switch each element off and measure the increase in total error on the training set. We then pick the element which has the least impact on the total error, and remove it. Rinse and repeat. This is extremely computationally expensive, given a reasonably large neural network and training set. Alternatively, we might accomplish this using any number of much faster off-the-shelf pruning algorithms, such as Skeletonization (Mozer & Smolensky (1989a)), Optimal Brain Damage (LeCun et al. (1989)), or later variants such as Optimal Brain Surgeon (Hassibi & Stork (1993)). In fact, we borrow much of our inspiration from these algorithms, with one major variation: Instead of pruning individual weights, we prune entire neurons, thereby eliminating all of their incoming and outgoing weight parameters in one go, resulting in more memory saved, faster.
The algorithm developed for this paper is targeted at reducing the total number of neurons in a trained network, which is one way of reducing its computational memory footprint. This is often a desirable criteria to minimize in the case of resource-constrained or embedded devices, and also allows us to probe the limitations of pruning down to the very last essential network elements. In terms of generalization as well, we can measure the error of the network on the test set as each element is sequentially removed from the network. With an oracle pruning algorithm, what we expect to observe is that the output of the network remains stable as the first few superfluous neurons are removed, and as we start to bite into the more crucial members of the function approximation, the error should start to rise dramatically. In this paper, the brute-force approach described at the beginning of this section serves as a proxy for an oracle pruning algorithm.
One reason to choose to rank and prune individual neurons as opposed to weights is that there are far fewer elements to consider. Furthermore, the removal of a single weight from a large network is a drop in the bucket in terms of reducing a network’s core memory footprint. If we want to reduce the size of a network as efficiently as possible, we argue that pruning neurons instead of weights is more efficient computationally as well as practically in terms of quickly reaching a hypothetical target reduction in memory consumption. This approach also offers downstream applications a realistic expectation of the minimal increase in error resulting from the removal of a specified percentage of neurons. Such trade-offs are unavoidable, but performance impacts can be limited if a principled approach is used to find the best candidate neurons for removal.
It is well known that too many free parameters in a neural network can lead to overfitting. Regardless of the number of weights used in a given network, as Segee & Carter (1991) assert, the representation of a learned function approximation is almost never evenly distributed over the hidden units, and thus the removal of any single hidden unit at random can actually result in a network fault. Mozer & Smolensky (1989b) argue that only a subset of the hidden units in a neural network actually
latch on to the invariant or generalizing properties of the training inputs, and the rest learn to either mutually cancel each other’s influence or begin overfitting to the noise in the data. We leverage this idea in the current work to rank all neurons in pre-trained networks based on their effective contributions to the overall performance. We then remove the unnecessary neurons to reduce the network’s footprint. Through our experiments we not only concretely validate the theory put forth by Mozer & Smolensky (1989b) but we also successfully build on it to prune networks to 40 to 60 % of their original size without any major loss in performance.

3 PRUNING NEURONS TO SHRINK NEURAL NETWORKS
As discussed in Section 1 our aim is to leverage the highly non-uniform distribution of the learning representation in pre-trained neural networks to eliminate redundant neurons, without focusing on individual weight parameters. Taking this approach enables us to remove all the weights (incoming and outgoing) associated with a non-contributing neuron at once. We would like to note here that in an ideal scenario, based on the neuron interdependency theory put forward by Mozer & Smolensky (1989a), one would evaluate all possible combinations of neurons to remove (one at a time, two at a time, three at a time and so forth) to find the optimal subset of neurons to keep. This is computationally unacceptable, and so we will only focus on removing one neuron at a time and explore more “greedy” algorithms to do this in a more efficient manner.
The general approach taken to prune an optimally trained neural network here is to create a ranked list of all the neurons in the network based off of one of the 3 proposed ranking criteria: a brute force approximation, a linear approximation and a quadratic approximation of the neuron’s impact on the output of the network. We then test the effects of removing neurons on the accuracy and error of the network. All the algorithms and methods presented here are easily parallelizable as well.
One last thing to note here before moving forward is that the methods discussed in this section involve some non-trivial derivations which are beyond the scope of this paper. We are more focused on analyzing the implications of these methods on our understanding of neural network learning representations. However, a complete step-by-step derivation and proof of all the results presented is provided in the Supplementary Material as an Appendix.

3.1 BRUTE FORCE REMOVAL APPROACH
This is perhaps the most naive yet the most accurate method for pruning the network. It is also the slowest and hence possibly unusable on large-scale neural networks with thousands of neurons. This method explicitly evaluates each neuron in the network. The idea is to manually check the effect of every single neuron on the output. This is done by running a forward propagation on the validation set K times (where K is the total number of neurons in the network), turning off exactly one neuron each time (keeping all other neurons active) and noting down the change in error. Turning a neuron off can be achieved by simply setting its output to 0. This results in all the outgoing weights from that neuron being turned off. This change in error is then used to generate the ranked list.

3.2 TAYLOR SERIES REPRESENTATION OF ERROR
Let us denote the total error from the optimally trained neural network for any given validation dataset by E. E can be seen as a function of O, where O is the output of any general neuron in the network. This error can be approximated at a particular neuron’s output (say Ok) by using the 2nd order Taylor Series as,
Ê(O) ≈ E(Ok) + (O −Ok) · ∂E
∂O ∣∣∣∣ Ok + 0.5 · (O −Ok)2 · ∂2E ∂O2 ∣∣∣∣ Ok , (1)
When a neuron is pruned, its output O becomes 0.
Replacing O by Ok in equation 1 shows us that the error is approximated perfectly by equation 1 at Ok. So:
∆Ek = Ê(0)− Ê(Ok) = −Ok · ∂E
∂O ∣∣∣∣ Ok + 0.5 ·O2k · ∂2E ∂O2 ∣∣∣∣ Ok , (2)
where ∆Ek is the change in the total error of the network when exactly one neuron (k) is turned off. Most of the terms in this equation are fairly easy to compute, as we have Ok already from the activations of the hidden units and we already compute ∂E∂O |Ok for each training instance during backpropagation. The ∂
2E ∂O2 |Ok terms are a little more difficult to compute. This is derived in the
appendix and summarized in the sections below.

3.2.1 LINEAR APPROXIMATION APPROACH
We can use equation 2 to get the linear error approximation of the change in error due to the kth neuron being turned off and represent it as ∆E1k as follows:
∆E1k = −Ok · ∂E
∂O ∣∣∣∣ Ok
(3)
The derivative term above is the first-order gradient which represents the change in error with respect to the output a given neuron. This term can be collected during back-propagation. As we shall see further in this section, linear approximations are not reliable indicators of change in error but they provide us with an interesting basis for comparison with the other methods discussed in this paper.

3.2.2 QUADRATIC APPROXIMATION APPROACH
As above, we can use equation 2 to get the quadratic error approximation of the change in error due to the kth neuron being turned off and represent it as ∆E2k as follows:
∆E2k = −Ok · ∂E
∂O ∣∣∣∣ Ok + 0.5 ·O2k · ∂2E ∂O2 ∣∣∣∣ Ok
(4)
The additional second-order gradient term appearing above represents the quadratic change in error with respect to the output of a given neuron. This term can be generated by performing backpropagation using second order derivatives. Collecting these quadratic gradients involves some non-trivial mathematics, the entire step-by-step derivation procedure of which is provided in the Supplementary Material as an Appendix.

3.3 PROPOSED PRUNING ALGORITHM
Figure 1 shows a random error function plotted against the output of any given neuron. Note that this figure is for illustration purposes only. The error function is minimized at a particular value of the neuron output as can be seen in the figure. The process of training a neural network is essentially the process of finding these minimizing output values for all the neurons in the network. Pruning this particular neuron (which translates to getting a zero output from it will result in a change in the total overall error. This change in error is represented by distance between the original minimum error (shown by the dashed line) and the top red arrow. This neuron is clearly a bad candidate for removal since removing it will result in a huge error increase.
The straight red line in the figure represents the first-order approximation of the error using Taylor Series as described before while the parabola represents a second-order approximation. It can be clearly seen that the second-order approximation is a much better estimate of the change in error.
One thing to note here is that it is possible in some cases that there is some thresholding required when trying to approximate the error using the 2nd order Taylor Series expansion. These cases might arise when the parabolic approximation undergoes a steep slope change. To take into account such cases, mean and median thresholding were employed, where any change above a certain threshold was assigned a mean or median value respectively.
Two pruning algorithms are proposed here. They are different in the way the neurons are ranked but both of them use ∆Ek, the approximation of the change in error as the basis for the ranking. ∆Ek can be calculated using the Brute Force method, or one of the two Taylor Series approximations discussed previously.
The first step in both the algorithms is to decide a stopping criterion. This can vary depending on the application but some intuitive stopping criteria can be: maximum number of neurons to remove, percentage scaling needed, maximum allowable accuracy drop etc.

3.3.1 ALGORITHM I: SINGLE OVERALL RANKING
The complete algorithm is shown in Algorithm 1. The idea here is to generate a single ranked list based on the values of ∆Ek. This involves a single pass of second-order back-propagation (without weight updates) to collect the gradients for each neuron. The neurons from this rank-list (with the lowest values of ∆Ek) are then pruned according to the stopping criterion decided. We note here that this algorithm is intentionally naive and is used for comparison only.
Data: optimally trained network, training set Result: A pruned network initialize and define stopping criterion ; perform forward propagation over the training set ; perform second-order back-propagation without updating weights and collect linear and quadratic
gradients ; rank the remaining neurons based on ∆Ek; while stopping criterion is not met do
remove the last ranked neuron ; end
Algorithm 1: Single Overall Ranking

3.3.2 ALGORITHM II: ITERATIVE RE-RANKING
In this greedy variation of the algorithm (Algorithm 2), after each neuron removal, the remaining network undergoes a single forward and backward pass of second-order back-propagation (without weight updates) and the rank list is formed again. Hence, each removal involves a new pass through
the network. This method is computationally more expensive but takes into account the dependencies the neurons might have on one another which would lead to a change in error contribution every time a dependent neuron is removed.
Data: optimally trained network, training set Result: A pruned network initialize and define stopping criterion ; while stopping criterion is not met do
perform forward propagation over the training set ; perform second-order back-propagation without updating weights and collect linear and
quadratic gradients ; rank the remaining neurons based on ∆Ek ; remove the worst neuron based on the ranking ;
end Algorithm 2: Iterative Re-Ranking

4 EXPERIMENTAL RESULTS

4.1 EXAMPLE REGRESSION PROBLEM
This problem serves as a quick example to demonstrate many of the phenomena described in previous sections. We trained two networks to learn the cosine function, with one input and one output. This is a task which requires no more than 11 sigmoid neurons to solve entirely, and in this case we don’t care about overfitting because the cosine function has a precise definition. Furthermore, the cosine function is a good toy example because it is a smooth continuous function and, as demonstrated by Nielsen (2015), if we were to tinker directly with the weights and bias parameters of the network, we could allocate individual units within the network to be responsible for constrained ranges of inputs, similar to a basis spline function with many control points. This would distribute the learned function approximation evenly across all hidden units, and thus we have presented the network with a problem in which it could productively use as many hidden units as we give it. In this case, a pruning algorithm would observe a fairly consistent increase in error after the removal of each successive unit. In practice however, regardless of the number of experimental trials, this is not what happens. The network will always use 10-11 hidden units and leave the rest to cancel each other’s influence.
Figure 2 shows two graphs. Both graphs demonstrate the use of the iterative re-ranking algorithm and the comparative performance of the brute-force pruning method (in blue), the first order method (in green), and the second order method (in red). The graph on the left shows the performance of these algorithms starting from a network with two layers of 10 neurons (20 total), and the graph on the right shows a network with two layers of 50 neurons (100 total).
In the left graph, we see that the brute-force method shows a graceful degradation, and the error only begins to rise sharply after 50% of the total neurons have been removed. The error is basically constant up to that point. In the first and second order methods, we see evidence of poor decision making in the sense that both made mistakes early on, which disrupted the output function approximation. The first order method made a large error early on, though we see after a few more neurons were removed this error was corrected somewhat (though it only got worse from there). This is direct evidence of the lack of fault tolerance in a trained neural network. This phenomenon is even more starkly demonstrated in the second order method. After making a few poor neuron removal decisions in a row, the error signal rose sharply, and then went back to zero after the 6th neuron was removed. This is due to the fact that the neurons it chose to remove were trained to cancel each others’ influence within a localized part of the network. After the entire group was eliminated, the approximation returned to normal. This can only happen if the output function approximation is not evenly distributed over the hidden units in a trained network.
This phenomenon is even more starkly demonstrated in the graph on the right. Here we see the first order method got “lucky” in the beginning and made decent decisions up to about the 40th removed neuron. The second order method had a small error in the beginning which it recovered from gracefully and proceeded to pass the 50 neuron point before finally beginning to unravel. The brute force method, in sharp contrast, shows little to no increase in error at all until 90% of the neurons in the network have been obliterated. Clearly first and second order methods have some value in that they do not make completely arbitrary choices, but the brute force method is far better at this task.
This also demonstrates the sharp dualism in neuron roles within a trained network. These networks were trained to near-perfect precision and each pruning method was applied without any re-training of any kind. Clearly, in the case of the brute force or oracle method, up to 90% of the network can be completely extirpated before the output approximation even begins to show any signs of degradation. This would be impossible if the learning representation were evenly or equitably distributed. Note, for example, that the degradation point in both cases is approximately the same. This example is not a real-world application of course, but it brings into very clear focus the kind of phenomena we will discuss in the following sections.

4.2 RESULTS ON MNIST DATASET
For all the results presented in this section, the MNIST database of Handwritten Digits by LeCun & Cortes (2010) was used. It is worth noting that due to the time taken by the brute force algorithm we rather used a 5000 image subset of the MNIST database in which we have normalized the pixel values between 0 and 1.0, and compressed the image sizes to 20x20 images rather than 28x28, so the starting test accuracy reported here appears higher than those reported by LeCun et al. We do not believe that this affects the interpretation of the presented results because the basic learning problem does not change with a larger dataset or input dimension.

4.3 PRUNING A 1-LAYER NETWORK
The network architecture in this case consisted of 1 layer, 100 neurons, 10 outputs, logistic sigmoid activations, and a starting test accuracy of 0.998.

4.3.1 SINGLE OVERALL RANKING ALGORITHM
We first present the results for a single-layer neural network in Figure 3, using the Single Overall algorithm (Algorithm 1) as proposed in Section 3. (We again note that this algorithm is intentionally naive and is used for comparison only. Its performance should be expected to be poor.) After training, each neuron is assigned its permanent ranking based on the three criteria discussed previously: A brute force “ground truth” ranking, and two approximations of this ranking using first and second order Taylor estimations of the change in network output error resulting from the removal of each neuron.
An interesting observation here is that with only a single layer, no criteria for ranking the neurons in the network (brute force or the two Taylor Series variants) using Algorithm 1 emerges superior, indicating that the 1st and 2nd order Taylor Series methods are actually reasonable approximations
of the brute force method under certain conditions. Of course, this method is still quite bad in terms of the rate of degradation of the classification accuracy and in practice we would likely follow Algorithm 2 which is takes into account Mozer & Smolensky (1989a)’s observations stated in the Related Work section. The purpose of the present investigation, however, is to demonstrate how much of a trained network can be theoretically removed without altering the network’s learned parameters in any way.

4.3.2 ITERATIVE RE-RANKING ALGORITHM
In Figure 4 we present our results using Algorithm 2 (The iterative re-ranking Algorithm) in which all remaining neurons are re-ranked after each successive neuron is switched off. We compute the same brute force rankings and Taylor series approximations of error deltas over the remaining active neurons in the network after each pruning decision. This is intended to account for the effects of cancelling interactions between neurons.
There are 2 key observations here. Using the brute force ranking criteria, almost 60% of the neurons in the network can be pruned away without any major loss in performance. The other noteworthy observation here is that the 2nd order Taylor Series approximation of the error performs consistently better than its 1st order version, in most situations, though Figure 21 is a poignant counter-example.

4.3.3 VISUALIZATION OF ERROR SURFACE & PRUNING DECISIONS
As explained in Section 3, these graphs are a visualization of the error surface of the network output with respect to the neurons chosen for removal using each of the 3 ranking criteria, represented in
intervals of 10 neurons. In each graph, the error surface of the network output is displayed in log space (left) and in real space (right) with respect to each candidate neuron chosen for removal. We create these plots during the pruning exercise by picking a neuron to switch off, and then multiplying its output by a scalar gain value α which is adjusted from 0.0 to 10.0 with a step size of 0.001. When the value of α is 1.0, this represents the unperturbed neuron output learned during training. Between 0.0 and 1.0, we are graphing the literal effect of turning the neuron off (α = 0), and when α > 1.0 we are simulating a boosting of the neuron’s influence in the network, i.e. inflating the value of its outgoing weight parameters.
We graph the effect of boosting the neuron’s output to demonstrate that for certain neurons in the network, even doubling, tripling, or quadrupling the scalar output of the neuron has no effect on the overall error of the network, indicating the remarkable degree to which the network has learned to ignore the value of certain parameters. In other cases, we can get a sense of the sensitivity of the network’s output to the value of a given neuron when the curve rises steeply after the red 1.0 line. This indicates that the learned value of the parameters emanating from a given neuron are relatively important, and this is why we should ideally see sharper upticks in the curves for the later-removed neurons in the network, that is, when the neurons crucial to the learning representation start to be picked off. Some very interesting observations can be made in each of these graphs.
Remember that lower is better in terms of the height of the curve and minimal (or negative) horizontal change between the vertical red line at 1.0 (neuron on, α = 1.0) and 0.0 (neuron off, α = 0.0) is indicative of a good candidate neuron to prune, i.e. there will be minimal effect on the network output when the neuron is removed.

4.3.4 VISUALIZATION OF BRUTE FORCE PRUNING DECISIONS
In Figure ??, we notice how low to the floor and flat most of the curves are. It’s only until the 90th removed neuron that we see a higher curve with a more convex shape (clearly a more sensitive, influential piece of the network).

4.3.5 VISUALIZATION OF 1ST ORDER APPROXIMATION PRUNING DECISIONS
It can be seen in Figure 6 that most choices seem to have flat or negatively sloped curves, indicating that the first order approximation seems to be pretty good, but examining the brute force choices shows they could be better.

4.3.6 VISUALIZATION OF 2ND ORDER APPROXIMATION PRUNING DECISIONS
The method in Figure 7 looks similar to the brute force method choices, though clearly not as good (they’re more spread out). Notice the difference in convexity between the 2nd and 1st order method
choices. It’s clear that the first order method is fitting a line and the 2nd order method is fitting a parabola in their approximation.

4.4 PRUNING A 2-LAYER NETWORK
The network architecture in this case consisted of 2 layers, 50 neurons per layer, 10 outputs, logistic sigmoid activations, and a starting test accuracy of 1.000.

4.4.1 SINGLE OVERALL RANKING ALGORITHM
Figure 8 shows the pruning results for Algorithm 1 on a 2-layer network. The ranking procedure is identical to the one used to generate Figure 3. (We again note that this algorithm is intentionally naive and is used for comparison only. Its performance should be expected to be poor.)
Unsurprisingly, a 2-layer network is harder to prune because a single overall ranking will never capture the interdependencies between neurons in different layers. It makes sense that this is worse
than the performance on the 1-layer network, even if this method is already known to be bad, and we’d likely never use it in practice.

4.4.2 ITERATIVE RE-RANKING ALGORITHM
Figure 9 shows the results from using Algorithm 2 on a 2-layer network. We compute the same brute force rankings and Taylor series approximations of error deltas over the remaining active neurons in the network after each pruning decision used to generate Figure 4. Again, this is intended to account for the effects of cancelling interactions between neurons.
It is clear that it becomes harder to remove neurons 1-by-1 with a deeper network (which makes sense because the neurons have more interdependencies in a deeper network), but we see an overall better performance with 2nd order method vs. 1st order, except for the first 20% of the neurons (but this doesn’t seem to make much difference for classification accuracy.)
Perhaps a more important observation here is that even with a more complex network, it is possible to remove up to 40% of the neurons with no major loss in performance which is clearly illustrated by the brute force curve. This shows the clear potential of an ideal pruning technique and also shows how inconsistent 1st and 2nd order Taylor Series approximations of the error can be as ranking criteria.

4.4.3 VISUALIZATION OF ERROR SURFACE & PRUNING DECISIONS
As seen in the case of a single layered network, these graphs are a visualization the error surface of the network output with respect to the neurons chosen for removal using each algorithm, represented in intervals of 10 neurons.

4.4.4 VISUALIZATION OF BRUTE FORCE PRUNING DECISIONS
In Figure 10, it is clear why these neurons got chosen, their graphs clearly show little change when neuron is removed, are mostly near the floor, and show convex behaviour of error surface, which argues for the rationalization of using 2nd order methods to estimate difference in error when they are turned off.

4.4.5 VISUALIZATION OF 1ST ORDER APPROXIMATION PRUNING DECISIONS
Drawing a flat line at the point of each neurons intersection with the red vertical line (no change in gain) shows that the 1st derivative method is actually accurate for estimation of change in error in these cases, but still ultimately leads to poor decisions.

4.4.6 VISUALIZATION OF 2ND ORDER APPROXIMATION PRUNING DECISIONS
Clearly these neurons are not overtly poor candidates for removal (error doesn’t change much between 1.0 & zero-crossing left-hand-side), but could be better (as described above in the brute force Criterion discussion).

4.5 INVESTIGATION OF PRUNING PERFORMANCE WITH IMPERFECT STARTING CONDITIONS
In our experiments thus far we have tacitly assumed that we start with a network which has learned an “optimal” representation of the training objective, i.e. it has been trained to the point where we accept its performance on the test set. Here we explore what happens when we prune with a sub-optimal starting network.
If the assumptions of this paper regarding the nature of neural network learning are correct, we expect that two processes are essentially at work during back-propagation training. First, we expect that the neurons which directly participate in the fundamental learning representation (even if redundantly) work together to reduce error on the training data. Second, we expect that neurons which do not directly participate in the learning representation work to cancel each other’s negative influence. Furthermore, we expect that these two groups are essentially distinct, as evinced by the fact that multiple neurons can often be removed as a group with little to no effect on the network output. Some non-trivial portion of the training time, then, is spent doing work which has nothing intrinsically to do with the learning representation and essentially functions as noise cancellation.
If this is the case, when we attempt to prune a network which has not fully canceled the noisy influence of extraneous or redundant units, we might expect to see the error actually improve after removing a few bad apples. This is in fact what we observe, as demonstrated in the following experiments.
For each experiment in this section we trained with the full MNIST training set (LeCun & Cortes (2010)), uncompressed and without any data normalization. We trained three different networks to learn to distinguish a single handwritten digit from the rest of the data. The network architectures were each composed of 784 inputs, 1 hidden layer with 100 neurons, and 2 soft-max outputs; one to say yes, and the other to say no. These networks were trained to distinguish the digits 0, 1, and 2, and their respective starting accuracies were a sub-optimal 0.9757, 0.9881, and 0.9513. Finally, we only consider the iterative re-ranking algorithm, as the single overall ranking algorithm is clearly nonviable.

4.5.1 MNIST SINGLE DIGIT CLASSIFICATION: DIGIT 0
Figure 13 shows the degradation in squared error after removing neurons from a network trained to distinguish the digit 0. What we observe is that the first and second order methods both fail in different ways, though clearly the second order method makes better decisions overall. The first order method explodes spectacularly in the first few iterations. The brute force method, in stark contrast, actually improves in the first few iterations, and remains essentially flat until around the 60% mark, at which point it begins to gradually increase and meet the other curves.
The behavior of the brute force method here demonstrates that the network was essentially working to cancel the effect of a few bad neurons when the training convergence criteria were met, i.e. the network was no longer able to make progress on the training set. After removing these neurons during pruning, the output improved. We can investigate this by looking at the error surface with respect to the neurons chosen for removal by each method in turn. Below in Figure 14 is the graph of the brute force method.
Figure 14 shows an interesting phenomenon, which we will see in later experiments as well. The high blue curve corresponding to neuron 0 is negatively sloped in the beginning and clearly after removing this neuron, the output will improve. The rest of the curves, in correspondence with the squared error degradation curve above, are mostly flat and tightly layered together, indicating that they are good neurons to remove.
In Figure 15 below, we observe a stark contrast to this. The curves corresponding to neurons 0 and 10 are mostly flat, and fairly lower than the rest, though clearly a mistake was made early on and the rest of the curves are clearly bad choices. In all of these cases however, we see that the curves are
easily approximated with a straight line and so the first order method may have been fairly accurate in its predictions, even though it still made poor decisions.
Figure 15 is an example of how things can go south once a few bad mistakes are made at the outset. Figure 16 shows a much better set of choices made by the second order method, though clearly not as good as the brute force method. The log-space plots make it a bit easier to see the difference between the brute force and second order methods in Figures 14 and 16, respectively.

4.5.2 MNIST SINGLE DIGIT CLASSIFICATION: DIGIT 1
Examining Figure 17, we see a much starker example of the previous phenomenon, in which the brute force method continues to improve the performance of the network after removing 80% of the neurons in the network. The first and second order methods fail early and proceed in fits and starts (clearly demonstrating evidence of interrelated groups of noise-canceling neurons), and never fully recover. It should be noted that it would be impossible to see curves like this if neural networks evenly distributed the learning representation evenly or equitably over their hidden units.
One of the most striking things about the blue curve in Figure 17 is the fact that the network never drops below its starting error until it crosses the 80% mark, indicating that only 20% of the neurons in this network are actually essential to the learning the training objective. In this sense, we can only
wonder how much of the training time was spent winnowing the error out of the remaining 80% of the network.
In Figures 18, 19 and 20 we can examine the choices made by the respective methods. The brute force method serves as our example of a near-optimal pruning regimen, and the rest are first and second order approximations of this. Small differences, clearly, can lead to large effects on the network output as shown in Figure 17.

4.5.3 MNIST SINGLE DIGIT CLASSIFICATION: DIGIT 2
Figure 21 is an interesting case because it shatters our confidence in the reliability of the second order method to make good pruning decisions, and further demonstrates the phenomenon of how much the error can improve if the right neurons are removed after training gets stuck. In this case, though still a poor performance overall, the first order method vastly outperforms the second order method.
Figure 22 shows us a clear example of the first element to remove having a negative error slope, and improving the output as a result. The rest of the pruning decisions are reasonable. Comparing with the blue curve in Figure 21, we see the correspondence between the first pruning decision improving the output, and the remaining pruning decisions keeping the output fairly flat. Clearly, however, there isn’t much room to get worse given our starting point with a sub-optimal network, and we see that the ending sum of squared errors is not much higher than the starting point. At the same time, we can still see the contrast in performance if we make optimal pruning decisions, and most of the neurons in this network were clearly doing nothing.
In Figure 23, we see a mixed bag in which the decisions are clearly sub-optimal, though much better than Figure 24, in which we can observe how a bad first decision essentially ruined the network for good. The jagged edges of the red curve in Figure 21 correspond with the positive and negative
slopes of the cluster of bad pruning decisions in 24. Once again, these are not necessarily bad decisions, but the starting point is already bad and this cannot be recovered without re-training the network.

4.5.4 ASIDE: IMPLICATIONS OF THIS EXPERIMENT
From the three examples above, we see that in each case, starting from a sub-optimal network, a brute force removal technique consistently improves performance for the first few pruning iterations, and the sum of squared errors does not degrade beyond the starting point until around 60-80% of the neurons have been removed. This is only possible if we have an essentially strict dichotomy between the roles of different neurons during training. If the network needs only 20-40% of the neurons it began with, the training process is essentially dominated by the task of canceling the residual noise of redundant neurons. Furthermore, the network can get stuck in training with redundant units and distort the final output. This is strong evidence of our thesis that the learning representation is neither equitably nor evenly distributed and that most of the neurons which do not directly participate in the learning representation can be removed without any retraining.

4.6 EXPERIMENTS ON TOY DATASETS
As can be seen from the experiments on MNIST, even though the 2nd-order approximation criterion is consistently better than 1st-order, its performance is not nearly as good as brute force based rank-
ing, especially beyond the first layer. What is interesting to note is that from some other experiments conducted on toy datasets (predicting whether a given point would lie inside a given shape on the Cartesian plane), the performance of the 2nd-order method was found to be exceptionally good and produced results very close to the brute force method. The 1st-order method, as expected, performed poorly here as well. Some of these results are illustrated in Figure 25.

5 CONCLUSIONS & FUTURE WORK
In conclusion, we must first re-assert that we do not present this work as a bench-marking study of the algorithm we derived and tested. We have merely used this algorithm as a jumping off point to investigate the nature of learning representations in neural networks. What we discovered is that first and second order methods do not make particularly good pruning decisions, and can get hopelessly lost after making a bad pruning decision resulting in a network fault. Furthermore, the brute-force algorithm does surprisingly well, despite being computationally expensive. This method does so well in fact, we argue that further investigation is warranted to make this algorithm computationally tractable, though we do not speculate on how that should be done here.
We also observed strong evidence for the hypotheses of Mozer & Smolensky (1989a) regarding the “dualist” nature of hidden units, i.e. that learning representations are divided between units which either participate in the output approximation or learn to cancel each others influence. This suggests that neural networks may in fact learn a minimal network implicitly, though we cannot say for sure that this is the case without further investigation. A necessary experiment to this end would be to compare the size of network constructed using cascade correlation (Fahlman & Lebiere (1989)) and compare it to the results described herein.
We have presented a novel algorithm for pruning whole neurons from a trained neural network using a second-order Taylor series approximation of the change in error resulting from the removal a given neuron as a pruning criteria. We compared this method to a first order method and a bruteforce serial removal method which exhaustively found the next best single neuron to remove at each stage. Our algorithm relies on a combination of assumptions similar to the ones made by Mozer & Smolensky (1989a) and LeCun et al. (1989) in the formulation of the Skeletonization and Optimal Brain Damage algorithms.
First, we assumed that the error function with respect to each individual neuron can be approximated with a straight line or more precisely with a parabola. Second, for second derivative terms we consider only the diagonal elements of the Hessian matrix, i.e. we assume that each neuron-weight connection can be treated independently of the other elements in the network. Third, we assumed that pruning could be done in a serial fashion in which we find the single least productive element in the network, remove it, and move on. We found that all of these assumptions are deeply flawed in the sense that the true relevance of a neuron can only be partially approximated by a first or second order method, and only at certain stages of the pruning process.
For most problems, these methods can usually remove between 10-30% of the neurons in a trained network, but beyond this point their reliability breaks down. For certain problems, none of the described methods seem to perform very well, though for obvious reasons the brute-force method always exhibits the best results. The reason for this is that the error function with respect to each hidden unit is more complex than a simple second-order Taylor series can approximate. Furthermore, we have not directly taken into account the interdependence of elements within a network, though the work of Hassibi & Stork (1993) could provide some guidance in this regard. This is another critical issue to investigate in the future.
Re-training may help in this regard. We freely admit that our algorithm does not use re-training to recover from errors made in pruning decisions. We argue that evaluating a network pruning algorithm using re-training does not allow us to make fair comparisons between the kinds of decisions made by these algorithms. Neural networks are very good at recovering from the removal of individual elements with re-training and so this compensates for sub-optimal pruning criteria.
We have observed that pruning whole neurons from an optimally trained network without major loss in performance is not only possible but also enables compressing networks to 40-70% of their original size, which is of great importance in constrained memory environments like embedded devices. We cite the results of our experiments using the brute force criterion as evidence of this conclusion. However expensive, it would be extremely easy to parallelize this method, or potentially approximate it using a subset of the training data to decide which neurons to prune. This avoids the problem of trying to approximate the importance of a unit and potentially making a mistake.
It would also be interesting to see how these methods perform on deeper networks and on some other popular and real world datasets. In our case, on the MNIST dataset, we observed that it was more difficult to prune neurons from a deeper network than from one with a single layer. We should expect
this trend to continue as networks get deeper and deeper, which also calls into further question the reliability of the described first and second order methods. We did investigate the order in which neurons were plucked from each layer of the networks and we found that the brute force method primarily removes neurons from the deepest layer of the network first, but there was no obvious pattern in layer preference for the other two methods.
Our experiments using the visualization of error surfaces and pruning decisions concretely establish the fact that not all neurons in a network contribute to its performance in the same way, and the observed complexity of these functions demonstrates limitations of the approximations we used.
Finally, we encourage the readers of this work to take these results into consideration when making decisions as to which methods to use to improve network generalization or compress their models. It should be remembered that various heuristics may perform well in practice for reasons which are in fact orthogonal to the accepted justifications given by their proponents.
","How much can pruning algorithms teach us about the fundamentals of learning representations in neural networks? A lot, it turns out. Neural network model compression has become a topic of great interest in recent years, and many different techniques have been proposed to address this problem. In general, this is motivated by the idea that smaller models typically lead to better generalization. At the same time, the decision of what to prune and when to prune necessarily forces us to confront our assumptions about how neural networks actually learn to represent patterns in data. In this work we set out to test several long-held hypotheses about neural network learning representations and numerical approaches to pruning. To accomplish this we first reviewed the historical literature and derived a novel algorithm to prune whole neurons (as opposed to the traditional method of pruning weights) from optimally trained networks using a second-order Taylor method. We then set about testing the performance of our algorithm and analyzing the quality of the decisions it made. As a baseline for comparison we used a first-order Taylor method based on the Skeletonization algorithm and an exhaustive brute-force serial pruning algorithm. Our proposed algorithm worked well compared to a first-order method, but not nearly as well as the brute-force method. Our error analysis led us to question the validity of many widely-held assumptions behind pruning algorithms in general and the trade-offs we often make in the interest of reducing computational complexity. We discovered that there is a straightforward way, however expensive, to serially prune 40-70% of the neurons in a trained network with minimal effect on the learning representation and without any re-training.",ICLR 2017 conference submission,False,,"The paper introduces a new pruning method for neural networks based on the second-order Taylor expansion and compares the results against a first-order method and brute-force pruning. It performs experiments of the three methods on several toy examples - including a two-layer network on MNIST - and shows that the second-order method behaves much worse then the brute-force baseline. In addition, from the success of the brute-force pruning the authors conclude that the hypothesis of Mozer et al - that neurons either contribute to performance or cancel out the effect of other neurons - is probably correct.

The authors put in considerable effort to explain all details of the paper clearly and at length, so the content of the paper is accessible even to people novel to pruning methods. Additionally, the authors have very carefully answered all questions that were coming up through the pre-review and have been very responsive.

My major criticism is that the paper lacks focus, does not have a concrete conclusion and does not explain what it adds to the literature. To make this apparent, I here summarise each paragraph of the conclusion section:

Paragraph 1: We do not benchmark / Pruning methods do not fare well against brute-force baseline / Some evidence for hypothesis of Mozer & Smolensky, but further investigation needed

Paragraph 2: Introduced 2nd order Taylor method / Does not fare well against baseline

Paragraph 3: Re-training may help but is not fair

Paragraph 4: Brute-force can prune 40-70% in shallow networks

Paragraph 5: Brute-force less effective in deep networks

Paragraph 6: Not all neurons contribute equally to performance of network

The title of the paper and answers of the authors to the pre-review questions seemed to strongly suggest that the paper is not about the new second-order method, is not about benchmarking pruning algorithms but is instead about the learnt representations. But only two or three sentences in the conclusion, and no sentence in the part on results in the abstract, even refers to neural representations. In an answer to the pre-review questions the authors stated:

> Furthermore, we do not have to accept the conclusion that re-training is a necessary part of pruning because a brute force search reveals that neurons can in fact be 
> pruned from trained networks in a piecemeal fashion with no retraining and minimal adverse effect on the overall performance of the network. This would be 
> impossible if neurons did not belong to the distinct classes we describe.""

But this can already be concluded from the 2nd order method, which has a similar characteristic and is based on other 2nd order methods (not shown here). What is the motivation to introduce a new 2nd order method here?

In addition, some other minor conclusions about representations - in particular the cancellation effect - might be based on side-effects of the greedy serial pruning method. Optimally, one would need to consider all the different ways of pruning (which, of course, scales exponentially with the number of neurons and is computationally infeasible). Notably, the authors do consider this limitation in the context of conventional pruning methods in the conclusions: ""Third, we assumed that pruning could be done in a serial fashion [...]. We found that all of these assumptions are deeply flawed in the sense that the true relevance of a neuron can only be partially approximated [...] at certain stages of the pruning process"". But the brute-force pruning process is also serial - why is that not a problem?

All in all it is unclear to me what the paper adds: there are little conclusions regarding the learnt representations nor is there sufficient benchmarking against state-of-the-art pruning methods. I would suggest to focus the paper in the following way: first, use a state-of-the-art pruning method from the literature (that works without re-training) or do not use any other pruning methods besides brute-force (depending on whether you want to compare pruning methods against brute-force, or want to learn something about the learnt representations). In this way you need to write little about this second-order tuning methods, and readers are not so easily confused about the purpose of this paper (plus it will be considerably shorter!). Then concentrate on 2-layer MNIST and a deeper CIFAR10 network. Further focus the paper by adding an itemised list of the exact contributions that you make, and streamline the paper accordingly. These measures could strongly boost the impact of your work but will require a major revision.

PS: I think the confusion starts with the following sentence in the abstract: ""In this work we set out to test several long-held hypothesis about neural network learning representations and numerical approaches to pruning."" Both aspects are pretty orthogonal, but are completely mixed up in the paper.

---

The paper does not seem to have enough novelty, and the contribution is not clear enough due to presentation issues.

---

1) Wen, Wei, et al. ""Learning structured sparsity in deep neural networks."" Advances in Neural Information Processing Systems. 2016.
2) Lebedev, Vadim, and Victor Lempitsky. ""Fast convnets using group-wise brain damage."" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016.
3) Alvarez, Jose M., and Mathieu Salzmann. ""Learning the Number of Neurons in Deep Networks."" Advances in Neural Information Processing Systems. 2016.

---

The authors have put forward a sincere effort to investigate the ""fundamental nature of learning representations in neural networks"", a topic of great interest and importance to our field.  They propose to do this via a few simplistic pruning algorithms, to essentially monitor performance decay as a function of unit pruning.  This is an interesting idea and one that could potentially be instructive, though in total I don't think that has been achieved here.  

First, I find the introduction of pruning lengthy and not particularly novel or surprising.  For example, Fig 1 is not necessary, nor is most of the preamble section 3.3.0.  The pruning algorithms themselves are sensible (though overly simplistic) approaches, which of course would not matter if they were effective in addressing the question.  However, in looking for contributions this paper makes, an interesting, pithy, or novel take on pruning is not one of them, in my opinion.

Second, and most relevant to my overall rating, Section 4 does not get deeper than scratching the surface.  The figures do not offer much beyond the expected decay in performance as a percentage of neurons removed or gain value.  The experiments themselves are not particularly deep, covering a toy problem and MNIST, which does not convince me that I can draw lessons to the broader story of neural networks more generally.  

Third, there is no essential algorithmic, architectural, or mathematical insight, which I expect out of all but the most heavily experimental papers.

---

I did enjoy reading some of the introductions and background, in particular that of reminding readers of popular papers from the late 1980s and early 1990s. The idea of the proposal is straight forward: remove neurons based on the estimated change in the loss function from the packpropagation estimate with either first or second order backpropagation. The results are as expected that the first order method is worse then the second order method which in turn is worse than the brute force method.

However, there are many reasons why I think that this work is not appropriate for ICLR. For one, there is now a much stronger comprehension of weight decay algorithms and their relation to Bayesian priors which has not been mentioned at all. I would think that any work in this regime would require at least some comments about this. Furthermore, there are many statements in the text that are not necessarily true, in particular in light of deep networks with modern regularization methods. For example, the authors state that the most accurate method is what they call brute-force. However, this assumes that the effects of each neurons are independent which might not be the case. So the serial order of removal is not necessarily the best. 

I also still think that this paper is unnecessarily long and the idea and the results could have been delivered in a much compressed way. I also don’t think just writing a Q&A section is not enough, and the points should be included in the paper.

---

The paper introduces a new pruning method for neural networks based on the second-order Taylor expansion and compares the results against a first-order method and brute-force pruning. It performs experiments of the three methods on several toy examples - including a two-layer network on MNIST - and shows that the second-order method behaves much worse then the brute-force baseline. In addition, from the success of the brute-force pruning the authors conclude that the hypothesis of Mozer et al - that neurons either contribute to performance or cancel out the effect of other neurons - is probably correct.

The authors put in considerable effort to explain all details of the paper clearly and at length, so the content of the paper is accessible even to people novel to pruning methods. Additionally, the authors have very carefully answered all questions that were coming up through the pre-review and have been very responsive.

My major criticism is that the paper lacks focus, does not have a concrete conclusion and does not explain what it adds to the literature. To make this apparent, I here summarise each paragraph of the conclusion section:

Paragraph 1: We do not benchmark / Pruning methods do not fare well against brute-force baseline / Some evidence for hypothesis of Mozer & Smolensky, but further investigation needed

Paragraph 2: Introduced 2nd order Taylor method / Does not fare well against baseline

Paragraph 3: Re-training may help but is not fair

Paragraph 4: Brute-force can prune 40-70% in shallow networks

Paragraph 5: Brute-force less effective in deep networks

Paragraph 6: Not all neurons contribute equally to performance of network

The title of the paper and answers of the authors to the pre-review questions seemed to strongly suggest that the paper is not about the new second-order method, is not about benchmarking pruning algorithms but is instead about the learnt representations. But only two or three sentences in the conclusion, and no sentence in the part on results in the abstract, even refers to neural representations. In an answer to the pre-review questions the authors stated:

> Furthermore, we do not have to accept the conclusion that re-training is a necessary part of pruning because a brute force search reveals that neurons can in fact be 
> pruned from trained networks in a piecemeal fashion with no retraining and minimal adverse effect on the overall performance of the network. This would be 
> impossible if neurons did not belong to the distinct classes we describe.""

But this can already be concluded from the 2nd order method, which has a similar characteristic and is based on other 2nd order methods (not shown here). What is the motivation to introduce a new 2nd order method here?

In addition, some other minor conclusions about representations - in particular the cancellation effect - might be based on side-effects of the greedy serial pruning method. Optimally, one would need to consider all the different ways of pruning (which, of course, scales exponentially with the number of neurons and is computationally infeasible). Notably, the authors do consider this limitation in the context of conventional pruning methods in the conclusions: ""Third, we assumed that pruning could be done in a serial fashion [...]. We found that all of these assumptions are deeply flawed in the sense that the true relevance of a neuron can only be partially approximated [...] at certain stages of the pruning process"". But the brute-force pruning process is also serial - why is that not a problem?

All in all it is unclear to me what the paper adds: there are little conclusions regarding the learnt representations nor is there sufficient benchmarking against state-of-the-art pruning methods. I would suggest to focus the paper in the following way: first, use a state-of-the-art pruning method from the literature (that works without re-training) or do not use any other pruning methods besides brute-force (depending on whether you want to compare pruning methods against brute-force, or want to learn something about the learnt representations). In this way you need to write little about this second-order tuning methods, and readers are not so easily confused about the purpose of this paper (plus it will be considerably shorter!). Then concentrate on 2-layer MNIST and a deeper CIFAR10 network. Further focus the paper by adding an itemised list of the exact contributions that you make, and streamline the paper accordingly. These measures could strongly boost the impact of your work but will require a major revision.

PS: I think the confusion starts with the following sentence in the abstract: ""In this work we set out to test several long-held hypothesis about neural network learning representations and numerical approaches to pruning."" Both aspects are pretty orthogonal, but are completely mixed up in the paper.

---

Here are answers to some common questions the authors have been asked about the current work in the past by readers of the manuscript. We hope these will help clarify any other questions our reviewers/readers might have.

Q: Why doesn't the paper present numerical comparision to state-of-the-art/recent pruning techniques?

A: Under certain motivational assumptions, it is understandable to demand benchmarking comparisons against state-of-the-art methods, but this may be missing the fundamental purpose of the present research. Our investigation is intended less to propose a competing alternative to existing pruning techniques and more to shed light on the limitations of generally accepted approaches to pruning and the degree to which increased numbers of parameters affect learning representations in neural networks. The paper does talk about most, if not all popoular pruning techniques out there. In fact, we examined the literature for numerical methods to approximate the importance of network elements, and the widely-cited 1st & 2nd order techniques proposed by Mozer, LeCun, Hassibi, Stork, et al. provided our initial inspiration. This is the jumping off point for our research in terms of key insights.

Q: The idea of using Taylor series approximations seems interesting but not really effective.

A: It is not effective when used as a pruning technique but it is VERY effective to test out the effectiveness of existing pruning techniques, which is what we do here. We have mentioned it multiple times in the paper that the motivation behind this work is NOT to propose a new pruning technique that will outperform all other techniques out there but to tap into learning representations to see how effective our established techniques are when seen from the perspective of representations. The Taylor series approximations play an important role here. A lot of pruning techniques out there use 2nd Order error gradients and assume that using them is the most effective way to prune networks. We have conclusively proved using the Taylor series that this is very much not the case. Our results with the brute-force method show us that there is a much larger extent to which networks can be pruned. This makes for a great starting-off point for future research to find methods that can produce similar results.

Q: Why did you decide in favor of sigmoid activation functions instead of something more recent and more popular like ReLUs? 

A: As mentioned above, the main contribution of this work is to demonstrate the feasibility of pruning entire neurons from trained networks, and offer novel insight on learning representations. We use Taylor methods to approximate the results achieved by the brute-force method but this is not an ideal solution to the problem, as we discuss. The 2nd order approximation technique will not work for ReLU networks because ReLUs do not have a 2nd derivative, unless we use the soft-plus function as a continuous approximation. Furthermore, due to the fact that we are approximating the error surface of a network element with respect to the output using a parabola, if there is no useful parabola to approximate this relationship, then the method breaks down. The derivatives of the activation function are simply parameters of the Taylor series. It doesn’t cease to be a parabolic approximation or become more effective if we use a different doubly-differentiable activation function. 

Q: Why carry out your experiments on the MNIST dataset and not go for a larger and more practical image dataset?

A: All experiments were necessarily carried out on optimally trained networks (not counting Section 4.5, which specifically examines non-optimally trained networks), so there is no way to improve them. We derived the algorithm assuming the well-studied sigmoid activation function. Furthermore, the MNIST dataset is a de-facto standard for demonstrating the potential of new techniques. A different dataset, task, activation function, or network architecture will not change the trends we see in the results but could make the results less interpretable. 

Q: The best setting is Iterative Re-ranking with Brute Force removal which is too expensive.

A: The brute-force method is highly parallelizable, so time complexity is not necessarily a deal-breaker. Our focus is the proof of concept, and we intend to investigate potential speedups in future work. Also, since pruning is anyways a single step carried out after the training process is over (which usually takes orders of magnitude more time), this is potentially acceptable.

---

First Revision: 27 November, 2016. Added Section 4.5: Investigation of Pruning Performance with Imperfect Starting Conditions. We discuss the impact of pruning sub-optimally trained networks in this section by specifically analyzing performance of networks classifying the digits 0, 1 and 2 of the MNIST database.

---

The paper introduces a new pruning method for neural networks based on the second-order Taylor expansion and compares the results against a first-order method and brute-force pruning. It performs experiments of the three methods on several toy examples - including a two-layer network on MNIST - and shows that the second-order method behaves much worse then the brute-force baseline. In addition, from the success of the brute-force pruning the authors conclude that the hypothesis of Mozer et al - that neurons either contribute to performance or cancel out the effect of other neurons - is probably correct.

The authors put in considerable effort to explain all details of the paper clearly and at length, so the content of the paper is accessible even to people novel to pruning methods. Additionally, the authors have very carefully answered all questions that were coming up through the pre-review and have been very responsive.

My major criticism is that the paper lacks focus, does not have a concrete conclusion and does not explain what it adds to the literature. To make this apparent, I here summarise each paragraph of the conclusion section:

Paragraph 1: We do not benchmark / Pruning methods do not fare well against brute-force baseline / Some evidence for hypothesis of Mozer & Smolensky, but further investigation needed

Paragraph 2: Introduced 2nd order Taylor method / Does not fare well against baseline

Paragraph 3: Re-training may help but is not fair

Paragraph 4: Brute-force can prune 40-70% in shallow networks

Paragraph 5: Brute-force less effective in deep networks

Paragraph 6: Not all neurons contribute equally to performance of network

The title of the paper and answers of the authors to the pre-review questions seemed to strongly suggest that the paper is not about the new second-order method, is not about benchmarking pruning algorithms but is instead about the learnt representations. But only two or three sentences in the conclusion, and no sentence in the part on results in the abstract, even refers to neural representations. In an answer to the pre-review questions the authors stated:

> Furthermore, we do not have to accept the conclusion that re-training is a necessary part of pruning because a brute force search reveals that neurons can in fact be 
> pruned from trained networks in a piecemeal fashion with no retraining and minimal adverse effect on the overall performance of the network. This would be 
> impossible if neurons did not belong to the distinct classes we describe.""

But this can already be concluded from the 2nd order method, which has a similar characteristic and is based on other 2nd order methods (not shown here). What is the motivation to introduce a new 2nd order method here?

In addition, some other minor conclusions about representations - in particular the cancellation effect - might be based on side-effects of the greedy serial pruning method. Optimally, one would need to consider all the different ways of pruning (which, of course, scales exponentially with the number of neurons and is computationally infeasible). Notably, the authors do consider this limitation in the context of conventional pruning methods in the conclusions: ""Third, we assumed that pruning could be done in a serial fashion [...]. We found that all of these assumptions are deeply flawed in the sense that the true relevance of a neuron can only be partially approximated [...] at certain stages of the pruning process"". But the brute-force pruning process is also serial - why is that not a problem?

All in all it is unclear to me what the paper adds: there are little conclusions regarding the learnt representations nor is there sufficient benchmarking against state-of-the-art pruning methods. I would suggest to focus the paper in the following way: first, use a state-of-the-art pruning method from the literature (that works without re-training) or do not use any other pruning methods besides brute-force (depending on whether you want to compare pruning methods against brute-force, or want to learn something about the learnt representations). In this way you need to write little about this second-order tuning methods, and readers are not so easily confused about the purpose of this paper (plus it will be considerably shorter!). Then concentrate on 2-layer MNIST and a deeper CIFAR10 network. Further focus the paper by adding an itemised list of the exact contributions that you make, and streamline the paper accordingly. These measures could strongly boost the impact of your work but will require a major revision.

PS: I think the confusion starts with the following sentence in the abstract: ""In this work we set out to test several long-held hypothesis about neural network learning representations and numerical approaches to pruning."" Both aspects are pretty orthogonal, but are completely mixed up in the paper.

---

The paper does not seem to have enough novelty, and the contribution is not clear enough due to presentation issues.

---

1) Wen, Wei, et al. ""Learning structured sparsity in deep neural networks."" Advances in Neural Information Processing Systems. 2016.
2) Lebedev, Vadim, and Victor Lempitsky. ""Fast convnets using group-wise brain damage."" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016.
3) Alvarez, Jose M., and Mathieu Salzmann. ""Learning the Number of Neurons in Deep Networks."" Advances in Neural Information Processing Systems. 2016.

---

The authors have put forward a sincere effort to investigate the ""fundamental nature of learning representations in neural networks"", a topic of great interest and importance to our field.  They propose to do this via a few simplistic pruning algorithms, to essentially monitor performance decay as a function of unit pruning.  This is an interesting idea and one that could potentially be instructive, though in total I don't think that has been achieved here.  

First, I find the introduction of pruning lengthy and not particularly novel or surprising.  For example, Fig 1 is not necessary, nor is most of the preamble section 3.3.0.  The pruning algorithms themselves are sensible (though overly simplistic) approaches, which of course would not matter if they were effective in addressing the question.  However, in looking for contributions this paper makes, an interesting, pithy, or novel take on pruning is not one of them, in my opinion.

Second, and most relevant to my overall rating, Section 4 does not get deeper than scratching the surface.  The figures do not offer much beyond the expected decay in performance as a percentage of neurons removed or gain value.  The experiments themselves are not particularly deep, covering a toy problem and MNIST, which does not convince me that I can draw lessons to the broader story of neural networks more generally.  

Third, there is no essential algorithmic, architectural, or mathematical insight, which I expect out of all but the most heavily experimental papers.

---

I did enjoy reading some of the introductions and background, in particular that of reminding readers of popular papers from the late 1980s and early 1990s. The idea of the proposal is straight forward: remove neurons based on the estimated change in the loss function from the packpropagation estimate with either first or second order backpropagation. The results are as expected that the first order method is worse then the second order method which in turn is worse than the brute force method.

However, there are many reasons why I think that this work is not appropriate for ICLR. For one, there is now a much stronger comprehension of weight decay algorithms and their relation to Bayesian priors which has not been mentioned at all. I would think that any work in this regime would require at least some comments about this. Furthermore, there are many statements in the text that are not necessarily true, in particular in light of deep networks with modern regularization methods. For example, the authors state that the most accurate method is what they call brute-force. However, this assumes that the effects of each neurons are independent which might not be the case. So the serial order of removal is not necessarily the best. 

I also still think that this paper is unnecessarily long and the idea and the results could have been delivered in a much compressed way. I also don’t think just writing a Q&A section is not enough, and the points should be included in the paper.

---

The paper introduces a new pruning method for neural networks based on the second-order Taylor expansion and compares the results against a first-order method and brute-force pruning. It performs experiments of the three methods on several toy examples - including a two-layer network on MNIST - and shows that the second-order method behaves much worse then the brute-force baseline. In addition, from the success of the brute-force pruning the authors conclude that the hypothesis of Mozer et al - that neurons either contribute to performance or cancel out the effect of other neurons - is probably correct.

The authors put in considerable effort to explain all details of the paper clearly and at length, so the content of the paper is accessible even to people novel to pruning methods. Additionally, the authors have very carefully answered all questions that were coming up through the pre-review and have been very responsive.

My major criticism is that the paper lacks focus, does not have a concrete conclusion and does not explain what it adds to the literature. To make this apparent, I here summarise each paragraph of the conclusion section:

Paragraph 1: We do not benchmark / Pruning methods do not fare well against brute-force baseline / Some evidence for hypothesis of Mozer & Smolensky, but further investigation needed

Paragraph 2: Introduced 2nd order Taylor method / Does not fare well against baseline

Paragraph 3: Re-training may help but is not fair

Paragraph 4: Brute-force can prune 40-70% in shallow networks

Paragraph 5: Brute-force less effective in deep networks

Paragraph 6: Not all neurons contribute equally to performance of network

The title of the paper and answers of the authors to the pre-review questions seemed to strongly suggest that the paper is not about the new second-order method, is not about benchmarking pruning algorithms but is instead about the learnt representations. But only two or three sentences in the conclusion, and no sentence in the part on results in the abstract, even refers to neural representations. In an answer to the pre-review questions the authors stated:

> Furthermore, we do not have to accept the conclusion that re-training is a necessary part of pruning because a brute force search reveals that neurons can in fact be 
> pruned from trained networks in a piecemeal fashion with no retraining and minimal adverse effect on the overall performance of the network. This would be 
> impossible if neurons did not belong to the distinct classes we describe.""

But this can already be concluded from the 2nd order method, which has a similar characteristic and is based on other 2nd order methods (not shown here). What is the motivation to introduce a new 2nd order method here?

In addition, some other minor conclusions about representations - in particular the cancellation effect - might be based on side-effects of the greedy serial pruning method. Optimally, one would need to consider all the different ways of pruning (which, of course, scales exponentially with the number of neurons and is computationally infeasible). Notably, the authors do consider this limitation in the context of conventional pruning methods in the conclusions: ""Third, we assumed that pruning could be done in a serial fashion [...]. We found that all of these assumptions are deeply flawed in the sense that the true relevance of a neuron can only be partially approximated [...] at certain stages of the pruning process"". But the brute-force pruning process is also serial - why is that not a problem?

All in all it is unclear to me what the paper adds: there are little conclusions regarding the learnt representations nor is there sufficient benchmarking against state-of-the-art pruning methods. I would suggest to focus the paper in the following way: first, use a state-of-the-art pruning method from the literature (that works without re-training) or do not use any other pruning methods besides brute-force (depending on whether you want to compare pruning methods against brute-force, or want to learn something about the learnt representations). In this way you need to write little about this second-order tuning methods, and readers are not so easily confused about the purpose of this paper (plus it will be considerably shorter!). Then concentrate on 2-layer MNIST and a deeper CIFAR10 network. Further focus the paper by adding an itemised list of the exact contributions that you make, and streamline the paper accordingly. These measures could strongly boost the impact of your work but will require a major revision.

PS: I think the confusion starts with the following sentence in the abstract: ""In this work we set out to test several long-held hypothesis about neural network learning representations and numerical approaches to pruning."" Both aspects are pretty orthogonal, but are completely mixed up in the paper.

---

Here are answers to some common questions the authors have been asked about the current work in the past by readers of the manuscript. We hope these will help clarify any other questions our reviewers/readers might have.

Q: Why doesn't the paper present numerical comparision to state-of-the-art/recent pruning techniques?

A: Under certain motivational assumptions, it is understandable to demand benchmarking comparisons against state-of-the-art methods, but this may be missing the fundamental purpose of the present research. Our investigation is intended less to propose a competing alternative to existing pruning techniques and more to shed light on the limitations of generally accepted approaches to pruning and the degree to which increased numbers of parameters affect learning representations in neural networks. The paper does talk about most, if not all popoular pruning techniques out there. In fact, we examined the literature for numerical methods to approximate the importance of network elements, and the widely-cited 1st & 2nd order techniques proposed by Mozer, LeCun, Hassibi, Stork, et al. provided our initial inspiration. This is the jumping off point for our research in terms of key insights.

Q: The idea of using Taylor series approximations seems interesting but not really effective.

A: It is not effective when used as a pruning technique but it is VERY effective to test out the effectiveness of existing pruning techniques, which is what we do here. We have mentioned it multiple times in the paper that the motivation behind this work is NOT to propose a new pruning technique that will outperform all other techniques out there but to tap into learning representations to see how effective our established techniques are when seen from the perspective of representations. The Taylor series approximations play an important role here. A lot of pruning techniques out there use 2nd Order error gradients and assume that using them is the most effective way to prune networks. We have conclusively proved using the Taylor series that this is very much not the case. Our results with the brute-force method show us that there is a much larger extent to which networks can be pruned. This makes for a great starting-off point for future research to find methods that can produce similar results.

Q: Why did you decide in favor of sigmoid activation functions instead of something more recent and more popular like ReLUs? 

A: As mentioned above, the main contribution of this work is to demonstrate the feasibility of pruning entire neurons from trained networks, and offer novel insight on learning representations. We use Taylor methods to approximate the results achieved by the brute-force method but this is not an ideal solution to the problem, as we discuss. The 2nd order approximation technique will not work for ReLU networks because ReLUs do not have a 2nd derivative, unless we use the soft-plus function as a continuous approximation. Furthermore, due to the fact that we are approximating the error surface of a network element with respect to the output using a parabola, if there is no useful parabola to approximate this relationship, then the method breaks down. The derivatives of the activation function are simply parameters of the Taylor series. It doesn’t cease to be a parabolic approximation or become more effective if we use a different doubly-differentiable activation function. 

Q: Why carry out your experiments on the MNIST dataset and not go for a larger and more practical image dataset?

A: All experiments were necessarily carried out on optimally trained networks (not counting Section 4.5, which specifically examines non-optimally trained networks), so there is no way to improve them. We derived the algorithm assuming the well-studied sigmoid activation function. Furthermore, the MNIST dataset is a de-facto standard for demonstrating the potential of new techniques. A different dataset, task, activation function, or network architecture will not change the trends we see in the results but could make the results less interpretable. 

Q: The best setting is Iterative Re-ranking with Brute Force removal which is too expensive.

A: The brute-force method is highly parallelizable, so time complexity is not necessarily a deal-breaker. Our focus is the proof of concept, and we intend to investigate potential speedups in future work. Also, since pruning is anyways a single step carried out after the training process is over (which usually takes orders of magnitude more time), this is potentially acceptable.

---

First Revision: 27 November, 2016. Added Section 4.5: Investigation of Pruning Performance with Imperfect Starting Conditions. We discuss the impact of pruning sub-optimally trained networks in this section by specifically analyzing performance of networks classifying the digits 0, 1 and 2 of the MNIST database.",,,,,,3.0,,,4.0,,
691,"Authors: PLAYING SNES, IN THE, RETRO LEARNING ENVIRONMENT, Nadav Bhonker, Shai Rozenberg
Source file: 691.pdf

ABSTRACT
Mastering a video game requires skill, tactics and strategy. While these attributes may be acquired naturally by human players, teaching them to a computer program is a far more challenging task. In recent years, extensive research was carried out in the field of reinforcement learning and numerous algorithms were introduced, aiming to learn how to perform human tasks such as playing video games. As a result, the Arcade Learning Environment (ALE) (Bellemare et al., 2013) has become a commonly used benchmark environment allowing algorithms to train on various Atari 2600 games. In many games the state-of-the-art algorithms outperform humans. In this paper we introduce a new learning environment, the Retro Learning Environment — RLE, that can run games on the Super Nintendo Entertainment System (SNES), Sega Genesis and several other gaming consoles. The environment is expandable, allowing for more video games and consoles to be easily added to the environment, while maintaining the same interface as ALE. Moreover, RLE is compatible with Python and Torch. SNES games pose a significant challenge to current algorithms due to their higher level of complexity and versatility.

1 INTRODUCTION
Controlling artificial agents using only raw high-dimensional input data such as image or sound is a difficult and important task in the field of Reinforcement Learning (RL). Recent breakthroughs in the field allow its utilization in real-world applications such as autonomous driving (Shalev-Shwartz et al., 2016), navigation (Bischoff et al., 2013) and more. Agent interaction with the real world is usually either expensive or not feasible, as the real world is far too complex for the agent to perceive. Therefore in practice the interaction is simulated by a virtual environment which receives feedback on a decision made by the algorithm. Traditionally, games were used as a RL environment, dating back to Chess (Campbell et al., 2002), Checkers (Schaeffer et al., 1992), backgammon (Tesauro, 1995) and the more recent Go (Silver et al., 2016). Modern games often present problems and tasks which are highly correlated with real-world problems. For example, an agent that masters a racing game, by observing a simulated driver’s view screen as input, may be usefull for the development of an autonomous driver. For high-dimensional input, the leading benchmark is the Arcade Learning Environment (ALE) (Bellemare et al., 2013) which provides a common interface to dozens of Atari 2600 games, each presents a different challenge. ALE provides an extensive benchmarking platform, allowing a controlled experiment setup for algorithm evaluation and comparison. The main challenge posed by ALE is to successfully play as many Atari 2600 games as possible (i.e., achieving a score higher than an expert human player) without providing the algorithm any game-specific information (i.e., using the same input available to a human - the game screen and score). A key work to tackle this problem is the Deep Q-Networks algorithm (Mnih et al., 2015), which made a breakthrough in the field of Deep Reinforcement Learning by achieving human level performance on 29 out of 49 games. In this work we present a new environment — the Retro Learning Environment (RLE). RLE sets new challenges by providing a unified interface for Atari 2600 games as well as more advanced gaming consoles. As a start we focused on the Super Nintendo Entertainment
System (SNES). Out of the five SNES games we tested using state-of-the-art algorithms, only one was able to outperform an expert human player. As an additional feature, RLE supports research of multi-agent reinforcement learning (MARL) tasks (Buşoniu et al., 2010). We utilize this feature by training and evaluating the agents against each other, rather than against a pre-configured in-game AI. We conducted several experiments with this new feature and discovered that agents tend to learn how to overcome their current opponent rather than generalize the game being played. However, if an agent is trained against an ensemble of different opponents, its robustness increases. The main contributions of the paper are as follows:
• Introducing a novel RL environment with significant challenges and an easy agent evaluation technique (enabling agents to compete against each other) which could lead to new and more advanced RL algorithms.
• A new method to train an agent by enabling it to train against several opponents, making the final policy more robust.
• Encapsulating several different challenges to a single RL environment.

2 RELATED WORK

2.1 ARCADE LEARNING ENVIRONMENT
The Arcade Learning Environment is a software framework designed for the development of RL algorithms, by playing Atari 2600 games. The interface provided by ALE allows the algorithms to select an action and receive the Atari screen and a reward in every step. The action is the equivalent to a human’s joystick button combination and the reward is the difference between the scores at time stamp t and t− 1. The diversity of games for Atari provides a solid benchmark since different games have significantly different goals. Atari 2600 has over 500 games, currently over 70 of them are implemented in ALE and are commonly used for algorithm comparison.

2.2 INFINITE MARIO
Infinite Mario (Togelius et al., 2009) is a remake of the classic Super Mario game in which levels are randomly generated. On these levels the Mario AI Competition was held. During the competition, several algorithms were trained on Infinite Mario and their performances were measured in terms of the number of stages completed. As opposed to ALE, training is not based on the raw screen data but rather on an indication of Mario’s (the player’s) location and objects in its surrounding. This environment no longer poses a challenge for state of the art algorithms. Its main shortcoming lie in the fact that it provides only a single game to be learnt. Additionally, the environment provides hand-crafted features, extracted directly from the simulator, to the algorithm. This allowed the use of planning algorithms that highly outperform any learning based algorithm.

2.3 OPENAI GYM
The OpenAI gym (Brockman et al., 2016) is an open source platform with the purpose of creating an interface between RL environments and algorithms for evaluation and comparison purposes. OpenAI Gym is currently very popular due to the large number of environments supported by it. For example ALE, Go, MouintainCar and VizDoom (Zhu et al., 2016), an environment for the learning of the 3D first-person-shooter game ”Doom”. OpenAI Gym’s recent appearance and wide usage indicates the growing interest and research done in the field of RL.

2.4 OPENAI UNIVERSE
Universe (Universe, 2016) is a platform within the OpenAI framework in which RL algorithms can train on over a thousand games. Universe includes very advanced games such as GTA V, Portal as well as other tasks (e.g. browser tasks). Unlike RLE, Universe doesn’t run the games locally and requires a VNC interface to a server that runs the games. This leads to a lower frame rate and thus longer training times.

2.5 MALMO
Malmo (Johnson et al., 2016) is an artificial intelligence experimentation platform of the famous game ”Minecraft”. Although Malmo consists of only a single game, it presents numerous challenges since the ”Minecraft” game can be configured differently each time. The input to the RL algorithms include specific features indicating the ”state” of the game and the current reward.

2.6 DEEPMIND LAB
DeepMind Lab (Dee) is a first-person 3D platform environment which allows training RL algorithms on several different challenges: static/random map navigation, collect fruit (a form of reward) and a laser-tag challenge where the objective is to tag the opponents controlled by the in-game AI. In LAB the agent observations are the game screen (with an additional depth channel) and the velocity of the character. LAB supports four games (one game - four different modes).

2.7 DEEP Q-LEARNING
In our work, we used several variant of the Deep Q-Network algorithm (DQN) (Mnih et al., 2015), an RL algorithm whose goal is to find an optimal policy (i.e., given a current state, choose action that maximize the final score). The state of the game is simply the game screen, and the action is a combination of joystick buttons that the game responds to (i.e., moving ,jumping). DQN learns through trial and error while trying to estimate the ”Q-function”, which predicts the cumulative discounted reward at the end of the episode given the current state and action while following a policy π. The Q-function is represented using a convolution neural network that receives the screen as input and predicts the best possible action at it’s output. The Q-function weights θ are updated according to:
θt+1(st, at) = θt + α(Rt+1 + γmax a
(Qt(st+1, a; θ ′ t))−Qt(st, at; θt))∇θQt(st, at; θt), (1)
where st, st+1 are the current and next states, at is the action chosen, α is the step size, γ is the discounting factor Rt+1 is the reward received by applying at at st. θ′ represents the previous weights of the network that are updated periodically. Other than DQN, we examined two leading algorithms on the RLE: Double Deep Q-Learning (D-DQN) (Van Hasselt et al., 2015), a DQN based algorithm with a modified network update rule. Dueling Double DQN (Wang et al., 2015), a modification of D-DQN’s architecture in which the Q-function is modeled using a state (screen) dependent estimator and an action dependent estimator.

3 THE RETRO LEARNING ENVIRONMENT

3.1 SUPER NINTENDO ENTERTAINMENT SYSTEM
The Super Nintendo Entertainment System (SNES) is a home video game console developed by Nintendo and released in 1990. A total of 783 games were released, among them, the iconic Super Mario World, Donkey Kong Country and The Legend of Zelda. Table (1) presents a comparison between Atari 2600, Sega Genesis and SNES game consoles, from which it is clear that SNES and Genesis games are far more complex.

3.2 IMPLEMENTATION
To allow easier integration with current platforms and algorithms, we based our environment on the ALE, with the aim of maintaining as much of its interface as possible. While the ALE is highly coupled with the Atari emulator, Stella1, RLE takes a different approach and separates the learning environment from the emulator. This was achieved by incorporating an interface named LibRetro (libRetro site), that allows communication between front-end programs to game-console emulators. Currently, LibRetro supports over 15 game consoles, each containing hundreds of games, at an estimated total of over 7,000 games that can potentially be supported using this interface. Examples of supported game consoles include Nintendo Entertainment System, Game Boy, N64, Sega Genesis,
1http://stella.sourceforge.net/
Saturn, Dreamcast and Sony PlayStation. We chose to focus on the SNES game console implemented using the snes9x2 as it’s games present interesting, yet plausible to overcome challenges. Additionally, we utilized the Genesis-Plus-GX3 emulator, which supports several Sega consoles: Genesis/Mega Drive, Master System, Game Gear and SG-1000.

3.3 SOURCE CODE
RLE is fully available as open source software for use under GNU’s General Public License4. The environment is implemented in C++ with an interface to algorithms in C++, Python and Lua. Adding a new game to the environment is a relatively simple process.

3.4 RLE INTERFACE
RLE provides a unified interface to all games in its supported consoles, acting as an RL-wrapper to the LibRetro interface. Initialization of the environment is done by providing a game (ROM file) and a gaming-console (denoted by ’core’). Upon initialization, the first state is the initial frame of the game, skipping all menu selection screens. The cores are provided with the RLE and installed together with the environment. Actions have a bit-wise representation where each controller button is represented by a one-hot vector. Therefore a combination of several buttons is possible using the bit-wise OR operator. The number of valid buttons combinations is larger than 700, therefore only the meaningful combinations are provided. The environments observation is the game screen, provided as a 3D array of 32 bit per pixel with dimensions which vary depending on the game. The reward can be defined differently per game, usually we set it to be the score difference between two consecutive frames. By setting different configuration to the environment, it is possible to alter in-game properties such as difficulty (i.e easy, medium, hard), its characters, levels, etc.

3.5 ENVIRONMENT CHALLENGES
Integrating SNES and Genesis with RLE presents new challenges to the field of RL where visual information in the form of an image is the only state available to the agent. Obviously, SNES games are significantly more complex and unpredictable than Atari games. For example in sports games, such as NBA, while the player (agent) controls a single player, all the other nine players’ behavior is determined by pre-programmed agents, each exhibiting random behavior. In addition, many SNES games exhibit delayed rewards in the course of their play (i.e., reward for an actions is given many time steps after it was performed). Similarly, in some of the SNES games, an agent can obtain a reward that is indirectly related to the imposed task. For example, in platform games, such as Super Mario, reward is received for collecting coins and defeating enemies, while the goal of the challenge is to reach the end of the level which requires to move to keep moving to the right. Moreover, upon completing a level, a score bonus is given according to the time required for its completion. Therefore collecting coins or defeating enemies is not necessarily preferable if it consumes too much time. Analysis of such games is presented in section 4.2. Moreover, unlike Atari that consists of
2http://www.snes9x.com/ 3https://github.com/ekeeke/Genesis-Plus-GX 4https://github.com/nadavbh12/Retro-Learning-Environment
eight directions and one action button, SNES has eight-directions pad and six actions buttons. Since combinations of buttons are allowed, and required at times, the actual actions space may be larger than 700, compared to the maximum of 18 actions in Atari. Furthermore, the background in SNES is very rich, filled with details which may move locally or across the screen, effectively acting as non-stationary noise since it provided little to no information regarding the state itself. Finally, we note that SNES utilized the first 3D games. In the game Wolfenstein, the player must navigate a maze from a first-person perspective, while dodging and attacking enemies. The SNES offers plenty of other 3D games such as flight and racing games which exhibit similar challenges. These games are much more realistic, thus inferring from SNES games to ”real world” tasks, as in the case of self driving cars, might be more beneficial. A visual comparison of two games, Atari and SNES, is presented in Figure (1).

4 EXPERIMENTS

4.1 EVALUATION METHODOLOGY
The evaluation methodology that we used for benchmarking the different algorithms is the popular method proposed by (Mnih et al., 2015). Each examined algorithm is trained until either it reached convergence or 100 epochs (each epoch corresponds to 50,000 actions), thereafter it is evaluated by performing 30 episodes of every game. Each episode ends either by reaching a terminal state or after 5 minutes. The results are averaged per game and compared to the average result of a human player. For each game the human player was given two hours for training, and his performances were evaluated over 20 episodes. As the various algorithms don’t use the game audio in the learning process, the audio was muted for both the agent and the human. From both, humans and agents
score, a random agent score (an agent performing actions randomly) was subtracted to assure that learning indeed occurred. It is important to note that DQN’s -greedy approach (select a random action with a small probability ) is present during testing thus assuring that the same sequence of actions isn’t repeated. While the screen dimensions in SNES are larger than those of Atari, in our experiments we maintained the same pre-processing of DQN (i.e., downscaling the image to 84x84 pixels and converting to gray-scale). We argue that downscaling the image size doesn’t affect a human’s ability to play the game, therefore suitable for RL algorithms as well. To handle the large action space, we limited the algorithm’s actions to the minimal button combinations which provide unique behavior. For example, on many games the R and L action buttons don’t have any use therefore their use and combinations were omitted.

4.1.1 RESULTS
A thorough comparison of the four different agents’ performances on SNES games can be seen in Figure (). The full results can be found in Table (3). Only in the game Mortal Kombat a trained agent was able to surpass a expert human player performance as opposed to Atari games where the same algorithms have surpassed a human player on the vast majority of the games.
One example is Wolfenstein game, a 3D first-person shooter game, requires solving 3D vision tasks, navigating in a maze and detecting object. As evident from figure (2), all agents produce poor results indicating a lack of the required properties. By using -greedy approach the agents weren’t able to explore enough states (or even other rooms in our case). The algorithm’s final policy appeared as a random walk in a 3D space. Exploration based on visited states such as presented in Bellemare et al. (2016) might help addressing this issue. An interesting case is Gradius III, a side-scrolling, flight-shooter game. While the trained agent was able to master the technical aspects of the game, which includes shooting incoming enemies and dodging their projectiles, it’s final score is still far from a human’s. This is due to a hidden game mechanism in the form of ”power-ups”, which can be accumulated, and significantly increase the players abilities. The more power-ups collected without being use — the larger their final impact will be. While this game-mechanism is evident to a human, the agent acts myopically and uses the power-up straight away5.

4.2 REWARD SHAPING
As part of the environment and algorithm evaluation process, we investigated two case studies. First is a game on which DQN had failed to achieve a better-than-random score, and second is a game on which the training duration was significantly longer than that of other games.
In the first case study, we used a 2D back-view racing game ”F-Zero”. In this game, one is required to complete four laps of the track while avoiding other race cars. The reward, as defined by the score of the game, is only received upon completing a lap. This is an extreme case of a reward delay. A lap may last as long as 30 seconds, which span over 450 states (actions) before reward is received. Since DQN’s exploration is a simple -greedy approach, it was not able to produce a useful strategy. We approached this issue using reward shaping, essentially a modification of the reward to be a function of the reward and the observation, rather than the reward alone. Here, we define the reward to be the sum of the score and the agent’s speed (a metric displayed on the screen of the game). Indeed when the reward was defined as such, the agents learned to finish the race in first place within a short training period.
The second case study is the famous game of Super Mario. In this game the agent, Mario, is required to reach the right-hand side of the screen, while avoiding enemies and collecting coins. We found this case interesting as it involves several challenges at once: dynamic background that can change drastically within a level, sparse and delayed rewards and multiple tasks (such as avoiding enemies and pits, advancing rightwards and collecting coins). To our surprise, DQN was able to reach the end of the level without any reward shaping, this was possible since the agent receives rewards for events (collecting coins, stomping on enemies etc.) that tend to appear to the right of the player, causing the agent to prefer moving right. However, the training time required for convergence was significantly longer than other games. We defined the reward as the sum of the in-game reward and a bonus granted according the the player’s position, making moving right preferable. This reward
5A video demonstration can be found at https://youtu.be/nUl9XLMveEU
proved useful, as training time required for convergence decreased significantly. The two games above can be seen in Figure (3).
Figure (4) illustrates the agent’s average value function . Though both were able complete the stage trained upon, the convergence rate with reward shaping is significantly quicker due to the immediate realization of the agent to move rightwards.

4.3 MULTI-AGENT REINFORCEMENT LEARNING
In this section we describe our experiments with RLE’s multi-agent capabilities. We consider the case where the number of agents, n = 2 and the goals of the agents are opposite, as in r1 = −r2. This scheme is known as fully competitive (Buşoniu et al., 2010). We used the simple singleagent RL approach (as described by Buşoniu et al. (2010) section 5.4.1) which is to apply to single agent approach to the multi-agent case. This approach was proved useful in Crites and Barto (1996) and Matarić (1997). More elaborate schemes are possible such as the minimax-Q algorithm (Littman, 1994), (Littman, 2001). These may be explored in future works. We conducted three experiments on this setup: the first use was to train two different agents against the in-game AI, as done in previous sections, and evaluate their performance by letting them compete against each other. Here, rather than achieving the highest score, the goal was to win a tournament which consist of 50 rounds, as common in human-player competitions. The second experiment was to initially train two agents against the in-game AI, and resume the training while competing against each other. In this case, we evaluated the agent by playing again against the in-game AI, separately. Finally, in our last experiment we try to boost the agent capabilities by alternated it’s opponents, switching between the in-game AI and other trained agents.

4.3.1 MULTI-AGENT REINFORCEMENT LEARNING RESULTS
We chose the game Mortal Kombat, a two character side viewed fighting game (a screenshot of the game can be seen in Figure (1), as a testbed for the above, as it exhibits favorable properties: both players share the same screen, the agent’s optimal policy is heavily dependent on the rival’s behavior, unlike racing games for example. In order to evaluate two agents fairly, both were trained using the same characters maintaining the identity of rival and agent. Furthermore, to remove the impact of the starting positions of both agents on their performances, the starting positions were initialized randomly.
In the first experiment we evaluated all combinations of DQN against D-DQN and Dueling D-DQN. Each agent was trained against the in-game AI until convergence. Then 50 matches were performed between the two agents. DQN lost 28 out of 50 games against Dueling D-DQN and 33 against D-DQN. D-DQN lost 26 time to Dueling D-DQN. This win balance isn’t far from the random case, since the algorithms converged into a policy in which movement towards the opponent is not
required rather than generalize the game. Therefore, in many episodes, little interaction between the two agents occur, leading to a semi-random outcome.
In our second experiment, we continued the training process of a the D-DQN network by letting it compete against the Dueling D-DQN network. We evaluated the re-trained network by playing 30 episodes against the in-game AI. After training, D-DQN was able to win 28 out of 30 games, yet when faced again against the in-game AI its performance deteriorated drastically (from an average of 17000 to an average of -22000). This demonstrated a form of catastrophic forgetting (Goodfellow et al., 2013) even though the agents played the same game.
In our third experiment, we trained a Dueling D-DQN agent against three different rivals: the ingame AI, a trained DQN agent and a trained Dueling-DQN agent, in an alternating manner, such that in each episode a different rival was playing as the opponent with the intention of preventing the agent from learning a policy suitable for just one opponent. The new agent was able to achieve a score of 162,966 (compared to the ”normal” dueling D-DQN which achieved 169,633). As a new and objective measure of generalization, we’ve configured the in-game AI difficulty to be ”very hard” (as opposed to the default ”medium” difficulty). In this metric the alternating version achieved 83,400 compared to -33,266 of the dueling D-DQN which was trained in default setting. Thus, proving that the agent learned to generalize to other policies which weren’t observed while training.

4.4 FUTURE CHALLENGES
As demonstrated, RLE presents numerous challenges that have yet to be answered. In addition to being able to learn all available games, the task of learning games in which reward delay is extreme, such as F-Zero without reward shaping, remains an unsolved challenge. Additionally, some games, such as Super Mario, feature several stages that differ in background and the levels structure. The task of generalizing platform games, as in learning on one stage and being tested on the other, is another unexplored challenge. Likewise surpassing human performance remains a challenge since current state-of-the-art algorithms still struggling with the many SNES games.

5 CONCLUSION
We introduced a rich environment for evaluating and developing reinforcement learning algorithms which presents significant challenges to current state-of-the-art algorithms. In comparison to other environments RLE provides a large amount of games with access to both the screen and the ingame state. The modular implementation we chose allows extensions of the environment with new consoles and games, thus ensuring the relevance of the environment to RL algorithms for years to come (see Table (2)). We’ve encountered several games in which the learning process is highly dependent on the reward definition. This issue can be addressed and explored in RLE as reward definition can be done easily. The challenges presented in the RLE consist of: 3D interpretation, delayed reward, noisy background, stochastic AI behavior and more. Although some algorithms were able to play successfully on part of the games, to fully overcome these challenges, an agent must incorporate both technique and strategy. Therefore, we believe, that the RLE is a great platform for future RL research.

6 ACKNOWLEDGMENTS
The authors are grateful to the Signal and Image Processing Lab (SIPL) staff for their support, Alfred Agrell and the LibRetro community for their support and Marc G. Bellemare for his valuable inputs.
","Mastering a video game requires skill, tactics and strategy. While these attributes may be acquired naturally by human players, teaching them to a computer program is a far more challenging task. In recent years, extensive research was carried out in the field of reinforcement learning and numerous algorithms were introduced, aiming to learn how to perform human tasks such as playing video games. As a result, the Arcade Learning Environment (ALE) (Bellemare et al., 2013) has become a commonly used benchmark environment allowing algorithms to train on various Atari 2600 games. In many games the state-of-the-art algorithms outperform humans. In this paper we introduce a new learning environment, the Retro Learning Environment — RLE, that can run games on the Super Nintendo Entertainment System (SNES), Sega Genesis and several other gaming consoles. The environment is expandable, allowing for more video games and consoles to be easily added to the environment, while maintaining the same interface as ALE. Moreover, RLE is compatible with Python and Torch. SNES games pose a significant challenge to current algorithms due to their higher level of complexity and versatility.",ICLR 2017 conference submission,False,,"The paper presents a new environment, called Retro Learning Environment (RLE), for reinforcement learning. The authors focus on Super Nintendo but claim that the interface supports many others (including ALE). Benchmark results are given for standard algorithms in 5 new Super Nintendo games, and some results using a new ""rivalry metric"".

These environments (or, more generally, standardized evaluation methods like public data sets, competitions, etc.) have a long history of improving the quality of AI and machine learning research. One example in the past few years was the Atari Learning Environment (ALE) which has now turned into a standard benchmark for comparison of algorithms and results. In this sense, the RLE could be a worthy contribution to the field by encouraging new challenging domains for research.

That said, the main focus of this paper is presenting this new framework and showcasing the importance of new challenging domains. The results of experiments themselves are for existing algorithms. There are some new results that show reward shaping and policy shaping (having a bias toward going right in Super Mario) help during learning. And, yes, domain knowledge helps, but this is obvious. The rivalry training is an interesting idea, when training against a different opponent, the learner overfits to that opponent and forgets to play against the in-game AI; but then oddly, it gets evaluated on how well it does against the in-game AI! 

Also the part of the paper that describes the scientific results (especially the rivalry training) is less polished, so this is disappointing. In the end, I'm not very excited about this paper.

I was hoping for a more significant scientific contribution to accompany in this new environment. It's not clear if this is necessary for publication, but also it's not clear that ICLR is the right venue for this work due to the contribution being mainly about the new code (for example, mloss.org could be a better 'venue', JMLR has an associated journal track for accompanying papers:

---

The authors present a new set of environments, similar to ALE but based on Super Nintendo rather than Atari. This is a great asset and could be important for RL research, but it doesn't merit ICLR publication because of the lack of novel research ideas. Hopefully the authors will consider another venue to publish this paper, such as perhaps a journal or workshop.

---

The paper presents a new environment, called Retro Learning Environment (RLE), for reinforcement learning. The authors focus on Super Nintendo but claim that the interface supports many others (including ALE). Benchmark results are given for standard algorithms in 5 new Super Nintendo games, and some results using a new ""rivalry metric"".

These environments (or, more generally, standardized evaluation methods like public data sets, competitions, etc.) have a long history of improving the quality of AI and machine learning research. One example in the past few years was the Atari Learning Environment (ALE) which has now turned into a standard benchmark for comparison of algorithms and results. In this sense, the RLE could be a worthy contribution to the field by encouraging new challenging domains for research.

That said, the main focus of this paper is presenting this new framework and showcasing the importance of new challenging domains. The results of experiments themselves are for existing algorithms. There are some new results that show reward shaping and policy shaping (having a bias toward going right in Super Mario) help during learning. And, yes, domain knowledge helps, but this is obvious. The rivalry training is an interesting idea, when training against a different opponent, the learner overfits to that opponent and forgets to play against the in-game AI; but then oddly, it gets evaluated on how well it does against the in-game AI! 

Also the part of the paper that describes the scientific results (especially the rivalry training) is less polished, so this is disappointing. In the end, I'm not very excited about this paper.

I was hoping for a more significant scientific contribution to accompany in this new environment. It's not clear if this is necessary for publication, but also it's not clear that ICLR is the right venue for this work due to the contribution being mainly about the new code (for example, mloss.org could be a better 'venue', JMLR has an associated journal track for accompanying papers:

---

This paper introduces a new reinforcement learning environment called « The Retro Learning Environment”, that interfaces with the open-source LibRetro API to offer access to various emulators and associated games (i.e. similar to the Atari 2600 Arcade Learning Environment, but more generic). The first supported platform is the SNES, with 5 games (more consoles and games may be added later). Authors argue that SNES games pose more challenges than Atari’s (due to more complex graphics, AI and game mechanics). Several DQN variants are evaluated in experiments, and it is also proposed to compare learning algorihms by letting them compete against each other in multiplayer games.

I like the idea of going toward more complex games than those found on Atari 2600, and having an environment where new consoles and games can easily be added sounds promising. With OpenAI Universe and DeepMind Lab that just came out, though, I am not sure we really need another one right now. Especially since using ROMs of emulated games we do not own is technically illegal: it looks like this did not cause too much trouble for Atari but it might start raising eyebrows if the community moves to more advanced and recent games, especially some Nintendo still makes money from.

Besides the introduction of the environment, it is good to have DQN benchmarks on five games, but this does not add a lot of value. The authors also mention as contribution ""A new benchmarking technique, allowing algorithms to compete against each other, rather than playing against the in-game AI"", but this seems a bit exaggerated to me: the idea of pitting AIs against each other has been at the core of many AI competitions for decades, so it is hardly something new. The finding that reinforcement learning algorithms tend to specialize to their opponent is also not particular surprising.

Overall I believe this is an ok paper but I do not feel it brings enough to the table for a major conference. This does not mean, however, that this new environment won't find a spot in the (now somewhat crowded) space of game-playing frameworks.

Other small comments:
- There are lots of typos (way too many to mention them all)
- It is said that Infinite Mario ""still serves as a benchmark platform"", however as far as I know it had to be shutdown due to Nintendo not being too happy about it
- ""RLE requires an emulator and a computer version of the console game (ROM file) upon initialization rather than a ROM file only. The emulators are provided with RLE"" => how is that different from ALE that requires the emulator Stella which is also provided with ALE?
- Why is there no DQN / DDDQN result on Super Mario?
- It is not clear if Figure 2 displays the F-Zero results using reward shaping or not
- The Du et al reference seems incomplete

---

This paper presents a valuable new collection of video game benchmarks, in an extendable framework, and establishes initial baselines on a few of them.

Reward structures: for how many of the possible games have you implemented the means to extract scores and incremental reward structures? From the github repo it looks like about 10 -- do you plan to add more, and when?

“rivalry” training: this is one of the weaker components of the paper, and it should probably be emphasised less. On this topic, there is a vast body of (uncited) multi-agent literature, it is a well-studied problem setup (more so than RL itself). To avoid controversy, I would recommend not claiming any novel contribution on the topic (I don’t think that you really invented “a new method to train an agent by enabling it to train against several opponents” nor “a new benchmarking technique for agents evaluation, by enabling them to compete against each other, rather than playing against the in-game AI”). Instead, just explain that you have established single-agent and multi-agent baselines for your new benchmark suite.

Your definition of Q-function (“predicts the score at the end of the game given the current state and selected action”) is incorrect. It should read something like: it estimates the cumulative discounted reward that can be obtained from state s, starting with action a (and then following a certain policy).

Minor:
* Eq (1): the Q-net inside the max() is the target network, with different parameters theta’
* the Du et al. reference is missing the year
* some of the other references should point at the corresponding published papers instead of the arxiv versions

---

The paper presents a new environment, called Retro Learning Environment (RLE), for reinforcement learning. The authors focus on Super Nintendo but claim that the interface supports many others (including ALE). Benchmark results are given for standard algorithms in 5 new Super Nintendo games, and some results using a new ""rivalry metric"".

These environments (or, more generally, standardized evaluation methods like public data sets, competitions, etc.) have a long history of improving the quality of AI and machine learning research. One example in the past few years was the Atari Learning Environment (ALE) which has now turned into a standard benchmark for comparison of algorithms and results. In this sense, the RLE could be a worthy contribution to the field by encouraging new challenging domains for research.

That said, the main focus of this paper is presenting this new framework and showcasing the importance of new challenging domains. The results of experiments themselves are for existing algorithms. There are some new results that show reward shaping and policy shaping (having a bias toward going right in Super Mario) help during learning. And, yes, domain knowledge helps, but this is obvious. The rivalry training is an interesting idea, when training against a different opponent, the learner overfits to that opponent and forgets to play against the in-game AI; but then oddly, it gets evaluated on how well it does against the in-game AI! 

Also the part of the paper that describes the scientific results (especially the rivalry training) is less polished, so this is disappointing. In the end, I'm not very excited about this paper.

I was hoping for a more significant scientific contribution to accompany in this new environment. It's not clear if this is necessary for publication, but also it's not clear that ICLR is the right venue for this work due to the contribution being mainly about the new code (for example, mloss.org could be a better 'venue', JMLR has an associated journal track for accompanying papers:

---

The authors present a new set of environments, similar to ALE but based on Super Nintendo rather than Atari. This is a great asset and could be important for RL research, but it doesn't merit ICLR publication because of the lack of novel research ideas. Hopefully the authors will consider another venue to publish this paper, such as perhaps a journal or workshop.

---

The paper presents a new environment, called Retro Learning Environment (RLE), for reinforcement learning. The authors focus on Super Nintendo but claim that the interface supports many others (including ALE). Benchmark results are given for standard algorithms in 5 new Super Nintendo games, and some results using a new ""rivalry metric"".

These environments (or, more generally, standardized evaluation methods like public data sets, competitions, etc.) have a long history of improving the quality of AI and machine learning research. One example in the past few years was the Atari Learning Environment (ALE) which has now turned into a standard benchmark for comparison of algorithms and results. In this sense, the RLE could be a worthy contribution to the field by encouraging new challenging domains for research.

That said, the main focus of this paper is presenting this new framework and showcasing the importance of new challenging domains. The results of experiments themselves are for existing algorithms. There are some new results that show reward shaping and policy shaping (having a bias toward going right in Super Mario) help during learning. And, yes, domain knowledge helps, but this is obvious. The rivalry training is an interesting idea, when training against a different opponent, the learner overfits to that opponent and forgets to play against the in-game AI; but then oddly, it gets evaluated on how well it does against the in-game AI! 

Also the part of the paper that describes the scientific results (especially the rivalry training) is less polished, so this is disappointing. In the end, I'm not very excited about this paper.

I was hoping for a more significant scientific contribution to accompany in this new environment. It's not clear if this is necessary for publication, but also it's not clear that ICLR is the right venue for this work due to the contribution being mainly about the new code (for example, mloss.org could be a better 'venue', JMLR has an associated journal track for accompanying papers:

---

This paper introduces a new reinforcement learning environment called « The Retro Learning Environment”, that interfaces with the open-source LibRetro API to offer access to various emulators and associated games (i.e. similar to the Atari 2600 Arcade Learning Environment, but more generic). The first supported platform is the SNES, with 5 games (more consoles and games may be added later). Authors argue that SNES games pose more challenges than Atari’s (due to more complex graphics, AI and game mechanics). Several DQN variants are evaluated in experiments, and it is also proposed to compare learning algorihms by letting them compete against each other in multiplayer games.

I like the idea of going toward more complex games than those found on Atari 2600, and having an environment where new consoles and games can easily be added sounds promising. With OpenAI Universe and DeepMind Lab that just came out, though, I am not sure we really need another one right now. Especially since using ROMs of emulated games we do not own is technically illegal: it looks like this did not cause too much trouble for Atari but it might start raising eyebrows if the community moves to more advanced and recent games, especially some Nintendo still makes money from.

Besides the introduction of the environment, it is good to have DQN benchmarks on five games, but this does not add a lot of value. The authors also mention as contribution ""A new benchmarking technique, allowing algorithms to compete against each other, rather than playing against the in-game AI"", but this seems a bit exaggerated to me: the idea of pitting AIs against each other has been at the core of many AI competitions for decades, so it is hardly something new. The finding that reinforcement learning algorithms tend to specialize to their opponent is also not particular surprising.

Overall I believe this is an ok paper but I do not feel it brings enough to the table for a major conference. This does not mean, however, that this new environment won't find a spot in the (now somewhat crowded) space of game-playing frameworks.

Other small comments:
- There are lots of typos (way too many to mention them all)
- It is said that Infinite Mario ""still serves as a benchmark platform"", however as far as I know it had to be shutdown due to Nintendo not being too happy about it
- ""RLE requires an emulator and a computer version of the console game (ROM file) upon initialization rather than a ROM file only. The emulators are provided with RLE"" => how is that different from ALE that requires the emulator Stella which is also provided with ALE?
- Why is there no DQN / DDDQN result on Super Mario?
- It is not clear if Figure 2 displays the F-Zero results using reward shaping or not
- The Du et al reference seems incomplete

---

This paper presents a valuable new collection of video game benchmarks, in an extendable framework, and establishes initial baselines on a few of them.

Reward structures: for how many of the possible games have you implemented the means to extract scores and incremental reward structures? From the github repo it looks like about 10 -- do you plan to add more, and when?

“rivalry” training: this is one of the weaker components of the paper, and it should probably be emphasised less. On this topic, there is a vast body of (uncited) multi-agent literature, it is a well-studied problem setup (more so than RL itself). To avoid controversy, I would recommend not claiming any novel contribution on the topic (I don’t think that you really invented “a new method to train an agent by enabling it to train against several opponents” nor “a new benchmarking technique for agents evaluation, by enabling them to compete against each other, rather than playing against the in-game AI”). Instead, just explain that you have established single-agent and multi-agent baselines for your new benchmark suite.

Your definition of Q-function (“predicts the score at the end of the game given the current state and selected action”) is incorrect. It should read something like: it estimates the cumulative discounted reward that can be obtained from state s, starting with action a (and then following a certain policy).

Minor:
* Eq (1): the Q-net inside the max() is the target network, with different parameters theta’
* the Du et al. reference is missing the year
* some of the other references should point at the corresponding published papers instead of the arxiv versions",,,,,,5.333333333333333,,,4.0,,
697,"SKIP-GRAM NEGATIVE SAMPLING
Authors: Alexander Fonarev, Alexey Grinchuk, Gleb Gusev, Pavel Serdyukov, Ivan Oseledets
Source file: 697.pdf

ABSTRACT
Skip-Gram Negative Sampling (SGNS) word embedding model, well known by its implementation in “word2vec” software, is usually optimized by stochastic gradient descent. It can be shown that optimizing for SGNS objective can be viewed as an optimization problem of searching for a good matrix with the low-rank constraint. The most standard way to solve this type of problems is to apply Riemannian optimization framework to optimize the SGNS objective over the manifold of required low-rank matrices. In this paper, we propose an algorithm that optimizes SGNS objective using Riemannian optimization and demonstrates its superiority over popular competitors, such as the original method to train SGNS and SVD over SPPMI matrix.

1 INTRODUCTION
In this paper, we consider the problem of embedding words into a low-dimensional space in order to measure the semantic similarity between them. As an example, how to find whether the word “table” is semantically more similar to the word “stool” than to the word “sky”? That is achieved by constructing a low-dimensional vector representation for each word and measuring similarity between the words as the similarity between the corresponding vectors.
One of the most popular word embedding models by Mikolov et al. (2013) is a discriminative neural network that optimizes Skip-Gram Negative Sampling (SGNS) objective (see Equation 3). It aims at predicting whether two words can be found close to each other within a text. As shown in Section 2, the process of word embeddings training using SGNS can be divided into two general steps with clear objectives:
Step 1. Search for a low-rank matrix X that provides a good SGNS objective value; Step 2. Search for a good low-rank representation X = WC⊤ in terms of linguistic metrics,
where W is a matrix of word embeddings and C is a matrix of so-called context embeddings.
Unfortunately, most previous approaches mixed these two steps into a single one, what entails a not completely correct formulation of the optimization problem. For example, popular approaches to train embeddings (including the original “word2vec” implementation) do not take into account that the objective from Step 1 depends only on the product X = WC⊤: instead of straightforward computing of the derivative w.r.t. X , these methods are explicitly based on the derivatives w.r.t. W and C, what complicates the optimization procedure. Moreover, such approaches do not take into account that parametrization WC⊤ of matrix X is non-unique and Step 2 is required. Indeed, for any invertible matrix S, we have X = W1C⊤1 = W1SS −1C⊤1 = W2C ⊤ 2 , therefore, solutions W1C1 and W2C2 are equally good in terms of the SGNS objective but entail different cosine similarities between embeddings and, as a result, different performance in terms of linguistic metrics (see Section 4.2 for details).
A successful attempt to follow the above described steps, which outperforms the original SGNS optimization approach in terms of various linguistic tasks, was proposed by Levy & Goldberg (2014). In order to obtain a low-rank matrix X on Step 1, the method reduces the dimensionality of Shifted Positive Pointwise Mutual Information (SPPMI) matrix via Singular Value Decomposition (SVD). On Step 2, it computes embeddings W and C via a simple formula that depends on the factors obtained by SVD. However, this method has one important limitation: SVD provides a solution to a surrogate optimization problem, which has no direct relation to the SGNS objective. In fact, SVD minimizes the Mean Squared Error (MSE) between X and SPPMI matrix, what does not lead to minimization of SGNS objective in general (see Section 6.1 and Section 4.2 in Levy & Goldberg (2014) for details).
These issues bring us to the main idea of our paper: while keeping the low-rank matrix search setup on Step 1, optimize the original SGNS objective directly. This leads to an optimization problem over matrixX with the low-rank constraint, which is often (Mishra et al. (2014)) solved by applying Riemannian optimization framework (Udriste (1994)). In our paper, we use the projector-splitting algorithm (Lubich & Oseledets (2014)), which is easy to implement and has low computational complexity. Of course, Step 2 may be improved as well, but we regard this as a direction of future work.
As a result, our approach achieves the significant improvement in terms of SGNS optimization on Step 1 and, moreover, the improvement on Step 1 entails the improvement on Step 2 in terms of linguistic metrics. That is why, the proposed two-step decomposition of the problem makes sense, what, most importantly, opens the way to applying even more advanced approaches based on it (e.g., more advanced Riemannian optimization techniques for Step 1 or a more sophisticated treatment of Step 2).
To summarize, the main contributions of our paper are:
• We reformulated the problem of SGNS word embedding learning as a two-step procedure with clear objectives;
• For Step 1, we developed an algorithm based on Riemannian optimization framework that optimizes SGNS objective over low-rank matrix X directly;
• Our algorithm outperforms state-of-the-art competitors in terms of SGNS objective and the semantic similarity linguistic metric (Levy & Goldberg (2014); Mikolov et al. (2013); Schnabel et al. (2015)).

2 PROBLEM SETTING

2.1 SKIP-GRAM NEGATIVE SAMPLING
In this paper, we consider the Skip-Gram Negative Sampling (SGNS) word embedding model (Mikolov et al. (2013)), which is a probabilistic discriminative model. Assume we have a text corpus given as a sequence of words w1, . . . , wn, where n may be larger than 1012 and wi ∈ VW belongs to a vocabulary of words VW . A context c ∈ VC of the word wi is a word from set {wi−L, ..., wi−1, wi+1, ..., wi+L} for some fixed window size L. Letw, c ∈ Rd be the word embeddings of word w and context c, respectively. Assume they are specified by the following mappings:
W : VW → Rd, C : VC → Rd. The ultimate goal of SGNS word embedding training is to fit good mappingsW and C. In the SGNS model, the probability that pair (w, c) is observed in the corpus is modeled as a following function:
P ((w, c) ∈ D|w, c) = σ(⟨w, c⟩) = 1 1 + exp(−⟨w, c⟩) , (1)
where D is the multiset of all word-context pairs (w, c) observed in the corpus and ⟨x,y⟩ is the scalar product of vectors x and y. Number d is a hyperparameter that adjusts the flexibility of the model. It usually takes values from tens to hundreds.
In order to collect a training set, we take all pairs (w, c) fromD as positive examples and k randomly generated pairs (w, c) as negative ones. Let #(w, c) be the number of times the pair (w, c) appears
in D. Thereby the number of times the word w and the context c appear in D can be computed as #(w) = ∑ c∈Vc #(w, c) and #(c) = ∑ w∈Vw #(w, c) accordingly. Then negative examples are generated from the distribution defined by #(c) counters: PD(c) = #(c) |D| . In this way, we have a model maximizing the following logarithmic likelihood objective for each word pair (w, c):
#(w, c)(log σ(⟨w, c⟩) + k · Ec′∼PD log σ(−⟨w, c′⟩)). (2) In order to maximize the objective over all observations for each pair (w, c), we arrive at the following SGNS optimization problem over all possible mappingsW and C:
l = ∑
w∈VW ∑ c∈VC #(w, c)(log σ(⟨w, c⟩) + k · Ec′∼PD log σ(−⟨w, c′⟩)) → maxW, . (3)
Usually, this optimization is done via the stochastic gradient descent procedure that is performed during passing through the corpus (Mikolov et al. (2013); Rong (2014)).

2.2 OPTIMIZATION OVER LOW-RANK MATRICES
Relying on the prospect proposed by Levy & Goldberg (2014), let us show that the optimization problem given by (3) can be considered as a problem of searching for a matrix that maximizes a certain objective function and has the rank-d constraint (Step 1 in the scheme described in Section 1).

2.2.1 SGNS LOSS FUNCTION
As shown by Levy & Goldberg (2014), the logarithmic likelihood (3) can be represented as the sum of lw,c(w, c) over all pairs (w, c), where lw,c(w, c) has the following form:
lw,c(w, c) =#(w, c) log σ(⟨w, c⟩) + k #(w)#(c)
|D| log σ(−⟨w, c⟩). (4)
A crucial observation is that this loss function depends only on the scalar product ⟨w, c⟩ but not on embeddings w and c separately:
lw,c(w, c) = fw,c(xw,c),
fw,c(xw,c) = aw,c log σ(xw,c) + bw,c log σ(−xw,c), where xw,c is the scalar product ⟨w, c⟩ and aw,c = #(w, c), bw,c = k#(w)#(c)|D| are constants.

2.2.2 MATRIX NOTATION
Denote |VW | as n and |VC | as m. Let W ∈ Rn×d and C ∈ Rm×d be matrices, where each row w ∈ Rd of matrix W is the word embedding of the corresponding word w and each row c ∈ Rd of matrix C is the context embedding of the corresponding context c. Then the elements of the product of these matrices
X = WC⊤
are the scalar products xw,c of all pairs (w, c):
X = (xw,c), w ∈ VW , c ∈ VC . Note that this matrix has rank d, becauseX equals to the product of two matrices with sizes (n× d) and (d×m). Now we can write SGNS objective given by (3) as a function of X:
F (X) = ∑
w∈VW ∑ c∈VC fw,c(xw,c), F : Rn×m → R. (5)
This arrives us at the following proposition:
Proposition 1 SGNS optimization problem given by (3) can be rewritten in the following constrained form:
maximize X∈Rn×m F (X), subject to X ∈ Md, (6)
whereMd is the manifold (Udriste (1994)) of all matrices in Rn×m with rank d: Md = {X ∈ Rn×m : rank(X) = d}.
The key idea of this paper is to solve the optimization problem given by (6) via the framework of Riemannian optimization, which we introduce in Section 3.
Important to note that this prospect does not suppose the optimization over parameters W and C directly. This entails the optimization in the space with ((n + m − d) · d) degrees of freedom (Mukherjee et al. (2015)) instead of ((n + m) · d), what simplifies the optimization process (see Section 5 for the experimental results).

2.3 COMPUTING EMBEDDINGS FROM A LOW-RANK SOLUTION
OnceX is found, we need to recoverW andC such thatX = WC⊤ (Step 2 in the scheme described in Section 1). This problem does not have a unique solution, since if (W,C) satisfy this equation, thenWS−1 and CS⊤ satisfy it as well for any non-singular matrix S. Moreover, different solutions may achieve different values of the linguistic metrics (see Section 4.2 for details). While our paper focuses on Step 1, we use, for Step 2, a heuristic approach that was proposed by Levy et al. (2015) and it shows good results in practice. We compute SVD of X in the form X = UΣV ⊤, where U and V have orthonormal columns, and Σ is the diagonal matrix, and use
W = U √ Σ, C = V √ Σ
as matrices of embeddings.
A simple justification of this solution is the following: we need to map words into vectors in a way that similar words would have similar embeddings in terms of cosine similarities:
cos(w1,w2) = ⟨w1,w2⟩
∥w1∥ · ∥w2∥ .
It is reasonable to assume that two words are similar, if they share contexts. Therefore, we can estimate the similarity of two words w1, w2 as s(w1, w2) = ∑ c∈VC xw1,c · xw2,c, what is the element of the matrix XX⊤ with indices (w1, w2). Note that XX⊤ = UΣV ⊤V ΣU⊤ = UΣ2U⊤. If we choose W = UΣ, we exactly obtain ⟨w1,w2⟩ = s(w1, w2), since WW⊤ = XX⊤ in this case. That is, the cosine similarity of the embeddingsw1,w2 coincides with the intuitive similarity s(w1, w2). However, scaling by √ Σ instead of Σ was shown by Levy et al. (2015) to be a better solution in experiments.

3 PROPOSED METHOD

3.1 RIEMANNIAN OPTIMIZATION

3.1.1 GENERAL SCHEME
The main idea of Riemannian optimization (Udriste (1994)) is to consider (6) as a constrained optimization problem. Assume we have an approximated solution Xi on a current step of the optimization process, where i is the step number. In order to improve Xi, the next step of the standard gradient ascent outputs Xi + ∇F (Xi), where ∇F (Xi) is the gradient of objective F at the point Xi. Note that the gradient ∇F (Xi) can be naturally considered as a matrix in Rn×m. Point Xi + ∇F (Xi) leaves the manifold Md, because its rank is generally greater than d. That is why Riemannian optimization methods map point Xi + ∇F (Xi) back to manifold Md. The standard Riemannian gradient method first projects the gradient step onto the tangent space at the current point Xi and then retracts it back to the manifold:
Xi+1 = R (PTM (Xi +∇F (Xi))),
where R is the retraction operator, and PTM is the projection onto the tangent space.

3.1.2 PROJECTOR-SPLITTING ALGORITHM
In our paper, we use a much simpler version of such approach that retracts point Xi + ∇F (Xi) directly to the manifold, as illustrated on Figure 1: Xi+1 = R(Xi +∇F (Xi)).
Under review as a conference paper at ICLR 2017
Fine-tuning word embeddings xxxxx xxxxx xxxxx xxxx xxxx xxxx xxx xxxxx xxxx xxxxx ABSTRACT Blah-blah
Keywords word embeddings, SGNS, word2vec, GLOVE
1. INTRODUCTION sdfdsf
2. CONCLUSIONS
3. RELATED WORK Mikolov main [?] Levi main [?]
rFi
Xi = UiSiV T i
Xi+1 = Ui+1Si+1V T i+1
retraction
4. CONCLUSIONS
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. WOODSTOCK ’97 El Paso, Texas USA Copyright 20XX ACM X-XXXXX-XX-X/XX/XX ...$15.00.
Fine-tuning word embeddings xxxxx xxxxx xxxxx xxxx xxxx xxxx xxx xxxxx xxxx xxxxx ABSTRACT Blah-blah Keywords word embeddings, SGNS, word2vec, GLOVE
1. INTRODUCTION sdfdsf
2. CONCLUSIONS
3. RELATED WORK Mikolov main [?] Levi main [?]
rFi
Xi = UiSiV T i
Xi+1 = Ui+1Si+1V T i+1
retraction
Md
4. CONCLUSIONS
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. WOODSTOCK ’97 El Paso, Texas USA Copyright 20XX ACM X-XXXXX-XX-X/XX/XX ...$15.00.
Fine-tuning word embeddings xxxxx xxxxx xxxxx xxxx xxxx xxxx xxx xxxxx xxxx xxxxx
ABSTRACT Blah-blah
Keywords word embeddings, SGNS, word2vec, GLOVE
1. INTRODUCTION sdfdsf
2. CONCLUSIONS
3. RELATED WORK Mikolov main [?] Levi main [?]
rF (Xi)
Xi +rF (Xi)
Xi = UiSiV T i
Xi
Xi+1
Xi+1 = Ui+1Si+1V T i+1
retraction
Md
4. CONCLUSIONS
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. WOODSTOCK ’97 El Paso, Texas USA Copyright 20XX ACM X-XXXXX-XX-X/XX/XX ...$15.00.
Fine-tuning word embeddings xxxxx xxxxx xxxxx xxx xxxx xxxx xxx xxxxx xxxx xxxxx ABSTRACT Blah-blah
Keywords word embeddings, SGNS, word2vec, GLOVE

1. INTRODUCTION
sdfdsf
2. CONCLUSIONS
3. RELATED WORK Mikolov main [?] Levi main [?]
rF (Xi)
Xi +rF (Xi)
Xi = UiSiV T i
Xi
Xi+1
Xi+1 = Ui+1Si+1V T i+1
retraction
Md
4. CONCLUSIONS
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. WOODSTOCK ’97 El Paso, Texas USA Copyright 20XX ACM X-XXXXX-XX-X/XX/XX ...$15.00.
Fine-tuning word embeddings xxxxx xxxxx xxxxx xxxx xxxx xxxx xxx xxxxx xxxx xxxxx
ABSTRACT Blah-blah
Keywords word embeddings, SGNS, word2vec, GLOVE
1. INTRODUCTION sdfdsf
2. CONCLUSIONS
3. RELATED WORK Mikolov main [?] Levi main [?]
rF (Xi)
Xi +rF (Xi)
Xi = UiSiV T i
Xi
Xi+1
Xi+1 = Ui+1Si+1V T i+1
retraction
Md
4. CONCLUSIONS
Permission to make digital or hard copies of all or part of this work for personal or classro m us is r nted without f e provided tha copies are not made or distributed for profit or commercial advantage an that copies bear this notice and the full citation n the first page. To copy otherwise, to republish, to post on servers or to redistribute o lists, requires prior specific permission and/or a fee. WOODSTOCK ’97 El Paso, Texas USA Copyright 20XX ACM X-XXXXX-XX-X/XX/XX ...$15.00.
Fine-tuning word embeddings xxxxx xx xxxxx xxxx xxxx xxxx xxx xxxxx xxxx xxxxx ABSTRACT Blah-blah
Keywords word embeddings, SGNS, word2vec, GLOVE
1. INT ODUCTION sdfdsf
2. CONCLUSIONS
3. RELATED WORK Mikolov main [?] Levi main [?]
rF (Xi)
Xi +rF (Xi)
Xi = UiSiV T i
Xi
Xi+1
Xi+1 = Ui+1Si+1V T i+1
retraction
Md
4. CONCLUSIONS
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. WOODSTOCK ’97 El Paso, Texas USA Copyright 20XX ACM X-XXXXX-XX-X/XX/XX ...$15.00.
Figure 1: Geometric interpretation of one step of projector-splitting optimization procedure: the gradient step an the retraction of the high-rank atr x Xi + ∇F (Xi) to the manifold of low-rank matrices Md.
Intuitively, retractor R finds rank-d matrix on t e manifold Md tha is similar to high-rank matrixXi+∇F (Xi) in terms o Frobeniu orm. How can e do it? The most straightforward way to reduce the rank ofXi +∇F (Xi) is to perform the SVD, which keeps d largest singular values of it:
1: Ui+1, Si+1, V ⊤i+1 ← SVD(Xi +∇F (Xi)), 2: Xi+1 ← Ui+1Si+1V ⊤i+1.
(7)
However, it is computationally expensive. Instead of this approach, we use the projector-splitting method (Lubich & Oseledets (2014)), which is a second-order retraction onto the manifold (for details, see the review by Absil & Oseledets (2015)). Its practical implementation is also quite intuitive: instead of computing the full SVD of Xi +∇F (Xi) according to the gradient projection method, we use just one step of the block power numerical method (Bentbib & Kanber (2015)) which computes the SVD, what reduces the computational complexity.
Let us keep the current point in the following factorized form: Xi = UiSiV ⊤ i , (8) where matrices Ui ∈ Rn×d and Vi ∈ Rm×d have d orthonormal columns and Si ∈ Rd×d. Then we need to perform two QR-decompositions to retract point Xi +∇F (Xi) back to the manifold:
1: Ui+1, Si+1 ← QR ((Xi +∇F (Xi))Vi) , 2: Vi+1, S⊤i+1 ← QR ( (Xi +∇F (Xi))⊤Ui+1 ) ,
3: Xi+1 ← Ui+1Si+1V ⊤i+1. In this way, we always keep the solution Xi+1 = Ui+1Si+1V ⊤i+1 on the manifold Md and in the form (8).
What is important, we only need to compute ∇F (Xi), so the gradients with respect to U , S and V are never computed explicitly, thus avoiding the subtle case where S is close to singular (so-called singular (critical) point on the manifold). Indeed, the gradient with respect to U (while keeping the orthogonality constraints) can be written (Koch & Lubich (2007)) as:
∂F ∂U = ∂F ∂X V S−1,
which means that the gradient will be large if S is close to singular. The projector-splitting scheme is free from this problem.

3.2 ALGORITHM
In case of SGNS objective given by (5), an element of gradient ∇F has the form:
(∇F (X))w,c = ∂fw,c(xw,c)
∂xw,c = #(w, c) · σ (−xw,c)− k
#(w)#(c)
|D| · σ (xw,c) .
To make the method more flexible in terms of convergence properties, we additionally use λ ∈ R, which is a step size parameter. In this case, retractor R returns Xi + λ∇F (Xi) instead of Xi +∇F (Xi) onto the manifold. The whole optimization procedure is summarized in Algorithm 1.
Algorithm 1 Riemannian Optimization for SGNS Require: Dimentionality d, initializationW0 and C0, step size λ, gradient function∇F : Rn×m →
Rn×m, number of iterationsK Ensure: FactorW ∈ Rn×d 1: X0 ← W0C⊤0 # get an initial point at the manifold 2: U0, S0, V ⊤0 ← SVD(X0) # compute the first point satisfying the low-rank constraint 3: i ← 0 4: while i < K do 5: Ui+1, Si+1 ← QR ((Xi + λ∇F (Xi))Vi) # perform one step of the block power method
with two QR-decompositions 6: Vi+1, S⊤i+1 ← QR ( (Xi + λ∇F (Xi))⊤Ui+1 ) 7: Xi+1 ← Ui+1Si+1V ⊤i+1 # update the point at the manifold 8: i ← i+ 1 9: end while 10: U,Σ, V ⊤ ← SVD(XK) 11: W ← U √ Σ # compute word embeddings 12: return W

4 EXPERIMENTAL SETUP

4.1 TRAINING MODELS
We compare our method (“RO-SGNS” in the tables) performance to two baselines: SGNS embeddings optimized via Stochastic Gradient Descent, implemented in the original “word2vec”, (“SGDSGNS” in the tables) by Mikolov et al. (2013) and embeddings obtained by SVD over SPPMI matrix (“SVD-SPPMI” in the tables) by Levy & Goldberg (2014). We have also experimented with the blockwise alternating optimization over factors W and C, but the results are almost the same to SGD results, that is why we do not to include them into the paper. The source code of our experiments is available online1.
The models were trained on English Wikipedia “enwik9” corpus2, which was previously used in most papers on this topic. Like in previous studies, we counted only the words which occur more than 200 times in the training corpus (Levy & Goldberg (2014); Mikolov et al. (2013)). As a result, we obtained a vocabulary of 24292 unique tokens (set of words VW and set of contexts VC are equal). The size of the context window was set to 5 for all experiments, as it was done by Levy & Goldberg (2014); Mikolov et al. (2013). We conduct two series of experiments: for dimensionality d = 100 and d = 200.
Optimization step size is chosen to be small enough to avoid huge gradient values. However, thorough choice of λ does not result in a significant difference in performance (this parameter was tuned on the training data only, the exact values used in experiments are reported below).

4.2 EVALUATION
We evaluate word embeddings via the word similarity task. We use the following popular datasets for this purpose: “wordsim-353” (Finkelstein et al. (2001); 3 datasets), “simlex-999” (Hill et al. (2016)) and “men” (Bruni et al. (2014)). Original “wordsim-353” dataset is a mixture of the word pairs for both word similarity and word relatedness tasks. This dataset was split (Agirre et al. (2009)) into two intersecting parts: “wordsim-sim” (“ws-sim” in the tables) and “wordsim-rel” (“ws-rel” in the tables) to separate the words from different tasks. In our experiments, we use both of them on a par with the full version of “wordsim-353” (“ws-full” in the tables). Each dataset contains word pairs together with assessor-assigned similarity scores for each pair. As a quality measure, we use Spearman’s correlation between these human ratings and cosine similarities for each pair. We call this quality metric linguistic in our paper.
1https://github.com/newozz/riemannian_sgns 2Enwik9 corpus can be found here: http://mattmahoney.net/dc/textdata

5 RESULTS OF EXPERIMENTS
First of all, we compare the value of SGNS objective obtained by the methods. The comparison is demonstrated in Table 1.
We see that SGD-SGNS and SVD-SPPMI methods provide quite similar results, however, the proposed method obtains significantly better SGNS values, what proves the feasibility of using Riemannian optimization framework in SGNS optimization problem. It is interesting to note that SVDSPPMI method, which does not optimize SGNS objective directly, obtains better results than SGDSGNS method, which aims at optimizing SGNS. This fact additionally confirms the idea described in Section 2.2.2 that the independent optimization over parameters W and C may decrease the performance.
However, the target performance measure of embedding models is the correlation between semantic similarity and human assessment (Section 4.2). Table 2 presents the comparison of the methods in terms of it. We see that our method outperforms the competitors on all datasets except for “men” dataset where it obtains slightly worse results. Moreover, it is important that the higher dimension entails higher performance gain of our method in comparison to the competitors.
In order to understand how exactly our model improves or degrades the performance in comparison to the baseline, we found several words, whose neighbors in terms of cosine distance change significantly. Table 3 demonstrates neighbors of words “five”, “he” and “main” in terms of our model and its nearest competitor according to the similarity task — SVD-SPPMI. These words were chosen as representative examples whose neighborhoods in terms of SVD-SPPMI and RO-SGNS models are strikingly different. A neighbour of a source word is bold if we suppose that it has a similar semantic meaning to the source word. First of all, we notice that our model produces much better neighbors of the words describing digits or numbers (see word “five” as an example). The similar situation happens for many other words, e.g. in case of word “main” — the nearest neighbors contain 4 similar words in case of our model instead of 2 in case of SVD-SPPMI. The neighbourhood of word “he” contains less semantically similar words in case of our model. However, it filters out completely irrelevant words, such as “promptly” and “dumbledore”.
Talking about the optimal number K of iterations in the optimization procedure and step size λ, we found that they depend on the particular value of dimensionality d. For d = 100, we have K = 25, λ ≈ 5 · 10−5, and for d = 200, we have K = 13, λ = 10−4. Moreover, it is interesting that the best results were obtained when SVD-SPPMI embeddings were used as an initialization of Riemannian optimization process.

6 RELATED WORK

6.1 WORD EMBEDDINGS
Skip-Gram Negative Sampling was introduced by Mikolov et al. (2013). The “negative sampling” approach was thoroughly described by Goldberg & Levy (2014), and the learning method is ex-
plained by Rong (2014). There are several open-source implementations of SGNS neural network, which is widely known as “word2vec” 34.
As shown in Section 2.2, Skip-Gram Negative Sampling optimization can be reformulated as a problem of searching for a low-rank matrix. In order to be able to use out-of-the-box SVD for this task, Levy & Goldberg (2014) used the surrogate version of SGNS as the objective function. There are two general assumptions made in their algorithm that distinguish it from the SGNS optimization:
1. SVD optimizes Mean Squared Error (MSE) objective instead of SGNS loss function.
2. In order to avoid infinite elements in SPMI matrix, it is transformed in ad-hoc manner (SPPMI matrix) before applying SVD.
This makes the objective not interpretable in terms of the original task (3). As mentioned by Levy & Goldberg (2014), SGNS objective weighs different (w, c) pairs differently, unlike the SVD, which works with the same weight for all pairs, what may entail the performance fall. The comprehensive explanation of the relation between SGNS, SPPMI, SVD-over-SPPMI methods is provided by Keerthi et al. (2015). Lai et al. (2015); Levy et al. (2015) give a good overview of highly practical methods to improve these word embedding models.

6.2 RIEMANNIAN OPTIMIZATION
An introduction to optimization over Riemannian manifolds can be found in the paper of Udriste (1994). The overview of retractions of high rank matrices to low-rank manifolds is provided by Absil & Oseledets (2015). The projector-splitting algorithm was introduced by Lubich & Oseledets (2014), and also was mentioned by Absil & Oseledets (2015) as “Lie-Trotter retraction”.
Riemannian optimization is succesfully applied to various data science problems: for example, matrix completion (Vandereycken (2013)), large-scale recommender systems (Tan et al. (2014)), and tensor completion (Kressner et al. (2014)).

7 CONCLUSIONS AND FUTURE WORK
In our paper, we proposed the general two-step scheme of training SGNS word embedding model and introduced the algorithm that performs the search of a solution in the low-rank form via Riemannian optimization framework. We also demonstrated the superiority of the proposed method, by providing the experimental comparison to the existing state-of-the-art approaches.
It seems to be an interesting direction of future work to apply more advanced optimization techniques to Step 1 of the scheme proposed in Section 1 and to explore the Step 2 — obtaining embeddings with a given low-rank matrix.
3Original Google word2vec: https://code.google.com/archive/p/word2vec/ 4Gensim word2vec: https://radimrehurek.com/gensim/models/word2vec.html
","Skip-Gram Negative Sampling (SGNS) word embedding model, well known by its implementation in “word2vec” software, is usually optimized by stochastic gradient descent. It can be shown that optimizing for SGNS objective can be viewed as an optimization problem of searching for a good matrix with the low-rank constraint. The most standard way to solve this type of problems is to apply Riemannian optimization framework to optimize the SGNS objective over the manifold of required low-rank matrices. In this paper, we propose an algorithm that optimizes SGNS objective using Riemannian optimization and demonstrates its superiority over popular competitors, such as the original method to train SGNS and SVD over SPPMI matrix.",ICLR 2017 conference submission,False,,"This paper presents a principled optimization method for SGNS (word2vec).

While the proposed method is elegant from a theoretical perspective, I am not sure what the tangible benefits of this approach are. For example, does using Riemannian optimization allow the model to converge faster than the alternatives? The evaluation doesn't show a dramatic advantage to RO-SGNS; the 1% difference on the word similarity benchmarks is within the range of hyperparameter effects (see ""Improving Distributional Similarity with Lessons Learned from Word Embeddings"", (Levy et al., 2015)). The theoretical connection to Riemannian optimization is nice though, and it might be useful for understanding related methods in the future.

---

The paper is mostly clearly written. The observation made in the paper that word-embedding models based on optimizing skip-gram negative sampling objective function can be formulated as a low-rank matrix estimation problem, and solved using manifold optimization techniques, is sound. However, this observation by itself is not new and has come up in various other contexts such as matrix completion. As such the reviewers do not see sufficient novelty in the algorithmic aspects of the paper, and empirical evaluation on the specific problem of learning word embeddings does not show striking enough gains relative to standard SGD methods. The authors are encouraged to explore complimentary algorithmic angles and benefits that their approach provides for this specific class of applications.

---

The paper considers Grassmannian SGD to optimize the skip gram negative sampling (SGNS) objective for learning better word embeddings. It is not clear why the proposed optimization approach has any advantage over the existing vanilla SGD-based approach - neither approach comes with theoretical guarantees - the empirical comparisons show marginal improvements. Furthermore, the key idea here - that of projector splitting algorithm - has been applied on numerous occasions to machine learning problems - see references by Vandereycken on matrix completion and by Sepulchre on matrix factorization. 

The computational cost of the two approaches is not carefully discussed. For instance, how expensive is the SVD in (7)? One can always perform an efficient low-rank update to the SVD - therefore, a rank one update requires O(nd) operations. What is the computational cost of each iteration of the proposed approach?

---

Dear authors,

The authors' response clarified some of my confusion. But I still have the following question:

-- The response said a first contribution is a different formulation: you divide the word embedding learning into two steps, step 1 looks for a low-rank X (by Riemannian optimization), step 2 factorizes X into two matrices (W, C). You are claiming that your model outperforms previous approaches that directly optimizes over (W, C). But since the end result (the factors) is the same, can the authors provide some intuition and justification why the proposed method works better?

As far as I can see, though parameterized differently, the first step of your method and previous methods (SGD) are both optimizing over low-rank matrices. Admittedly, Riemannian optimization avoids the rotational degree of freedom (the invertible matrix S you are mentioning in sec 2.3), but I am not 100% certain at this point this is the source of your gain; learning curves of objectives would help to see if Riemannian optimization is indeed more effective. 

-- Another detail I could not easily find is the following. You said a disadvantage of other approaches is that their factors W and C do not directly reflect similarity. Did you try to multiply the factors W and C from other optimizers and then factorize the product using the method in section 2.3, and use the new W for your downstream tasks? I am not sure if this would cause much difference in the performance.

Overall, I think it is always interesting to apply advanced optimization techniques to machine learning problems. The current paper would be stronger from the machine learning perspective, if more thorough comparison and discussion (as mentioned above) are provided. On the other hand, my expertise is not in NLP and I leave it to other reviewers to decide the significance in experimental results.

---

This paper presents a principled optimization method for SGNS (word2vec).

While the proposed method is elegant from a theoretical perspective, I am not sure what the tangible benefits of this approach are. For example, does using Riemannian optimization allow the model to converge faster than the alternatives? The evaluation doesn't show a dramatic advantage to RO-SGNS; the 1% difference on the word similarity benchmarks is within the range of hyperparameter effects (see ""Improving Distributional Similarity with Lessons Learned from Word Embeddings"", (Levy et al., 2015)). The theoretical connection to Riemannian optimization is nice though, and it might be useful for understanding related methods in the future.

---

This paper presents a principled optimization method for SGNS (word2vec).

While the proposed method is elegant from a theoretical perspective, I am not sure what the tangible benefits of this approach are. For example, does using Riemannian optimization allow the model to converge faster than the alternatives? The evaluation doesn't show a dramatic advantage to RO-SGNS; the 1% difference on the word similarity benchmarks is within the range of hyperparameter effects (see ""Improving Distributional Similarity with Lessons Learned from Word Embeddings"", (Levy et al., 2015)). The theoretical connection to Riemannian optimization is nice though, and it might be useful for understanding related methods in the future.

---

The paper is mostly clearly written. The observation made in the paper that word-embedding models based on optimizing skip-gram negative sampling objective function can be formulated as a low-rank matrix estimation problem, and solved using manifold optimization techniques, is sound. However, this observation by itself is not new and has come up in various other contexts such as matrix completion. As such the reviewers do not see sufficient novelty in the algorithmic aspects of the paper, and empirical evaluation on the specific problem of learning word embeddings does not show striking enough gains relative to standard SGD methods. The authors are encouraged to explore complimentary algorithmic angles and benefits that their approach provides for this specific class of applications.

---

The paper considers Grassmannian SGD to optimize the skip gram negative sampling (SGNS) objective for learning better word embeddings. It is not clear why the proposed optimization approach has any advantage over the existing vanilla SGD-based approach - neither approach comes with theoretical guarantees - the empirical comparisons show marginal improvements. Furthermore, the key idea here - that of projector splitting algorithm - has been applied on numerous occasions to machine learning problems - see references by Vandereycken on matrix completion and by Sepulchre on matrix factorization. 

The computational cost of the two approaches is not carefully discussed. For instance, how expensive is the SVD in (7)? One can always perform an efficient low-rank update to the SVD - therefore, a rank one update requires O(nd) operations. What is the computational cost of each iteration of the proposed approach?

---

Dear authors,

The authors' response clarified some of my confusion. But I still have the following question:

-- The response said a first contribution is a different formulation: you divide the word embedding learning into two steps, step 1 looks for a low-rank X (by Riemannian optimization), step 2 factorizes X into two matrices (W, C). You are claiming that your model outperforms previous approaches that directly optimizes over (W, C). But since the end result (the factors) is the same, can the authors provide some intuition and justification why the proposed method works better?

As far as I can see, though parameterized differently, the first step of your method and previous methods (SGD) are both optimizing over low-rank matrices. Admittedly, Riemannian optimization avoids the rotational degree of freedom (the invertible matrix S you are mentioning in sec 2.3), but I am not 100% certain at this point this is the source of your gain; learning curves of objectives would help to see if Riemannian optimization is indeed more effective. 

-- Another detail I could not easily find is the following. You said a disadvantage of other approaches is that their factors W and C do not directly reflect similarity. Did you try to multiply the factors W and C from other optimizers and then factorize the product using the method in section 2.3, and use the new W for your downstream tasks? I am not sure if this would cause much difference in the performance.

Overall, I think it is always interesting to apply advanced optimization techniques to machine learning problems. The current paper would be stronger from the machine learning perspective, if more thorough comparison and discussion (as mentioned above) are provided. On the other hand, my expertise is not in NLP and I leave it to other reviewers to decide the significance in experimental results.

---

This paper presents a principled optimization method for SGNS (word2vec).

While the proposed method is elegant from a theoretical perspective, I am not sure what the tangible benefits of this approach are. For example, does using Riemannian optimization allow the model to converge faster than the alternatives? The evaluation doesn't show a dramatic advantage to RO-SGNS; the 1% difference on the word similarity benchmarks is within the range of hyperparameter effects (see ""Improving Distributional Similarity with Lessons Learned from Word Embeddings"", (Levy et al., 2015)). The theoretical connection to Riemannian optimization is nice though, and it might be useful for understanding related methods in the future.",,,,,,5.0,,,3.3333333333333335,,
719,"CLASSLESS ASSOCIATION USING NEURAL NETWORKS
Authors: Federico Raue, Sebastian Palacio, Andreas Dengel, Marcus Liwicki
Source file: 719.pdf

ABSTRACT
The goal of this paper is to train a model based on the relation between two instances that represent the same unknown class. This scenario is inspired by the Symbol Grounding Problem and the association learning in infants. We propose a novel model called Classless Association. It has two parallel Multilayer Perceptrons (MLP) that uses one network as a target of the other network, and vice versa. In addition, the presented model is trained based on an EM-approach, in which the output vectors are matched against a statistical distribution. We generate four classless datasets based on MNIST, where the input is two different instances of the same digit. In addition, the digits have a uniform distribution. Furthermore, our classless association model is evaluated against two scenarios: totally supervised and totally unsupervised. In the first scenario, our model reaches a good performance in terms of accuracy and the classless constraint. In the second scenario, our model reaches better results against two clustering algorithms.

1 INTRODUCTION
Infants are able to learn the binding between abstract concepts to the real world via their sensory input. For example, the abstract concept ball is binding to the visual representation of a rounded object and the auditory representation of the phonemes /b/ /a/ /l/. This scenario can be seen as the Symbol Grounding Problem (Harnad, 1990). Moreover, infants are also able to learn the association between different sensory input signals while they are still learning the binding of the abstract concepts. Several results have shown a correlation between object recognition (visual) and vocabulary acquisition (auditory) in infants (Balaban & Waxman, 1997; Asano et al., 2015). One example of this correlation is the first words that infants have learned. In that case, the words are mainly nouns, which are visible concepts, such as, dad, mom, ball, dog, cat (Gershkoff-Stowe & Smith, 2004). As a result, we can define the previous scenario in terms of a machine learning tasks. More formally, the task is defined by learning the association between two parallel streams of data that represent the same unknown class (or abstract concept). Note that this task is different from the supervised association where the data has labels. First, the semantic concept is unknown in our scenario whereas it is known in the supervised case. Second, both classifiers needs to agree on the same coding scheme for each sample pair during training. In contrast, the coding-scheme is already pre-defined before training in the supervised case. Figure 1 shows an example of the difference between a supervised association task and our scenario.
Usually, classifiers requires labeled data for training. However, the presented scenario needs an alternative training mechanism. One way is to train based on statistical distributions. Casey (1986) proposed to solve the OCR problem using language statistics for inferring form images to characters. Later on, Knight et al. (2006) applied a similar idea to machine translation. Recently, Sutskever et al. (2015) defined the Output Distribution Matching (ODM) cost function for dual autoencoders and generative networks.
In this paper, we are proposing a novel model that is trained based on the association of two input samples of the same unknown class. The presented model has two parallel Multilayer Perceptrons (MLPs) with an Expectation-Maximization (EM) (Dempster et al., 1977) training rule that matches the network output against a statistical distribution. Also, both networks agree on the same classification because one network is used as target of the other network, and vice versa. Our model has some
similarities with Siamese Networks proposed by Chopra et al. (2005). They introduced their model for supervised face verification where training is based on constraints of pairs of faces. The constraints exploit the relation of two faces that may or may not be instances of the same person. However, there are some differences to our work. First, our training rule does not have pre-defined classes before training, whereas the Siamese Network requires labeled samples. Second, our model only requires instances of the same unknown class, whereas the Siamese network requires two types of input pairs: a) instances of the same person and b) instances of two different persons. Our contributions in this paper are
• We define a novel training rule based on matching the output vectors of the presented model and a statistical distribution. Note that the output vectors are used as symbolic features similar to the Symbol Grounding Problem. Furthermore, the proposed training rule is based on an EM-approach and classified each sample based on generated pseudo-classes (Section 2.1).
• We propose a novel architecture for learning the association in the classless scenario. Moreover, the presented model uses two parallel MLPs, which require to agree on the same class for each input sample. This association is motivated by the correlation between different sensory input signals in infants development. In more detail, one network is the target of the other network, and vice versa. Also, note that our model is gradient-based and can be extended to deeper architectures (Section 2.2).
• We evaluate our classless association task against two cases: totally supervised and totally unsupervised. In this manner, we can verify the range of our results in terms of supervised and unsupervised cases since our model is neither totally supervised nor totally unsupervised. We compare against a MLP trained with labels as the supervised scenario (upper bound) and two clustering algorithms (K-means and Hierarchical Agglomerative) as the unsupervised scenario (lower bound). First, our model reaches better results than the clustering. Second, our model shows promising results with respect to the supervised scenario (Sections 3 and 4).

2 METHODOLOGY
In this paper, we are interested in the classless association task in the following scenario: two input instances x(1) and x(2) belong to the same unknown class c, where x(1) ∈ X(1) and x(2) ∈ X(2) are two disjoint sets, and the goal is to learn the output classification of x(1) and x(2) is the same c(1) = c(2), where c(1) and c(2) ∈ C is the set of possible classes. With this in mind, we present a model that has two parallel Multilayer Perceptrons (MLPs) that are trained with an EM-approach that associates both networks in the following manner: one network uses the other network as a target, and vice versa. We explain how the output vectors of the network are matched to a statistical distribution in Section 2.1 and the classless association learning is presented in Section 2.2.

2.1 STATISTICAL CONSTRAINT
One of our constraint is to train a MLP without classes. As a result, we use an alternative training rule based on matching the output vectors and a statistical distribution. For simplicity, we explain our training rule using a single MLP with one hidden layer, which is defined by
z = network(x;θ) (1)
where x ∈ Rn is the input vector, θ encodes the parameters of the MLP, and z ∈ Rc is the output vector. Moreover, the output vectors (z1, . . . ,zm) of a mini-batch of size m are matched to a target distribution (E[z1, . . . ,zm] ∼ φ ∈ Rc), e.g., uniform distribution. We have selected a uniform distribution because it is an ideal case to have a balanced dataset for any classifier. However, it is possible to extend to different distribution. We introduce a new parameter that is a weighting vector γ ∈ Rc. The intuition behind it is to guide the network based on a set of generated pseudo-classes c. These pseudo-classes can be seen as cluster indexes that group similar elements. With this in mind, we also propose an EM-training rule for learning the unknown class given a desired target distribution. We want to point out that the pseudo-classes are internal representations of the network that are independent of the labels.
The E-step obtains the current statistical distribution given the output vectors (z1, . . . ,zm) and the weighting vector (γ). In this case, an approximation of the distribution is obtained by the following equation
ẑ = 1
M M∑ i=1 power(zi,γ) (2)
where γ is the weighting vector, zi is the output vector of the network, M is the number of elements, and the function power1 is the element-wise power operation between the output vector and the weighting vector. We have used the power function because the output vectors (z1, . . . , zm) are quite similar between them at the initial state of the network, and the power function provides an initial boost for learning to separate the input samples in different pseudo-classes in the first iterations. Moreover, we can retrieve the pseudo-classes by the maximum value of the following equation
c∗ = arg maxc power(zi,γ) (3)
where c∗ is the pseudo-class, which are used in the M-step for updating the MLP weights. Also, note that the pseudo-classes are not updated in an online manner. Instead, the pseudo-classes are updated after a certain number of iterations. The reason is the network requires a number of iterations to learn the common features.
The M-step updates the weighting vector γ given the current distribution ẑ. Also, the MLP parameters (θ) are updated given the current classification given by the pseudo-classes. The cost function is the variance between the distribution and the desired statistical distribution, which is defined by
cost = (ẑ − φ)2 (4) 1We decide to use power function instead of zγi in order to simplify the index notation
where ẑ is the current statistical distribution of the output vectors, and φ is a vector that represent the desired statistical distribution, e.g. uniform distribution. Then, the weighting vector is updated via gradient descent
γ = γ − α ∗ ∇γcost (5)
where α is the learning rate and ∇γcost is the derivative w.r.t γ. Also, the MLP weights are updated via the generated pseudo-classes, which are used as targets in the backpropagation step.
In summary, we propose an EM-training rule for matching the network output vectors and a desired target statistical distribution. The E-Step generates pseudo-classes and finds an approximation of the current statistical distribution of the output vectors. The M-Step updates the MLP parameters and the weighting vector. With this in mind, we adapt the mentioned training rule for the classless association task. Figure 2 summarizes the presented EM training rule and its components.

2.2 CLASSLESS ASSOCIATION LEARNING
Our second constraint is to classify both input samples as the same class and different from the other classes. Note that the pseudo-class (Equation 3) is used as identification for each input sample and it is not related to the semantic concept or labels. The presented classless association model is trained based on a statistical constraint. Formally, the input is represented by the pair x(1) ∈ Rn1 and x(2) ∈ Rn2 where x(1) and x(2) are two different instances of the same unknown label. The classless association model has two parallel Multilayer Perceptron MLP (1) and MLP (2) with training rule that follows an EM-approach (cf. Section 2.1). Moreover, input samples are divided into several mini-batches of size m.
Initially, all input samples have random pseudo-classes c(1)i and c (2) i . The pseudo-classes have the same desired statistical distribution φ. Also, the weighting vectors γ(1) and γ(2) are initialized to one. Then each input element from the mini-batch is propagated forward to each MLP. Afterwards, an estimation of the statistical distribution for each MLP (ẑ(1) and ẑ(2)) is obtained. Furthermore, a new set of pseudo-classes (c(1)1 , . . . , c (1) m and c (2) 1 , . . . , c (2) m ) is obtained for each network. Note that this first part can be seen as an E-step from Section 2.1. We want to point out that the pseudo-classes are updated only after a number of iterations.
The second part of our association training updates the MLP parameters and the weighting vector (γ(1) and γ(2)). In this step, one network (MLP (1)) uses pseudo-classes (c(2)1 , . . . , c (2) m ) obtained from the other network (MLP (2)), and vice versa. In addition, the weighting vector is updated
between the output approximation (ẑ(1) and ẑ(2)) and the desired target distribution (φ). Figure 3 shows an overview of the presented model.

3 EXPERIMENTS
In this paper, we are interested in a simplified scenario inspired by the Symbol Grounding Problem and the association learning between sensory input signal in infants. We evaluated our model in four classless datasets that are generated from MNIST (Lecun & Cortes, 2010). The procedure of generating classless datasets from labeled datasets have been already applied in (Sutskever et al., 2015; Hsu & Kira, 2015). Each dataset has two disjoint sets input 1 and input 2. The first dataset (MNIST) has two different instances of the same digit. The second dataset (Rotated-90 MNIST) has two different instances of the same digit, and all input samples in input 2 are rotated 90 degrees. The third dataset (Inverted MNIST ) follows a similar procedures as the second dataset, but the transformation of the elements in input 2 is the invert function instead of rotation. The last dataset (Random Rotated MNIST) is more challenging because all elements in input 2 are randomly rotated between 0 and 2π. All datasets have a uniform distribution between the digits and the dataset size is 21,000 samples for training and 4,000 samples for validation and testing.
The following parameters turned out being optimal on the validation set. For the first three datasets, each internal MLP relies on two fully connected layers of 200 and 100 neurons respectively. The learning rate for the MLPs was set to start at 1.0 and was continuously decaying by half after every 1,000 iterations. We set the initial weighting vector to 1.0 and updated after every 1,000 iterations as well. Moreover, the best parameters for the fourth dataset were the same forMLP (1) and different for MLP (2), which has two fully connected layers of 400 and 150 neurons respectively and the learning rate stars at 1.2. The target distribution φ is uniform for all datasets. The decay of the learning rate (Equation 5) for the weighting vector was given by 1/(100+epoch)0.3, where epoch was the number of training iterations so far. The mini-batch size M is 5,250 sample pairs (corresponding to 25% of the training set) and the mean of the derivatives for each mini-batch is used for the back-propagation step of MLPs. Note that the mini-batch is quite big comparing with common setups. We infer from this parameter that the model requires a sample size big enough for estimating the uniform distribution and also needs to learn slower than traditional approaches. Our model was implemented in Torch.
MLP(1) MLP (2) Association Matrix (%) Purity (%)
10.9 10.9 __________Initial State
Epoch 1,000
0 1 2 3 4 5 6 7 8 9 MLP (2)
0 1 2 3 4 5 6 7 8 9 M LP (1 ) 0.7 0.6 2.4 0.4 3.0 2.2 1.0 1.4 1.2 0.6
0
2
4
6
8
10
12
14
16
18
20
24.8 22.6
Epoch 3,000
0 1 2 3 4 5 6 7 8 9 MLP (2)
0 1 2 3 4 5 6 7 8 9 M LP (1 ) 0.1 1.7 2.8 0.0 5.7 8.9 0.3 10.0 8.1 0.1
0
2
4
6
8
10
12
14
16
18
20
64.4 65.8
Epoch 49,000
0 1 2 3 4 5 6 7 8 9 MLP (2)
0 1 2 3 4 5 6 7 8 9 M LP (1 ) 9.8 8.9 9.4 9.0 9.6 9.6 9.7 9.6 9.6 9.0
0
2
4
6
8
10
12
14
16
18
20
95.5 95.6
Figure 4: Example of the presented model during classless training. In this example, there are ten pseudo-classes represented by each row of MLP (1) and MLP (2). Note that the output classification are randomly selected (not cherry picking). Initially, the pseudo-classes are assigned randomly to all input pair samples, which holds a uniform distribution (first row). Then, the classless association model slowly start learning the features and grouping similar input samples. Afterwards, the output classification of both MLPs slowly agrees during training, and the association matrix shows the relation between the occurrences of the pseudo-classes.
To determine the baseline of our classless constraint, we compared our model against two cases: totally supervised and totally unsupervised. In the supervised case, we used the same MLP parameters and training for a fair comparison. In the unsupervised scenario, we used k-means and agglomerative clustering to each set (input 1 and input 2) independently. The clustering algorithm implementation are provided by scikit-learn (Pedregosa et al., 2011).

4 RESULTS AND DISCUSSION
In this work, we have generated ten different folds for each dataset and report the average results. We introduce the Association Accuracy for measuring association, and it is defined by the following equation
Association Accuracy = 1
N N∑ i=1 1(c (1) i = c (2) i ) (6)
where the indicator function is one if c(1)i = c (2) i , zero otherwise; c (1) i and c (2) i are the pseudo-classes for MLP (1) and MLP (2), respectively, and N is the number of elements. In addition, we also reported the Purity of each set (input 1 and input 2). Purity is defined by
Purity(Ω, C) = 1 N k∑ i=1 maxj |ci ∩ gtj | (7)
where Ω = {gt1, gt2, . . . , gtj} is the set of ground-truth labels and C = {c1, c2, . . . , ck} is the set of pseudo-classes in our model or the set of cluster indexes of K-means or Hierarchical Agglomerative clustering, and N is the number of elements.
Table 1 shows the Association Accuracy between our model and the supervised association task and the Purity between our model and two clustering algorithms. First, the supervised association task performances better that the presented model. This was expected because our task is more complex in relation to the supervised scenario. However, we can infer from our results that the presented model has a good performance in terms of the classless scenario and supervised method. Second, our model not only learns the association between input samples but also finds similar elements covered under the same pseudo-class. Also, we evaluate the purity of our model and found that the performance of our model reaches better results than both clustering methods for each set (input 1 and input 2).
Figure 4 illustrates an example of the proposed learning rule. The first two columns (MLP (1) and MLP (2)) are the output classification (Equation 3) and each row represents a pseudo-class. We have randomly selected 15 output samples for each MLP (not cherry picking). Initially, the pseudo classes are random selected for each MLP. As a result, the output classification of both networks does not show any visible discriminant element and the initial purity is close to random choice (first row). After 1,000 epochs, the networks start learning some features in order to discriminate the input samples. Some groups of digits are grouped together after 3,000 epochs. For example, the first row of MLP (2) shows several digits zero, whereas MLP (1) has not yet agree on the same digit for that pseudo-class. In contrast, both MLPs have almost agree on digit one at the fifth row. Finally, the association is learned using only the statistical distribution of the input samples and each digit is represented by each pseudo-class.
Best Results
Worst Results
MLP (1) MLP (2) Association Matrix (%)
0 1 2 3 4 5 6 7 8 9 MLP (2)
0 1 2 3 4 5 6 7 8 9 M LP (1 ) 9.7 8.9 9.0 8.7 9.7 9.0 9.2 9.1 9.4 8.5
0
2
4
6
8
10
12
14
16
18
20
Purity (%)
0 1 2 3 4 5 6 7 8 9 MLP (2)
0 1 2 3 4 5 6 7 8 9 M LP (1 ) 9.7 7.9 5.1 4.5 0.0 5.1 9.2 0.0 10.3 12.8
0
2
4
6
8
10
12
14
16
18
20
95.9 95.2
72.9 59.4
Figure 5: Example of the best and worst results among all folds and datasets. It can be observed our model is able to learn to discriminate each digit (first row). However, the presented model has a limitation that two or more digits are assigned to the same pseudo-class (last row of MLP (1) and MLP (2)).
Figure 5 shows the best and worst results of our model in two cases. The first row is the best result from MNIST dataset. Each row of MLP (1) and MLP (2) represent a pseudo-class, and it can be observed that all digits are grouped together. In addition, the association matrix shows a distribution per digit close to the desired uniform distribution, and the purity of each input is close to the supervised scenario. In contrast, the second row is our worst result from Random Rotated MNIST dataset. In this example, we can observe that some digits are recognized by the same pseudo-class, for example, digit one and seven (first two rows). However, there two or more digits that are recognized by the samepseudo-class. For example, the last row shows that nine and four are merged. Our model is still able to reach better results than the unsupervised scenario.

5 CONCLUSION
In this paper, we have shown the feasibility to train a model that has two parallel MLPs under the following scenario: pairs of input samples that represent the same unknown classes. This scenario was motivated by the Symbol Grounding Problem and association learning between sensory input signal in infants development. We proposed a model based on gradients for solving the classless association. Our model has an EM-training that matches the network output against a statistical distribution and uses one network as a target of the other network, and vice versa. Our model reaches better performance than K-means and Hierarchical Agglomerative clustering. In addition, we compare the presented model against a supervised method. We find that the presented model with respect to the supervised method reaches good results because of two extra conditions in the unsupervised association: unlabeled data and agree on the same pseudo-class. We want to point out that our model was evaluated in an optimal case where the input samples are uniform distributed and the number of classes is known. However, we will explore the performance of our model if the number of classes and the statistical distrubtion are unknown. One way is to change the number of pseudo-classes. This can be seen as changing the number of clusters k in k-means. With this in mind, we are planning to do more exhaustive analysis of the learning behavior with deeper architectures. Moreover, we will work on how a small set of labeled classes affects the performance of our model (similar to semi-supervised learning). Furthermore, we are interested in replicating our findings in more complex scenarios, such as, multimodal datasets like TVGraz (Khan et al., 2009) or Wikipedia featured articles (Rasiwasia et al., 2010). Finally, our work can be applied to more classless scenarios where the data can be extracted simultaneously from different input sources at the same time. Also, transformation functions can be applied to input samples for creating the association without classes.

ACKNOWLEDGMENTS
We would like to thank Damian Borth, Christian Schulze, Jörn Hees, Tushar Karayil, and Philipp Blandfort for helpful discussions.
","The goal of this paper is to train a model based on the relation between two instances that represent the same unknown class. This scenario is inspired by the Symbol Grounding Problem and the association learning in infants. We propose a novel model called Classless Association. It has two parallel Multilayer Perceptrons (MLP) that uses one network as a target of the other network, and vice versa. In addition, the presented model is trained based on an EM-approach, in which the output vectors are matched against a statistical distribution. We generate four classless datasets based on MNIST, where the input is two different instances of the same digit. In addition, the digits have a uniform distribution. Furthermore, our classless association model is evaluated against two scenarios: totally supervised and totally unsupervised. In the first scenario, our model reaches a good performance in terms of accuracy and the classless constraint. In the second scenario, our model reaches better results against two clustering algorithms.",ICLR 2017 conference submission,False,,"The paper looks correct but still i am not convinced about the experimentation performed. Perhaps another experiment with more challenging data would be welcome. Honestly i don't find a clear motivation for this work however it could have some potential and it would be interested to be presented in conference.

---

The paper explores neural-network learning on pairs of samples that are labeled as either similar or dissimilar. The proposed model appears to be different from standard siamese architectures, but it is poorly motivated. The experimental evaluation of the proposed model is very limited.

---

We have updated our paper.  The changes are
* We have improved the clarity and motivation of our model
* We have evaluated our model to three more classless datasets (Rotated-90 MNIST, Inverted MNIST, and Random Rotation MNIST).
* We have updated Figure 4 and 5 for showing some random output classification samples instead of the mean of all images.
* We have added two more examples and demo as supplemental material

---

The paper presents an alternative way of supervising the training of neural network without explicitly using labels when only link/not-link information is available between pairs of examples. A pair of network is trained each of which is used to supervise the other one.


The presentation of the paper is not very clear, the writing can be improved.
Some design choice are not explained: Why is the power function used in the E-step for approximating the distribution (section 2.1)? Why do the authors only consider a uniform distribution? I understand that using a different prior breaks the assumption that nothing is known about the classes. However I do not see a practical situations where the proposed setting/work would be useful.  

Also, there exist a large body of work in semi-supervised learning with co-training based on a similar idea. 

Overall, I think this work should be clarified and improved to be a good fit for this venue.

---

The paper explores a new technique for classless association, a milder unsupervised learning where we do not know the class labels exactly, but we have a prior about the examples that belong to the same class. Authors proposed a two stream architecture with two neural networks, as streams process examples from the same class simultaneously. Both streams rely on the target (pseudo classes or cluster indices) of each other, and the outputs an intermediate representation z, which is forced to match with a statistical distribution (uniform in their case). The model is trained with EM where the E step obtains the current statistical distribution given output vectors z, and M step updates the weights of the architecture given z and pseudo-classes. Experimental results on re-organized MNIST exhibits better performance compared to classical clustering algorithms (in terms of association accuracy and purity). The authors further provide comparison against a supervised method, where proposed architecture expectedly performs worse but with promising results.

The basic motivation of the architecture apparently relies on unlabeled data and agreement of the same pseudo-labels generated by two streams. But the paper is hard to follow and the motivation for the proposed architecture itself, is hidden in details. What is trying to be achieved by matching distributions and using the pseudo-targets of the each other? Perhaps the statistical distribution of the classes is assumed to be uniform but how will it extend to other priors, or even the case where we do not assume that we know the prior? The current setup needs justifications. 

What would be very interesting is to see two examples having the same class but one from MNIST, the other from Rotated-MNIST or Background-MNIST. Because it is hard to guess how different the examples in two streams. 

At the end, I feel like the authors have found a very interesting approach for classless association which can be extended to lots of many-to-one problems. This is a good catch. I would like to see the idea in the future with some extensive experiments on large scale datasets and tasks. But the current version lacks the theoretical motivations and convincing experiments. I would definitely recommend this paper to be presented in ICLR workshop.

Few more points:
Typo: Figure1. second line in the caption ""that"" -> ""than""
Necessity of Equation 2 is not clear
Batch size M is enormous compared to classical models, there is no explanation for this
Why uniform? should be clarified (of course it is the simplest prior to pick but just a few words about it would be good for completeness)
Typo: Page 6, second paragraph line 3: ""that"" -> ""than""

---

The paper looks correct but still i am not convinced about the experimentation performed. Perhaps another experiment with more challenging data would be welcome. Honestly i don't find a clear motivation for this work however it could have some potential and it would be interested to be presented in conference.

---

The paper looks correct but still i am not convinced about the experimentation performed. Perhaps another experiment with more challenging data would be welcome. Honestly i don't find a clear motivation for this work however it could have some potential and it would be interested to be presented in conference.

---

The paper explores neural-network learning on pairs of samples that are labeled as either similar or dissimilar. The proposed model appears to be different from standard siamese architectures, but it is poorly motivated. The experimental evaluation of the proposed model is very limited.

---

We have updated our paper.  The changes are
* We have improved the clarity and motivation of our model
* We have evaluated our model to three more classless datasets (Rotated-90 MNIST, Inverted MNIST, and Random Rotation MNIST).
* We have updated Figure 4 and 5 for showing some random output classification samples instead of the mean of all images.
* We have added two more examples and demo as supplemental material

---

The paper presents an alternative way of supervising the training of neural network without explicitly using labels when only link/not-link information is available between pairs of examples. A pair of network is trained each of which is used to supervise the other one.


The presentation of the paper is not very clear, the writing can be improved.
Some design choice are not explained: Why is the power function used in the E-step for approximating the distribution (section 2.1)? Why do the authors only consider a uniform distribution? I understand that using a different prior breaks the assumption that nothing is known about the classes. However I do not see a practical situations where the proposed setting/work would be useful.  

Also, there exist a large body of work in semi-supervised learning with co-training based on a similar idea. 

Overall, I think this work should be clarified and improved to be a good fit for this venue.

---

The paper explores a new technique for classless association, a milder unsupervised learning where we do not know the class labels exactly, but we have a prior about the examples that belong to the same class. Authors proposed a two stream architecture with two neural networks, as streams process examples from the same class simultaneously. Both streams rely on the target (pseudo classes or cluster indices) of each other, and the outputs an intermediate representation z, which is forced to match with a statistical distribution (uniform in their case). The model is trained with EM where the E step obtains the current statistical distribution given output vectors z, and M step updates the weights of the architecture given z and pseudo-classes. Experimental results on re-organized MNIST exhibits better performance compared to classical clustering algorithms (in terms of association accuracy and purity). The authors further provide comparison against a supervised method, where proposed architecture expectedly performs worse but with promising results.

The basic motivation of the architecture apparently relies on unlabeled data and agreement of the same pseudo-labels generated by two streams. But the paper is hard to follow and the motivation for the proposed architecture itself, is hidden in details. What is trying to be achieved by matching distributions and using the pseudo-targets of the each other? Perhaps the statistical distribution of the classes is assumed to be uniform but how will it extend to other priors, or even the case where we do not assume that we know the prior? The current setup needs justifications. 

What would be very interesting is to see two examples having the same class but one from MNIST, the other from Rotated-MNIST or Background-MNIST. Because it is hard to guess how different the examples in two streams. 

At the end, I feel like the authors have found a very interesting approach for classless association which can be extended to lots of many-to-one problems. This is a good catch. I would like to see the idea in the future with some extensive experiments on large scale datasets and tasks. But the current version lacks the theoretical motivations and convincing experiments. I would definitely recommend this paper to be presented in ICLR workshop.

Few more points:
Typo: Figure1. second line in the caption ""that"" -> ""than""
Necessity of Equation 2 is not clear
Batch size M is enormous compared to classical models, there is no explanation for this
Why uniform? should be clarified (of course it is the simplest prior to pick but just a few words about it would be good for completeness)
Typo: Page 6, second paragraph line 3: ""that"" -> ""than""

---

The paper looks correct but still i am not convinced about the experimentation performed. Perhaps another experiment with more challenging data would be welcome. Honestly i don't find a clear motivation for this work however it could have some potential and it would be interested to be presented in conference.",,,,,,5.333333333333333,,,3.3333333333333335,,
739,"Source file: 739.pdf

ABSTRACT
We provide an algorithm for polynomial feature expansion that both operates on and produces a compressed sparse row matrix without any densification. For a vector of dimension D, density d, and degree k the algorithm has time complexity O(dD) where k is the polynomial-feature order; this is an improvement by a factor d over the standard method.

1 INTRODUCTION
Polynomial feature expansion has long been used in statistics to approximate nonlinear functions Gergonne (1974); Smith (1918). The compressed sparse row (CSR) matrix format is a widelyused data structure to hold design matrices for statistics and machine learning applications. However, polynomial expansions are typically not performed directly on sparse CSR matrices, nor on any sparse matrix format for that matter, without intermediate densification steps. This densification not only adds extra overhead, but wastefully computes combinations of features that have a product of zero, which are then discarded during conversion into a sparse format.
We provide an algorithm that allows CSR matrices to be the input of a polynomial feature expansion without any densification. The algorithm leverages the CSR format to only compute products of features that result in nonzero values. This exploits the sparsity of the data to achieve an improved time complexity of O(dkDk) on each vector of the matrix where k is the degree of the expansion, D is the dimensionality, and d is the density. The standard algorithm has time complexity O(Dk). Since 0 ≤ d ≤ 1, our algorithm is a significant improvement. While the algorithm we describe uses CSR matrices, it could be modified to operate on other sparse formats.

2 PRELIMINARIES
Matrices are denoted by uppercase bold letters thus: A. The ithe row of A is written ai. All vectors are written in bold, and a, with no subscript, is a vector.
A compressed sparse row (CSR) matrix representation of an r-row matrix A consists of three vectors: c, d, and p and a single number: the number of columns of A. The vectors c and d contain the same number of elements, and hold the column indices and data values, respectively, of all nonzero elements of A. The vector p has r entries. The values in p index both c and d. The ith entry pi of p tells where the data describing nonzero columns of ai are within the other two vectors: cpi:pi+1 contain the column indices of those entries; dpi:pi+1 contain the entries themselves. Since only nonzero elements of each row are held, the overall number of columns of A must also be stored, since it cannot be derived from the other data.
Scalars, vectors, and matrices are often referenced with the superscript k. This is not to be interpreted as an exponent, but to indicate that it is the analogous aspect of that which procedes it, but in its polynomial expansion form. For example, c2 is the vector that holds columns for nonzero values in A’s quadratic feature expansion CSR representation.
For simplicity in the presentation, we work with polynomial expansions of degree 2, but continue to use the exponent k to show how the ideas apply in the general case. ∗Now at Google †The authors contributed equally important and fundamental aspects of this work.
We do provide an algorithm for third degree expansions, and derive the big-O time complexity of the general case.
We have also developed an algorithm for second and third degree interaction features (combinations without repetition), which can be found in the implementation.

3 MOTIVATION
In this section, we present a strawman algorithm for computing polynomial feature expansions on dense matrices. We then modify the algorithm slightly to operate on a CSR matrix, in order to expose its infeasibility in that context. We then show how the algorithm would be feasible with an added component, which we then derive in the following section.

3.1 DENSE EXPANSION ALGORITHM
A natural way to calculate polynomial features for a matrix A is to walk down its rows and, for each row, take products of all k-combinations of elements. To determine in which column of Aki products of elements in Ai belong, a simple counter can be set to zero for each row of A and incremented efter each polynomial feature is generated. This counter gives the column of Aki into which each expansion feature belongs.
SECOND ORDER (k = 2) DENSE POLYNOMIAL EXPANSION ALGORITHM(A) 1 N = row count of A 2 D = column count of A 3 Ak = empty N × ( D 2 ) matrix 4 for i = 0 to N − 1 5 cp = 0 6 for j1 = 0 to D − 1 7 for j2 = j1 to D − 1 8 Akicp = Aij1 ·Aij2 9 cp = cp + 1

3.2 IMPERFECT CSR EXPANSION ALGORITHM
Now consider how this algorithm might be modified to accept a CSR matrix. Instead of walking directly down rows of A, we will walk down sections of c and d partitioned by p, and instead of inserting polynomial features into Ak, we will insert column numbers into ck and data elements into dk.
INCOMPLETE SECOND ORDER (k = 2) CSR POLYNOMIAL EXPANSION ALGORITHM(A) 1 N = row count of A 2 pk = vector of size N + 1 3 pk0 = 0 4 nnzk = 0 5 for i = 0 to N − 1 6 istart = pi 7 istop = pi+1 8 ci = cistart:istop 9 nnzki = (|ci| 2
) 10 nnzk = nnzk + nnzki 11 pki+1 = p k i + nnz k i
// Build up the elements of pk, ck, and dk
12 pk = vector of size N + 1 13 ck = vector of size nnzk 14 dk = vector of size nnzk 15 n = 0 16 for i = 0 to N − 1 17 istart = pi 18 istop = pi+1 19 ci = cistart:istop 20 di = distart:istop 21 for c1 = 0 to |ci| − 1 22 for c2 = c1 to |ci| − 1 23 dkn = dc0 · dc1 24 ckn =? 25 n = n+ 1
The crux of the problem is at line 24. Given the arbitrary columns involved in a polynomial feature of Ai, we need to determine the corresponding column of Aki . We cannot simply reset a counter for each row as we did in the dense algorithm, because only columns corresponding to nonzero values are stored. Any time a column that would have held a zero value is implicitly skipped, the counter would err.
To develop a general algorithm, we require a mapping from columns of A to a column of Ak. If there are D columns of A and ( D k ) columns of Ak, this can be accomplished by a bijective mapping of the following form:
(j0, j1, . . . , jk−1) pj0j1...ik−1 ∈ {0, 1, . . . , ( D
k
) − 1} (1)
such that 0 ≤ j0 ≤ j1 ≤ · · · ≤ jk−1 < D where (j0, j1, . . . , jk−1) are elements of c and pj0j1...ik−1 is an element of ck.

4 CONSTRUCTION OF MAPPING
Within this section, i, j, and k denote column indices. For the second degree case, we seek a map from matrix indices (i, j) (with 0 ≤ i < j < D ) to numbers f(i, j) with 0 ≤ f(i, j) < D(D−1)2 , one that follows the pattern indicated by x 0 1 3x x 2 4x x x 5
x x x x  (2) where the entry in row i, column j, displays the value f(i, j). We let T2(n) = 12n(n + 1) be the nth triangular number; then in Equation 2, column j (for j > 0) contains entries with T2(j − 1) ≤
e < T2(j); the entry in the ith row is just i + T2(j − 1). Thus we have f(i, j) = i + T2(j − 1) = 1 2 (2i + j
2 − j). For instance, in column j = 2 in our example (the third column), the entry in row i = 1 is i+ T2(j − 1) = 1 + 1 = 2. With one-based indexing in both the domain and codomain, the formula above becomes f1(i, j) = 1 2 (2i+ j
2 − 3j + 2). For polynomial features, we seek a similar map g, one that also handles the case i = j. In this case, a similar analysis yields g(i, j) = i+ T2(j) = 12 (2i+ j 2 + j + 1).
To handle three-way interactions, we need to map triples of indices in a 3-index array to a flat list, and similarly for higher-order interactions. For this, we’ll need the tetrahedral numbers T3(n) =∑n
i=1 T2(n) = 1 6 (n 3 + 3n2 + 2n).
For three indices, i, j, k, with 0 ≤ i < j < k < D, we have a similar recurrence. Calling the mapping h, we have
h(i, j, k) = i+ T2(j − 1) + T3(k − 2); (3) if we define T1(i) = i, then this has the very regular form
h(i, j, k) = T1(i) + T2(j − 1) + T3(k − 2); (4) and from this the generalization to higher dimensions is straightforward. The formulas for “higher triangular numbers”, i.e., those defined by
Tk(n) = n∑ i=1 Tk−1(n) (5)
for k > 1 can be determined inductively.
The explicit formula for 3-way interactions, with zero-based indexing, is
h(i, j, k) = 1 + (i− 1) + (j − 1)j 2 + (6)
(k − 2)3 + 3(k − 2)2 + 2(k − 2) 6 . (7)

5 FINAL CSR EXPANSION ALGORITHM
With the mapping from columns of A to a column of Ak, we can now write the final form of the innermost loop of the algorithm from 3.2. Let the mapping for k = 2 be denoted h2. Then the innermost loop becomes:
for c2 = c1 to |ci| − 1 j0 = cc0 j1 = cc1 cp = h
2(j0, j1) dkn = dc0 · dc1 ckn = cp n = n+ 1
The algorithm can be generalized to higher degrees by simply adding more nested loops, using higher order mappings, modifying the output dimensionality, and adjusting the counting of nonzero polynomial features in line 9.

6 TIME COMPLEXITY

6.1 ANALYTICAL
Calculating k-degree polynomial features via our method for a vector of dimensionality D and density d requires ( dD k ) (with repetition) products. The complexity of the algorithm, for fixed k
dD, is therefore
O
(( dD + k − 1
k
)) = O ( (dD + k − 1)! k!(dD − 1)! ) (8)
= O
( (dD + k − 1)(dD + k − 2) . . . (dD)
k!
) (9)
= O ((dD + k − 1)(dD + k − 2) . . . (dD)) for k dD (10) = O ( dkDk ) (11)

6.2 EMPIRICAL
To demonstrate how our algorithm scales with the density of a matrix, we compare it to the traditional polynomial expansion algorithm in the popular machine library scikit-learn Pedregosa et al. (2011) in the task of generating second degree polynomial expansions. Matrices of size 100× 5000 were randomly generated with densities of 0.2, 0.4, 0.6, 0.8, and 1.0. Thirty matrices of each density were randomly generated, and the mean times (gray) of each algorithm were plotted. The red or blue width around the mean marks the third standard deviation from the mean. The time to densify the input to the standard algorithm was not counted.
The standard algorithm’s runtime stays constant no matter the density of the matrix. This is because it does not avoid products that result in zero, but simply multiplies all second order combinations of features. Our algorithm scales quadratically with respect to the density. If the task were third degree expansions rather than second, the plot would show cubic scaling.
The fact that our algorithm is approximately 6.5 times faster than the scikit-learn algorithm on 100× 5000 matrices that are entirely dense is likely a language implementation difference. What matters is that the time of our algorithm increases quadratically with respect to the density in accordance with the big-O analysis.

7 CONCLUSION
We have developed an algorithm for performing polynomial feature expansions on CSR matrices that scales polynomially with respect to the density of the matrix. The areas within machine learning that this work touches are not en vogue, but they are workhorses of industry, and every improvement in core representations has an impact across a broad range of applications.
","We provide an algorithm for polynomial feature expansion that both operates on and produces a compressed sparse row matrix without any densification. For a vector of dimension D, density d, and degree k the algorithm has time complexity O(dD) where k is the polynomial-feature order; this is an improvement by a factor d over the standard method.",ICLR 2017 conference submission,False,,"The paper is beyond my expertise. I cannot give any solid review comments regarding the techniques that are better than an educated guess.

However, it seems to me that the topic is not very relevant to the focus of ICLR. Also the quality of writing requires improvement, especially literature review and experiment analysis.

---

The approach/problem seems interesting, and several reviewers commented on this. However, the experimental evaluation is quite preliminary and the paper would be helped a lot with a connection to a motivating application. All of the reviewers pointed out that the work is not written in the usual in the scope of ICLR papers, and putting these together at this time it makes sense to reject the paper.

---

The authors present here a new algorithm for the effective calculation of polynomial features on Sparse Matrices. The key idea is to use a proper mapping between matrices and their polynomial versions, in order to derive an effective CSR expansion algorithm. The authors analyse the time complexity in a convincing way with experiments.

Overall, the algorithm is definitely interesting, quite simple and nice, with many possible applications. The paper is however very superficial in terms of experiments, or applications of the proposed scheme. Most importantly, the fit with the main scope of ICLR is far from obvious with this work, that should probably re-submitted to better targets.

---

This paper proposes an algorithm for polynomial feature expansion on CSR matrices, which reduces the time complexity of the standard method by a factor d^k where d is the density of the sparse matrix. The main contribution of this work is not significant enough. The experiments are incomplete and not convincing.

The background of the problem is not sufficiently introduced. There are only two references in the introduction part (overall only three papers are cited), which are from decades ago. Many more relevant papers should be cited from the recent literature.

The experiment part is very weak. This paper claims that the time complexity of their algorithm is O(d^k D^k), which is an improvement over standard method O(D^k) by a factor d^k. But in the experiments, when d=1, there is still a large gap (~14s vs. ~90s) between the proposed method and the standard one. The authors explain this as ""likely a language implementation"", which is not convincing. To fairly compare the two methods, of course you need to implement both in the same programming language and run experiments in the same environment. For higher degree feature expansion, there is no empirical experiments to show the advantage of the proposed method.

Some minor problems are listed below.
1) In Section 2, the notation ""p_i:p_i+1"" is not clearly defined.
2) In Section 3.1, typo: ""efter"" - ""after""
3) All the algorithms in this paper are not titled. The input and output is not clearly listed.
4) In Figure 1, the meaning of the colored area is not described. Is it standard deviation or some quantile of the running time? How many runs of each algorithm are used to generate the ribbons? Many details of the experimental settings are missing.

---

The paper is beyond my expertise. I cannot give any solid review comments regarding the techniques that are better than an educated guess.

However, it seems to me that the topic is not very relevant to the focus of ICLR. Also the quality of writing requires improvement, especially literature review and experiment analysis.

---

The paper is beyond my expertise. I cannot give any solid review comments regarding the techniques that are better than an educated guess.

However, it seems to me that the topic is not very relevant to the focus of ICLR. Also the quality of writing requires improvement, especially literature review and experiment analysis.

---

The approach/problem seems interesting, and several reviewers commented on this. However, the experimental evaluation is quite preliminary and the paper would be helped a lot with a connection to a motivating application. All of the reviewers pointed out that the work is not written in the usual in the scope of ICLR papers, and putting these together at this time it makes sense to reject the paper.

---

The authors present here a new algorithm for the effective calculation of polynomial features on Sparse Matrices. The key idea is to use a proper mapping between matrices and their polynomial versions, in order to derive an effective CSR expansion algorithm. The authors analyse the time complexity in a convincing way with experiments.

Overall, the algorithm is definitely interesting, quite simple and nice, with many possible applications. The paper is however very superficial in terms of experiments, or applications of the proposed scheme. Most importantly, the fit with the main scope of ICLR is far from obvious with this work, that should probably re-submitted to better targets.

---

This paper proposes an algorithm for polynomial feature expansion on CSR matrices, which reduces the time complexity of the standard method by a factor d^k where d is the density of the sparse matrix. The main contribution of this work is not significant enough. The experiments are incomplete and not convincing.

The background of the problem is not sufficiently introduced. There are only two references in the introduction part (overall only three papers are cited), which are from decades ago. Many more relevant papers should be cited from the recent literature.

The experiment part is very weak. This paper claims that the time complexity of their algorithm is O(d^k D^k), which is an improvement over standard method O(D^k) by a factor d^k. But in the experiments, when d=1, there is still a large gap (~14s vs. ~90s) between the proposed method and the standard one. The authors explain this as ""likely a language implementation"", which is not convincing. To fairly compare the two methods, of course you need to implement both in the same programming language and run experiments in the same environment. For higher degree feature expansion, there is no empirical experiments to show the advantage of the proposed method.

Some minor problems are listed below.
1) In Section 2, the notation ""p_i:p_i+1"" is not clearly defined.
2) In Section 3.1, typo: ""efter"" - ""after""
3) All the algorithms in this paper are not titled. The input and output is not clearly listed.
4) In Figure 1, the meaning of the colored area is not described. Is it standard deviation or some quantile of the running time? How many runs of each algorithm are used to generate the ribbons? Many details of the experimental settings are missing.

---

The paper is beyond my expertise. I cannot give any solid review comments regarding the techniques that are better than an educated guess.

However, it seems to me that the topic is not very relevant to the focus of ICLR. Also the quality of writing requires improvement, especially literature review and experiment analysis.",,,,,,3.0,,,2.3333333333333335,,
745,"Authors: SOUND COMBINERS, Saeed Maleki, Madanlal Musuvathi, Todd Mytkowicz, Yufei Ding
Source file: 745.pdf

ABSTRACT
Stochastic gradient descent (SGD) is a well-known method for regression and classification tasks. However, it is an inherently sequential algorithm — at each step, the processing of the current example depends on the parameters learned from the previous examples. Prior approaches to parallelizing SGD, such as HOGWILD! and ALLREDUCE, do not honor these dependences across threads and thus can potentially suffer poor convergence rates and/or poor scalability. This paper proposes SYMSGD, a parallel SGD algorithm that retains the sequential semantics of SGD in expectation. Each thread in this approach learns a local model and a probabilistic model combiner that allows the local models to be combined to produce the same result as what a sequential SGD would have produced, in expectation. This SYMSGD approach is applicable to any linear learner whose update rule is linear. This paper evaluates SYMSGD’s accuracy and performance on 9 datasets on a shared-memory machine shows up-to 13× speedup over our heavily optimized sequential baseline on 16 cores.

1 INTRODUCTION
Stochastic Gradient Descent (SGD) is an effective method for many regression and classification tasks. It is a simple algorithm with few hyper-parameters and its convergence rates are well understood both theoretically and empirically. However, its performance scalability is severely limited by its inherently sequential computation. SGD iteratively processes its input dataset where the computation at each iteration depends on the model parameters learned from the previous iteration.
Current approaches for parallelizing SGD do not honor this inter-step dependence across threads. Each thread learns a local model independently and combine these models in ways that can break sequential behavior. For instance, threads in HOGWILD! Recht et al. (2011) racily update a shared global model without holding any locks. In parameter-server Li et al. (2014a), each thread (or machine) periodically sends its model deltas to a server that applies them to a global model. In ALLREDUCE Agarwal et al. (2014), threads periodically reach a barrier where they compute a weightedaverage of the local models. Although these asynchronous parallel approaches reach the optimal solution eventually, they can produce a model that is potentially different from what a sequential SGD would have produced after processing a certain number of examples. Our experiments indicate that this makes their convergence rate slower than sequential SGD in terms of total number of examples studied. Our experiments show that all these algorithms either do not scale or their accuracy on the same number of examples falls short of a sequential baseline.
To address this problem, this paper presents SYMSGD, a parallel SGD algorithm that seeks to retain its sequential semantics. The key idea is for each thread to generate a sound combiner that allows the local models to be combined into a model that is the same as the sequential model. This paper describes a method for generating sound combiners for a class of SGD algorithms in which the inter-step dependence is linear in the model parameters. This class includes linear regression, linear regression with L2 regularization, and polynomial regression. While logistic regression is not in this class, our experiments show that linear regression performs equally well in classification tasks as logistic regression for the datasets studied in this paper. Also, this approach works even if the SGD
computation is non-linear on the input examples and other parameters such as the learning rate; only the dependence on model parameters has to be linear.
Generating sound combiners can be expensive. SYMSGD uses random projection techniques to reduce this overhead but still retaining sequential semantics in expectation. We call this approach probabilistically sound combiners. Even though SYMSGD is expected to produce the same answer as the sequential SGD, controlling the variance introduced by the random projection requires care — a large variance can result in reduced accuracy. This paper describes the factors that affect this variance and explores the ensuing design trade-offs.
The resulting algorithm is fast, scales well on multiple cores, and achieves the same accuracy as sequential SGD on sparse and dense datasets. When compared to our optimized sequential baseline, SYMSGD achieves a speedup of 3.5× to 13× on 16 cores, with the algorithm performing better for denser datasets. Moreover, the cost of computing combiners can be efficiently amortized in a multiclass regression as a single combiner is sufficient for all of the classes. Finally, SYMSGD (like ALLREDUCE) is deterministic, producing the same result for a given dataset, configuration, and random seed. Determinism greatly simplifies the task of debugging and optimizing learning.

2 SOUND AND PROBABILISTIC MODEL COMBINERS
Stochastic gradient descent (SGD) is a robust method for finding the parameters of a model that minimize a given error function. Figure 1 shows an example of a (convex) error function over two dimensions x and y reaching the minimum at parameter w∗. SGD starts from some, not necessarily optimal, parameter wg (as shown in Figure 1), and repeatedly modifies w by taking a step along the gradient of the error function for a randomly selected example at the currentw. The magnitude of the step is called the learning rate and is usually denoted by α. The gradient computed from one example is not necessarily the true gradient at w. Nevertheless, SGD enjoys robust convergence behavior by moving along the “right” direction over a large number of steps. This is shown pictorially in Figure 1, where SGD processes examples in dataset D1 to reach w1 from wg . Subsequently, SGD starts from w1 and processes a different set D2 to reach wh. There is a clear dependence between the processing ofD1 and the processing ofD2 — the latter starts from w1, which is only determined after processing D1. Our goal is to parallelize SGD despite this dependence.
State of the art parallelization techniques such as HOGWILD! and ALLREDUCE approach this problem by processing D1 and D2 starting from the same model wg (let us assume that there only two processors for now), and respectively reaching w1 and w2. Then, they combine their local models into a global model, but do so in an ad-hoc manner. For instance, ALLREDUCE computes a weighted average of w1 and w2, where the per-feature weights are chosen so as to prefer the processor that has larger update for that feature. This weighted average is depicted pictorially as wa. Similarly, in HOGWILD!, the two processors race to update the global model with their respective local model without any locking. (HOGWILD! performs this udpate after every example, thus the size of D1 and D2 is 1.) Both approaches do not necessarily reach wh, the model that a sequential SGD would have reached on D1 and D2. While SGD is algorithmically robust to errors, such ad-hoc combinations can result in slower convergence or poor performance, as we demonstrate in Section 4.
Sound Combiner: The goal of this paper is to soundly combine local models. Looking at Figure 1, a sound combiner combines local models w1 and w2, respectively generated from datasets D1 and D2, into a global model wh that is guaranteed to be the same as the model achieved by the sequential
SGD processing D1 and then D2. In effect, a sound combiner allows us to parallelize the sequential computation without changing its semantics.
If we look at the second processor, it starts its computation at wg , while in a sequential execution it would have started at w1, the output of the first processor. To obtain sequential semantics, we need to “adjust” its computation from wg to w1. To do so, the second processor performs its computation starting fromwg +∆w, where ∆w is an unknown symbolic vector. This allows the second processor to both compute a local model (resulting from the concrete part) and a sound combiner (resulting from the symbolic part) that accounts for changes in the initial state. Once both processors are done learning, second processor finds wh by setting ∆w to w1 − wg where w1 is computed by the first processor. This parallelization approach of SGD can be extended to multiple processors where all processor produce a local model and a combiner (except for the first processor) and the local models are combined sequentially using the combiners.
When the update to the model parameters is linear in a SGD computation, then the dependence on the unknown ∆w can be concisely represented by a combiner matrix, as formally described in Section 3. Many interesting machine learning algorithms, such as linear regression, linear regression with L2 regularization, and polynomial regression already have linear update to the model parameters (but not necessarily linear on the input example).
Probabilistically Sound Combiner: The main problem with generating a sound combiner is that the combiner matrix has as many rows and columns as the total number of features. Thus, it can be effectively generated only for datasets with modest number of features. Most interesting machine learning problems involve learning over tens of thousands to billions of features, for which maintaining a combiner matrix is clearly not feasible.
We solve this problem through dimensionality reduction. Johnson-Lindenstrauss (JL) lemma Johnson & Lindenstrauss (1984) allows us to project a set of vectors from a high-dimensional space to a random low-dimensional space while preserving distances. We use this property to reduce the size of the combiner matrix without losing the fidelity of the computation — our parallel algorithm produces the same result as the sequential SGD in expectation.
Of course, a randomized SGD algorithm that generates the exact result in expectation is only useful if the resulting variance is small enough to maintain accuracy and the rate of convergence. We observe that for the variance to be small, the combiner matrix should have small singular values. Interestingly, the combiner matrix resulting from SGD is dominated by the diagonal entries as the learning rate has to be small for effective learning. We use this property to perform the JL projection only after subtracting the identity matrix. Also, other factors that control the singular values are the learning rate, number of processors, and the frequency of combining local models. This paper explores this design space and demonstrates the feasibility of efficient parallelization of SGD that retains the convergence properties of sequential SGD while enjoying parallel scalability.

3 PARALLEL SYMSGD ALGORITHM
Consider a training dataset (Xn×f , yn×1), where f is the number of features, n is the number of examples in the dataset, the ith row of matrix X , Xi, represents the features of the ith example, and yi is the dependent value (or label) of that example. A linear model seeks to find a
w∗ = arg min w∈Rf n∑ i=0 Q(Xi · w, yi)
that minimizes an error function Q. For linear regression, Q(Xi · w, yi) = (Xi · w − yi)2. When (Xi, yi) is evident from the context, we will simply refer to the error function as Qi(w).
SGD iteratively finds w∗ by updating the current model w with a gradient of Qr(w) for a randomly selected example r. For the linear regression error function above, this amounts to the update
wi = wi−1 − α∇Qr(wi−1) = wi−1 − α(Xr · wi−1 − yr)XTr (1) Here, α is the learning rate that determines the magnitude of the update along the gradient. As it is clear from this equation, wi is dependent on wi−1 which creates a loop-carried dependence and consequently makes parallelization of SGD across iterations using naı̈ve approaches impossible.
The complexity of SGD for each iteration is as follows. Assume thatXr has z non-zeros. Therefore, the computation in Equation 1 requires O(z) amount of time for the inner product computation, Xr ·wi−1, and the sameO(z) amount of time for scalar-vector multiplication, α(Xr ·wi−1−yr)XTr . If the updates to the weight vector happen in-place meaning thatwi andwi−1 share the same memory location, the computation in Equation 1 takes O(z) amount of time.

3.1 SYMBOLIC STOCHASTIC GRADIENT DESCENT
This section explains a new approach to parallelize the SGD algorithm despite its loop-carried dependence. As shown in Figure 1, the basic idea is to start each processor (except the first) on a concrete model w along with a symbolic unknown ∆w that captures the fact that the starting model can change based on the output of the previous processor. If the dependence on ∆w is linear during an SGD update, which is indeed the case for linear regression, then the symbolic dependence on ∆w on the final output can be captured by an appropriate matrix Ma→b that is a function of the input examples Xa, . . . , Xb processed (ya, . . . , yb do not affect this matrix). Specifically, as Lemma A.1 in the Appendix shows, this combiner matrix is given by
Ma→b = a∏ i=b (I − αXTi ·Xi) (2)
In effect, the combiner matrix above is the symbolic representation of how a ∆w change in the input will affect the output of a processor. Ma→b is referred by M when the inputs are not evident.
The parallel SGD algorithm works as follows (see Figure 1). In the learning phase, each processor i starting from w0, computes both a local model li and a combiner matrix Mi. In a subsequent reduction phase, each processor in turn computes its true output using
wi = li +Mi · (wi−1 − w0) (3) Lemma A.1 ensures that this combination of local models will produce the same output as what these processors would have generated had they run sequentially. We call such combiners sound.
One can compute a sound model combiner for other SGD algorithms provided the loop-carried dependence on w is linear. In other words, there should exist a matrix Ai and vector bi in iteration i such that wi = Ai · wi−1 + bi. Note that Ai and bi can be nonlinear in terms of input datasets.

3.2 DIMENSIONALITY REDUCTION OF A SOUND COMBINER
The combiner matrixM generate above can be quite large and expensive to compute. The sequential SGD algorithm maintains and updates the weight vector w, and thus requires O(f) space and time, where f is the number of features. In contrast, M is a f × f matrix and consequently, the space and time complexity of parallel SGD is O(f2). In practice, this would mean that we would need O(f) processors to see constant speedups, an infeasible proposition particularly for datasets that can have thousands if not millions of features.
SYMSGD resolves this issue by projecting M into a smaller space while maintaining its fidelity. This projection is inspired by the Johnson-Lindenstrauss (JL) lemma Johnson & Lindenstrauss (1984) and follows the treatment of Achlioptas Achlioptas (2001). Lemma 3.1. 1 Let A be a random f × k matrix with
aij = dij/ √ k
where aij is the element of A at the ith row and jth column and dij is independently sampled from a random distribution D with IE[D] = 0 and Var[D] = 1. Then
IE[A ·AT ] = If×f
The matrix A from Lemma 3.1 projects from Rf → Rk where k can be much smaller than f . This allows us to approximate Equation 3 as
wi ≈ li +Mi ·A ·AT (wi−1 − w0) (4) 1See proof in Appendix A.2.
Lemma 3.1 guarantees that the approximation above is unbiased.
IE[li +Mi ·A ·AT (wi−1 − w0)] = li +Mi · IE[A ·AT ](wi−1 − w0) = wi
This allows an efficient algorithm that only computes the projected version of the combiner matrix while still producing the same answer as the sequential algorithm in expectation. We call such combiners probabilistically sound.
Algorithm 1: SYMSGD learning a local model and a model combiner. 1 <vector,matrix,matrix> SymSGD( 2 float α, vector: w0, X1..Xn, 3 scalar: y1..yn) { 4 vector w = w0; 5 matrix A = 1√
k random(D,f,k);
6 matrix MA = A; 7 for i in (1..n) { 8 w = w - α(Xi·w - yi)XiT; 9 MA = MA - α Xi·(XiTMA); }
10 return <w,MA,A>; }
Algorithm 2: SYMSGD combining local models using model combiners. 1 vector SymSGDCombine(vector w0, 2 vector w, vector l, 3 matrix MA, matrix A) { 4 parallel { 5 matrix NA = MA - A; 6 w = l+w-w0+NA·AT(w-w0); 7 } 8 return w; }
Algorithm 1 shows the resulting symbolic SGD learner. The random function in line 5 returns a f × k matrix with elements chosen independently from the random distribution D according to Lemma 3.1. When compared to the sequential SGD, the additional work is the computation of MA in Line 9. It is important to note that this algorithm maintains the invariant that MA = M · A at every step. This projection incurs a space and time overhead of O(z × k) where z is the number of non-zeros in Xi. This overhead is acceptable for small k and infact in our experiments in Section 4, k is between 7 to 15 across all benchmarks. Most of the overhead for such a small k is hidden by utilizing SIMD hardware within a processor (SymSGD with one thread is only half as slow as the sequential SGD as discussed in Section 4.1). After learning a local model and a probabilistically sound combiner in each processor, Algorithm 2 combines the resulting local model using the combiners, but additionally employs the optimizations discussed in Section 3.3.
Note that the correctness and performance of SYMSGD do not depend on the sparsity of a dataset and as Section 4 demonstrates, it works for very sparse and completely dense datasets. Also, note that X1, . . . ,Xn may contain a subset of size f’ of all f features. Our implementation of Algorithm 1 takes advantage of this property and allocates and initializes A for only the observed features. This optimization is omitted from the pseudo code in Algorithm 1 for the sake of simplicity.

3.3 CONTROLLING THE VARIANCE
While the dimensionality reduction discussed above is expected to produce the right answer, this is useful only if the variance of the approximation is acceptably small. Computing the variance is involved and is discussed in the associated technical report SymSGDTR. But we discuss the main result that motivates the rest of the paper.
Consider the approximation ofM ·∆w with v = M ·A ·AT ·∆w. Let C(v) be the covariance matrix of v. The trace of the covariance matrix tr(C(v)) is the sum of the variance of individual elements of v. Let λi(M) by the ith eigenvalue of M and σi(M) = √ λi(MTM) the ith singular value of M . Let σmax(M) be the maximum singular value of M . Then the following holds SymSGDTR:
‖∆w‖22 k ∑ i σ2i (M) ≤ tr(C(v)) ≤ ‖∆w‖22 k ( ∑ i σ2i (M) + σ 2 max(M))
The covariance is small if k, the dimension of the projected space, is large. But increasing k proportionally increases the overhead of the parallel algorithm. Similarly, covariance is small if the projection happens on small ∆w. Looking at Equation 4, this means that wi−1 should be as close to w0 as possible, implying that processors should communicate frequently enough such that their
models are roughly in sync. Finally, the singular values of M should be as small as possible. The next section describes a crucial optimization that achieves this.
Taking Identity Off: Expanding Equation 2, we see that the combiner matrices are of the form
I − αR1 + α2R2 − α3R3 + · · ·
where Ri matrices are formed from the sum of products of Xj · XTj matrices. Since α is a small number, this sum is dominated by I . In fact, for a combiner matrix M generated from n examples, M − I has at most n non-zero singular values SymSGDTR. We use these observation to lower the variance of dimensionality reduction by projecting matrix N = M − I instead of M . Appendix A.3 empirically shows the impact of this optimization. Rewriting Equations 3 and 4, we have
wi = li + (Ni + I) · (wi−1 − w0) = li + wi−1 − w0 +Ni · (wi−1 − w0) ≈ li + wi−1 − w0 +Ni ·A ·AT · (wi−1 − w0) (5)
Lemma 3.1 guarantees that the approximation above is unbiased. Algorithm 2 shows the pseudo code for the resulting probabilistically sound combination of local models. The function SymSGDCombine is called iteratively to combine the model of the first processor with the local models of the rest. Note that each model combination is executed in parallel (Line 4) by parallelizing the underlying linear algebra operations.
An important factor in controlling the singular values of N is the frequency of model combinations which is a tunable parameter in SYMSGD. As it is shown in Appendix A.3, the fewer the number of examples learned, the smaller the singular values of N and the less variance (error) in Equation 5.
Implementation For the implementation of SymSGD function, matrix M and weight vector w are stored next to each other. This enables better utilization of vector units in the processor and improves the performance of our approach significantly. Also, most of datasets are sparse and therefore, SGD and SymSGD only copy the observed features from w0 to their learning model w. Moreover, for the implementation of matrix A, we used Achlioptas (2001) theorem to minimize the overhead of creating A. In this approach, each element of A is independently chosen from { 13 ,− 1 3 , 0} with probability { 16 , 1 6 , 2 3}, respectively.

4 EVALUATION
All experiments described in this section were performed on an Intel Xeon E5-2630 v3 machine clocked at 2.4 GHz with 256 GB of RAM. The machine has two sockets with 8 cores each, allowing us to study the scalability of the algorithms across sockets. We disabled hyper-threading and turbo boost. We also explicitly pinned threads to cores in a compact way which means that thread i + 1 was placed as close as possible to thread i. The machine runs Windows 10. All of our implementations were compiled with Intel C/C++ compiler 16.0 and relied heavily on OpenMP primitives for parallelization and MKL for efficient linear algebra computations. And, finally, to measure runtime, we use the average of five independent runs on an otherwise idle machine.
There are several algorithms and implementations that we used for our comparison: Vowpal Wabbit Langford et al. (2007), a widely used public library, Baseline, a fast sequential implementation, HW-Paper, the implementation from Recht et al. (2011), HW-Release, an updated version, HogWild, which runs Baseline in multiple threads without any synchronization, and ALLREDUCE, the implementation from Agarwal et al. (2014). Each of these algorithms have different parameters and settings and we slightly modified to ensure a fair comparison; see Appendix A.4 for more details.
When studying the scalability of a parallel algorithm, it is important to compare the algorithms against an efficient baseline Bailey (1991); McSherry et al. (2015). Otherwise, it is empirically not possible to differentiate between the scalability achieved from the parallelization of the inefficiencies and the scalability inherent in the algorithm. We spent a significant effort to implement a well-tuned sequential algorithm which we call Baseline in our comparisons. Baseline is between 1.97 to 7.62 (3.64 on average) times faster than Vowpal Wabbit and it is used for all speedup graphs in this paper.
Datasets Table 1 describes the datasets used for evaluation. The number of features, training instances, test instances, classes and the sparsity of each dataset is shown in Table 1. We used Vowpal
Wabbit with the configurations discussed in Appendix A.4 to measure the maximum accuracies that can be achieved using linear and logistic regression and the result is presented in columns 8 and 9 of Table 1. In the case of aloi dataset, even after 500 passes (the default for our evaluation was 100 passes) the accuracies did not saturate to the maximum possible and we reported that both linear and logistic achieved at least 80% accuracy. The last two columns show the maximum speedup of SYMSGD and HOGWILD! over the baseline.
Parameters Hyper-parameters searching is essential for performance and accuracy. The learning rate, α, for each dataset was selected by searching for a constant value among {.5, .05, .005, . . . } where Baseline reached close to maximum accuracy for each benchmark. The parameters for the projection size, k, and the frequency of model combination were searched to pick the best performing configuration. The parameters for ALLREDUCE were similarly searched.

4.1 RESULTS
Figure 2 shows the accuracy and speedup measurements on three benchmarks: rcv1.binary, a sparse binary dataset, rcv1.multiclass, a sparse multiclass dataset, and epsilon, a dense binary dataset. The results for the other six benchmarks are presented in Appendix A.5.
Sparse Binary, rcv1.binary: Figure 2a compares the scalability of all the algorithms studied in this paper. HW-Paper is around six times slower than HW-Release. While this could potentially be a result of us running HW-Release on a Ubuntu VM, our primary aim of this comparison was to ensure that HogWild is a competitive implementation of HOGWILD!. Thus, we remove HW-Paper and HW-Release in our subsequent comparisons.
SYMSGD is half as slow as the Baseline on one thread as it performs lot more computation, but scales to a 3.5× speedup to 16 cores. Note, this represents a roughly 7× strong-scaling speedup with respect to its own performance on one thread. Analysis of the hardware performance counters shows the current limit to SYMSGD’s scalability arises from load-imbalance across barrier synchronization, which provides an opportunity for future improvement.
Figure 2d shows the accuracy as a function of the number of examples processed by different algorithms. SYMSGD “stutters” at the beginning, but it too matches the accuracy of Baseline. The initial stuttering happens because the magnitude of the local models on each processor are large during the first set of examples. This directly affect the variance of the combiner matrix approximation. However, as more examples are given to SYMSGD, the magnitude of the local models are smaller and thus SYMSGD better matches the Baseline accuracy. One way to avoid this stuttering is to combine models more frequently (lower variance) or running single threaded for the first few iterations.
HogWild does approach sequential accuracy, however, it does so at the cost of scalablity (i.e., see Figure 2a (a)). Likewise, ALLREDUCE scales slightly better but does so at the cost of accuracy.
Sparse Multiclass, rcv1.multiclass: Figure 2b shows the scalability on rcv1.multiclass. Since this is a multiclass dataset, SYMSGD is competitive with the baseline on one thread as it is able to amortize the combiner matrix computation across all of the classes (M is the same across different classes). Thus, it enjoys much better scalability of 7× when compared to rcv1.binary. HogWild scales similar to SYMSGD up-to 8 threads but suffers when 16 threads across multiple sockets are used. Figure 2e shows that SYMSGD meets the sequential accuracy after an initial stutter. ALLREDUCE suffers from accuracy.
Dense Binary, epsilon: Figure 2c in Appendix A.5 shows that SYMSGD achieves a 7× speedup over the baseline on 16 cores. This represents a 14× strong scaling speedup over SYMSGD on one thread. As HOGWILD! is not designed for dense workloads, its speedup suffers when 16 cores across multiple sockets are used. This shows that SYMSGD scales to both sparse and dense datasets. Similarly, ALLREDUCE suffers from accuracy.

5 RELATED WORK
Most schemes for parallelizing SGD learn local models independently and communicate to update the global model. The algorithms differ in how and how often the update is performed. These choices determine the applicability of the algorithm to shared-memory or distributed systems.
To the best of our knowledge, our approach is the only one that retain the semantics of the sequential SGD algorithm. While some prior work provides theoretical analysis of the convergence rates that justify a specific parallelization, convergence properties of SYMSGD simply follow from the sequential SGD algorithm. On the other hand, SYMSGD is currently restricted to class of SGD computations where the inter-step dependence is linear in the model parameters.
Given a tight coupling of the processing units, Langford et al. Langford et al. (2009) suggest on a round-robin scheme to update the global model allowing for some staleness. However, as the SGD computation per example is usually much smaller when compared to the locking overhead, HOGWILD! Recht et al. (2011) improves on this approach to perform the update in a “racy” manner. While HOGWILD! is theoretically proven to achieve good convergence rates provided the dataset is sparse enough and the processors update the global model fast enough, our experiments show that the generated cache-coherence traffic limits its scalability particularly across multiple sockets. Moreover, as HOGWILD! does not update the model atomically, it potentially loses correlation among more frequent features resulting in loss of accuracy. Lastly, unlike SYMSGD, which works for both sparse and dense datasets, HOGWILD! is expclitly designed for sparse data. Recently, Sallinen et al. (2016) proposed applying lock-free HOGWILD! approach to mini-batch. However, mini-batch converges slower than SGD and also they did not study multi-socket scaling.
Zinkevich et al. Zinkevich et al. (2010) propose a MapReduce-friendly framework for SGD. The basic idea is for each machine/thread to run a sequential SGD on its local data. At the end, the global model is obtained by averaging these local models. Alekh et al. Agarwal et al. (2014) extend this approach by using MPI AllReduce operation. Additionally, they use the adagrad Duchi et al. (2011) approach for the learning rates at each node and use weighted averaging to combine local models
with processors that processed a feature more frequently having a larger weight. Our experiments on our datasets and implementation shows that it does not achieve the sequential accuracy.
Several distributed frameworks for machine learning are based on parameter server Li et al. (2014b;a) where clients perform local learning and periodically send the changes to a central parameter server that applies the changes. For additional parallelism, the models themselves can be split across multiple servers and clients only contact a subset of the servers to perform their updates.

6 CONCLUSION
With terabytes of memory available on multicore machines today, our current implementation has the capability of learning from large datasets without incurring the communication overheads of a distributed system. That said, we believe the ideas in this paper apply to distributed SGD algorithms and how to pursue in future work.
Many machine learning SGD algorithms require a nonlinear dependence on the parameter models. While SYMSGD does not directly apply to such algorithms, it is an interesting open problem to devise linear approximations (say using Taylor expansion) to these problems and subsequently parallelize with probabilistically sound combiners. This is an interesting study for future work.

A APPENDIX
A.1 COMBINER MATRIX
Lemma A.1. If the SGD algorithm for linear regression processes examples (Xa, ya), (Xa+1, ya+1), . . . , (Xb, yb) starting from model ws to obtain wb, then its outcome starting on model ws + ∆w is given by wb + Ma→b · ∆w where the combiner matrix Ma→b is given by
Ma→b = a∏ i=b (I − αXTi ·Xi)
Proof. The proof follows from a simple induction. Starting from ws, let the models computed by SGD after processing (Xa, ya), (Xa+1, ya+1), . . . , (Xb, yb) respectively be wa, wa+1, . . . wb. Consider the base case of processing of (Xa, ya). Starting from ws + ∆w, SGD computes the model w′a using Equation 1 (reminder: wi = wi−1 − α(Xi · wi−1 − yi)XTi ):
w′a = ws + ∆w − α(Xa · (ws + ∆w)− ya)XTa = ws + ∆w − α(Xa · ws − ya)XTa − α(Xa ·∆w)XTa = ws − α(Xa · ws − ya)XTa + ∆w − α(Xa ·∆w)XTa = wa + ∆w − α(Xa ·∆w)XTa (6) = wa + ∆w − αXTa (Xa ·∆w) (7) = wa + ∆w − α(XTa ·Xa) ·∆w (8) = wa + (I − αXTa ·Xa) ·∆w
Step 6 uses Equation 1, Step 7 uses the fact that Xa ·∆w is a scalar (allowing it to be rearranged), and Step 8 follows from associativity property of matrix multiplication.
The induction is very similar and follows from replacing ∆w with Ma→i−1∆w and the property that
Ma→i = (I − αXTi ·Xi) ·Ma→i−1
A.2 PROOF OF LEMMA 3.1 Proof. Let’s call B = A · AT . Then bij , the element of B at row i and column j, is ∑
s aisajs. Therefore, IE[bij ] = ∑k s=1 IE[aisajs] = ( 1√ k )2 ∑k s=1 IE[disdjs] = 1 k ∑k s=1 IE[disdjs]. For i 6= j, IE[bij ] = 1 k ∑k s=1 IE[dis]IE[djs] because dij are chosen independently. Since IE[D] = 0 and dis, djs ∈ D, IE[dis] = IE[djs] = 0 and consequently, IE[bij ] = 0. For i = j, IE[bii] = 1 k ∑ s IE[disdis] = 1 k ∑ s IE[d 2 is]. Since IE[D
2] = 1 and dis ∈ D, IE[d2is] = 1. As a result, IE[bii] = 1 k ∑k s=1 IE[d 2 is] = 1 k ∑k s=1 1 = 1.
A.3 EMPIRICAL EVALUATING SINGULAR VALUES OF M
Figure 3 empirically demonstrates the benefit of taking identity off. This figure plots the singular values of M for the rcv1.binary dataset (described in Section 4) after processing 64, 128, 192, 256 examples for two different learning rates. As it can be seen, the singular values are close to 1. However, the singular values of N = M − I are roughly the same as those of M minus 1 and consequently, are small. Finally, the smaller α, the closer the singular values of M are to 1 and the singular values of N are to 0. Also, note that the singular values of M decrease as the numbers of examples increase and therefore, the singular values of N increase. As a result, the more frequent the models are combined, the less variance (and error) is introduced into Equation 5.
A.4 ALGORITHM DETAILS AND SETTINGS
This section provides details of all algorithms we used in this paper. Each algorithm required slight modification to ensure fair comparison.
Vowpal Wabbit: Vowpal Wabbit Langford et al. (2007) is one of the widely used public libraries for machine learning algorithms. We used this application as a baseline for accuracy of different datasets and as a comparison of logistic and linear regression and also an independent validation of the learners without any of our implementation bias. Vowpal Wabbit applies accuracy-improving optimizations such as adaptive and individual learning steps or per feature normalize updates. While all of these optimizations are applicable to SYMSGD, we avoided them since the focus of this paper is the running time performance of our learner. The non-default flags that we used are: --sgd, --power t 0, --holdout off, --oaa nc for multiclass datasets where nc is the number of classes, --loss function func where func is squared or logistic. For learning rate, we searched for α, the learning rate, in the set of {.1, .5, .01, .05, .001, .005, . . . } and used --learning rate α. We went through dataset 100 times for each dataset (--passes 100) and saved the learned model after each pass (--save per pass). At the end, for linear and logistic regressions, we reported the maximum accuracies achieved among different passes and different learning rates.
Baseline: Baseline uses a mixture of MKL Intel and manually vectorized implementations of linear algebra primitives in order to deliver the fastest performance. Baseline processes up-to 3.20 billion features per second at 6.4 GFLOPS.
HOGWILD!: HOGWILD! Recht et al. (2011) is a lock-free approach to parallelize SGD where multiple thread apply Equation 1 simultaneously. Although this approach may have race condition
when two threads process instances with a shared feature but the authors discuss that this does not hurt the accuracy significantly for sparse datasets. There are multiple implementations of this approach that we studied and evaluated in this section. Below is a description of each:
• HW-Paper: This is the implementation used to report the measurements in Recht et al. (2011) which is publicly available Hogwild. This code implements SVM algorithm. Therefore, we modified the update rule to linear regression. The modified code was compiled and run on our Windows machine described above using an Ubuntu VM since the code is configured for Linux systems.
• HW-Release: This is an optimized implementation that the authors built after the HOGWILD! paper Recht et al. (2011) was published. Similar to HW-Paper, we changed the update rule accordingly and executed it on the VM.
• HogWild: We implemented this version which runs Baseline by multiple threads without any synchronizations. This code runs natively on Windows and enjoys all the optimizations applied to our Baseline such as call to MKL library and manual vectorization of linear algebra primitives.
ALLREDUCE: ALLREDUCE Agarwal et al. (2014) is an approach where each thread makes a copy from the global model and applies the SGD update rule to the local model for certain number of instances. Along with the local model, another vector g is computed which indicates the confidence in an update for the weight of a feature in the local model. After the learning phase, the local weight vectors are averaged based on the confidence vectors from each thread. We implemented this approach similarly using MKL calls and manual vectorization.
A.5 SPEEUPS ON REMAINING BENCHMARKS
","Stochastic gradient descent (SGD) is a well-known method for regression and classification tasks. However, it is an inherently sequential algorithm — at each step, the processing of the current example depends on the parameters learned from the previous examples. Prior approaches to parallelizing SGD, such as HOGWILD! and ALLREDUCE, do not honor these dependences across threads and thus can potentially suffer poor convergence rates and/or poor scalability. This paper proposes SYMSGD, a parallel SGD algorithm that retains the sequential semantics of SGD in expectation. Each thread in this approach learns a local model and a probabilistic model combiner that allows the local models to be combined to produce the same result as what a sequential SGD would have produced, in expectation. This SYMSGD approach is applicable to any linear learner whose update rule is linear. This paper evaluates SYMSGD’s accuracy and performance on 9 datasets on a shared-memory machine shows up-to 13× speedup over our heavily optimized sequential baseline on 16 cores.",ICLR 2017 conference submission,False,,"This paper describes a correction technique to combine updates from multiple SGD to make it statistically equivalent to sequential technique.
Comments
1) The proposed method is novel and interesting to allow update to be corrected even when the update is delayed.
2) The proposed theory can only be applied to square loss setting (with linear update rule), making it somewhat limited. This paper would be much more interesting to ICLR community, if the technique is applicable to general objective function and settings of deep neural networks.
3) The resulting technique requires book-keeping of a dimensional reduced combiner matrix, which causes more computation in terms of complexity. The authors argue that the overhead can be canceled with SIMD support for symbolic update. However, the normal update of SGD might also benefit from SIMD, especially when the dataset is dense.

Overall, even though the practical value of this work is limited by 2) and 3), the technique(specifically the correction rule) proposed in the paper could be of interest to people scaling up learning. I would encourage the author to extend the method to the cases of non-linear objective function which could make it more interesting to the ICLR community

---

The reviewers largely agree that this paper is well written and presents an interesting, novel approach to parallelizing Stochastic Gradient Descent. However, the current formulation is restricted to linear regression models and requires sketching techniques to handle large number of features, although it is striking that very aggressive sketching (k~10) still works well. In this setting though, there are specialized randomized solvers such as Blendenpick (Avron et al) which sketch the data upfront to construct a high quality pre-conditioner for use with iterative methods. 
 The authors are encouraged to either compare with state of the art parallel randomized least squares solvers developed in the numerical linear algebra community (see papers by Michael Mahoney, Petros Drineas and others), or broaden the scope of the proposed methods to include models of current interest (e.g., DNNs). The latter would of course be of specific interest to the ICLR community.

---

This paper propose a parallel mechanism for stochastic gradient descent method (SGD) in case of gradient can be computed via linear operations (including least square linear regression and polynomial regression problems). The motivation is to recover the same effect compared with sequential SGD, by using a proposed sound combiner. To make such combiner more efficient, the authors also use a randomized projection matrix to do dimension reduction. Experiments shows the proposed method has better speedup than previous methods like Hogwild! and Allreduce.

I feel that there might be some fundamental misunderstanding on SGD.

''The combiner matrixM  generate above can be quite large and expensive to compute. The sequential SGD algorithm maintains and updates the weight vector w , and thus requires O(f)  space and time, where f  is the number of features. In contrast,M  is a f f  matrix and consequently, the space and time complexity of parallel SGD is O(f^2) . In practice, this would mean that we would need O(f) processors to see constant speedups, an infeasible proposition particularly for datasets that can have
thousands if not millions of features.""

I do not think one needs O(f^2) space and complexity for updating M_i * v, where v is an f-dimensional vector. Note that M_i is a low rank matrix in the form of (I - a_i a_i'). The complexity and space can be reduced to O(f) if compute it by O(v - a_i (a_i' v)) equivalently. If M_i is defined in the form of the product of n number of rank 1 matrices. The complexity and space complexity is O(fn). In the context of this paper, n should be much smaller than f. I seriously doubt that all author's assumptions, experiments, and strategies in this paper are based on this incorrect assumption on space and complexity of SGD. 

Why one can have speedup is unclear for me. It is unclear what computations are in parallel and why this sequential algorithms can bring speedup if M_i*v is computed in the most efficient way.
I suggest authors to make the following changes to make this paper more clear and theoretically solid
- provide computational complexity per step of the proposed algorithm
- convergence rate analysis (convergence analysis is not enough): we would like to see how the dimension reduction can affect the complexity.

---

Overall, the idea in this paper is interesting and the paper is well-written and well-motivated.  However, I think it is not ready to publish in ICLR for the following reasons:

- This paper is not related to representation learning. It may be more suitable for a general machine learning or data mining conference. 

- The proposed approach can only work for a small class of models and cannot apply to popular formulations,  such as SVM, logistic regression, and neural network. It is unclear why we want to use SGD for this specific type of formulations. For model like linear regression, the authors should compare their methods with linear programming approaches. Also, it is unclear why we need to develope parallel algorithm for linear regressio problems as they are relatively easy to solve unless the data are big (see next comment). 

- The dataset used in the paper are relatively small and can be only used for proving the concept. Most datasets considered in the paper can be solved in a few second using a single core CPU. Hogwild! is suitable for sparse dataset because of its asynchronized nature. On data that are very sparse, the proposed approach is only slightly better or is worse than Hogwild. For dense dataset, it is unclear why we need to use SYMSGD instead of simply parallelizing the gradient computation using GPUs. Put them together, the experiment results are not convincing.

---

This paper describes a correction technique to combine updates from multiple SGD to make it statistically equivalent to sequential technique.
Comments
1) The proposed method is novel and interesting to allow update to be corrected even when the update is delayed.
2) The proposed theory can only be applied to square loss setting (with linear update rule), making it somewhat limited. This paper would be much more interesting to ICLR community, if the technique is applicable to general objective function and settings of deep neural networks.
3) The resulting technique requires book-keeping of a dimensional reduced combiner matrix, which causes more computation in terms of complexity. The authors argue that the overhead can be canceled with SIMD support for symbolic update. However, the normal update of SGD might also benefit from SIMD, especially when the dataset is dense.

Overall, even though the practical value of this work is limited by 2) and 3), the technique(specifically the correction rule) proposed in the paper could be of interest to people scaling up learning. I would encourage the author to extend the method to the cases of non-linear objective function which could make it more interesting to the ICLR community

---

This paper describes a correction technique to combine updates from multiple SGD to make it statistically equivalent to sequential technique.
Comments
1) The proposed method is novel and interesting to allow update to be corrected even when the update is delayed.
2) The proposed theory can only be applied to square loss setting (with linear update rule), making it somewhat limited. This paper would be much more interesting to ICLR community, if the technique is applicable to general objective function and settings of deep neural networks.
3) The resulting technique requires book-keeping of a dimensional reduced combiner matrix, which causes more computation in terms of complexity. The authors argue that the overhead can be canceled with SIMD support for symbolic update. However, the normal update of SGD might also benefit from SIMD, especially when the dataset is dense.

Overall, even though the practical value of this work is limited by 2) and 3), the technique(specifically the correction rule) proposed in the paper could be of interest to people scaling up learning. I would encourage the author to extend the method to the cases of non-linear objective function which could make it more interesting to the ICLR community

---

The reviewers largely agree that this paper is well written and presents an interesting, novel approach to parallelizing Stochastic Gradient Descent. However, the current formulation is restricted to linear regression models and requires sketching techniques to handle large number of features, although it is striking that very aggressive sketching (k~10) still works well. In this setting though, there are specialized randomized solvers such as Blendenpick (Avron et al) which sketch the data upfront to construct a high quality pre-conditioner for use with iterative methods. 
 The authors are encouraged to either compare with state of the art parallel randomized least squares solvers developed in the numerical linear algebra community (see papers by Michael Mahoney, Petros Drineas and others), or broaden the scope of the proposed methods to include models of current interest (e.g., DNNs). The latter would of course be of specific interest to the ICLR community.

---

This paper propose a parallel mechanism for stochastic gradient descent method (SGD) in case of gradient can be computed via linear operations (including least square linear regression and polynomial regression problems). The motivation is to recover the same effect compared with sequential SGD, by using a proposed sound combiner. To make such combiner more efficient, the authors also use a randomized projection matrix to do dimension reduction. Experiments shows the proposed method has better speedup than previous methods like Hogwild! and Allreduce.

I feel that there might be some fundamental misunderstanding on SGD.

''The combiner matrixM  generate above can be quite large and expensive to compute. The sequential SGD algorithm maintains and updates the weight vector w , and thus requires O(f)  space and time, where f  is the number of features. In contrast,M  is a f f  matrix and consequently, the space and time complexity of parallel SGD is O(f^2) . In practice, this would mean that we would need O(f) processors to see constant speedups, an infeasible proposition particularly for datasets that can have
thousands if not millions of features.""

I do not think one needs O(f^2) space and complexity for updating M_i * v, where v is an f-dimensional vector. Note that M_i is a low rank matrix in the form of (I - a_i a_i'). The complexity and space can be reduced to O(f) if compute it by O(v - a_i (a_i' v)) equivalently. If M_i is defined in the form of the product of n number of rank 1 matrices. The complexity and space complexity is O(fn). In the context of this paper, n should be much smaller than f. I seriously doubt that all author's assumptions, experiments, and strategies in this paper are based on this incorrect assumption on space and complexity of SGD. 

Why one can have speedup is unclear for me. It is unclear what computations are in parallel and why this sequential algorithms can bring speedup if M_i*v is computed in the most efficient way.
I suggest authors to make the following changes to make this paper more clear and theoretically solid
- provide computational complexity per step of the proposed algorithm
- convergence rate analysis (convergence analysis is not enough): we would like to see how the dimension reduction can affect the complexity.

---

Overall, the idea in this paper is interesting and the paper is well-written and well-motivated.  However, I think it is not ready to publish in ICLR for the following reasons:

- This paper is not related to representation learning. It may be more suitable for a general machine learning or data mining conference. 

- The proposed approach can only work for a small class of models and cannot apply to popular formulations,  such as SVM, logistic regression, and neural network. It is unclear why we want to use SGD for this specific type of formulations. For model like linear regression, the authors should compare their methods with linear programming approaches. Also, it is unclear why we need to develope parallel algorithm for linear regressio problems as they are relatively easy to solve unless the data are big (see next comment). 

- The dataset used in the paper are relatively small and can be only used for proving the concept. Most datasets considered in the paper can be solved in a few second using a single core CPU. Hogwild! is suitable for sparse dataset because of its asynchronized nature. On data that are very sparse, the proposed approach is only slightly better or is worse than Hogwild. For dense dataset, it is unclear why we need to use SYMSGD instead of simply parallelizing the gradient computation using GPUs. Put them together, the experiment results are not convincing.

---

This paper describes a correction technique to combine updates from multiple SGD to make it statistically equivalent to sequential technique.
Comments
1) The proposed method is novel and interesting to allow update to be corrected even when the update is delayed.
2) The proposed theory can only be applied to square loss setting (with linear update rule), making it somewhat limited. This paper would be much more interesting to ICLR community, if the technique is applicable to general objective function and settings of deep neural networks.
3) The resulting technique requires book-keeping of a dimensional reduced combiner matrix, which causes more computation in terms of complexity. The authors argue that the overhead can be canceled with SIMD support for symbolic update. However, the normal update of SGD might also benefit from SIMD, especially when the dataset is dense.

Overall, even though the practical value of this work is limited by 2) and 3), the technique(specifically the correction rule) proposed in the paper could be of interest to people scaling up learning. I would encourage the author to extend the method to the cases of non-linear objective function which could make it more interesting to the ICLR community",,,,,,4.666666666666667,,,4.666666666666667,,
749,"DEEP CONVOLUTIONAL NEURAL NETWORK DESIGN PATTERNS
Authors: Leslie N. Smith, Nicholay Topin
Source file: 749.pdf

ABSTRACT
Recent research in the deep learning field has produced a plethora of new architectures. At the same time, a growing number of groups are applying deep learning to new applications. Some of these groups are likely to be composed of inexperienced deep learning practitioners who are baffled by the dizzying array of architecture choices and therefore opt to use an older architecture (i.e., Alexnet). Here we attempt to bridge this gap by mining the collective knowledge contained in recent deep learning research to discover underlying principles for designing neural network architectures. In addition, we describe several architectural innovations, including Fractal of FractalNet network, Stagewise Boosting Networks, and Taylor Series Networks (our Caffe code and prototxt files is available at https://github.com/iPhysicist/CNNDesignPatterns). We hope others are inspired to build on our preliminary work.

1 INTRODUCTION
Many recent articles discuss new architectures for neural networking, especially regarding Residual Networks (He et al. (2015; 2016); Larsson et al. (2016); Zhang et al. (2016); Huang et al. (2016b)). Although the literature covers a wide range of network architectures, we take a high-level view of the architectures as the basis for discovering universal principles of the design of network architecture. We discuss 14 original design patterns that could benefit inexperienced practitioners who seek to incorporate deep learning in various new applications. This paper addresses the current lack of guidance on design, a deficiency that may prompt the novice to rely on standard architecture, e.g., Alexnet, regardless of the architecture’s suitability to the application at hand.
This abundance of research is also an opportunity to determine which elements provide benefits in what specific contexts. We ask: Do universal principles of deep network design exist? Can these principles be mined from the collective knowledge on deep learning? Which architectural choices work best in any given context? Which architectures or parts of architectures seem elegant?
Design patterns were first described by Christopher Alexander (Alexander (1979)) in regards to the architectures of buildings and towns. Alexander wrote of a timeless quality in architecture that “lives” and this quality is enabled by building based on universal principles. The basis of design patterns is that they resolve a conflict of forces in a given context and lead to an equilibrium analogous to the ecological balance in nature. Design patterns are both highly specific, making them clear to follow, and flexible so they can be adapted to different environments and situations. Inspired by Alexander’s work, the “gang of four” (Gamma et al. (1995)) applied the concept of design patterns to the architecture of object-oriented software. This classic computer science book describes 23 patterns that resolve issues prevalent in software design, such as “requirements always change”. We were inspired by these previous works on architectures to articulate possible design patterns for convolutional neural network (CNN) architectures.
Design patterns provide universal guiding principles, and here we take the first steps to defining network design patterns. Overall, it is an enormous task to define design principles for all neural networks and all applications, so we limit this paper to CNNs and their canonical application of image classification. However, we recognize that architectures must depend upon the application by defining our first design pattern; Design Pattern 1: Architectural Structure follows the Application
(we leave the details of this pattern to future work). In addition, these principles allowed us to discover some gaps in the existing research and to articulate novel networks (i.e, Fractal of FractalNets, Stagewise Boosting and Taylor Series networks) and units (i.e., freeze-drop-path). We hope the rules of thumb articulated here are valuable for both the experienced and novice practitioners and that our preliminary work serves as a stepping stone for others to discover and share additional deep learning design patterns.

2 RELATED WORK
To the best of our knowledge, there has been little written recently to provide guidance and understanding on appropriate architectural choices1. The book ”Neural Networks: Tricks of the Trade” (Orr & Müller, 2003) contains recommendations for network models but without reference to the vast amount of research in the past few years. Perhaps the closest to our work is Szegedy et al. (2015b) where those authors describe a few design principles based on their experiences.
Much research has studied neural network architectures, but we are unaware of a recent survey of the field. Unfortunately, we cannot do justice to this entire body of work, so we focus on recent innovations in convolutional neural networks architectures and, in particular, on Residual Networks (He et al., 2015) and its recent family of variants. We start with Network In Networks (Lin et al., 2013), which describes a hierarchical network with a small network design repeatedly embedded in the overall architecture. Szegedy et al. (2015a) incorporated this idea into their Inception architecture. Later, these authors proposed modifications to the original Inception design (Szegedy et al., 2015b). A similar concept was contained in the multi-scale convolution architecture (Liao & Carneiro, 2015). In the meantime, Batch Normalization (Ioffe & Szegedy, 2015) was presented as a unit within the network that makes training faster and easier.
Before the introduction of Residual Networks, a few papers suggested skip connections. Skip connections were proposed by Raiko et al. (2012). Highway Networks (Srivastava et al., 2015) use a gating mechanism to decide whether to combine the input with the layer’s output and showed how these networks allowed the training of very deep networks. The DropIn technique (Smith et al., 2015; 2016) also trains very deep networks by allowing a layer’s input to skip the layer. The concept of stochastic depth via a drop-path method was introduced by Huang et al. (2016b).
Residual Networks were introduced by He et al. (2015), where the authors describe their network that won the 2015 ImageNet Challenge. They were able to extend the depth of a network from tens to hundreds of layers and in doing so, improve the network’s performance. The authors followed up with another paper (He et al., 2016) where they investigate why identity mappings help and report results for a network with more than a thousand layers. The research community took notice of this architecture and many modifications to the original design were soon proposed.
The Inception-v4 paper (Szegedy et al., 2016) describes the impact of residual connections on their Inception architecture and compared these results with the results from an updated Inception design. The Resnet in Resnet paper (Targ et al., 2016) suggests a duel stream architecture. Veit et al. (2016) provided an understanding of Residual Networks as an ensemble of relatively shallow networks. These authors illustrated how these residual connections allow the input to follow an exponential number of paths through the architecture. At the same time, the FractalNet paper (Larsson et al., 2016) demonstrated training deep networks with a symmetrically repeating architectural pattern. As described later, we found the symmetry introduced in their paper intriguing. In a similar vein, Convolutional Neural Fabrics (Saxena & Verbeek, 2016) introduces a three dimensional network, where the usual depth through the network is the first dimension.
Wide Residual Networks (Zagoruyko & Komodakis, 2016) demonstrate that simultaneously increasing both depth and width leads to improved performance. In Swapout (Singh et al., 2016), each layer can be dropped, skipped, used normally, or combined with a residual. Deeply Fused Nets (Wang et al., 2016) proposes networks with multiple paths. In the Weighted Residual Networks paper (Shen & Zeng, 2016), the authors recommend a weighting factor for the output from the convolutional layers, which gradually introduces the trainable layers. Convolutional Residual Memory Networks (Moniz & Pal, 2016) proposes an architecture that combines a convolutional Residual Network with
1After submission we became aware of an online book being written on deep learning design patterns at http://www.deeplearningpatterns.com
an LSTM memory mechanism. For Residual of Residual Networks (Zhang et al., 2016), the authors propose adding a hierarchy of skip connections where the input can skip a layer, a module, or any number of modules. DenseNets (Huang et al., 2016a) introduces a network where each module is densely connected; that is, the output from a layer is input to all of the other layers in the module. In the Multi-Residual paper (Abdi & Nahavandi, 2016), the authors propose expanding a residual block width-wise to contain multiple convolutional paths. Our Appendix A describes the close relationship between many of these Residual Network variants.

3 DESIGN PATTERNS
We reviewed the literature specifically to extract commonalities and reduce their designs down to fundamental elements that might be considered design patterns. It seemed clear to us that in reviewing the literature some design choices are elegant while others are less so. In particular, the patterns described in this paper are the following:
1. Architectural Structure follows the Application
2. Proliferate Paths
3. Strive for Simplicity
4. Increase Symmetry
5. Pyramid Shape
6. Over-train
7. Cover the Problem Space
8. Incremental Feature Construction
9. Normalize Layer Inputs
10. Input Transition
11. Available Resources Guide Layer Widths
12. Summation Joining
13. Down-sampling Transition
14. Maxout for Competition

3.1 HIGH LEVEL ARCHITECTURE DESIGN
Several researchers have pointed out that the winners of the ImageNet Challenge (Russakovsky et al., 2015) have successively used deeper networks (as seen in, Krizhevsky et al. (2012), Szegedy et al. (2015a), Simonyan & Zisserman (2014), He et al. (2015)). It is also apparent to us from the ImageNet Challenge that multiplying the number of paths through the network is a recent trend that is illustrated in the progression from Alexnet to Inception to ResNets. For example, Veit et al. (2016) show that ResNets can be considered to be an exponential ensemble of networks with different lengths. Design Pattern 2: Proliferate Paths is based on the idea that ResNets can be an exponential ensemble of networks with different lengths. One proliferates paths by including a multiplicity of branches in the architecture. Recent examples include FractalNet (Larsson et al. 2016), Xception (Chollet 2016), and Decision Forest Convolutional Networks (Ioannou et al. 2016).
Scientists have embraced simplicity/parsimony for centuries. Simplicity was exemplified in the paper ”Striving for Simplicity” (Springenberg et al. 2014) by achieving state-of-the-art results with fewer types of units. Design Pattern 3: Strive for Simplicity suggests using fewer types of units and keeping the network as simple as possible. We also noted a special degree of elegance in the FractalNet (Larsson et al. 2016) design, which we attributed to the symmetry of its structure. Design Pattern 4: Increase Symmetry is derived from the fact that architectural symmetry is typically considered a sign of beauty and quality. In addition to its symmetry, FractalNets also adheres to the Proliferate Paths design pattern so we used it as the baseline of our experiments in Section 4.
An essential element of design patterns is the examination of trade-offs in an effort to understand the relevant forces. One fundamental trade-off is the maximization of representational power versus
elimination of redundant and non-discriminating information. It is universal in all convolutional neural networks that the activations are downsampled and the number of channels increased from the input to the final layer, which is exemplified in Deep Pyramidal Residual Networks (Han et al. (2016)). Design Pattern 5: Pyramid Shape says there should be an overall smooth downsampling combined with an increase in the number of channels throughout the architecture.
Another trade-off in deep learning is training accuracy versus the ability of the network to generalize to non-seen cases. The ability to generalize is an important virtue of deep neural networks. Regularization is commonly used to improve generalization, which includes methods such as dropout (Srivastava et al. 2014a) and drop-path (Huang et al. 2016b). As noted by Srivastava et al. 2014b, dropout improves generalization by injecting noise in the architecture. We believe regularization techniques and prudent noise injection during training improves generalization (Srivastava et al. 2014b, Gulcehre et al. 2016). Design Pattern 6: Over-train includes any training method where the network is trained on a harder problem than necessary to improve generalization performance of inference. Design Pattern 7: Cover the Problem Space with the training data is another way to improve generalization (e.g., Ratner et al. 2016, Hu et al. 2016, Wong et al. 2016, Johnson-Roberson et al. 2016). Related to regularization methods, cover the problem space includes the use of noise (Rasmus et al. 2015, Krause et al. 2015, Pezeshki et al. 2015), synthetic data, and data augmentation, such as random cropping, flipping, and varying brightness, contrast, and the like.

3.2 DETAILED ARCHITECTURE DESIGN
A common thread throughout many of the more successful architectures is to make each layer’s “job” easier. Use of very deep networks is an example because any single layer only needs to incrementally modify the input, and this partially explains the success of Residual Networks, since in very deep networks, a layer’s output is likely similar to the input; hence adding the input to the layer’s output makes the layer’s job incremental. Also, this concept is part of the motivation behind design pattern 2 but it extends beyond that. Design Pattern 8: Incremental Feature Construction recommends using short skip lengths in ResNets. A recent paper (Alain & Bengio (2016)) showed in an experiment that using an identity skip length of 64 in a network of depth 128 led to the first portion of the network not being trained.
Design Pattern 9: Normalize Layer Inputs is another way to make a layer’s job easier. Normalization of layer inputs has been shown to improve training and accuracy but the underlying reasons are not clear (Ioffe & Szegedy 2015, Ba et al. 2016, Salimans & Kingma 2016). The Batch Normalization paper (Ioffe & Szegedy 2015) attributes the benefits to handling internal covariate shift, while the authors of streaming normalization (Liao et al. 2016) express that it might be otherwise. We feel that normalization puts all the layer’s input samples on more equal footing (analogous to a units conversion scaling), which allows back-propagation to train more effectively.
Some research, such as Wide ResNets (Zagoruyko & Komodakis 2016), has shown that increasing the number of channels improves performance but there are additional costs with extra channels. The input data for many of the benchmark datasets have 3 channels (i.e., RGB). Design Pattern 10: Input Transition is based on the common occurrence that the output from the first layer of a CNN significantly increases the number of channels from 3. A few examples of this increase in channels/outputs at the first layer for ImageNet are AlexNet (96 channels), Inception (32), VGG (224), and ResNets (64). Intuitively it makes sense to increase the number of channels from 3 in the first layer as it allows the input data to be examined many ways but it is not clear how many outputs are best. Here, the trade-off is that of cost versus accuracy. Costs include the number of parameters in the network, which directly affects the computational and storage costs of training and inference. Design Pattern 11: Available Resources Guide Layer Widths is based on balancing costs against an application’s requirements. Choose the number of outputs of the first layer based on memory and computational resources and desired accuracy.

3.2.1 JOINING BRANCHES: CONCATENATION, SUMMATION/MEAN, MAXOUT
When there are multiple branches, three methods have been used to combine the outputs; concatenation, summation (or mean), or Maxout. It seems that different researchers have their favorites and there hasn’t been any motivation for using one in preference to another. In this Section, we propose some rules for deciding how to combine branches.
Summation is one of the most common ways of combining branches. Design Pattern 12: Summation Joining is where the joining is performed by summation/mean. Summation is the preferred joining mechanism for Residual Networks because it allows each branch to compute corrective terms (i.e., residuals) rather than the entire approximation. The difference between summation and mean (i.e., fractal-join) is best understood by considering drop-path (Huang et al. 2016b). In a Residual Network where the input skip connection is always present, summation causes the layers to learn the residual (the difference from the input). On the other hand, in networks with several branches, where any branch can be dropped (e.g., FractalNet (Larsson et al. (2016))), using the mean is preferable as it keeps the output smooth as branches are randomly dropped.
Some researchers seem to prefer concatenation (e.g., Szegedy et al. (2015a)). Design Pattern 13: Down-sampling Transition recommends using concatenation joining for increasing the number of outputs when pooling. That is, when down-sampling by pooling or using a stride greater than 1, a good way to combine branches is to concatenate the output channels, hence smoothly accomplishing both joining and an increase in the number of channels that typically accompanies down-sampling.
Maxout has been used for competition, as in locally competitive networks (Srivastava et al. 2014b) and competitive multi-scale networks Liao & Carneiro (2015). Design Pattern 14: Maxout for Competition is based on Maxout choosing only one of the activations, which is in contrast to summation or mean where the activations are “cooperating”; here, there is a “competition” with only one “winner”. For example, when each branch is composed of different sized kernels, Maxout is useful for incorporating scale invariance in an analogous way to how max pooling enables translation invariance.

4 EXPERIMENTS

4.1 ARCHITECTURAL INNOVATIONS
In elucidating these fundamental design principles, we also discovered a few architectural innovations. In this section we will describe these innovations.
First, we recommended combining summation/mean, concatenation, and maxout joining mechanisms with differing roles within a single architecture, rather than the typical situation where only one is used throughout. Next, Design Pattern 2: Proliferate Branches led us to modify the overall sequential pattern of modules in the FractalNet architecture. Instead of lining up the modules for maximum depth, we arranged the modules in a fractal pattern as shown in 1b, which we named a Fractal of FractalNet (FoF) network, where we exchange depth for a greater number of paths.

4.1.1 FREEZE-DROP-PATH AND STAGEWISE BOOSTING NETWORKS (SBN)
Drop-path was introduced by Huang et al. (2016b), which works by randomly removing branches during an iteration of training, as though that path doesn’t exist in the network. Symmetry considerations led us to an opposite method that we named freeze-path. Instead of removing a branch from the network during training, we freeze the weights, as though the learning rate was set to zero. A similar idea has been proposed for recurrent neural networks (Krueger et al. 2016).
The potential usefulness of combining drop-path and freeze-path, which we named freeze-drop-path, is best explained in the non-stochastic case. Figure 1 shows an example of a fractal of FractalNet architecture. Let’s say we start training only the leftmost branch in Figure 1b and use drop-path on all of the other branches. This branch should train quickly since it has only a relatively few parameters compared to the entire network. Next we freeze the weights in that branch and allow the next branch to the right to be active. If the leftmost branch is providing a good function approximation, the next branch works to produce a “small” corrective term. Since the next branch contains more layers than the previous branch and the corrective term should be easier to approximate than the original function, the network should attain greater accuracy. One can continue this process from left to right to train the entire network. We used freeze-drop-path as the final/bottom join in the FoF architecture in Figure 1b and named this the Stagewise Boosting Networks (SBN) because they are analogous to stagewise boosting (Friedman et al. 2001). The idea of boosting neural networks is not new (Schwenk & Bengio 2000) but this architecture is new. In Appendix B we discuss the implementation we tested.

4.1.2 TAYLOR SERIES NETWORKS (TSN)
Taylor series expansions are classic and well known as a function approximation method, which is: f(x+ h) = f(x) + hf ′(x) + h2f ′′(x)/2 + ... (1)
Since neural networks are also function approximators, it is a short leap from FoFs and SBNs to consider the branches of that network as terms in a Taylor series expansion. Hence, the Taylor series implies squaring the second branch before the summation joining unit, analogous to the second order term in the expansion. Similarly, the third branch would be cubed. We call this “Taylor Series Networks” (TSN) and there is precedence for this idea in the literature with polynomial networks (Livni et al. 2014) and multiplication in networks (e.g. Lin et al. 2015. The implementation details of this TSN are discussed in the Appendix.

4.2 RESULTS
The experiments in this section are primarily to empirically validate the architectural innovations described above but not to fully test them. We leave a more complete evaluation to future work.
Table 1 and Figures 2 and 3 compare the final test accuracy results for CIFAR-10 and CIFAR-100 in a number of experiments. An accuracy value in Table 1 is computed as the mean of the last 6 test
accuracies computed over the last 3,000 iterations (out of 100,000) of the training. The results from the original FractalNet (Larsson et al. 2016) are given in the first row of the table and we use this as our baseline. The first four rows of Table 1 and Figure 2 compare the test accuracy of the original FractalNet architectures to architectures with a few modifications advocated by design patterns. The first modification is to use concatenation instead of fractal-joins at all the downsampling locations in the networks. The results for both CIFAR-10 (2a) and CIFAR-100 (2b indicate that the results are equivalent when concatenation is used instead of fractal-joins at all the downsampling locations in the networks. The second experiment was to change the kernel sizes in the first module from 3x3 such that the left most column used a kernel size of 7x7, the second column 5x5, and the third used 3x3. The fractal-join for module one was replaced with Maxout. The results in Figure 2 are a bit worse, indicating that the more cooperative fractal-join (i.e., mean/summation) with 3x3 kernels has better performance than the competitive Maxout with multiple scales. Figure 2 also illustrates how an experiment replacing max pooling with average pooling throughout the architecture changes the training profile. For CIFAR-10, the training accuracy rises quickly, plateaus, then lags behind the original FractalNet but ends with a better final performance, which implies that average pooling might significantly reduce the length of the training (we plan to examine this in future work). This behavior provides some evidence that “cooperative” average pooling might be preferable to “competitive” max pooling.
Table 1 and Figure 3 compare the test accuracy results for the architectural innovations described in Section 4.1. The FoF architecture ends with a similar final test accuracy as FractalNet but the SBN and TSN architectures (which use freeze-drop-path) lag behind when the learning rate is dropped. However, it is clear from both Figures 3a and 3b that the new architectures train more quickly than FractalNet. The FoF network is best as it trains more quickly than FractalNet and achieves similar accuracy. The use of freeze-drop-path in SBN and TSN is questionable since the final performance lags behind FractalNet, but we are leaving the exploration for more suitable applications of these new architectures for future work.

5 CONCLUSION
In this paper we describe convolutional neural network architectural design patterns that we discovered by studying the plethora of new architectures in recent deep learning papers. We hope these design patterns will be useful to both experienced practitioners looking to push the state-of-the-art and novice practitioners looking to apply deep learning to new applications. There exists a large expanse of potential follow up work (some of which we have indicated here as future work). Our effort here is primarily focused on Residual Networks for classification but we hope this preliminary work will inspire others to follow up with new architectural design patterns for Recurrent Neural Networks, Deep Reinforcement Learning architectures, and beyond.

ACKNOWLEDGMENTS
The authors want to thank the numerous researchers upon whose work these design patterns are based and especially Larsson et al. 2016 for making their code publicly available. This work was supported by the US Naval Research Laboratory base program.

A RELATIONSHIPS BETWEEN RESIDUAL ARCHITECTURES
The architectures mentioned in Section 2 commonly combine outputs from two or more layers using concatenation along the depth axis, element-wise summation, and element-wise average. We show here that the latter two are special cases of the former with weight-sharing enforced. Likewise, we show that skip connections can be considered as introducing additional layers into a network that share parameters with existing layers. In this way, any of the Residual Network variants can be reformulated into a standard form where many of the variants are equivalent.
A filter has three dimensions: two spatial dimensions, along which convolution occurs, and a third dimension, depth. Each input channel corresponds to a different depth for each filter of a layer. As a result, a filter can be considered to consist of “slices,” each of which is convolved over one input channel. The results of these convolutions are then added together, along with a bias, to produce a single output channel. The output channels of multiple filters are concatenated to produce the output of a single layer. When the outputs of several layers are concatenated, the behavior is similar to that of a single layer. However, instead of each filter having the same spatial dimensions, stride, and padding, each filter may have a different structure. As far as the function within a network, though, the two cases are the same. In fact, a standard layer, one where all filters have the same shape, can be considered a special case of concatenating outputs of multiple layer types.
If summation is used instead of concatenation, the network can be considered to perform concatenation but enforce weight-sharing in the following layer. The results of first summing several channels element-wise and then convolving a filter slice over the output yields the same result as convolving the slice over the channels and then performing an element-wise summation afterwards. Therefore, enforcing weight-sharing such that the filter slices applied to the nth channel of all inputs share weight results in behavior identical to summation, but in a form similar to concatenation, which highlights the relationship between the two. When Batch Normalization (BN) (Ioffe & Szegedy 2015 is used, as is the current standard practice, performing an average is essentially identical to performing a summation, since BN scales the output. Therefore, scaling the input by a constant (i.e., averaging instead of a summation) is rendered irrelevant. The details of architecture-specific manipulations of summations and averages is described further in Section 3.2.1.
Due to the ability to express depth-wise concatenation, element-wise sum, and element-wise mean as variants of each other, architectural features of recent works can be combined within a single network, regardless of choice of combining operation. However, this is not to say that concatenation has the most expressivity and is therefore strictly better than the others. Summation allows networks to divide up the network’s task. Also, there is a trade-off between the number of parameters and the expressivity of a layer; summation uses weight-sharing to significantly reduce the number of parameters within a layer at the expense of some amount of expressivity.
Different architectures can further be expressed in a similar fashion through changes in the connections themselves. A densely connected series of layers can be “pruned” to resemble any desired architecture with skip connections through zeroing specific filter slices. This operation removes the dependency of the output on a specific input channel; if this is done for all channels from a given layer, the connection between the two layers is severed. Likewise, densely connected layers can be turned into linearly connected layers while preserving the layer dependencies; a skip connection can be passed through the intermediate layers. A new filter can be introduced for each input channel passing through, where the filter performs the identity operation for the given input channel. All existing filters in the intermediate layers can have zeroed slices for this input so as to not introduce new dependencies. In this way, arbitrarily connected layers can be turned into a standard form.
We certainly do not recommend this representation for actual experimentation as it introduces fixed parameters. We merely describe it to illustrate the relationship between different architectures. This representation illustrates how skip connections effectively enforce specific weights in intermediate layers. Though this restriction reduces expressivity, the number of stored weights is reduced, the number of computations performed is decreased, and the network might be more easily trainable.
B IMPLEMENTATION DETAILS
Our implementations are in Caffe (Jia et al. 2014; downloaded October 9, 2016) using CUDA 8.0. These experiments were run on a 64 node cluster with 8 Nvidia Titan Black GPUs, 128 GB memory, and dual Intel Xenon E5-2620 v2 CPUs per node. We used the CIFAR10 and CIFAR-100 datasets (Krizhevsky & Hinton 2009 for our classification tests. These datasets consist of 60,000 32x32 colour images (50,000 for training and 10,000 for testing) in 10 or 100 classes, respectively. Our Caffe code and prototxt files are publicly available at https://github.com/iPhysicist/CNNDesignPatterns.
B.1 ARCHITECTURES
We started with the FractalNet implementation 2 as our baseline and it is described in Larsson et al. 2016. We used the three column module as shown in Figure 1a. In some of our experiments, we replaced the fractal-join with concatenation at the downsampling locations. In other experiments, we modified the kernel sizes in module one and combined the branches with Maxout. A FractalNet module is shown in Figure 1a and the architecture consists of five sequential modules.
Our fractal of FractalNet (FoF) architecture uses the same module but has an overall fractal design as in Figure 1b rather than the original sequential one. We limited our investigation to this one realization and left the study of other (possibly more complex) designs for future work. We followed the FractalNet implementation in regards to dropout where the dropout rate for a module were 0%, 10%, 20%, or 30%, depending on the depth of the module in the architecture. This choice for dropout rates were not found by experimentation and better values are possible. The local drop-path rate in the fractal-joins were fixed at 15%, which is identical to the FractalNet implementation.
Freeze-drop-path introduces four new parameters. The first is whether the active branch is chosen stochastically or deterministically. If it is chosen stochastically, a random number is generated and the active branch is assigned based on which interval it falls in (intervals will be described shortly). If it is deterministically, a parameter is set by the user as to the number of iterations in one cycle through all the branches (we called this parameter num iter per cycle). In our Caffe implementation of the freeze-drop-path unit, the bottom input specified first is assigned as branch 1, the next is branch 2, then branch 3, etc. The next parameter indicates the proportion of iterations each branch should be active relative to all the other branches. The first type of interval uses the square of the branch number (i.e., 1, 4, 9, 16, ...) to assign the interval length for that branch to be active, which gives the more update iterations to the higher numbered branches. The next type gives the same amount of iterations to each branch. Our experiments showed that the first interval type works better (as we expected) and was used to obtained the results in Section 4.2. In addition, our experiments showed that the stochastic option works better than the deterministic option (unexpected) and was used for Section 4.2 results.
2https://github.com/gustavla/fractalnet/tree/master/caffe
The Stagewise Boosting Network’s (SBN) architecture is the same as the FoF architecture except that branches 2 and 3 are combined with a fractal-join and then combined with branch 1 in a freezedrop-path join. The reason for combining branches 2 and 3 came out of our first experiments; if branches 2 and 3 were separate, the performance deteriorated when branch 2 was frozen and branch 3 was active. In hindsight, this is due to the weights in the branch 2 path that are also in branch 3’s path being modified by the training of branch 3. The Taylor series network has the same architecture as SBN with the addition of squaring the branch 2 and 3 combined activations before the freeze-drop-path join.
For all of our experiments, we trained for 400 epochs. Since the training used 8 GPUs and each GPU had a batchsize of 25, 400 epochs amounted to 100,000 iterations. We adopted the same learning rate as the FractalNet implementation, which started at 0.002 and dropped the learning rate by a factor of 10 at epochs 200, 300, and 350.
","Recent research in the deep learning field has produced a plethora of new architectures. At the same time, a growing number of groups are applying deep learning to new applications. Some of these groups are likely to be composed of inexperienced deep learning practitioners who are baffled by the dizzying array of architecture choices and therefore opt to use an older architecture (i.e., Alexnet). Here we attempt to bridge this gap by mining the collective knowledge contained in recent deep learning research to discover underlying principles for designing neural network architectures. In addition, we describe several architectural innovations, including Fractal of FractalNet network, Stagewise Boosting Networks, and Taylor Series Networks (our Caffe code and prototxt files is available at https://github.com/iPhysicist/CNNDesignPatterns). We hope others are inspired to build on our preliminary work.",ICLR 2017 conference submission,False,,"The authors agree with the reviewers that this manuscript is not yet ready.

---

We wish to thank the esteemed Reviewers for their time and this valuable feedback to our paper.  We believe that the Reviewers are correct in their evaluations. Hence, our paper will require a significant rewrite and will not be ready for the ICLR conference paper deadline

---

The paper formulates a number of rules for designing convolutional neural network architectures for image processing and computer vision problems. Essentially, it reads like a review paper about modern CNN architectures. It also proposes a few new architectural ideas inspired by these rules. These are experimentally evaluated on CIFAR-10 and CIFAR-100, but seem to achieve relatively poor performance on these datasets (Table 1), so their merit is unclear to me.

I'm not sure if such a collection of rules extracted from prior work warrants publication as a research paper. It is not a bad idea to try and summarise some of these observations now that CNNs have been the model of choice for computer vision tasks for a few years, and such a summary could be useful for newcomers. However, a lot of it seems to boil down to common sense (e.g. #1, #3, #7, #11). The rest of it might be more suited for an ""introduction to training CNNs"" course / blog post. It also seems to be a bit skewed towards recent work that was fairly incremental (e.g. a lot of attention is given to the flurry of ResNet variants).

The paper states that ""it is universal in all convolutional neural networks that the activations are downsampled and the number of channels increased from the input to the final layer"", which is wrong. We already discussed this previously re: my question about design pattern 5, but I think the answer that was given (""the nature of design patterns is that they only apply some of the time"") does not excuse making such sweeping claims. This should probably be removed.

""We feel that normalization puts all the layer's input samples on more equal footing, which allows backprop to train more effectively"" (section 3.2, 2nd paragraph) is very vague language that has many possible interpretations and should probably be clarified. It also seems odd to start this sentence with ""we feel"", as this doesn't seem like the kind of thing one should have an opinion about. Such claims should be corroborated by experiments and measurements. There are several other instances of this issue across the paper.

The connection between Taylor series and the proposed Taylor Series Networks seems very tenuous and I don't think the name is appropriate. The resulting function is not even a polynomial as all the terms represent different functions -- f(x) + g(x)**2 + h(x)**3 + ... is not a particularly interesting object, it is just a nonlinear function of x.

Overall, the paper reads like a collection of thoughts and ideas that are not very well delineated, and the experimental results are unconvincing.

---

The authors have grouped recent work in convolutional neural network design (specifically with respect to image classification) to identify core design principles guiding the field at large. The 14 principles they produce (along with associated references) include a number of useful and correct observations that would be an asset to anyone unfamiliar with the field. The authors explore a number of architectures on CIFAR-10 and CIFAR-100 guided by these principles.

The authors have collected a quality set of references on the subject and grouped them well which is valuable for young researchers. Clearly the authors explored a many of architectural changes as part of their experiments and publicly available code base is always nice.

Overall the writing seems to jump around a bit and the motivations behind some design principles feel lost in the confusion. For example, ""Design Pattern 4: Increase Symmetry argues for architectural symmetry as a sign of beauty and quality"" is presented as one of 14 core design principles without any further justification. Similarly ""Design Pattern 6: Over-train includes any training method where the network is trained on a harder problem than necessary to improve generalization performance of inference"" is presented in the middle of a paragraph with no supporting references or further explanation.

The experimental portion of this paper feels scattered with many different approaches being presented based on subsets of the design principles. In general, these approaches either are minor modifications of existing networks (different FractalNet pooling strategies) or are novel architectures that do not perform well. The exception being the Fractal-of-Fractal network which achieves slightly improved accuracy but also introduces many more network parameters (increased capacity) over the original FractalNet.


Preliminary rating:
It is a useful and perhaps noble task to collect and distill research from many sources to find patterns (and perhaps gaps) in the state of a field; however, some of the patterns presented do not seem well developed and include principles that are poorly explained. Furthermore, the innovative architectures motivated by the design principles either fall short or achieve slightly better accuracy by introducing many more parameters (Fractal-Of-Fractal networks). For a paper addressing the topic of higher level design trends, I would appreciate additional rigorous experimentation around each principle rather than novel architectures being presented.

---

The authors take on the task of figuring out a set of design patterns for current deep architectures - namely themes that are recurring in the literature.  If one may say so, a distributed representation of deep architectures. 

There are two aspects of the paper that I particularly valued: firstly, the excellent review of recent works, which made me realize how many things I have been missing myself.  Secondly, the ""community service"" aspect of helping someone who starts figure out the ""coordinate system"" for deep architectures - this could potentially be more important than introducing yet-another trick of the trade, as most other submissions may do.

However I think this work is still half-done, and even though working on this project is a great idea, the authors do not yet do it properly. 

Firstly, I am not too sure how the choice of these 14 patterns was made. Maxout for instance (pattern 14) is one of the many nonlinearities (PreLU, ReLU, ...) and I do not see how it stands on the same grounds as something as general as ""3 Strive for simplicity"".

Similarly some of the patterns are as vague as ""Increase symmetry"" and are backed up by statements such as ""we noted a special degree of elegance in the FractalNet"". I do not see how this leads to a design pattern that can be applied to a new architecture - or if it applies to anything other than the FractalNet. 

Some other patterns are phrased with weird names ""7 Cover the problem space"" - which I guess stands for dataset augmentation; or ""6 over-train"" which is not backed up by a single reference. Unless the authors relate it to regularization (text preceding ""overtrain""), which then has no connection to the description of ""over-train"" provided by the authors (""training a network on a harder problem to improve generalization""). If ""harder problem"" means one where one adds an additional term (i.e. the regularizer), the authors are doing harm to the unexperienced reader, confusing ""regularization"" with something that sounds like ""overfitting"" (i.e. the exact opposite).

Furthermore, the extensions proposed in Section 4 seem a bit off tune - in particular I could not figure out 
-how the Taylor Series networks stem from any of the design patterns proposed in the rest of the paper. 
-whether the text between 4.1 and 4.1.1 is another of the architecture innovations (and if yes, why it is not in the 4.1.2, or 4.1.0) 
-and, most importantly, how these design patterns would be deployed in practice to think of a new network. 

To be more concrete, the authors mention that they propose the ""freeze-drop-path"" variant from ""symmetry considerations"" to ""drop-path"". 
Is this an application of the ""increase symmetry"" pattern? How would ""freeze-drop-path"" be more symmetric that ""drop-path""?
 Can this be expressed concretely, or is it some intuitive guess? If the second, it is not really part of applying a pattern, in my understanding. If the first, this is missing. 


What I would have appreciated more (and would like to see in a revised version) would have been a table of ""design patterns"" on one axis, ""Deep network"" on another, and a breakdown of which network applies which design pattern. 

A big part of the previous work is also covered in cryptic language - some minimal explanation of what is taking place in the alternative works would be useful.

---

The authors agree with the reviewers that this manuscript is not yet ready.

---

We wish to thank the esteemed Reviewers for their time and this valuable feedback to our paper.  We believe that the Reviewers are correct in their evaluations. Hence, our paper will require a significant rewrite and will not be ready for the ICLR conference paper deadline

---

The paper formulates a number of rules for designing convolutional neural network architectures for image processing and computer vision problems. Essentially, it reads like a review paper about modern CNN architectures. It also proposes a few new architectural ideas inspired by these rules. These are experimentally evaluated on CIFAR-10 and CIFAR-100, but seem to achieve relatively poor performance on these datasets (Table 1), so their merit is unclear to me.

I'm not sure if such a collection of rules extracted from prior work warrants publication as a research paper. It is not a bad idea to try and summarise some of these observations now that CNNs have been the model of choice for computer vision tasks for a few years, and such a summary could be useful for newcomers. However, a lot of it seems to boil down to common sense (e.g. #1, #3, #7, #11). The rest of it might be more suited for an ""introduction to training CNNs"" course / blog post. It also seems to be a bit skewed towards recent work that was fairly incremental (e.g. a lot of attention is given to the flurry of ResNet variants).

The paper states that ""it is universal in all convolutional neural networks that the activations are downsampled and the number of channels increased from the input to the final layer"", which is wrong. We already discussed this previously re: my question about design pattern 5, but I think the answer that was given (""the nature of design patterns is that they only apply some of the time"") does not excuse making such sweeping claims. This should probably be removed.

""We feel that normalization puts all the layer's input samples on more equal footing, which allows backprop to train more effectively"" (section 3.2, 2nd paragraph) is very vague language that has many possible interpretations and should probably be clarified. It also seems odd to start this sentence with ""we feel"", as this doesn't seem like the kind of thing one should have an opinion about. Such claims should be corroborated by experiments and measurements. There are several other instances of this issue across the paper.

The connection between Taylor series and the proposed Taylor Series Networks seems very tenuous and I don't think the name is appropriate. The resulting function is not even a polynomial as all the terms represent different functions -- f(x) + g(x)**2 + h(x)**3 + ... is not a particularly interesting object, it is just a nonlinear function of x.

Overall, the paper reads like a collection of thoughts and ideas that are not very well delineated, and the experimental results are unconvincing.

---

The authors have grouped recent work in convolutional neural network design (specifically with respect to image classification) to identify core design principles guiding the field at large. The 14 principles they produce (along with associated references) include a number of useful and correct observations that would be an asset to anyone unfamiliar with the field. The authors explore a number of architectures on CIFAR-10 and CIFAR-100 guided by these principles.

The authors have collected a quality set of references on the subject and grouped them well which is valuable for young researchers. Clearly the authors explored a many of architectural changes as part of their experiments and publicly available code base is always nice.

Overall the writing seems to jump around a bit and the motivations behind some design principles feel lost in the confusion. For example, ""Design Pattern 4: Increase Symmetry argues for architectural symmetry as a sign of beauty and quality"" is presented as one of 14 core design principles without any further justification. Similarly ""Design Pattern 6: Over-train includes any training method where the network is trained on a harder problem than necessary to improve generalization performance of inference"" is presented in the middle of a paragraph with no supporting references or further explanation.

The experimental portion of this paper feels scattered with many different approaches being presented based on subsets of the design principles. In general, these approaches either are minor modifications of existing networks (different FractalNet pooling strategies) or are novel architectures that do not perform well. The exception being the Fractal-of-Fractal network which achieves slightly improved accuracy but also introduces many more network parameters (increased capacity) over the original FractalNet.


Preliminary rating:
It is a useful and perhaps noble task to collect and distill research from many sources to find patterns (and perhaps gaps) in the state of a field; however, some of the patterns presented do not seem well developed and include principles that are poorly explained. Furthermore, the innovative architectures motivated by the design principles either fall short or achieve slightly better accuracy by introducing many more parameters (Fractal-Of-Fractal networks). For a paper addressing the topic of higher level design trends, I would appreciate additional rigorous experimentation around each principle rather than novel architectures being presented.

---

The authors take on the task of figuring out a set of design patterns for current deep architectures - namely themes that are recurring in the literature.  If one may say so, a distributed representation of deep architectures. 

There are two aspects of the paper that I particularly valued: firstly, the excellent review of recent works, which made me realize how many things I have been missing myself.  Secondly, the ""community service"" aspect of helping someone who starts figure out the ""coordinate system"" for deep architectures - this could potentially be more important than introducing yet-another trick of the trade, as most other submissions may do.

However I think this work is still half-done, and even though working on this project is a great idea, the authors do not yet do it properly. 

Firstly, I am not too sure how the choice of these 14 patterns was made. Maxout for instance (pattern 14) is one of the many nonlinearities (PreLU, ReLU, ...) and I do not see how it stands on the same grounds as something as general as ""3 Strive for simplicity"".

Similarly some of the patterns are as vague as ""Increase symmetry"" and are backed up by statements such as ""we noted a special degree of elegance in the FractalNet"". I do not see how this leads to a design pattern that can be applied to a new architecture - or if it applies to anything other than the FractalNet. 

Some other patterns are phrased with weird names ""7 Cover the problem space"" - which I guess stands for dataset augmentation; or ""6 over-train"" which is not backed up by a single reference. Unless the authors relate it to regularization (text preceding ""overtrain""), which then has no connection to the description of ""over-train"" provided by the authors (""training a network on a harder problem to improve generalization""). If ""harder problem"" means one where one adds an additional term (i.e. the regularizer), the authors are doing harm to the unexperienced reader, confusing ""regularization"" with something that sounds like ""overfitting"" (i.e. the exact opposite).

Furthermore, the extensions proposed in Section 4 seem a bit off tune - in particular I could not figure out 
-how the Taylor Series networks stem from any of the design patterns proposed in the rest of the paper. 
-whether the text between 4.1 and 4.1.1 is another of the architecture innovations (and if yes, why it is not in the 4.1.2, or 4.1.0) 
-and, most importantly, how these design patterns would be deployed in practice to think of a new network. 

To be more concrete, the authors mention that they propose the ""freeze-drop-path"" variant from ""symmetry considerations"" to ""drop-path"". 
Is this an application of the ""increase symmetry"" pattern? How would ""freeze-drop-path"" be more symmetric that ""drop-path""?
 Can this be expressed concretely, or is it some intuitive guess? If the second, it is not really part of applying a pattern, in my understanding. If the first, this is missing. 


What I would have appreciated more (and would like to see in a revised version) would have been a table of ""design patterns"" on one axis, ""Deep network"" on another, and a breakdown of which network applies which design pattern. 

A big part of the previous work is also covered in cryptic language - some minimal explanation of what is taking place in the alternative works would be useful.",,,,,,3.3333333333333335,,,3.6666666666666665,,
756,"TIONS WITH NEURAL SIMILARITY AND CONTEXT EN- CODERS
Authors: Franziska Horn
Source file: 756.pdf

ABSTRACT
We introduce similarity encoders (SimEc), which learn similarity preserving representations by using a feed-forward neural network to map data into an embedding space where the original similarities can be approximated linearly. The model can easily compute representations for novel (out-of-sample) data points, even if the original pairwise similarities of the training set were generated by an unknown process such as human ratings. This is demonstrated by creating embeddings of both image and text data. Furthermore, the idea behind similarity encoders gives an intuitive explanation of the optimization strategy used by the continuous bag-of-words (CBOW) word2vec model trained with negative sampling. Based on this insight, we define context encoders (ConEc), which can improve the word embeddings created with word2vec by using the local context of words to create out-of-vocabulary embeddings and representations for words with multiple meanings. The benefit of this is illustrated by using these word embeddings as features in the CoNLL 2003 named entity recognition task.

1 INTRODUCTION
Many dimensionality reduction or manifold learning algorithms optimize for retaining the pairwise similarities, distances, or local neighborhoods of data points. Classical scaling (Cox & Cox, 2000), kernel PCA (Schölkopf et al., 1998), isomap (Tenenbaum et al., 2000), and LLE (Roweis & Saul, 2000) achieve this by performing an eigendecomposition of some similarity matrix to obtain a low dimensional representation of the original data. However, this is computationally expensive if a lot of training examples are available. Additionally, out-of-sample representations can only be created when the similarities to the original training examples can be computed (Bengio et al., 2004).
For some methods such as t-SNE (van der Maaten & Hinton, 2008), great effort was put into extending the algorithm to work with large datasets (van der Maaten, 2013) or to provide an explicit mapping function which can be applied to new data points (van der Maaten, 2009). Current attempts at finding a more general solution to these issues are complex and require the development of specific cost functions and constraints when used in place of existing algorithms (Bunte et al., 2012), which limits their applicability to new objectives.
In this paper we introduce a new neural network architecture, that we will denote as similarity encoder (SimEc), which is able to learn representations that can retain arbitrary pairwise relations present in the input space, even those obtained from unknown similarity functions such as human ratings. A SimEc can learn a linear or non-linear mapping function to project new data points into a lower dimensional embedding space. Furthermore, it can take advantage of large datasets since the objective function is optimized iteratively using stochastic mini-batch gradient descent. We show on both image and text datasets that SimEcs can, on the one hand, recreate solutions found by traditional methods such as kPCA or isomap, and, on the other hand, obtain meaningful embeddings from similarities based on human labels.
Additionally, we propose the new context encoder (ConEc) model, a variation of similarity encoders for learning word embeddings, which extends word2vec (Mikolov et al., 2013b) by using the local context of words as input to the neural network to create representations for out-of-vocabulary words and to distinguish between multiple meanings of words. This is shown to be advantageous, for example, if the word embeddings are used as features in a named entity recognition task as demonstrated on the CoNLL 2003 challenge.

2 SIMILARITY ENCODERS
We propose a novel dimensionality reduction framework termed similarity encoder (SimEc), which can be used to learn a linear or non-linear mapping function for computing low dimensional representations of data points such that the original pairwise similarities between the data points in the input space are preserved in the embedding space. For this, we borrow the “bottleneck” neural network (NN) architecture idea from autoencoders (Tishby et al., 2000; Hinton & Salakhutdinov, 2006). Autoencoders aim to transform the high dimensional data points into low dimensional embeddings such that most of the data’s variance is retained. Their network architecture has two parts: The first part of the network maps the data points from the original feature space to the low dimensional embedding (at the bottleneck). The second part of the NN mirrors the first part and projects the embedding back to a high dimensional output. This output is then compared to the original input to compute the reconstruction error of the training samples, which is used in the backpropagation procedure to tune the network’s parameters. After the training is complete, i.e. the low dimensional embeddings encode enough information about the original input samples to allow for their reconstruction, the second part of the network is discarded and only the first part is used to project data points into the low dimensional embedding space. Similarity encoders have a similar two fold architecture, where in the first part of the network, the data is mapped to a low dimensional embedding, and then in the second part (which is again only used during training), the embedding is transformed such that the error of the representation can be computed. However, since here the objective is to retain the (non-linear) pairwise similarities instead of the data’s variance, the second part of the NN does not mirror the first like it does in the autoencoder architecture.
The similarity encoder architecture (Figure 1) uses as the first part of the network a flexible non-linear feed-forward neural network to map the high dimensional input data points xi ∈ RD to a low dimensional embedding yi ∈ Rd (at the bottleneck). As we make no assumptions on the range of values the embedding can take, the last layer of the first part of the NN (i.e. the one resulting in the embedding) is always linear. For example, with two additional non-linear hidden layers, the embedding would be computed as
yi = σ1(σ0(xiW0)W1)W2,
where σ0 and σ1 denote your choice of non-linear activation functions (e.g. tanh, sigmoid, or relu), but there is no non-linearity applied after multiplying with W2. The second part of the network then
consists of a single additional layer with the weight matrix W−1 ∈ Rd×N to project the embedding to the output, the approximated similarities s′ ∈ RN :
s′ = σ−1(yiW−1).
These approximated similarities are then compared to the target similarities (for one data point this is the corresponding row si ∈ RN of the similarity matrix S ∈ RN×N of the N training samples) and the computed error is used to tune the network’s parameters with backpropagation.
For the model to learn most efficiently, the exact form of the cost function to optimize as well as the type of non-linearity σ−1 applied when computing the network’s output should be chosen with respect to the type of target similarities that the model is supposed to preserve. In the experimental section of the paper we are considering two application scenarios of SimEcs: a) to obtain the same low dimensional embedding as found by spectral methods such as kPCA, and b) to embed data points such that binary similarity relations obtained from human labels are preserved. In the first case (further discussed in the next section), we omit the non-linearity when computing the output of the network, i.e. s′ = yiW−1, since the target similarities, computed by some kernel function, are not necessarily constrained to lie in a specific interval. As the cost function to minimize we choose the mean squared error between the output (approximated similarities) and the original (target) similarities. A regularization term is added to encourage the weights of the last layer (W−1) to be orthogonal.1 The model’s objective function optimized during training is therefore:
min 1
N N∑ i=1 ‖si − s′‖22 + λ 1 d2 − d ∥∥W−1W>−1 − diag(W−1W>−1)∥∥1
where ‖ · ‖p denotes the respective p-norms for vectors and matrices and λ is a hyperparameter to control the strength of the regularization. In the second case, the target similarities are binary and it therefore makes sense to use a nonlinear activation function in the final layer when computing the output of the network to ensure the approximated similarities are between 0 and 1 as well:2
s′ = σ−1(yiW−1) with σ−1(z) = 1
1 + e−10(z−0.5) .
While the mean squared error between the target and approximated similarities would still be a natural choice of cost function to optimize, with the additional non-linearity in the output layer, learning might be slow due to small gradients and we therefore instead optimize the cross-entropy:
min − 1 N
∑ [si ln(s ′) + (1− si) ln(1− s′)] .
For a different application scenario, yet another setup might lead to the best results. When using SimEcs in practice, we recommend to first try the first setup, i.e. keeping the output layer linear and minimizing the mean squared error, as this often already gives quite good results.
After the training is completed, only the first part of the neural network, which maps the input to the embedding, is used to create the representations of new data points. Depending on the complexity of the feed-forward NN, the mapping function learned by similarity encoders can be linear or non-linear, and because of the iterative optimization using stochastic mini-batch gradient descent, large amounts of data can be utilized to learn optimal representations.3

2.1 RELATION TO KERNEL PCA
Kernel PCA (kPCA) is a popular non-linear dimensionality reduction algorithm, which performs the eigendecomposition of a kernel matrix to obtain low dimensional representations of the data points
1To get embeddings similar to those obtained by kPCA, orthogonal weights in the last layer of the NN help as they correspond to the orthogonal eigenvectors of the kernel matrix found by kPCA.
2This scaled and shifted sigmoid function maps values between 0 and 1 almost linearly while thresholding values outside this interval.
3To speed up the training procedure and limit memory requirements for large datasets, the columns of the similarity matrix can also be subsampled (yielding S ∈ RN×n), i.e. the number of target similarities (and the dimensionality of the output layer) is n < N , however all N training examples can still be used as input to train the network.
(Schölkopf et al., 1998). However, if the kernel matrix is very large this becomes computationally very expensive. Additionally, there are constraints on possible kernel functions (should be positive semi-definite) and new data points can only be embedded in the lower dimensional space if their kernel map (i.e. the similarities to the original training points) can be computed. As we show below, SimEc can optimize the same objective as kPCA but addresses these shortcomings.
The general idea is that both kPCA and SimEc embed the N data points in a feature space where the given target similarities can be approximated linearly (i.e. with the scalar product of the embedding vectors). When the error between the approximated (S′) and the target similarities (S) is computed as the mean squared error, kPCA finds the optimal approximation by performing the eigendecomposition of the (centered) target similarity matrix, i.e.
S′ = Y Y >,
where Y ∈ RN×d is the low dimensional embedding of the data based on the eigenvectors belonging to the d largest eigenvalues of S.
In addition to the embedding itself, it is often desired to have a parametrized mapping function, which can be used to project new (out-of-sample) data points into the embedding space. If the target similarity matrix is the linear kernel, i.e. S = XX> where X ∈ RN×D is the given input data, this can easily be accomplished with traditional PCA. Here, the covariance matrix of the centered input data, i.e. C = X>X is decomposed to obtain a matrix with parameters, W̃ ∈ RD×d, based on the eigenvectors belonging to the d largest eigenvalues of the covariance matrix. Then the optimal embedding (i.e. the same solution obtained by linear kPCA) can be computed as
Y = XW̃.
This serves as a mapping function, with which new data points can be easily projected into the lower dimensional embedding space.
When using a similarity encoder to embed data in a low dimensional space where the linear similarities are preserved, the SimEc’s architecture would consist of a neural network with a single linear layer, i.e. the parameter matrix W0, to project the input data X to the embedding Y = XW0, and another matrix W−1 ∈ Rd×N used to approximate the similarities as S′ = YW−1. From these formulas one can immediately see the link between linear similarity encoders and PCA / linear kPCA: once the parameters of the neural network are tuned correctly, W0 would correspond to the mapping matrix W̃ found by PCA and W−1 could be interpreted as Y >, i.e. Y would be the same eigenvector based embedding as found with linear kPCA.
Finding the corresponding function to map new data points into the embedding space is trivial for linear kPCA, but this is not the case for other kernel functions. While it is still possible to find the optimal embedding with kPCA for non-linear kernel functions, the mapping function remains unknown and new data points can only be projected into the embedding space if we can compute their kernel map, i.e. the similarities to the original training examples (Bengio et al., 2004). Some attempts were made to manually define an explicit mapping function to represent data points in the kernel feature space, however this only works for specific kernels and there exists no general solution (Rahimi & Recht, 2007). As neural networks are universal function approximators, with the right architecture similarity encoders could instead learn arbitrary mapping functions for unknown similarities to arrive at data driven kernel learning solutions.

2.2 MODEL OVERVIEW
The properties of similarity encoders are summarized in the following. The objective of this dimensionality reduction approach is to retain pairwise similarities between data points in the embedding space. This is achieved by tuning the parameters of a neural network to obtain a linear or non-linear mapping (depending on the network’s architecture) from the high dimensional input to the low dimensional embedding. Since the cost function is optimized using stochastic mini-batch gradient descent, we can take advantage of large datasets for training. The embedding for new test points can be easily computed with the explicit mapping function in the form of the tuned neural network. And since there is no need to compute the similarity of new test examples to the original training data for out-of-sample solutions (like with kPCA), the target similarities can be generated by an unknown process such as human similarity judgments.

2.3 EXPERIMENTS
In the following experiments we demonstrate that similarity encoders can, on the one hand, reach the same solution as kPCA, and, on the other hand, generate meaningful embeddings from human labels. To illustrate that this is independent of the type of data, we present results obtained both on the well known MNIST handwritten digits dataset as well as the 20 newsgroups text corpus. Further details as well as the code to replicate these experiments and more is available online.4
We compare the embedding found with linear kPCA to that created with a linear similarity encoder (consisting of one linear layer mapping the input to the embedding and a second linear layer to project the embedding to the output, i.e. computing the approximated similarities). Additionally, we show that a non-linear SimEc can approximate the solution found with isomap (i.e. the eigendecomposition of the geodesic distance matrix). We found that for optimal results the kernel matrix used as the target similarity matrix for the SimEc should first be centered (as it is being done for kPCA as well (Müller et al., 2001)).
In a second step, we show that SimEcs can learn the mapping to a low dimensional embedding for arbitrary similarity functions and reliably create representations for new test samples without the need to compute their similarities to the original training examples, thereby going beyond the capabilities of kPCA. For both datasets we illustrate this by using the class labels assigned to the samples by human annotators to create the target similarity matrix for the training fold of the data, i.e. S is 1 for data points belonging to the same class and 0 everywhere else. We compare the solutions found by SimEc architectures with a varying number of additional non-linear hidden layers in the first part of the network (while keeping the embedding layer linear as before) to show how a more complex network improves the ability to map the data into an embedding space in which the class-based similarities are retained.
MNIST The MNIST dataset contains 28× 28 pixel images depicting handwritten digits. For our experiments we randomly subsampled 10k images from all classes, of which 80% are assigned to the training fold and the remaining 20% to the test fold (in the following plots, data points belonging to the training set are displayed transparently while the test points are opaque). As shown in Figure 2, the embeddings of the MNIST dataset created with linear kPCA and a linear similarity encoder, which uses as target similarities the linear kernel matrix, are almost identical (up to a rotation). The same holds true for the isomap embedding, which is well approximated by a non-linear SimEc with two hidden layers using the geodesic distances between the data points as targets (Figure 8 in the Appendix). When optimizing SimEcs to retain the class-based similarities (Figure 3), additional
non-linear hidden layers in the feed-forward NN can improve the embedding by further separating data points belonging to different classes in tight clusters. As it can be seen, the test points (opaque) are nicely mapped into the same locations as the corresponding training points (transparent), i.e. the model learns to associate the input pixels with the class clusters only based on the imposed similarities between the training data points.
4https://github.com/cod3licious/simec/examples_simec.ipynb
20 newsgroups The 20 newsgroups dataset consists of around 18k newsgroup posts assigned to 20 different topics. We take a subset of seven categories and use the original train/test split (∼4.1k and ∼2.7k samples respectively) and remove metadata such as headers to avoid overfitting.5 All text documents are transformed into 46k dimensional tf-idf feature vectors, which are used as input to the SimEc and to compute the linear kernel matrix of the training fold. The embedding created with linear kPCA is again well approximated by the solution found with a corresponding linear SimEc (Figure 9 in the Appendix). Additionally, this serves as an example where traditional PCA is not an option to obtain the corresponding mapping matrix for the linear kPCA solution, as due to the high dimensionality of the input data and comparatively low number of samples, the empirical covariance matrix would be poorly estimated and too large to decompose into eigenvalues and -vectors. With the objective to retain the class-based similarities, a SimEc with a non-linear hidden layer clusters documents by their topics (Figure 4).

3 CONTEXT ENCODERS
Representation learning is very prominent in the field of natural language processing (NLP). For example, word embeddings learned by neural network language models were shown to improve the performance when used as features for supervised learning tasks such as named entity recognition (NER) (Collobert et al., 2011; Turian et al., 2010). The popular word2vec model (Figure 5) learns meaningful word embeddings by considering only the words’ local contexts and thanks to its shallow architecture it can be trained very efficiently on large corpora. However, an important limiting factor of current word embedding models is that they only learn the representations for words from a fixed vocabulary. This means, if in a task we encounter a new word which was not present in the texts used for training, we can not create an embedding for this word without repeating the time consuming
5http://scikit-learn.org/stable/datasets/twenty_newsgroups.html
training procedure of the model.6 Additionally, word2vec, like many other approaches, only learns a single representation for every word. However, it is often the case that a single word can have multiple meanings, e.g. “Washington” is both the name of a US state as well as a former president. It is only the local context in which these words appear that lets humans resolve this ambiguity and identify the proper sense of the word in question. While attempts were made to improve this, they lack flexibility as they require a clustering of word contexts beforehand (Huang et al., 2012), which still does not guarantee that all possible meanings of a word have been identified prior in the training documents. Other approaches require additional labels such part-of-speech tags (Trask et al., 2015) or other lexical resources like WordNet (Rothe & Schütze, 2015) to create word embeddings which distinguish between the different senses of a word.
As a further contribution of this paper we provide a link between the successful word2vec natural language model and similarity encoders and thereby propose a new model we call context encoder (ConEc), which can efficiently learn word embeddings from huge amounts of training data and additionally make use of local contexts to create representations for out-of-vocabulary words and help distinguish between multiple meanings of words.
6In practice these models are trained on such a large vocabulary that it is rare to encounter a word which does not have an embedding. However, there are still scenarios where this is the case, for example, it is unlikely that the term “W10281545” is encountered in a regular training corpus, but we might still want its embedding to represent a search query like “whirlpool W10281545 ice maker part”.
Formally, word embeddings are d -dimensional vector representations learned for all N words in the vocabulary. Word2vec is a shallow model with parameter matrices W0,W1 ∈ RN×d, which are tuned iteratively by scanning huge amounts of texts sentence by sentence (see Figure 5). Based on some context words the algorithm tries to predict the target word between them. Mathematically this is realized by first computing the sum of the embeddings of the context words by selecting the appropriate rows from W0. This vector is then multiplied by several rows selected from W1: one of these rows corresponds to the target word, while the others correspond to k ‘noise’ words, selected at random (negative sampling). After applying a non-linear activation function, the backpropagation error is computed by comparing this output to a label vector t ∈ Rk+1, which is 1 at the position of the target word and 0 for all k noise words. After the training of the model is complete, the word embedding for a target word is the corresponding row of W0.
The main principle utilized when learning word embeddings is that similar words appear in similar contexts (Harris, 1954; Melamud et al., 2015). Therefore, in theory one could compute the similarities between all words by checking how many context words any two words generally have in common (possibly weighted somehow to reduce the influence of frequent words such as ‘the’ and ‘and’). However, such a word similarity matrix would be very large, as typically the vocabulary for which word embeddings are learned comprises several 10, 000 words, making it computationally too expensive to be used with similarity encoders. But this matrix would also be quite sparse, because many words in fact do not occur in similar contexts and most words only have a handful of synonyms which could be used in their place. Therefore, we can view the negative sampling approach used for word2vec (Mikolov et al., 2013b) as an approximation of the words’ context based similarities: while the similarity of a word to itself is 1, if for one word we select k random words out of the huge vocabulary, it is very unlikely that they are similar to the target word, i.e. we can approximate their similarities with 0. This is the main insight necessary for adapting similarity encoders to be used for learning (context sensitive) word embeddings.
Figure 6 shows the architecture of the context encoder. For the training procedure we stick very closely to the optimization strategy used by word2vec: while parsing a document, we again select a target word and its context words. As input to the context encoder network, we use a vector xi of length N (i.e. the size of the vocabulary), which indicates the context words by non-zero values (either binary or e.g. giving lower weight to context words further away from the target word). This vector is then multiplied by a first matrix of weights W0 ∈ RN×d yielding a low dimensional embedding yi, comparable to the summed context embedding created as a first step when training the word2vec model. This embedding is then multiplied by a second matrix W1 ∈ Rd×N to yield the output. Instead of comparing this output vector to a whole row from a word similarity matrix (as we would with similarity encoders), only k + 1 entries are selected, namely those belonging to
the target word as well as k random and unrelated noise words. After applying a non-linearity we compare these entries s′ ∈ Rk+1 to the binary target vector exactly as in the word2vec model and use error backpropagation to tune the parameters.
Up to now, there are no real differences between the word2vec model and our context encoders, we have merely provided an intuitive interpretation of the training procedure and objective. The main deviation from the word2vec model lies in the computation of the word embedding for a target word after the training is complete. In the case of word2vec, the word embedding is simply the row of the tuned W0 matrix. However, when considering the idea behind the optimization procedure, we instead propose to compute a target word’s representation by multiplying W0 with the word’s average context vector. This is closer to what is being done in the training procedure and additionally it enables us to compute the embeddings for out-of-vocabulary words (assuming at least most of such a new word’s context words are in the vocabulary) as well as to place more emphasis on a word’s local context (which helps to identify the proper meaning of the word (Melamud et al., 2015)) by creating a weighted sum between the word’s average global and local context vectors used as input to the ConEc.
With this new perspective on the model and optimization procedure, another advancement is feasible. Since the context words are merely a sparse feature vector used as input to a neural network, there is no reason why this input vector should not contain other features about the target word as well. For example, the feature vector could be extended to contain information about the word’s case, part-of-speech (POS) tag, or other relevant details. While this would increase the dimensionality of the first weight matrix W0 to include the additional features when mapping the input to the word’s embedding, the training objective and therefore also W1 would remain unchanged. These additional features could be especially helpful if details about the words would otherwise get lost in preprocessing (e.g. by lowercasing) or to retain information about a word’s position in the sentence, which is ignored in a BOW approach. These extended ConEcs are expected to create embeddings which distinguish even better between the words’ different senses by taking into account, for example, if the word is used as a noun or verb in the current context, similar to the sense2vec algorithm (Trask et al., 2015). However, unlike sense2vec, not multiple embeddings per term are learned, instead the dimensionality of the input vector is increased to include the POS tag of the current word as a feature.

3.1 EXPERIMENTS
The word embeddings learned with word2vec and context encoders are evaluated on a word analogy task (Mikolov et al., 2013a) as well as the CoNLL 2003 NER benchmark task (Tjong et al., 2003). The word2vec model used is a continuous BOW model trained with negative sampling as described above where k = 13, the embedding dimensionality d is 200 and we use a context window of 5. The word embeddings created by the context encoders are build directly on top of the word2vec model by multiplying the original embeddings (W0) with the respective context vectors. Code to replicate the experiments can be found online.7 The results of the analogy task can be found in the Appendix.8
Named Entity Recognition The main advantage of context encoders is that they can use local context to create out-of-vocabulary (OOV) embeddings and distinguish between the different senses of words. The effects of this are most prominent in a task such as named entity recognition (NER) where the local context of a word can make all the difference, e.g. to distinguish between the “Chicago Bears” (an organization) and the city of Chicago (a location). To test this, we used the word embeddings as features in the CoNLL 2003 NER benchmark task (Tjong et al., 2003). The word2vec embeddings were trained on the documents used in the training part of the task.9 For the context encoders we experimented with different combinations of local and global context vectors. The global context vectors were computed on only the training documents as well, i.e. just as with
7https://github.com/cod3licious/conec 8As it was recently demonstrated that a good performance on intrinsic evaluation tasks such as word similarity or analogy tasks does not necessarily transfer to extrinsic evaluation measures when using the word embeddings as features (Chiu et al., 2016; Linzen, 2016), we consider the performance on the NER challenge as more relevant.
9Since this is a very small corpus, we trained word2vec for 25 iterations on these documents (afterwards the performance on the development split stopped improving significantly) while usually the model is trained in a single pass through a much larger corpus.
the word2vec model, when applied to the test documents there are some words which don’t have a word embedding available as they did not occur in the training texts. The local context vectors on the other hand can be computed for all words occurring in the current document for which the model should identify the named entities. When combining these local context vectors with the global ones we always use the local context vector as is in case there is no global vector available and otherwise compute a weighted average between the two context vectors as wl · CVlocal + (1− wl) · CVglobal.10 The different word embeddings were used as features with a logistic regression classifier trained on the labels obtained from the training part of the task and the reported F1-scores were computed using the official evaluation script. Please note that we are using this task to show the potential of ConEc word embeddings as features in a real world task and to illustrate their advantages over the regular word2vec embeddings and did not optimize for competitive performance on this NER challenge.
Figure 7 shows the results achieved with various word embeddings on the training, development and test part of the CoNLL task. As it can be seen there, taking into account the local context can yield large improvements, especially on the dev and test data. Context encoders using only the global context vectors already perform better than word2vec. When using the local context vectors only where the global ones are not available (wl = 0) we can see a jump in the development and test performance, while of course the training performance stays the same as here we have global context vectors for all words. The best performances on all folds are achieved when averaging the global and local context vectors with around wl = 0.4 before multiplying them with the word2vec embeddings. This clearly shows that using ConEcs with local context vectors can be very beneficial as they let us compute word embeddings for out-of-vocabulary words as well as help distinguish between multiple meanings of words.
10The global context matrix is computed without taking the word itself into account (i.e. zero on the diagonal) to make the context vectors comparable to the local context vectors of OOV words where we can’t count the target word either. Both global and local context vectors are normalized by their respective maximum values, then multiplied with the length normalized word2vec embeddings and again renormalized to have unit length.

4 CONCLUSION
Representing intrinsically complex data is an ubiquitous challenge in data analysis. While kernel methods and manifold learning have made very successful contributions, their ability to scale is somewhat limited. Neural autoencoders offer scalable nonlinear embeddings, but their objective is to minimize the reconstruction error of the input data which does not necessarily preserve important pairwise relations between data points. In this paper we have proposed SimEcs as a neural network framework which bridges this gap by optimizing the same objective as spectral methods, such as kPCA, for creating similarity preserving embeddings while retaining the favorable properties of autoencoders.
Similarity encoders are a novel method to learn similarity preserving embeddings and can be especially useful when it is computationally infeasible to perform the eigendecomposition of a kernel matrix, when the target similarities are obtained through an unknown process such as human similarity judgments, or when an explicit mapping function is required. To accomplish this, a feed-forward neural network is constructed to map the data into an embedding space where the original similarities can be approximated linearly.
As a second contribution we have defined context encoders, a practical extension of SimEcs, that can be readily used to enhance the word2vec model with further local context information and global word statistics. Most importantly, ConEcs allow to easily create word embeddings for out-of-vocabulary words on the spot and distinguish between different meanings of a word based its local context.
Finally, we have demonstrated the usefulness of SimEcs and ConEcs for practical tasks such as the visualization of data from different domains and to create meaningful word embedding features for a NER task, going beyond the capabilities of traditional methods.
Future work will aim to further the theoretical understanding of SimEcs and ConEcs and explore other application scenarios where using this novel neural network architecture can be beneficial. As it is often the case with neural network models, determining the optimal architecture as well as other hyperparameter choices best suited for the task at hand can be difficult. While so far we mainly studied SimEcs based on fairly simple feed-forward networks, it appears promising to consider also deeper neural networks and possibly even more elaborate architectures, such as convolutional networks, for the initial mapping step to the embedding space, as in this manner hierarchical structures in complex data could be reflected. Note furthermore that prior knowledge as well as more general error functions could be employed to tailor the embedding to the desired application target(s).

ACKNOWLEDGMENTS
We would like to thank Antje Relitz, Christoph Hartmann, Ivana Balažević, and other anonymous reviewers for their helpful comments on earlier versions of this manuscript. Additionally, Franziska Horn acknowledges funding from the Elsa-Neumann scholarship from the TU Berlin.
","We introduce similarity encoders (SimEc), which learn similarity preserving representations by using a feed-forward neural network to map data into an embedding space where the original similarities can be approximated linearly. The model can easily compute representations for novel (out-of-sample) data points, even if the original pairwise similarities of the training set were generated by an unknown process such as human ratings. This is demonstrated by creating embeddings of both image and text data. Furthermore, the idea behind similarity encoders gives an intuitive explanation of the optimization strategy used by the continuous bag-of-words (CBOW) word2vec model trained with negative sampling. Based on this insight, we define context encoders (ConEc), which can improve the word embeddings created with word2vec by using the local context of words to create out-of-vocabulary embeddings and representations for words with multiple meanings. The benefit of this is illustrated by using these word embeddings as features in the CoNLL 2003 named entity recognition task.",ICLR 2017 conference submission,False,,"This paper presents a method for embedding data instances into a low-dimensional space that preserves some form of similarity.

Although the paper presents this notion as new, basically every pre-trained embedding (be it auto-encoders or word2vec) has been doing the same: representing items in a low-dimensional space that inherently encodes their similarities. Even when looking at the specific case of word/context embeddings, the method is not novel either: this method is almost identical to one of the similarity functions presented in ""A Simple Word Embedding Model for Lexical Substitution"" (Melamud et al., 2015). The novelty claim must be more accurate and position itself with respect to existing work.

In addition, I think the evaluation could be done better. There are plenty of benchmarks for word embeddings in context, for example: 
*

---

There is consensus among the reviewers that the novelty of the paper is limited, and that the experimental evaluation of the proposed method needs to be improved.

---

This paper presents a method for embedding data instances into a low-dimensional space that preserves some form of similarity.

Although the paper presents this notion as new, basically every pre-trained embedding (be it auto-encoders or word2vec) has been doing the same: representing items in a low-dimensional space that inherently encodes their similarities. Even when looking at the specific case of word/context embeddings, the method is not novel either: this method is almost identical to one of the similarity functions presented in ""A Simple Word Embedding Model for Lexical Substitution"" (Melamud et al., 2015). The novelty claim must be more accurate and position itself with respect to existing work.

In addition, I think the evaluation could be done better. There are plenty of benchmarks for word embeddings in context, for example: 
*

---

this paper proposes to use feed-forward neural networks to learn similarity preserving embeddings. They also use the proposed idea to represent out-of-vocabulary words using the words in given context. 

First, considering the related work [1,2] the proposed approach brings marginal novelty. Especially
Context Encoders is just a small improvement over word2vec. 

Experimental setup should provide more convincing results other than visualizations and non-standard benchmark for NER evaluation with word vectors [3].

[1]

---

This paper introduces a similarity encoder based on a standard feed-forward neural network with the aim of generating similarity-preserving embeddings. The approach is utilized to generate a simple extension of the CBOW word2vec model that transforms the learned embeddings by their average context vectors. Experiments are performed on an analogy task and named entity recognition.

While this paper offers some reasonable intuitive arguments for why a feed-forward neural network can generate good similarity-preserving embeddings, the architecture and approach is far from novel. As far as I can tell, the model is nothing more than the most vanilla neural network trained with SGD on similarity signals.

Slightly more original is the idea to use context embeddings to augment the expressive capacity of learned word representations. Of course, using explicit contextual information is not a new idea, especially for tasks like word sense disambiguation (see, e.g., 'Efficient Non-parametric Estimation of Multiple Embeddings per Word in Vector Space' by Neelakantan et al, which should also be cited), but the specific method used here is original, as far as I know.

The evaluation of the method is far from convincing. The corpora used to train the embeddings are far smaller than would ever be used in practice for unsupervised or semi-supervised embedding learning. The performance on the analogy task says little about the benefit of this method for larger corpora, and, as the authors mentioned in the comments, they expect ""the gain will be less significant, as the global context statistics brought in by the ConEc can also be picked up by word2vec with more training.""

The argument can be made (and the authors do claim) that extrinsic evaluations are more important for real-world applications, so it is good to see experiments on NER. However, again the embeddings were trained on a very small corpus and I am not convinced that the observed benefit will persist when trained on larger corpora.

Overall, I believe this paper offers little novelty and weak experimental evidence supporting its claims. I cannot recommend it for acceptance.

---

This paper presents a method for embedding data instances into a low-dimensional space that preserves some form of similarity.

Although the paper presents this notion as new, basically every pre-trained embedding (be it auto-encoders or word2vec) has been doing the same: representing items in a low-dimensional space that inherently encodes their similarities. Even when looking at the specific case of word/context embeddings, the method is not novel either: this method is almost identical to one of the similarity functions presented in ""A Simple Word Embedding Model for Lexical Substitution"" (Melamud et al., 2015). The novelty claim must be more accurate and position itself with respect to existing work.

In addition, I think the evaluation could be done better. There are plenty of benchmarks for word embeddings in context, for example: 
*

---

There is consensus among the reviewers that the novelty of the paper is limited, and that the experimental evaluation of the proposed method needs to be improved.

---

This paper presents a method for embedding data instances into a low-dimensional space that preserves some form of similarity.

Although the paper presents this notion as new, basically every pre-trained embedding (be it auto-encoders or word2vec) has been doing the same: representing items in a low-dimensional space that inherently encodes their similarities. Even when looking at the specific case of word/context embeddings, the method is not novel either: this method is almost identical to one of the similarity functions presented in ""A Simple Word Embedding Model for Lexical Substitution"" (Melamud et al., 2015). The novelty claim must be more accurate and position itself with respect to existing work.

In addition, I think the evaluation could be done better. There are plenty of benchmarks for word embeddings in context, for example: 
*

---

this paper proposes to use feed-forward neural networks to learn similarity preserving embeddings. They also use the proposed idea to represent out-of-vocabulary words using the words in given context. 

First, considering the related work [1,2] the proposed approach brings marginal novelty. Especially
Context Encoders is just a small improvement over word2vec. 

Experimental setup should provide more convincing results other than visualizations and non-standard benchmark for NER evaluation with word vectors [3].

[1]

---

This paper introduces a similarity encoder based on a standard feed-forward neural network with the aim of generating similarity-preserving embeddings. The approach is utilized to generate a simple extension of the CBOW word2vec model that transforms the learned embeddings by their average context vectors. Experiments are performed on an analogy task and named entity recognition.

While this paper offers some reasonable intuitive arguments for why a feed-forward neural network can generate good similarity-preserving embeddings, the architecture and approach is far from novel. As far as I can tell, the model is nothing more than the most vanilla neural network trained with SGD on similarity signals.

Slightly more original is the idea to use context embeddings to augment the expressive capacity of learned word representations. Of course, using explicit contextual information is not a new idea, especially for tasks like word sense disambiguation (see, e.g., 'Efficient Non-parametric Estimation of Multiple Embeddings per Word in Vector Space' by Neelakantan et al, which should also be cited), but the specific method used here is original, as far as I know.

The evaluation of the method is far from convincing. The corpora used to train the embeddings are far smaller than would ever be used in practice for unsupervised or semi-supervised embedding learning. The performance on the analogy task says little about the benefit of this method for larger corpora, and, as the authors mentioned in the comments, they expect ""the gain will be less significant, as the global context statistics brought in by the ConEc can also be picked up by word2vec with more training.""

The argument can be made (and the authors do claim) that extrinsic evaluations are more important for real-world applications, so it is good to see experiments on NER. However, again the embeddings were trained on a very small corpus and I am not convinced that the observed benefit will persist when trained on larger corpora.

Overall, I believe this paper offers little novelty and weak experimental evidence supporting its claims. I cannot recommend it for acceptance.",,,,,,2.6666666666666665,,,4.333333333333333,,
767,"AN ACTOR-CRITIC ALGORITHM FOR LEARNING RATE LEARNING
Authors: Chang Xu, Tao Qin
Source file: 767.pdf

ABSTRACT
Stochastic gradient descent (SGD), which updates the model parameters by adding a local gradient times a learning rate at each step, is widely used in model training of machine learning algorithms such as neural networks. It is observed that the models trained by SGD are sensitive to learning rates and good learning rates are problem specific. To avoid manually searching of learning rates, which is tedious and inefficient, we propose an algorithm to automatically learn learning rates using actor-critic methods from reinforcement learning (RL). In particular, we train a policy network called actor to decide the learning rate at each step during training, and a value network called critic to give feedback about quality of the decision (e.g., the goodness of the learning rate outputted by the actor) that the actor made. Experiments show that our method leads to good convergence of SGD and can prevent overfitting to a certain extent, resulting in better performance than human-designed competitors.

1 INTRODUCTION
While facing large scale of training data, stochastic learning such as stochastic gradient descent (SGD) is usually much faster than batch learning and often results in better models. An observation for SGD methods is that their performances are highly sensitive to the choice of learning rate LeCun et al. (2012). Clearly, setting a static learning rate for the whole training process is insufficient, since intuitively the learning rate should decrease when the model becomes more and more close to a (local) optimum as the training goes on over time Maclaurin et al. (2015). Although there are some empirical suggestions to guide how to adjust the learning rate over time in training, it is still a difficult task to find a good policy to adjust the learning rate, given that good policies are problem specific and depend on implementation details of a machine learning algorithm. One usually needs to try many times and adjust the learning rate manually to accumulate knowledge about the problem. However, human involvement often needs domain knowledge about the target problems, which is inefficient and difficult to scale up to different problems. Thus, a natural question arises: can we automatically adjust the learning rate? This is exactly the focus of this work and we aim to automatically learn the learning rates for SGD based machine learning (ML) algorithms without human-designed rules or hand-crafted features.
By examining the current practice of learning rate control/adjustment, we have two observations. First, learning rate control is a sequential decision process. At the beginning, we set an initial learning rate. Then at each step, we decide whether to change the learning rate and how to change it, based on the current model and loss, training data at hand, and maybe history of the training process. As suggested in Orr & Müller (2003), one well-principled method for estimating the ideal learning rate that is to decrease the learning rate when the weight vector oscillates, and increase it when the weight vector follows a relatively steady direction. Second, although at each step some immediate reward (e.g., the loss decrement) can be obtained by taking actions, we care more about the performance of the final model found by the ML algorithm. Consider two different learning rate
control policies: the first one leads to fast loss decrease at the beginning but gets saturated and stuck in a local minimum quickly, while the second one starts with slower loss decrease but results in much smaller final loss. Obviously, the second policy is better. That is, we prefer long-term rewards over short-term rewards.
Combining the two observations, it is easy to see that the problem of finding a good policy to control/adjust learning rate falls into the scope of reinforcement learning (RL) Sutton & Barto (1998), if one is familiar with RL. Inspired by the recent success of RL for sequential decision problems, in this work, we leverage RL techniques and try to learn the learning rate for SGD based methods.
We propose an algorithm to learn the learning rate within the actor-critic framework Sutton (1984); Sutton et al. (1999); Barto et al. (1983); Silver et al. (2014) from RL. In particular, an actor network is trained to take an action that decides the learning rate for current step, and a critic network is trained to give feedbacks to the actor network about long-term performance and help the actor network to adjust itself so as to perform better in the future steps. The main contributions of this paper include:
• We propose an actor-critic algorithm to automatically learn the learning rate for ML algorithms.
• Long-term rewards are exploited by the critic network in our algorithm to choose a better learning rate at each step.
• We propose to feed different training examples to the actor network and the critic network, which improve the generalization performance of the learnt ML model.
• A series of experiments validate the effectiveness of our proposed algorithm for learning rate control.

2 RELATED WORK

2.1 IMPROVED GRADIENT METHODS
Our focus is to improve gradient based ML algorithm through automatic learning of learning rate. Different approaches have been proposed to improve gradient methods, especially for deep neural networks.
Since SGD solely rely on a given example (or a mini-batch of examples) to compare gradient, its model update at each step tends to be unstable and it takes many steps to converge. To solve this problem, momentum SGD Jacobs (1988) is proposed to accelerate SGD by using recent gradients. RMSprop Tieleman & Hinton (2012) utilizes the magnitude of recent gradients to normalize the gradients. It always keeps a moving average over the root mean squared gradients, by which it divides the current gradient. Adagrad Duchi et al. (2011) adapts component-wise learning rates, and performs larger updates for infrequent and smaller updates for frequent parameters. Adadelta Zeiler (2012) extends Adagrad by reducing its aggressive, monotonically decreasing learning rate. Instead of accumulating all past squared gradients, Adadelta restricts the window of accumulated past gradients to some fixed size. Adam Kingma & Ba (2014) computes component-wise learning rates using the estimates of first and second moments of the gradients, which combines the advantages of AdaGrad and RMSProp.
Senior et al. (2013); Sutton (1992); Darken & Moody (1990) focus on predefining update rules to adjust learning rates during training. A limitation of these methods is that they have additional free parameters which need to be set manually. Another recent work Daniel et al. (2016) studies how to automatically select step sizes, but it still requires hand-tuned features. Schaul et al. (2013) proposes a method to choose good learning rate for SGD, which relies on the square norm of the expectation of the gradient, and the expectation of the square norm of the gradient. The method is much more constrained than ours and several assumption should be met.

2.2 REINFORCEMENT LEARNING
Since our proposed algorithm is based on RL techniques, here we give a very brief introduction to RL, which will ease the description of our algorithm in next section.
Reinforcement learning Sutton (1988) is concerned with how an agent acts in a stochastic environment by sequentially choosing actions over a sequence of time steps, in order to maximize a cumulative reward. In RL, a state st encodes the agents observation about the environment at a time step t, and a policy function π(st) determines how the agent behaves (e.g., which action to take) at state st. An action-value function (or, Q function) Qπ(st, at) is usually used to denote the cumulative reward of taking action at at state st and then following policy π afterwards.
Many RL algorithms have been proposed Sutton & Barto (1998); Watkins & Dayan (1992), and many RL algorithms Sutton (1984); Sutton et al. (1999); Barto et al. (1983); Silver et al. (2014) can be described under the actor-critic framework. An actor-critic algorithm learns the policy function and the value function simultaneously and interactively. The policy structure is known as the actor, and is used to select actions; the estimated value function is known as the critic, and it criticizes the actions made by the actor.
Recently, deep reinforcement learning, which uses deep neural networks to approximate/represent the policy function and/or the value function, have shown promise in various domains, including Atari games Mnih et al. (2015), Go Silver et al. (2016), machine translation Bahdanau et al. (2016), image recognition Xu et al. (2015), etc.

3 METHOD
In this section, we present an actor-critic algorithm that can automate the learning rate control for SGD based machine learning algorithms.
Many machine learning tasks need to train a model with parameters ω by minimizing a loss function f defined over a set X of training examples:
ω∗ = arg min ω fω(X). (1)
A standard approach for the loss function minimization is gradient descent, which sequentially updates the parameters using gradients step by step:
ωt+1 = ωt − at∇f t, (2) where at is the learning rate at step t, and ∇f t is the local gradient of f at ωt. Here one step can be the whole batch of all the training data, a mini batch of tens/hundreds of examples, or a random sample.
It is observed that the performance of SGD based methods is quite sensitive to the choice of at for non-convex loss function f . Unfortunately, f is usually non-convex with respect to the parameters
w in many ML algorithms, especially for deep neural networks. We aim to learn a learning rate controller using RL techniques that can automatically control at.
Figure 1 illustrates our automatic learning rate controller, which adopts the actor-critic framework in RL. The basic idea is that at each step, given the current model ωt and training sample x, an actor network is used to take an action (the learning rate at, and it will be used to update the model ωt), and a critic network is used to estimate the goodness of the action. The actor network will be updated using the estimated goodness of at, and the critic network will be updated by minimizing temporal difference (TD) Sutton & Barto (1990) error. We describe the details of our algorithm in the following subsections.

3.1 ACTOR NETWORK
The actor network, which is called policy network in RL, plays the key role in our algorithm: it determines the learning rate control policy for the primary ML algorithm1 based on the current model, training data, and maybe historical information during the training process.
Note that ωt could be of huge dimensions, e.g., one widely used image recognition model VGGNet Simonyan & Zisserman (2014) has more than 140 million parameters. If the actor network takes all of those parameters as the inputs, its computational complexity would dominate the complexity of the primary algorithm, which is unfordable. Therefore, we propose to use a function χ(·) to process and yield a compact vector st as the input of the actor network. Following the practice in RL, we call χ(·) the state function, which takes ωt and the training data x as inputs:
st = χ(ωt, X). (3)
Then the actor network πθ(·) parameterized by θ yields an action at:
πθ(s t) = at, (4)
where the action at ∈ R is a continuous value. When at is determined, we update the model of the primary algorithm by Equation 2.
Note that the actor network has its own parameters and we need to learn them to output a good action. To learn the actor network, we need to know how to evaluate the goodness of an actor network. The critic network exactly plays this role.

3.2 CRITIC NETWORK
Recall that our goal is to find a good policy for learning rate control to ensure that a good model can be learnt eventually by the primary ML algorithm. For this purpose, the actor network needs to output a good action at at state st so that finally a low training loss f(·) can be achieved. In RL, the Q function Qπ(s, a) is often used to denote the long term reward of the state-action pair s, a while following the policy π to take future actions. In our problem, Qπ(st, at) indicates the accumulative decrement of training loss starting from step t. We define the immediate reward at step t as the one step loss decrement:
rt = f t − f t+1. (5)
The accumulative value Rtπ of policy π at step t is the total discounted reward from step t:
Rtπ = Σ T k=tγ k−tr(sk, ak),
where γ ∈ (0, 1] is the discount factor. Considering that both the states and actions are uncountable in our problem, the critic network uses a parametric function Qϕ(s, a) with parameters ϕ to approximate the Q value function Qπ(s, a).
1Here we have two learning algorithms. We call the one with learning rate to adjust as the primary ML algorithm, and the other one which optimizes the learning rate of the primary one as the secondary ML algorithm.

3.3 TRAINING OF ACTOR AND CRITIC NETWORKS
The critic network has its own parameters ϕ, which is updated at each step using TD learning. More precisely, the critic is trained by minimizing the square error between the estimation Qϕ(st, at) and the target yt:
yt = rt + γQϕ(s t+1, at+1). (6)
The TD error is defined as:
δt = yt −Qϕ(st, at) = rt + γQϕ(s t+1, πθ(s t+1))−Qϕ(st, at)
(7)
The weight update rule follows the on-policy deterministic actor-critic algorithm. The gradients of critic network are:
∇ϕ = δt∇ϕQϕ(st, at), (8)
The policy parameters θ of the actor network is updated by ensuring that it can output the action with the largest Q value at state st, i.e., a∗ = arg maxaQϕ(st, a). Mathematically,
∇θ = ∇θπθ(st+1)∇aQϕ(st+1, at+1)|a=πθ(s). (9)
Algorithm 1 Actor-Critic Algorithm for Learning Rate Learning Require: Training steps T ; training set X; loss function f ; state function χ; discount factor: γ ; Ensure: Model parameters w, policy parameters θ of the actor network, and value parameters ϕ of
the critic network; 1: Initial parameters ω0, θ0, ϕ0; 2: for t = 0, ..., T do 3: Sample xi ∈ X, i ∈ 1, ..., N . 4: Extract state vector: sti = χ(ω
t, xi). 5: //Actor network selects an action. 6: Computes learning rate ati = πθ(s t i). 7: //Update model parameters ω. 8: Compute∇f t(xi). 9: Update ω: ωt+1 = ωt − ati∇f t(xi).
10: //Update critic network by minimizing square error between estimation and label. 11: rt = f t(xi)− f t+1(xi) 12: Extract state vector: st+1i = χ(ω t+1, xi) 13: Compute Qϕ(st+1i , πθ(s t+1 i )), Qϕ(s t i, a t i) 14: Compute δt according to Equation 7: δt = rt + γQϕ(s t+1 i , πθ(s t+1 i ))−Qϕ(sti, ati) 15: Update ϕ using the following gradients according to Equation 8 : ∇ϕ = δt∇ϕQϕ(sti, ati) 16: // Update actor network 17: Sample xj ∈ X, j ∈ 1, ..., N, j 6= i. 18: Extract state vector: st+1j = χ(ω
t+1, xj). 19: Compute at+1j = πθ(s t+1 j ). 20: Update θ from Equation 9: ∇θ = ∇θπθ(st+1j )∇aQϕ(s t+1 j , a t+1 j )|a=πθ(s) 21: end for 22: return ω, θ, ϕ;

3.4 THE ALGORITHM
The overall algorithm is shown in Algorithm 1. In each step, we sample an example (Line 3), extract the current state vector (Line 4), compute the learning rate using the actor network (Line 6), update the model (Lines 8-9), compute TD error (Lines 11-14), update the critic network (Line 15), and sample another example (Line 17) to update the actor network (Line 18-20). We would like to make some discussions about the algorithm.
First, in the current algorithm, for simplicity, we consider using only one example for model update. It is easy to generalize to a mini batch of random examples.
Second, one may notice that we use one example (e.g., xi) for model and the critic network update, but a different example (e.g., xj) for the actor network update. Doing so we can avoid that the algorithm will overfit on some (too) hard examples and can improve the generalization performance of the algorithm on the test set. Consider a hard example2 in a classification task. Since such an example is difficult to be classified correctly, intuitively its gradient will be large and the learning rate given by the actor network at this step will also be large. In other words, this hard example will greatly change the model, while itself is not a good representative of its category and the learning algorithm should not pay much attention to it. If we feed the same example to both the actor network and the critic network, both of them will encourage the model to change a lot to fit the example, consequently resulting in oscillation of the training, as shown in our experiments. By feeding different examples to the actor and critic networks, it is very likely the critic network will find that the gradient direction of the example fed into the actor network is inconsistent with its own training example and thus criticize the large learning rate suggested by the actor network. More precisely, the update of ω is based on xi and the learning rate suggested by the actor network, while the training target of the actor network is to maximize the output of the critic network on xj . If there is big gradient disagreement between xi and xj , the update of ω, which is affected by actor’s decision, would cause the critic’s output on xj to be small. To compensate this effect, the actor network is forced to predict a small learning rate for a too hard xi in this situation.

4 EXPERIMENTS
We conducted a set of experiments to test the performance of our learning rate learning algorithm and compared with several baseline methods. We report the experimental results in this section.

4.1 EXPERIMENTAL SETUP
We tested our method on two widely used image classification datasets: MNIST LeCun et al. (1998) and CIFAR-10 Krizhevsky & Hinton (2009). Convolutional neural networks (CNNs) are the standard model for image classification tasks in recent years, and thus the primary ML algorithm adopted the CNN model in all our experiments.
We specified our actor-critic algorithm in experiments as follows. Given that stochastic mini-batch training is a common practice in deep learning, the actor-critic algorithm also operated on minibatches, i.e., each step is a mini batch in our experiments. We defined the state st = χ(ωt, Xi) as the average loss of learning model ωt on the input min-batch Xi. We specified the actor network as a two-layer long short-term memory (LSTM) network with 20 units in each layer, considering that a good learning rate for step t depends on and correlates with the learning rates at previous steps while LSTM is well suited to model sequences with long-distance dependence. We used the absolute value activation function for the output layer of the LSTM to ensure a positive learning rate. The LSTM was unrolled for 20 steps during training. We specified the critic network as a simple neural network with one hidden layer and 10 hidden units. We use Adam with the default setting in TensorFlow optimizer toolbox Abadi et al. (2015) to train the actor and critic networks in all the experiments.
We compared our method with several mainstream SGD algorithms, including SGD, Adam Kingma & Ba (2014), Adagrad Duchi et al. (2011) and RMSprop Tieleman & Hinton (2012). For each of these algorithms and each dataset, we tried the following learning rates 10−4, 10−3, ..., 100. We report the best performance of these algorithms over those learning rates. If an algorithm needs some other parameters to set, such as decay coefficients for Adam, we used the default setting in TensorFlow optimizer toolbox. For each benchmark and our proposed method, five independent runs are averaged and reported in all of the following experiments.

4.2 RESULTS ON MNIST
MNIST is a dataset for handwritten digit classification task. Each example in the dataset is a 28×28 black and white image containing a digit in {0, 1, · · · , 9}. The CNN model used in the primary
2For example, an example may has an incorrect label because of the limited quality of labelers.
ML algorithm is consist of two convolutional layers, each followed by a pooling layer, and finally a fully connected layer. The first convolutional layer filters each input image using 32 kernels of size 5 × 5. The max-pooling layer following the first convolutional layer is performed over 2 × 2 pixel windows, with stride 2. The second convolutional layer takes the outputs of the first max-pooling layer as inputs and filters them with 64 kernels of size 5 × 5. The max-pooling layer following the second convolutional layer is performed over 2 × 2 pixel windows, with stride 2. The outputs of second max pooling layer are fed to a fully connected layer with 512 neurons. Dropout was conducted on the fully connect layer with a dropout rate of 0.5. ReLU activation functions are used in the CNN model. There are 60,000 training images and 10,000 test images in this dataset. We scaled the pixel values to the [0,1] range before inputting to all the algorithms. Each mini batch contains 50 randomly sampled images.
Figure 2 shows the results of our actor-critic algorithm for learning rate learning and the baseline methods, including the curves of training loss, test loss, and test accuracy. The final accuracies of these methods are summarized in Table 1. We have the following observations.
• In terms of training loss, our algorithm has similar convergence speed to the baseline methods. One may expect that our algorithm should have significantly faster convergence speed considering that our algorithm learns both the learning rate and the CNN model while the baselines only learn the CNN model and choose the learning rates per some predefined rules. However, this is not correct. As discussed in Section 3.4, we carefully design the algorithm and feed different samples to the actor network and critic network. Doing so we can focus more on generalization performance than training loss: as shown in Figure 4, our algorithm achieves the best test accuracy.
10-2 10-1 100 101
Epoch
0.010
0.015
0.020
0.025
0.030
0.035
0.040
0.045
0.050
0.055
Le a rn
in g r
a te
CIFAR-10
Figure 5: The learning rate learned by actor network for CIFAR-10.
• Our algorithm achieves the lowest error rate on MNIST. Although the improvement looks small, we would like to point out that given that the accuracy of CNN is already close to 100%, it is a very difficult task to further improve accuracy, not to mention that we only changed learning rate policy without changing the CNN model.

4.3 RESULTS ON CIFAR-10
CIFAR-10 is a dataset consisting of 60000 natural 32 × 32 RGB images in 10 classes: 50,000 imagesfor training and 10,000 for test. We used a CNN with 2 convolutional layers (each followed by max-pooling layer) and 2 fully connected layers for this task. There is a max pooling layer which performed over 2× 2 pixel windows, with stride 2 after each convolutional layer. All convolutional layers filter the input with 64 kernels of size 5× 5. The outputs of the second pooling layer are fed to a fully connected layer with 384 neurons. The last fully connected layer has 192 neurons. Before inputting an image to the CNN, we subtracted the per-pixel mean computed over the training set from each image.
Figure 3 shows the results of all the algorithms on CIFAR-10, including the curves of training loss, the test loss and test accuracy. Table 2 shows the final test accuracy. We get similar observations as MNIST: our algorithm achieves similar convergence speed in terms of training loss and slightly better test accuracy than baselines. Figure 5 shows the learning rate learned by our method on CIFAR-10. To further understand the generalization performance of our algorithm, we ran all the
algorithms on two subsets of training data on CIFAR-10: one with only 20% training data The curves of training loss and test loss are shown in Figure 4. As can be seen from the figure, those baseline methods are easy to overfit and their test loss increases after 5000 steps (mini batches). In contrast, our algorithm is relatively robust and can prevent overfitting to some extent.
As we explained in Section 3.4, feeding different examples to the actor and critic networks is important to guarantee generalization ability. Here we conducted another experiment to verify our intuitive explanation. Figure 6 shows the results of two different implementations of our actor-critic algorithm on CIFAR-10. In the first implementation, we fed the sample examples to the two net-
works, i.e., xi = xj in the algorithm, and in the second implementation, the input xj of the critic network is different from the input xi of the actor network. It is easy to see from the figure that setting xi = xj tends to oscillate during training and leads to poor test performance. Thus, we need to feed different training data to the actor network and the critic network to ensure the performance of the algorithm.

4.4 COMPARISON WITH OTHER ADAPTIVE LEARNING RATE METHOD
We also compare our method with “vSGD” from previous by work Schaul et al. (2013), which can automatically adjust learning rates to minimize the expected error. This method tries to compute learning rate at each update by optimizing the expected loss after the next update according to the square norm of the expectation of the gradient, and the expectation of the square norm of the gradient. Note that our method learns to predict a learning rate at each time step by utilizing the long term reward predicted by a critic network.
For a fair comparison, we followed the experiments settings of Schaul et al. (2013), which designed three different network architectures for MNIST task to measure the performance. The first one is denoted by ‘M0’ which is simple softmax regression (i.e. a network with no hidden layer). The second one (‘M1’) is a fully connected multi-layer perceptron, with a single hidden layer. The third one (denoted ‘M2’) is a deep, fully connected multi-layer perceptron with two hidden layers. The vSGD has three variants in their paper. We referred to the results reported in their paper and compared our method with all of three variants of their algorithm (vSGD-l, vSGD-b, vSGD-g). The learning rates of SGD are decreased according to a human designed schedule, and the hyperparameters of SGD, ADAM, Adagrad, RMSprop are carefully determined by their lowest test error among a set of hyper-parameters. All hyper-parameters can be found in Schaul et al. (2013).
The experimental results are reported in Table 3. It shows that our proposed method performs better than vSGD and other baseline methods, and is stable across different network architectures.

5 CONCLUSIONS AND FUTURE WORK
In this work, we have studied how to automatically learn learning rates for gradient based machine learning methods and proposed an actor-critic algorithm, inspired by the recent success of reinforcement learning. The experiments on two image classification datasets have shown that our method (1) has comparable convergence speed with expert-designed optimizer while achieving better test accuracy, and (2) can successfully adjust learning rate for different datasets and CNN model structures.
For the future work, we will explore the following directions. In this work, we have applied our algorithm to control the learning rates of SGD. We will apply to other variants of SGD methods. We have focused on learning a learning rate for all the model parameters. We will study how to learn an individual learning rate for each parameter. We have considered learning learning rates using RL techniques. We will consider learning other hyperparameters such as step-dependent dropout rates for deep neural networks.

A APPENDIX
A method of automatically controlling learning rate is proposed in the main body of the paper. The learning rate controller adjusts itself during training to control the learning rate. Here, we propose an improved version that can leverage experiences from several repeated training runs to learn a fixed learning rate controller. Empirically, this algorithm can achieve better performance than the previous one. Given that it requires more time for training the learning rate controller, this method is more suitable for training offline models.
In this algorithm, during every training run, we fix the actor network and compute the weighted sum of the gradients of its parameter θ. The parameter is updated after each run (modified from Equation 9):
∇θ = ΣTt=1h(t)∇θπθ(st+1)∇aQϕ(st+1, at+1)|a=πθ(s). (10)
h(t) is weighted function which is used to amplify the feedback signal from the initial training stage. It is defined as h(t) = 1/t in our experiments. An error rate of 0.48% was achieved with 5 repeated training runs in MNIST experiment (the same setting as Table 1), and in CIFAR-10 experiment (the same setting as Table 2), 80.23% accuracy was achieved with 10 training runs. This method showed better performance in both experiments.
","Stochastic gradient descent (SGD), which updates the model parameters by adding a local gradient times a learning rate at each step, is widely used in model training of machine learning algorithms such as neural networks. It is observed that the models trained by SGD are sensitive to learning rates and good learning rates are problem specific. To avoid manually searching of learning rates, which is tedious and inefficient, we propose an algorithm to automatically learn learning rates using actor-critic methods from reinforcement learning (RL). In particular, we train a policy network called actor to decide the learning rate at each step during training, and a value network called critic to give feedback about quality of the decision (e.g., the goodness of the learning rate outputted by the actor) that the actor made. Experiments show that our method leads to good convergence of SGD and can prevent overfitting to a certain extent, resulting in better performance than human-designed competitors.",ICLR 2017 conference submission,False,,"The paper proposes using an actor-critic RL algorithm for training learning rate controllers for supervised learning. The proposed method outperforms standard optimizers like SGD, ADAM and RMSprop in experiments conducted on MNIST and CIFAR 10.

I have two main concerns. One is the lack of comparisons to similar recently proposed methods - ""Learning Step Size Controllers for Robust Neural Network Training"" by Daniel et al. and ""Learning to learn by gradient descent by gradient descent"" by Andrychowicz et al. The work of Daniel et al. is quite similar because it also proposes using a policy search RL method (REPS) and it is not clear what the downsides of their approach are. Their work does use more prior knowledge as the authors stated, but why is this a bad thing?

My second concern is with the experiments. Some of the numbers reported for the other methods are surprisingly low. For example, why is RMSprop so bad in Table 2 and Table 3? These results suggest that the methods are not being tuned properly, which reinforces the need for comparisons on standard architectures with previously reported results. For example, if the baselines used a better architecture like a ResNet or, for simplicty, Network in Network from this list:

---

The authors use actor-critic reinforcement learning to adjust the step size of a supervised learning algorithm. There are no comparisons made to other, similar approaches, and the baselines are suspiciously weak, making the proposed method difficult to justify.

---

The authors present a method for adaptively setting the step size for SGD by treating the learning rate as an action in an MDP whose reward is the change in loss function. The method is presented against popular adaptive first-order methods for training deep networks (Adagrad, Adam, RMSProp, etc). The results are interesting but difficult to assess in a true apples-to-apples manner. Some specific comments:

-What is the computational overhead of the actor-critic algorithm relative to other algorithms? No plots with the wall-time of optimization are presented, even though the success of methods like Adagrad was due to their wall-time performance, not the number of iterations.
-Why was only a single learning rate learned? To accurately compare against other popular first order methods, why not train a separate RL model for each parameter, similar to how popular first-order methods adaptively change the learning rate for each parameter.
-Since learning is a non-stationary process, while RL algorithms assume a stationary environment, why should we expect an RL algorithm to work for learning a learning rate?
-In figure 6, how does the proposed method compare to something like early stopping? It may be that the actor-critic method is overfitting less simply because it is worse at optimization.

---

The paper proposes using an actor-critic RL algorithm for training learning rate controllers for supervised learning. The proposed method outperforms standard optimizers like SGD, ADAM and RMSprop in experiments conducted on MNIST and CIFAR 10.

I have two main concerns. One is the lack of comparisons to similar recently proposed methods - ""Learning Step Size Controllers for Robust Neural Network Training"" by Daniel et al. and ""Learning to learn by gradient descent by gradient descent"" by Andrychowicz et al. The work of Daniel et al. is quite similar because it also proposes using a policy search RL method (REPS) and it is not clear what the downsides of their approach are. Their work does use more prior knowledge as the authors stated, but why is this a bad thing?

My second concern is with the experiments. Some of the numbers reported for the other methods are surprisingly low. For example, why is RMSprop so bad in Table 2 and Table 3? These results suggest that the methods are not being tuned properly, which reinforces the need for comparisons on standard architectures with previously reported results. For example, if the baselines used a better architecture like a ResNet or, for simplicty, Network in Network from this list:

---

In the question response the authors mention and compare other works such as ""Learning to Learn by Gradient Descent by Gradient Descent"", but the goal of current work and that work is quite different. That work is a new form of optimization algorithm which is not the case here. And bayesian hyper-parameter optimization methods aim for multiple hyper-parameters but this work only tune one hyper-parameter.
The network architecture used for the experiments on CIFAR-10 is quite outdated and the performances are much poorer than any work that has published in last few years. So the comparison are not valid here, as if the paper claim the advantage of their method, they should use the state of the art network architecture and see if their claim still holds in that setting too.
As discussed before, the extra cost of hyper-parameter optimizers are only justified if the method could push the SOTA results in multiple modern datasets.
In summary, the general idea of having an actor-critic network as a meta-learner is an interesting idea. But the particular application proposed here does not seems to have any practical value and the reported results are very limited and it's hard to draw any conclusion about the effectiveness of the method.

---

The paper proposes using an actor-critic RL algorithm for training learning rate controllers for supervised learning. The proposed method outperforms standard optimizers like SGD, ADAM and RMSprop in experiments conducted on MNIST and CIFAR 10.

I have two main concerns. One is the lack of comparisons to similar recently proposed methods - ""Learning Step Size Controllers for Robust Neural Network Training"" by Daniel et al. and ""Learning to learn by gradient descent by gradient descent"" by Andrychowicz et al. The work of Daniel et al. is quite similar because it also proposes using a policy search RL method (REPS) and it is not clear what the downsides of their approach are. Their work does use more prior knowledge as the authors stated, but why is this a bad thing?

My second concern is with the experiments. Some of the numbers reported for the other methods are surprisingly low. For example, why is RMSprop so bad in Table 2 and Table 3? These results suggest that the methods are not being tuned properly, which reinforces the need for comparisons on standard architectures with previously reported results. For example, if the baselines used a better architecture like a ResNet or, for simplicty, Network in Network from this list:

---

The authors use actor-critic reinforcement learning to adjust the step size of a supervised learning algorithm. There are no comparisons made to other, similar approaches, and the baselines are suspiciously weak, making the proposed method difficult to justify.

---

The authors present a method for adaptively setting the step size for SGD by treating the learning rate as an action in an MDP whose reward is the change in loss function. The method is presented against popular adaptive first-order methods for training deep networks (Adagrad, Adam, RMSProp, etc). The results are interesting but difficult to assess in a true apples-to-apples manner. Some specific comments:

-What is the computational overhead of the actor-critic algorithm relative to other algorithms? No plots with the wall-time of optimization are presented, even though the success of methods like Adagrad was due to their wall-time performance, not the number of iterations.
-Why was only a single learning rate learned? To accurately compare against other popular first order methods, why not train a separate RL model for each parameter, similar to how popular first-order methods adaptively change the learning rate for each parameter.
-Since learning is a non-stationary process, while RL algorithms assume a stationary environment, why should we expect an RL algorithm to work for learning a learning rate?
-In figure 6, how does the proposed method compare to something like early stopping? It may be that the actor-critic method is overfitting less simply because it is worse at optimization.

---

The paper proposes using an actor-critic RL algorithm for training learning rate controllers for supervised learning. The proposed method outperforms standard optimizers like SGD, ADAM and RMSprop in experiments conducted on MNIST and CIFAR 10.

I have two main concerns. One is the lack of comparisons to similar recently proposed methods - ""Learning Step Size Controllers for Robust Neural Network Training"" by Daniel et al. and ""Learning to learn by gradient descent by gradient descent"" by Andrychowicz et al. The work of Daniel et al. is quite similar because it also proposes using a policy search RL method (REPS) and it is not clear what the downsides of their approach are. Their work does use more prior knowledge as the authors stated, but why is this a bad thing?

My second concern is with the experiments. Some of the numbers reported for the other methods are surprisingly low. For example, why is RMSprop so bad in Table 2 and Table 3? These results suggest that the methods are not being tuned properly, which reinforces the need for comparisons on standard architectures with previously reported results. For example, if the baselines used a better architecture like a ResNet or, for simplicty, Network in Network from this list:

---

In the question response the authors mention and compare other works such as ""Learning to Learn by Gradient Descent by Gradient Descent"", but the goal of current work and that work is quite different. That work is a new form of optimization algorithm which is not the case here. And bayesian hyper-parameter optimization methods aim for multiple hyper-parameters but this work only tune one hyper-parameter.
The network architecture used for the experiments on CIFAR-10 is quite outdated and the performances are much poorer than any work that has published in last few years. So the comparison are not valid here, as if the paper claim the advantage of their method, they should use the state of the art network architecture and see if their claim still holds in that setting too.
As discussed before, the extra cost of hyper-parameter optimizers are only justified if the method could push the SOTA results in multiple modern datasets.
In summary, the general idea of having an actor-critic network as a meta-learner is an interesting idea. But the particular application proposed here does not seems to have any practical value and the reported results are very limited and it's hard to draw any conclusion about the effectiveness of the method.",,,,,,4.0,,,4.333333333333333,,
773,"RECTIFIED FACTOR NETWORKS FOR BICLUSTERING
Authors: Djork-Arné Clevert, Thomas Unterthiner
Source file: 773.pdf

ABSTRACT
Biclustering is evolving into one of the major tools for analyzing large datasets given as matrix of samples times features. Biclustering has several noteworthy applications and has been successfully applied in life sciences and e-commerce for drug design and recommender systems, respectively. FABIA is one of the most successful biclustering methods and is used by companies like Bayer, Janssen, or Zalando. FABIA is a generative model that represents each bicluster by two sparse membership vectors: one for the samples and one for the features. However, FABIA is restricted to about 20 code units because of the high computational complexity of computing the posterior. Furthermore, code units are sometimes insufficiently decorrelated. Sample membership is difficult to determine because vectors do not have exact zero entries and can have both large positive and large negative values. We propose to use the recently introduced unsupervised Deep Learning approach Rectified Factor Networks (RFNs) to overcome the drawbacks of existing biclustering methods. RFNs efficiently construct very sparse, non-linear, highdimensional representations of the input via their posterior means. RFN learning is a generalized alternating minimization algorithm based on the posterior regularization method which enforces non-negative and normalized posterior means. Each code unit represents a bicluster, where samples for which the code unit is active belong to the bicluster and features that have activating weights to the code unit belong to the bicluster. On 400 benchmark datasets with artificially implanted biclusters, RFN significantly outperformed 13 other biclustering competitors including FABIA. In biclustering experiments on three gene expression datasets with known clusters that were determined by separate measurements, RFN biclustering was two times significantly better than the other 13 methods and once on second place. On data of the 1000 Genomes Project, RFN could identify DNA segments which indicate, that interbreeding with other hominins starting already before ancestors of modern humans left Africa.

1 INTRODUCTION
Biclustering is widely-used in statistics (A. Kasim & Talloen, 2016), and recently it also became popular in the machine learning community (O´ Connor & Feizi, 2014; Lee et al., 2015; Kolar et al., 2011), e.g., for analyzing large dyadic data given in matrix form, where one dimension are the samples and the other the features. A matrix entry is a feature value for the according sample. A bicluster is a pair of a sample set and a feature set for which the samples are similar to each other on the features and vice versa. Biclustering simultaneously clusters rows and columns of a matrix. In particular, it clusters row elements that are similar to each other on a subset of column elements. In contrast to standard clustering, the samples of a bicluster are only similar to each other on a subset of features. Furthermore, a sample may belong to different biclusters or to no bicluster at all. Thus, biclusters can overlap in both dimensions. For example, in drug design biclusters are compounds which activate the same gene module and thereby indicate a side effect. In this example different chemical compounds are added to a cell line and the gene expression is measured (Verbist et al., 2015). If multiple pathways are active in a sample, it belongs to different biclusters and may
have different side effects. In e-commerce often matrices of costumers times products are available, where an entry indicates whether a customer bought the product or not. Biclusters are costumers which buy the same subset of products. In a collaboration with the internet retailer Zalando the biclusters revealed outfits which were created by customers which selected certain clothes for a particular outfit.
FABIA (factor analysis for bicluster acquisition, (Hochreiter et al., 2010)) evolved into one of the most successful biclustering methods. A detailed comparison has shown FABIA’s superiority over existing biclustering methods both on simulated data and real-world gene expression data (Hochreiter et al., 2010). In particular FABIA outperformed non-negative matrix factorization with sparseness constraints and state-of-the-art biclustering methods. It has been applied to genomics, where it identified in gene expression data task-relevant biological modules (Xiong et al., 2014). In the large drug design project QSTAR, FABIA was used to extract biclusters from a data matrix that contains bioactivity measurements across compounds (Verbist et al., 2015). Due to its successes, FABIA has become part of the standard microarray data processing pipeline at the pharmaceutical company Janssen Pharmaceuticals. FABIA has been applied to genetics, where it has been used to identify DNA regions that are identical by descent in different individuals. These individuals inherited an IBD region from a common ancestor (Hochreiter, 2013; Povysil & Hochreiter, 2014). FABIA is a generative model that enforces sparse codes (Hochreiter et al., 2010) and, thereby, detects biclusters. Sparseness of code units and parameters is essential for FABIA to find biclusters, since only few samples and few features belong to a bicluster. Each FABIA bicluster is represented by two membership vectors: one for the samples and one for the features. These membership vectors are both sparse since only few samples and only few features belong to the bicluster.
However, FABIA has shortcomings, too. A disadvantage of FABIA is that it is only feasible with about 20 code units (the biclusters) because of the high computational complexity which depends cubically on the number of biclusters, i.e. the code units. If less code units were used, only the large and common input structures would be detected, thereby, occluding the small and rare ones. Another shortcoming of FABIA is that units are insufficiently decorrelated and, therefore, multiple units may encode the same event or part of it. A third shortcoming of FABIA is that the membership vectors do not have exact zero entries, that is the membership is continuous and a threshold have to be determined. This threshold is difficult to adjust. A forth shortcoming is that biclusters can have large positive but also large negative members of samples (that is positive or negative code values). In this case it is not clear whether the positive pattern or the negative pattern has been recognized.
Rectified Factor Networks (RFNs; (Clevert et al., 2015)) RFNs overcome the shortcomings of FABIA. The first shortcoming of only few code units is avoided by extending FABIA to thousands of code units. RFNs introduce rectified units to FABIA’s posterior distribution and, thereby, allow for fast computations on GPUs. They are the first methods which apply rectification to the posterior distribution of factor analysis and matrix factorization, though rectification it is well established in Deep Learning by rectified linear units (ReLUs). RFNs transfer the methods for rectification from the neural network field to latent variable models. Addressing the second shortcoming of FABIA, RFNs achieve decorrelation by increasing the sparsity of the code units using dropout from field of Deep Learning. RFNs also address the third FABIA shortcoming, since the rectified posterior means yield exact zero values. Therefore, memberships to biclusters are readily obtained by values that are not zero. Since RFNs only have non-negative code units, the problem of separating the negative from the positive pattern disappears.

2 IDENTIFYING BICLUSTERS BY RECTIFIED FACTOR NETWORKS

2.1 RECTIFIED FACTOR NETWORKS
We propose to use the recently introduced Rectified Factor Networks (RFNs; (Clevert et al., 2015)) for biclustering to overcome the drawbacks of the FABIA model. The factor analysis model and the construction of a bicluster matrix are depicted in Fig. 1. RFNs efficiently construct very sparse, nonlinear, high-dimensional representations of the input. RFN models identify rare and small events in the input, have a low interference between code units, have a small reconstruction error, and explain the data covariance structure.
RFN learning is a generalized alternating minimization algorithm derived from the posterior regularization method which enforces non-negative and normalized posterior means. These posterior means are the code of the input data. The RFN code can be computed very efficiently. For nonGaussian priors, the computation of the posterior mean of a new input requires either to numerically solve an integral or to iteratively update variational parameters. In contrast, for Gaussian priors the posterior mean is the product between the input and a matrix that is independent of the input. RFNs use a rectified Gaussian posterior, therefore, they have the speed of Gaussian posteriors but lead to sparse codes via rectification. RFNs are implemented on GPUs.
The RFN model is a factor analysis model
v = Wh + ✏ , (1)
which extracts the covariance structure of the data. The prior h ⇠ N (0, I) of the hidden units (factors) h 2 Rl and the noise ✏ ⇠ N (0, ) of visible units (observations) v 2 Rm are independent. The model parameters are the weight (factor loading) matrix W 2 Rm⇥l and the noise covariance matrix 2 Rm⇥m. RFN models are selected via the posterior regularization method (Ganchev et al., 2010). For data {v} = {v1, . . . ,vn}, it maximizes the objective F :
F = 1 n
nX
i=1
log p(vi) 1
n
nX
i=1
DKL(Q(hi | vi) k p(hi | vi)), (2)
where DKL is the Kullback-Leibler distance. Maximizing F achieves two goals simultaneously: (1) extracting desired structures and information from the data as imposed by the generative model and (2) ensuring sparse codes via Q from the set of rectified Gaussians.
For Gaussian posterior distributions, and mean-centered data {v} = {v1, . . . ,vn}, the posterior p(hi | vi) is Gaussian with mean vector (µp)i and covariance matrix ⌃p:
(µp)i = I + W T 1W 1 W T 1 vi , ⌃p = I + W T 1W 1 . (3)
For rectified Gaussian posterior distributions, ⌃p remains as in the Gaussian case, but minimizing the second DKL of Eq. (2) leads to constrained optimization problem (see Clevert et al. (2015))
min
µi
1
n
nX
i=1
(µi (µp)i)T ⌃ 1p (µi (µp)i)
s.t. 8i : µi 0 , 8j : 1
n
nX
i=1
µ2ij = 1 , (4)
where “ ” is component-wise. In the E-step of the generalized alternating minimization algorithm (Ganchev et al., 2010), which is used for RFN model selection, we only perform a step of the gradient projection algorithm (Bertsekas, 1976; Kelley, 1999), in particular a step of the projected Newton method for solving Eq. (4) (Clevert et al., 2015). Therefore, RFN model selection is extremely efficient but still guarantees the correct solution.

2.2 RFN BICLUSTERING
For a RFN model, each code unit represents a bicluster, where samples, for which the code unit is active, belong to the bicluster. On the other hand features that activates the code unit belong to the bicluster, too. The vector of activations of a unit across all samples is the sample membership vector. The weight vector which activates the unit is the feature membership vector. The un-constraint posterior mean vector is computed by multiplying the input with a matrix according to Eq. (3). The constraint posterior of a code unit is obtained by multiplying the input by a vector and subsequently rectifying and normalizing the code unit (Clevert et al., 2015).
To keep feature membership vector sparse, we introduce a Laplace prior on the parameters. Therefore only few features contribute to activating a code unit, that is, only few features belong to a bicluster. Sparse weights Wi are achieved by a component-wise independent Laplace prior for the weights:
p(Wi) = ⇣
1p 2
⌘n nY
k=1
e p 2 |Wki| (5)
The weight update for RFN (Laplace prior on the weights) is
W = W + ⌘ U S 1 W
↵ sign(W ) . (6)
Whereby the sparseness of the weight matrix can be controlled by the hyper-parameter ↵ and U and S are defined as U = 1n Pn i=1 viµ T i and S = 1 n Pn i=1 µiµ T i +⌃, respectively. In order to enforce more sparseness of the sample membership vectors, we introduce dropout of code units. Dropout means that during training some code units are set to zero at the same time as they get rectified. Dropout avoids co-adaptation of code units and reduces correlation of code units — a problem of FABIA which is solved.
RFN biclustering does not require a threshold for determining sample memberships to a bicluster since rectification sets code units to zero. Further crosstalk between biclusters via mixing up negative and positive memberships is avoided, therefore spurious biclusters do less often appear.

3 EXPERIMENTS
In this section, we will present numerical results on multiple synthetic and real data sets to verify the performance of our RFN biclustering algorithm, and compare it with various other biclustering methods.

3.1 METHODS COMPARED
To assess the performance of rectified factor networks (RFNs) as unsupervised biclustering methods, we compare the following 14 biclustering methods:
(1) RFN: rectified factor networks (Clevert et al., 2015), (2) FABIA: factor analysis with Laplace prior on the hidden units (Hochreiter et al., 2010; Hochreiter, 2013), (3) FABIAS: factor analysis with sparseness projection (Hochreiter et al., 2010), (4) MFSC: matrix factorization with sparseness constraints (Hoyer, 2004), (5) plaid: plaid model (Lazzeroni & Owen, 2002; T. Chekouo & Raffelsberger, 2015), (6) ISA: iterative signature algorithm (Ihmels et al., 2004), (7) OPSM: orderpreserving sub-matrices (Ben-Dor et al., 2003), (8) SAMBA: statistical-algorithmic method for bicluster analysis (Tanay et al., 2002), (9) xMOTIF: conserved motifs (Murali & Kasif, 2003), (10) Bimax: divide-and-conquer algorithm (Prelic et al., 2006), (11) CC: Cheng-Church -biclusters
(Cheng & Church, 2000), (12) plaid t: improved plaid model (Turner et al., 2003), (13) FLOC: flexible overlapped biclustering, a generalization of CC (Yang et al., 2005), and (14) spec: spectral biclustering (Kluger et al., 2003).
For a fair comparison, the parameters of the methods were optimized on auxiliary toy data sets. If more than one setting was close to the optimum, all near optimal parameter settings were tested. In the following, these variants are denoted as method variant (e.g. plaid ss). For RFN we used the following parameter setting: 13 hidden units, a dropout rate of 0.1, 500 iterations with a learning rate of 0.1, and set the parameter ↵ (controlling the sparseness on the weights) to 0.01.

3.2 SIMULATED DATA SETS WITH KNOWN BICLUSTERS
In the following subsections, we describe the data generation process and results for synthetically generated data according to either a multiplicative or additive model structure.

3.2.1 DATA WITH MULTIPLICATIVE BICLUSTERS
We assumed n = 1000 genes and l = 100 samples and implanted p = 10 multiplicative biclusters. The bicluster datasets with p biclusters are generated by following model:
X = pX
i=1
i z T i + ⌥ , (7)
where ⌥ 2 Rn⇥l is additive noise; i 2 Rn and zi 2 Rl are the bicluster membership vectors for the i-th bicluster. The i’s are generated by (i) randomly choosing the number N i of genes in bicluster i from {10, . . . , 210}, (ii) choosing N i genes randomly from {1, . . . , 1000}, (iii) setting i components not in bicluster i to N (0, 0.22) random values, and (iv) setting i components that are in bicluster i to N (±3, 1) random values, where the sign is chosen randomly for each gene. The zi’s are generated by (i) randomly choosing the number Nzi of samples in bicluster i from {5, . . . , 25}, (ii) choosing Nzi samples randomly from {1, . . . , 100}, (iii) setting zi components not in bicluster i to N (0, 0.22) random values, and (iv) setting zi components that are in bicluster i to N (2, 1) random values. Finally, we draw the ⌥ entries (additive noise on all entries) according to N (0, 32) and compute the data X according to Eq. (7). Using these settings, noisy biclusters of random sizes between 10⇥5 and 210⇥25 (genes⇥samples) are generated. In all experiments, rows (genes) were standardized to mean 0 and variance 1.

3.2.2 DATA WITH ADDITIVE BICLUSTERS
In this experiment we generated biclustering data where biclusters stem from an additive two-way ANOVA model:
X = pX
i=1
✓i ( i zTi ) + ⌥ , ✓ikj = µi + ↵ik + ij , (8)
where is the element-wise product of matrices and both i and zi are binary indicator vectors which indicate the rows and columns belonging to bicluster i. The i-th bicluster is described by an ANOVA model with mean µi, k-th row effect ↵ik (first factor of the ANOVA model), and jth column effect ij (second factor of the ANOVA model). The ANOVA model does not have interaction effects. While the ANOVA model is described for the whole data matrix, only the effects on rows and columns belonging to the bicluster are used in data generation. Noise and bicluster sizes are generated as in previous Subsection 3.2.1.
Data was generated for three different signal-to-noise ratios which are determined by distribution from which µi is chosen: A1 (low signal) N (0, 22), A2 (moderate signal) N (±2, 0.52), and A3 (high signal) N (±4, 0.52), where the sign of the mean is randomly chosen. The row effects ↵ki are chosen from N (0.5, 0.22) and the column effects ij are chosen from N (1, 0.52).

3.2.3 RESULTS ON SIMULATED DATA SETS
For method evaluation, we use the previously introduced biclustering consensus score for two sets of biclusters (Hochreiter et al., 2010), which is computed as follows:
Step (3) penalizes different numbers of biclusters in the sets. The highest consensus score is 1 and only obtained for identical sets of biclusters.
Table 1 shows the biclustering results for these data sets. RFN significantly outperformed all other methods (t-test and McNemar test of correct elements in biclusters).

3.3 GENE EXPRESSION DATA SETS
In this experiment, we test the biclustering methods on gene expression data sets, where the biclusters are gene modules. The genes that are in a particular gene module belong to the according bicluster and samples for which the gene module is activated belong to the bicluster. We consider three gene expression data sets which have been provided by the Broad Institute and were previously clustered by Hoshida et al. (2007) using additional data sets. Our goal was to study how well biclustering methods are able to recover these clusters without any additional information. (A) The “breast cancer” data set (van’t Veer et al., 2002) was aimed at a predictive gene signature for the outcome of a breast cancer therapy. We removed the outlier array S54 which leads to a data set with 97 samples and 1213 genes. In Hoshida et al. (2007), three biologically meaningful subclasses were found that should be re-identified. (B) The “multiple tissue types” data set (Su et al., 2002) are gene expression profiles from human cancer samples from diverse tissues and cell lines. The data set contains 102 samples with 5565 genes. Biclustering should be able to re-identify the tissue types. (C) The “diffuse large-B-cell lymphoma (DLBCL)” data set (Rosenwald et al., 2002) was aimed at predicting the survival after chemotherapy. It contains 180 samples and 661 genes. The three classes found by Hoshida et al. (2007) should be re-identified.
For methods assuming a fixed number of biclusters, we chose five biclusters — slightly higher than the number of known clusters to avoid biases towards prior knowledge about the number of actual clusters. Besides the number of hidden units (biclusters) we used the same parameters as described in Sec. 3.1. The performance was assessed by comparing known classes of samples in the data sets with the sample sets identified by biclustering using the consensus score defined in Subsection 3.2.3 — here the score is evaluated for sample clusters instead of biclusters. The biclustering results are summarized in Table 2. RFN biclustering yielded in two out of three datasets significantly better results than all other methods and was on second place for the third dataset (significantly according to a McNemar test of correct samples in clusters).

3.4 1000 GENOMES DATA SETS
In this experiment, we used RFN for detecting DNA segments that are identical by descent (IBD). A DNA segment is IBD in two or more individuals, if they have inherited it from a common ancestor, that is, the segment has the same ancestral origin in these individuals. Biclustering is well-suited to detect such IBD segments in a genotype matrix (Hochreiter, 2013; Povysil & Hochreiter, 2014), which has individuals as row elements and genomic structural variations (SNVs) as column elements. Entries in the genotype matrix usually count how often the minor allele of a particular SNV is present in a particular individual. Individuals that share an IBD segment are similar to each other because they also share minor alleles of SNVs (tagSNVs) within the IBD segment. Individuals that share an IBD segment represent a bicluster.
For our IBD-analysis we used the next generation sequencing data from the 1000 Genomes Phase 3. This data set consists of low-coverage whole genome sequences from 2,504 individuals of the main continental population groups (Africans (AFR), Asians (ASN), Europeans (EUR), and Admixed Americans (AMR)). Individuals that showed cryptic first degree relatedness to others were removed, so that the final data set consisted of 2,493 individuals. Furthermore, we also included archaic human and human ancestor genomes, in order to gain insights into the genetic relationships between humans, Neandertals and Denisovans. The common ancestor genome was reconstructed from human, chimpanzee, gorilla, orang-utan, macaque, and marmoset genomes. RFN IBD detec-
tion is based on low frequency and rare variants, therefore we removed common and private variants prior to the analysis. Afterwards, all chromosomes were divided into intervals of 10,000 variants with adjacent intervals overlapping by 5,000 variants
In the data of the 1000 Genomes Project, we found IBD-based indications of interbreeding between ancestors of humans and other ancient hominins within Africa (see Fig. 2 as an example of an IBD segment that matches the Neandertal genome).

4 CONCLUSION
We have introduced rectified factor networks (RFNs) for biclustering and benchmarked it with 13 other biclustering methods on artificial and real-world data sets.
On 400 benchmark data sets with artificially implanted biclusters, RFN significantly outperformed all other biclustering competitors including FABIA. On three gene expression data sets with previously verified ground-truth, RFN biclustering yielded twice significantly better results than all other methods and was once the second best performing method. On data of the 1000 Genomes Project, RFN could identify IBD segments which support the hypothesis that interbreeding between ancestors of humans and other ancient hominins already have taken place in Africa.
RFN biclustering is geared to large data sets, sparse coding, many coding units, and distinct membership assignment. Thereby RFN biclustering overcomes the shortcomings of FABIA and has the potential to become the new state of the art biclustering algorithm.
Acknowledgment. We thank the NVIDIA Corporation for supporting this research with several Titan X GPUs.
","Biclustering is evolving into one of the major tools for analyzing large datasets given as matrix of samples times features. Biclustering has several noteworthy applications and has been successfully applied in life sciences and e-commerce for drug design and recommender systems, respectively. FABIA is one of the most successful biclustering methods and is used by companies like Bayer, Janssen, or Zalando. FABIA is a generative model that represents each bicluster by two sparse membership vectors: one for the samples and one for the features. However, FABIA is restricted to about 20 code units because of the high computational complexity of computing the posterior. Furthermore, code units are sometimes insufficiently decorrelated. Sample membership is difficult to determine because vectors do not have exact zero entries and can have both large positive and large negative values. We propose to use the recently introduced unsupervised Deep Learning approach Rectified Factor Networks (RFNs) to overcome the drawbacks of existing biclustering methods. RFNs efficiently construct very sparse, non-linear, highdimensional representations of the input via their posterior means. RFN learning is a generalized alternating minimization algorithm based on the posterior regularization method which enforces non-negative and normalized posterior means. Each code unit represents a bicluster, where samples for which the code unit is active belong to the bicluster and features that have activating weights to the code unit belong to the bicluster. On 400 benchmark datasets with artificially implanted biclusters, RFN significantly outperformed 13 other biclustering competitors including FABIA. In biclustering experiments on three gene expression datasets with known clusters that were determined by separate measurements, RFN biclustering was two times significantly better than the other 13 methods and once on second place. On data of the 1000 Genomes Project, RFN could identify DNA segments which indicate, that interbreeding with other hominins starting already before ancestors of modern humans left Africa.",ICLR 2017 conference submission,False,,"This paper applies RFN for biclustering to overcome the drawbacks in FABIA. The proposed method performs best among 14 biclustering methods, However, my first concern is that from the methodological point of view, the novelty of the proposed method seems small. The authors replied to the same question which another reviewer gave, but the replies were not so convincing. 

This paper was actually difficult for me to follow. For instance, in Figure 1, a bicluster matrix is constructed as an outer product of $h$ and $w$. $h$ is a hidden unit, but what is $w$? I could not find any definition. Furthermore, I could not know how $h$ is estimated in this method. Therefore, I do NOT understand how this method performs biclustering. 

Totally, I am not sure that this paper is suitable for publication. 

Prons:
Empirical performance is good.

Cons:
Novelty of the proposed method
Some description in the paper is unclear.

---

The reviewers pointed out several issues with the paper, and all recommended rejection.

---

The paper presents a repurposing of rectified factor networks proposed
earlier by the same authors to biclustering. The method seems
potentially quite interesting but the paper has serious problems in
the presentation.


Quality:

The method relies mainly on techniques presented in a NIPS 2015 paper
by (mostly) the same authors. The experimental procedure should be
clarified further. The results (especially Table 2) seem to depend
critically upon the sparsity of the reported clusters, but the authors
do not explain in sufficient detail how the sparsity hyperparameter is
determined.


Clarity:

The style of writing is terrible and completely unacceptable as a
scientific publication. The text looks more like an industry white
paper or advertisement, not an objective scientific paper. A complete
rewrite would be needed before the paper can be considered for
publication. Specifically, all references to companies using your
methods must be deleted.

Additionally, Table 1 is essentially unreadable. I would recommend
using a figure or cleaning up the table by removing all engineering
notation and reporting numbers per 1000 so that e.g. ""0.475 +/- 9e-4""
would become ""475 +/- 0.9"". In general figures would be preferred as a
primary means for presenting the results in text while tables can be
included as supplementary information.


Originality:

The novelty of the work appears limited: the method is mostly based on
a NIPS 2015 paper by the same authors. The experimental evaluation
appears at least partially novel, but for example the IBD detection is
very similar to Hochreiter (2013) but without any comparison.


Significance:

The authors' strongest claim is based on strong empirical performance
in their own benchmark problems. It is however unclear how useful this
would be to others as there is no code available and the details of
the implementation are less than complete. Furthermore, the method
depends on many specific tuning parameters whose tuning method is not
fully defined, leaving it unclear how to guarantee the generalisation
of the good performance.

---

Clarity: The novel contribution of the paper --- Section 2.2 --- was very difficult to understand. The notation seemed inconsistent (particularly the use of l, p, and m), and I am still not confident that I understand the model being used.

Originality: The novelty comes from applying the RFN model (including the ReLU non-linearity and dropout training) to the problem of biclustering. It sounds like a good idea. 

Significance: The proposed algorithm appears to be a useful tool for unsupervised data modelling, and the authors make a convincing argument that it is significant. (I.E. The previous state-of-the-art, FABIA, is widely used and this method both outperforms and addresses some of the practical difficulties with that method.)

Quality: The experiments are high-quality. 

Comments:
1) The introduction claims that this method is much faster than FABIA because the use of rectified units allow it to be run on GPUs. It is not clear to me how this works. How many biclusters can be supported with this method? It looks like the number of biclusters used for this method in the experiments is only 3-5?
2) The introduction claims that using dropout during training increases sparsity in the bicluster assignments. This seems like a reasonable hypothesis, but this claim should be supported with a better argument or experiments.
3) How is the model deep? The model isn't deep just because it uses a relu and dropout.

---

This paper applies RFN for biclustering to overcome the drawbacks in FABIA. The proposed method performs best among 14 biclustering methods, However, my first concern is that from the methodological point of view, the novelty of the proposed method seems small. The authors replied to the same question which another reviewer gave, but the replies were not so convincing. 

This paper was actually difficult for me to follow. For instance, in Figure 1, a bicluster matrix is constructed as an outer product of $h$ and $w$. $h$ is a hidden unit, but what is $w$? I could not find any definition. Furthermore, I could not know how $h$ is estimated in this method. Therefore, I do NOT understand how this method performs biclustering. 

Totally, I am not sure that this paper is suitable for publication. 

Prons:
Empirical performance is good.

Cons:
Novelty of the proposed method
Some description in the paper is unclear.

---

This paper applies RFN for biclustering to overcome the drawbacks in FABIA. The proposed method performs best among 14 biclustering methods, However, my first concern is that from the methodological point of view, the novelty of the proposed method seems small. The authors replied to the same question which another reviewer gave, but the replies were not so convincing. 

This paper was actually difficult for me to follow. For instance, in Figure 1, a bicluster matrix is constructed as an outer product of $h$ and $w$. $h$ is a hidden unit, but what is $w$? I could not find any definition. Furthermore, I could not know how $h$ is estimated in this method. Therefore, I do NOT understand how this method performs biclustering. 

Totally, I am not sure that this paper is suitable for publication. 

Prons:
Empirical performance is good.

Cons:
Novelty of the proposed method
Some description in the paper is unclear.

---

The reviewers pointed out several issues with the paper, and all recommended rejection.

---

The paper presents a repurposing of rectified factor networks proposed
earlier by the same authors to biclustering. The method seems
potentially quite interesting but the paper has serious problems in
the presentation.


Quality:

The method relies mainly on techniques presented in a NIPS 2015 paper
by (mostly) the same authors. The experimental procedure should be
clarified further. The results (especially Table 2) seem to depend
critically upon the sparsity of the reported clusters, but the authors
do not explain in sufficient detail how the sparsity hyperparameter is
determined.


Clarity:

The style of writing is terrible and completely unacceptable as a
scientific publication. The text looks more like an industry white
paper or advertisement, not an objective scientific paper. A complete
rewrite would be needed before the paper can be considered for
publication. Specifically, all references to companies using your
methods must be deleted.

Additionally, Table 1 is essentially unreadable. I would recommend
using a figure or cleaning up the table by removing all engineering
notation and reporting numbers per 1000 so that e.g. ""0.475 +/- 9e-4""
would become ""475 +/- 0.9"". In general figures would be preferred as a
primary means for presenting the results in text while tables can be
included as supplementary information.


Originality:

The novelty of the work appears limited: the method is mostly based on
a NIPS 2015 paper by the same authors. The experimental evaluation
appears at least partially novel, but for example the IBD detection is
very similar to Hochreiter (2013) but without any comparison.


Significance:

The authors' strongest claim is based on strong empirical performance
in their own benchmark problems. It is however unclear how useful this
would be to others as there is no code available and the details of
the implementation are less than complete. Furthermore, the method
depends on many specific tuning parameters whose tuning method is not
fully defined, leaving it unclear how to guarantee the generalisation
of the good performance.

---

Clarity: The novel contribution of the paper --- Section 2.2 --- was very difficult to understand. The notation seemed inconsistent (particularly the use of l, p, and m), and I am still not confident that I understand the model being used.

Originality: The novelty comes from applying the RFN model (including the ReLU non-linearity and dropout training) to the problem of biclustering. It sounds like a good idea. 

Significance: The proposed algorithm appears to be a useful tool for unsupervised data modelling, and the authors make a convincing argument that it is significant. (I.E. The previous state-of-the-art, FABIA, is widely used and this method both outperforms and addresses some of the practical difficulties with that method.)

Quality: The experiments are high-quality. 

Comments:
1) The introduction claims that this method is much faster than FABIA because the use of rectified units allow it to be run on GPUs. It is not clear to me how this works. How many biclusters can be supported with this method? It looks like the number of biclusters used for this method in the experiments is only 3-5?
2) The introduction claims that using dropout during training increases sparsity in the bicluster assignments. This seems like a reasonable hypothesis, but this claim should be supported with a better argument or experiments.
3) How is the model deep? The model isn't deep just because it uses a relu and dropout.

---

This paper applies RFN for biclustering to overcome the drawbacks in FABIA. The proposed method performs best among 14 biclustering methods, However, my first concern is that from the methodological point of view, the novelty of the proposed method seems small. The authors replied to the same question which another reviewer gave, but the replies were not so convincing. 

This paper was actually difficult for me to follow. For instance, in Figure 1, a bicluster matrix is constructed as an outer product of $h$ and $w$. $h$ is a hidden unit, but what is $w$? I could not find any definition. Furthermore, I could not know how $h$ is estimated in this method. Therefore, I do NOT understand how this method performs biclustering. 

Totally, I am not sure that this paper is suitable for publication. 

Prons:
Empirical performance is good.

Cons:
Novelty of the proposed method
Some description in the paper is unclear.",,,,,,4.666666666666667,,,2.6666666666666665,,
778,"Authors: Mitsuru Ambai, Takuya Matsumoto, Takayoshi Yamashita, Hironobu Fujiyoshi
Source file: 778.pdf

ABSTRACT
This paper aims to reduce test-time computational load of a deep neural network. Unlike previous methods which factorize a weight matrix into multiple real-valued matrices, our method factorizes both weights and activations into integer and noninteger components. In our method, the real-valued weight matrix is approximated by a multiplication of a ternary matrix and a real-valued co-efficient matrix. Since the ternary matrix consists of three integer values, {−1, 0,+1}, it only consumes 2 bits per element. At test-time, an activation vector that passed from a previous layer is also transformed into a weighted sum of binary vectors, {−1,+1}, which enables fast feed-forward propagation based on simple logical operations: AND, XOR, and bit count. This makes it easier to deploy a deep network on low-power CPUs or to design specialized hardware. In our experiments, we tested our method on three different networks: a CNN for handwritten digits, VGG-16 model for ImageNet classification, and VGG-Face for large-scale face recognition. In particular, when we applied our method to three fully connected layers in the VGG-16, 15× acceleration and memory compression up to 5.2% were achieved with only a 1.43% increase in the top-5 error. Our experiments also revealed that compressing convolutional layers can accelerate inference of the entire network in exchange of slight increase in error.

1 INTRODUCTION
It is widely believed that deeper networks tend to achieve better performance than shallow ones in various computer vision tasks. As a trade-off of such impressive improvements, deeper networks impose heavy computational load both in terms of processing time and memory consumption due to an enormous amount of network parameters. For example, VGG-16 model (Simonyan & Zisserman, 2015) requires about 528 MBytes to store the network weights where fully connected layers account for 89% of them. A large number of multiplications and additions must also be processed at each layer which prevent real-time processing, consume vast amounts of electricity, and require a large number of logic gates when implementing a deep network on a FPGA or ASIC.
This article addresses the above issues. Specifically, we aimed to reduce the test-time computational load of a pre-trained network. Since our approach does not depend on a network configuration (e.g. a choice of an activation function, layer structures, and a number of neurons) and acts as a post-processing of network training, pre-trained networks shared in a download site of MatConvNet (Vedaldi & Lenc, 2015) and Model Zoo (BVLC) can be compressed and accelerated. Our method is outlined in Figure 1. The main idea is to factorize both weights and activations into integer and non-integer components. Our method is composed of two building blocks, as shown below.
Ternary weight decomposition for memory compression: We introduce a factored representation where the real-valued weight matrix is approximated by a multiplication of a ternary basis matrix and a real-valued co-efficient matrix. While the ternary basis matrix is sufficiently informative to reconstruct the original weights, it only consumes 2 bits per element. The number of rows of the coefficient matrix is also smaller than that of the original weight matrix. These compact representations result in efficient memory compression.
Binary activation encoding for fast feed-forward propagation: It has been reported that an inner product between a ternary and binary vector can be computed extremely fast by using three logical operations: AND, XOR, and bit count (Ambai & Sato, 2014). To use this technique, we approximate the activation vector by a weighted sum of binary vectors. This binary encoding must be processed as fast as possible at test-time. To overcome this issue, we use a fast binary encoding method based on a small lookup table.

1.1 RELATED WORK
There have been extensive studies on accelerating and compressing deep neural networks, e.g., on an FFT-based method (Mathieu et al., 2014), re-parameterization of a weight matrix (Yang et al., 2015), pruning network connection (Han et al., 2015; 2016), and hardware-specific optimization (Vanhoucke et al., 2011). In the following paragraphs, we only review previous studies that are intimately connected to ours.
It was pointed out by Denil et al. (2013) that network weights have a significant redundancy. Motivated by this fact, researchers have been involved in a series of studies on matrix/tensor factorization (Jaderberg et al., 2014; Zhang et al., 2015). In these studies, a weight matrix (or tensor) was factorized by minimizing an approximation error of original weights or activations. Jaderberg et al. (2014) exploited 1-D separable filter decomposition to accelerate feed-forward propagation. Zhang et al. (2015) proposed low-rank approximation based on generalized SVD to compress an entire deep network. Taking into account the lessons learned from these best practices, we also exploit the redundancy of the weights.
There is an another series of studies, integer decomposition (Hare et al., 2012; Yuji et al., 2014; Ambai & Sato, 2014), which involved accelerating test-time speed of a classifier by using fast logical operations. Although their contributions are limited to a shallow architecture such as a linear SVM, they achieved a noticeable acceleration. In these approaches, a real-valued weight vector is approximated by a weighted sum of a few binary or ternary basis vectors. To use fast logical operations, they extracted binary features from an image. Hare et al. (2012) and Yuji et al. (2014) exploited binary basis vectors, and Ambai & Sato (2014) investigated a case of ternary basis to improve approximation quality.
In a manner of speaking, our method is a unified framework of matrix/tensor factorization and integer decomposition reviewed in the above and inherits both their advantages. While the weight matrix is factorized to exploit low-rank characteristics, the basis matrix is restricted to take only three integer values, {−1, 0,+1}. In contrast to recent binary weighted networks such as XNOR-Net (Rastegari et al., 2016) which quantizes both activations and weights during backpropagation, it is not necessary for our method to change training algorithms at all. We can benefit from recent sophisticated training techniques, e.g. batch normalization (Ioffe & Szegedy, 2015), in combination with our method. Furthermore, our method does not need (iterative) end-to-end retraining which is needed for several previous studies such as network pruning (Han et al., 2015; 2016) and distillation (Hinton et al., 2014).

2 NETWORK COMPRESSION MODEL
In this section, we introduce our compression model and discuss time and space complexity. We consider a convolutional layer with a filter size of wx × wy × c, where wx and wy are the spacial size and c is a number of input channels. If wx = wy = 1, we can regard this layer as a fully connected layer. This three dimensional volume is reshaped to form a DI dimensional vector where DI = wx×wy×c. The filter weights and biases can be formulated by W ∈ RDI×DO and b ∈ RDO , where DO is a number of output channels. Let x ∈ RDI denote an activation vector obtained by
vectorizing the corresponding three dimensional volume. In test-time, we need to compute W>x+b followed by a non-linear activation function.
In our compressed network, W is decomposed into two matrices before test-time as follows:
W ≈MwCw, (1)
where Mw ∈ {−1, 0,+1}DI×kw is a ternary basis matrix, Cw ∈ Rkw×DO is a co-efficient matrix, and kw is the number of basis vectors, respectively. Since Mw only takes the three values, it consumes only 2 bits per element. Setting a sufficiently small value to kw further reduces total memory consumption. From the viewpoint of approximation quality, it should be noted that a large number of elements in W takes close to zero values. To fit them well enough, a zero value must be included in the basis. The ternary basis satisfies this characteristic. In practice, the ternary basis gives better approximation than the binary basis, as we discuss in Section 3.
The activation vector x is also factored to the following form:
x ≈Mxcx + bx1, (2)
where Mx ∈ {−1,+1}DI×kx is a binary basis matrix, cx ∈ Rkx is a real-valued co-efficient vector, bx ∈ R is a bias, and kx is the number of basis vectors, respectively. Since elements of x are often biased, e.g., activations from ReLU take non-negative values and have a non-zero mean, bx is added to this decomposition model. While cx and bx reflect a range of activation values, Mx determines approximated activation values within the defined range. This factorization must be computed at test-time because the intermediate activations depend on an input to the first layer. However, in practice, factorizing x into Mx, cx, and bx requires an iterative optimization, which is very slow. Since a scale of activation values within a layer is almost similar regardless of x, we pre-computed canonical cx and bx in advance and only optimized Mx at test-time. As we discuss in Section 4, an optimal Mx under fixed cx and bx can be selected using a lookup table resulting in fast factorization.
Substituting Eqs.(1) and (2) into W>x+ b, approximated response values are obtained as follows:
W>x+ b ≈ (MwCw)>(Mxcx + bx1) + b = C>wM > wMxcx + bxC > wM > w1+ b. (3)
A new bias bxC>wM > w1+ b in Eq.(3) is pre-computable in advance, because Cw,Mw, and bx are fixed at test-time. It should be noted that M>wMx is a multiplication of the ternary and binary matrix, which is efficiently computable using three logical operations: XOR, AND, and bit count, as previously investigated (Ambai & Sato, 2014). After computing M>wMx, the two co-efficient components, cx and Cw, are multiplied from the right and left in this order. Since cx and Cw are much smaller than W, the total number of floating point computations is drastically reduced.
The time and space complexity are summarized in Tables 1 and 2. As can be seen from Table 1, most of the floating operations are replaced with logical operations. In this table, B means the bit width of a variable used in the logical operations, e.g., B = 64 if a type of unsigned long long is used in C/C++ language. Table 2 suggests that if kw is sufficiently smaller than DI and DO, the total size of Mw and Cw is reduced compared to the original parameterization.
Algorithm 1 Decompose W into Mw and Cw Require: W, kw Ensure: factorized components Mw and Cw.
1: R←W 2: for i← 1 to kw do 3: Initialize m(i)w by three random values {−1, 0,+1}. 4: Minimize ||R−m(i)w c(i)w ||2F by repeating the following two steps until convergence. 5: [Step 1] c(i)w ←m(i)>w R/m(i)>w m(i)w 6: [Step 2] mij ← arg min
α∈{−1,0,+1} ||rj − αc(i)w ||22, for j = 1, · · · , DI
7: R← R−m(i)w c(i)w 8: end for

3 TERNARY WEIGHT DECOMPOSITION
To factorize W, we need to solve the following optimization problem.
Jw = min Mw,Cw
||W −MwCw||2F . (4)
However, the ternary constraint makes this optimization very difficult. Therefore, we take an iterative approach that repeats rank-one approximation one by one, as shown in Algorithm 1. Let m (i) w ∈ {−1, 0,+1}DI×1 denote an i-th column vector of Mw and c(i)w ∈ R1×DO denote an i-th row vector of Cw. Instead of directly minimizing Eq. (4), we iteratively solve the following rank-one approximation,
J (i)w = min m(i)w ,c(i)w ||R−m(i)w c(i)w ||2F , (5)
where R is a residual matrix initialized by W. We applied alternating optimization to obtain m(i)w and c(i)w . If m (i) w is fixed, c (i) w can be updated using a least squares method, as shown in line 5 of Algorithm 1. If c(i)w is fixed, mij , the j-th element of m (i) w , can be independently updated by exhaustively verifying three choices {−1, 0,+1} for each j = 1, · · · , DI , as shown in line 6 of Algorithm 1, where rj is a j-th row vector of R. After the alternating optimization is converged, R is updated by subtracting m(i)w c (i) w and passed to the next (i+ 1)-th iteration. Comparison of binary constraints with ternary constraints can be seen in Appendix A.

4 BINARY ACTIVATION ENCODING
Binary decomposition for a given activation vector x can be performed by minimizing
Jx(Mx, cx, bx;x) = ||x− (Mxcx + bx1)||22. (6)
In contrast to the case of decomposing W, a number of basis vectors kx can be set to a very small value (from 2 to 4 in practice) because x is not a matrix but a vector. This characteristic enables an exhaustive search for updating Mx. Algorithm 2 is an alternating optimization with respect to Mx, cx, and bx. By fixing Mx, we can apply a least squares method to update cx and bx (in lines 3-4 of Algorithm 2). If cx and bx are fixed, m (j) x , the j-th row vector of Mx, is independent of any other m (j′) x , j′ 6= j. We separately solve DI sub-problems formulated as follows:
m(j)x = arg min β∈{−1,+1}1×kx (xj − (βcx + bx))2, j = 1, · · · , DI , (7)
where xj is a j-th element of x. Since kx is sufficiently small, 2kx possible solutions can be exhaustively verified (in line 5 of Algorithm 2).
Our method makes this decomposition faster by pre-computing canonical cx and bx from training data and only optimizing Mx at test-time using lookup table. This compromise is reasonable because of the following two reasons: (1) scale of activation values is similar regardless of vector elements
Algorithm 2 Decompose x into Mx, cx, and bx Require: x, kx Ensure: factorized components Mx, cx, and bx.
1: Initialize Mx by three random values {−1,+1}. 2: Minimize ||x− (Mxcx + bx1)||22 by repeating the following two steps until convergence. 3: [Step 1] Update cx and bx using a least squares method. 4: cx ← (M>xMx)−1M > x (x− bx1), bx ← 1>(x−Mxcx)/DI 5: [Step 2] Update m(j)x for each j = 1, · · ·DI by an exhaustive search that minimizes Eq.(7).
within a layer, and (2) cx and bx reflect a scale of approximated activation values. Knowing these properties, cx and bx are obtained by minimizing Jx(M̂x, cx, bx; x̂) ,where x̂ is constructed as follows. First, NT different activation vectors T ∈ {xi}NTi=1 are collected from randomly chosen NT training data. Second, n elements are randomly sampled from xi. The sampled nNT elements are concatenated to form a vector x̂ ∈ RnNT . We use cx and bx as constants at test-time, and discard M̂x.
At test-time, we only need to solve the optimization of Eq. (7) for each xj . This can be regarded as the nearest neighbour search in one-dimensional space. We call βcx + bx a prototype. There are 2kx possible prototypes because β takes 2kx possible combinations. The nearest prototype to xj and an optimal solution m(j)x can be efficiently found using a lookup table as follows.
Preparing lookup table: We define L bins that evenly divide one-dimensional space in a range from the smallest to largest prototype. Let x̂l denote a representative value of the l-th bin. This is located at the center of the bin. For each x̂l, we solve Eq. (7) and assign the solution to the bin.
Activation encoding: At test-time, xj is quantized into L-levels. In other words, xj is transformed to an index of the lookup table. Let pmax and pmin denote the largest and smallest prototype, respectively. We transform xj as follows:
q = (L− 1)(xj − pmin)/(pmax − pmin) + 1, (8) l̂ = min(max(bq + 1/2c, 1), L). (9)
The range from pmin to pmax is linearly mapped to the range from 1 to L by Eq. (8). The term q is rounded and truncated from 1 to L by the max and min function in Eq. (9). If L is sufficiently large, the solution assigned to the l̂-th bin can be regarded as a nearly optimal solution because the difference between xj and the center of the bin x̂l̂ becomes very small. We found that L = 4096 is sufficient. The time complexity of this encoding is O(DI).

5 EXPERIMENTS
We tested our method on three different convolutional neural networks: CNN for handwritten digits (LeCun et al., 1998), VGG-16 for ImageNet classification (Simonyan & Zisserman, 2015), and VGGFace for large-scale face recognition (Parkhi et al., 2015). To compute memory compression rate, a size of W and a total size of Mw and Cw were compared. To obtain a fair evaluation of computation time, a test-time code of forward propagation was implemented without using any parallelization scheme, e.g., multi-threading or SIMD, and was used for both compressed and uncompressed networks. The computation time includes both binary activation encoding and calculation of Eq. (3). We used an Intel Core i7-5500U 2.40-GHz processor.

5.1 CNN FOR HANDWRITTEN DIGITS
MNIST is a database of handwritten digits which consists of 60000 training and 10000 test sets of 28× 28 gray-scale images with ground-truth labels from 0 to 9. We trained our CNN by using an example code in MatConvNet 1.0-beta18 (Vedaldi & Lenc, 2015). Our architecture is similar to LeNet-5 (LeCun et al., 1998) but has a different number of input and output channels. Each layer’s configuration is shown below:
(conv5-20)(maxpool)(conv5-64)(maxpool)(fc1024-640)(relu)(fc640-10)(softmax), (10)
where the parameters of a convolutional layer are denoted as (conv<receptive field size>-<number of output channels>), and parameters of a fully connected layer are denoted as (fc<number of input channels>-<number of output channels>). The (maxpool) is 2×2 subsampling without overlapping. The error rate of this network is 0.86%.
We applied our method to the first fully connected layer (fc1024-640) and set n = 10 andNT = 1000 to learn cx and bx from randomly chosen nNT activations. The cases of kx = 1, 2, 3, 4 and kw = DO, DO/2, DO/5 were tested. This means that kw was set to 640, 320, and 128.
Figures 2(a) and (b) show the relationships among the increases in error rates, memory compression rates, and acceleration rates. It was observed that error rates basically improved along with increasing kx and saturated at kx = 4. It is interesting that kx = 2, only 2 bits per element for encoding an activation x, still achieved good performance. While the smaller kw achieved better compression and acceleration rate, error rates rapidly increased when kw = DO/5. One of the well balanced parameters was (kx, kw) = (4, DO/2) which resulted in 1.95× faster processing and a 34.4% memory compression rate in exchange of a 0.19% increase in the error rate.

5.2 VGG-16 FOR IMAGENET CLASSIFICATION TASK
A dataset of ILSVRC2012 (Russakovsky et al., 2015) consists of 1.2 million training, 50,000 validation, and 100,000 test sets. Each image represents one of 1000 object categories. In this experiment, we used a network model of VGG-16 (model D in (Simonyan & Zisserman, 2015)) that consists of 13 convolutional layers and 3 fully connected layers followed by a softmax layer. The architecture is shown below:
(input) · · · (fc25088-4096)(relu)(fc4096-4096)(relu)(fc4096-1000)(softmax), (11) where layers before the first fully connected layer are omitted.
First, all three fully connected layers were compressed with our algorithm. We set n = 10 and NT = 1000 to learn cx and bx from randomly chosen nNT activations. The cases of kx = 2, 3, 4 and kw = DO/2, DO/4, DO/8, DO/16 were tested. The case of kx = 1 was omitted because this setting resulted in a very high error rate. Note that each of the fully connected layers has different DO. The kw was independently set for each layer according to its DO. The top-5 error rates were evaluated on the validation dataset. The top-5 error rate of the original network is 13.4%.
The three lines with circles in Figure 3 show these results. It should be noted that much higher acceleration rates and smaller compression rates with small loss of accuracies were achieved than the case of the network for MNIST. Interestingly, the case of kw = DO/4 still performed well due to the low-rank characteristics of weights in the VGG-16 network.
Although the error rates rapidly increased when kw took much smaller values, we found that this could be improved by tuning kw of the third layer. More specifically, we additionally tested the
following cases. While kw was set to DO/2, DO/4, DO/8, and DO/16 for the first and second layers, kw was fixed to DO for the third layer. The kx was set to 4. This is plotted with a red line in Figure 3. In this way, the memory compression rate and acceleration rate noticeably improved. Setting appropriate parameters for each layer is important to improve the total performance. Table 3 shows the details of the best balanced case in which 15× faster processing and 5.2% compression rate were achieved in exchange of a 1.43% increase in error rate.
Next, we also tested to compress convolutional layers. In this experiment, kw and kx were set to DO and 4. This setting accelerates each of the layers averagely 2.5 times faster. Table 4 shows positions of compressed layers, top-5 errors, and acceleration rates of the entire network. Although kw and kx must be larger than those of fully connected layers to avoid error propagation, it is still beneficial for entire acceleration. In summary, while compressing fully connected layers is beneficial for reducing memory, compressing convolutional layers is beneficial for reducing entire computation time.

5.3 VGG-FACE FOR FACE RECOGNITION TASK
The VGG-Face (Parkhi et al., 2015) is a model for extracting a face descriptor. It consists of a similar structure to VGG-16. The difference is that VGG-Face has only two fully connected layers, as shown below. (input) · · · (fc25088-4096)(relu)(fc4096-4096). (12) This network outputs a 4096-dimensional descriptor. We can verify whether two face images are identical, by evaluating the Euclidean distance of two l2-normalized descriptors extracted from
them. In our experiment, we did not apply a descriptor embedding technique based on triplet loss minimization (Parkhi et al., 2015). Following the evaluation protocol introduced in a previous paper (Parkhi et al., 2015), we used Labeled Faces in the Wild dataset (LFW) (Huang et al., 2007), which includes 13,233 face images with 5,749 identities. The LFW defines 1200 positive and 1200 negative pairs for testing. We used the 2400 test pairs to compute ROC curve and equal error rate (EER). The EER is defined as an error rate at the ROC operating point where the false positive and false negative rates are equal. The EER of the original network is 3.8%.
First, the two fully connected layers were compressed using our algorithm. We set n = 10 and NT = 1000 to learn cx and bx from randomly chosen nNT activations. We tested the cases of kx = 1, 2, 3, 4, and kw = DO/2, DO/4, DO/8, DO/16. Figure 4 reveals an interesting fact that even the fastest and smallest network configuration, kx = 1 and kw = DO/16, had less impact on the EER, in contrast to the previous ImageNet classification task in which the recognition results were corrupted when kx = 1. This indicates that the 4096-dimensional feature space is well preserved regardless of such coarse discretization of both weights and activations.
Next, we also tested to compress convolutional layers. In this experiment, kw and kx were set to DO and 4 which are the the same setting used in Table 4. Table 5 shows positions of compressed layers and EERs. The acceleration rates were almost the same as the results shown in Table 4. This is because architecture of VGG-face is the same as VGG-16 and we used the same parameter for kw and kx. Interestingly, compressing multiple layers from 2nd to 10th still preserves the original EER. As can be seen from this table, our method works very well depending on a certain kind of machine learning task.

6 CONCLUSION
We proposed a network compression model that consists of two components: ternary matrix decomposition and binary activation encoding. Our experiments revealed that the proposed compression model is available not only for multi-class recognition but also for feature embedding. Since our approach is post-processing for a pre-trained model, it is promising that recent networks designed for semantic segmentation, describing images, stereo matching, depth estimation, and much more can also be compressed with our method. For future work, we plan to improve approximation error further by investigating the discrete optimization algorithm.

A BINARY VS. TERNARY
Figure 5 illustrates the reconstruction errors of a 4096×1000 weight matrix of the last fully connected layer in VGG-16 model (Simonyan & Zisserman, 2015). We tested both the binary and ternary constraints on Mw for comparison. The reconstruction error Jw monotonically decreased along with an increase in kw. It was clear that the ternary basis provided better reconstruction than the binary basis.
","This paper aims to reduce test-time computational load of a deep neural network. Unlike previous methods which factorize a weight matrix into multiple real-valued matrices, our method factorizes both weights and activations into integer and noninteger components. In our method, the real-valued weight matrix is approximated by a multiplication of a ternary matrix and a real-valued co-efficient matrix. Since the ternary matrix consists of three integer values, {−1, 0,+1}, it only consumes 2 bits per element. At test-time, an activation vector that passed from a previous layer is also transformed into a weighted sum of binary vectors, {−1,+1}, which enables fast feed-forward propagation based on simple logical operations: AND, XOR, and bit count. This makes it easier to deploy a deep network on low-power CPUs or to design specialized hardware. In our experiments, we tested our method on three different networks: a CNN for handwritten digits, VGG-16 model for ImageNet classification, and VGG-Face for large-scale face recognition. In particular, when we applied our method to three fully connected layers in the VGG-16, 15× acceleration and memory compression up to 5.2% were achieved with only a 1.43% increase in the top-5 error. Our experiments also revealed that compressing convolutional layers can accelerate inference of the entire network in exchange of slight increase in error.",ICLR 2017 conference submission,False,,"This paper explores a new quantization method for both the weights and the activations that does not need re-training. In VGG-16 the method reaches compression ratios of 20x and experiences a speed-up of 15x. The paper is very well written and clearly exposes the details of the methodology and the results.

My major criticisms are three-fold: for one, the results are not compared to one of the many other pruning methods that are described in section 1.1, and as such the performance of the method is difficult to judge from the paper alone. Second, there have been several other compression schemes involving pruning, re-training and vector-quantization [e.g. 1, 2, 3] that seem to achieve much higher accuracies, compression ratios and speed-ups. Hence, for the practical application of running such networks on low-power, low-memory devices, other methods seem to be much more suited. The advantage of the given method - other then possibly reducing the time it takes to compress the network - is thus unclear. In particular, taking a pre-trained network as a starting point for a quantized model that is subsequently fine-tuned might not take much longer to process then the method given here (but maybe the authors can quantify this?). Finally, much of the speed-up and memory reduction in the VGG-model seems to arise from the three fully-connected layers, in particular the last one. The speed-up in the convolutional layers is comparably small, making me wonder how well the method would work in all-convolutional networks such as the Inception architecture.

[1] Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding,

---

The paper presents a method for quantizing neural network weights and activations. The method is not compared to related state-of-the-art quantization techniques, so in the current form the paper is not ready for acceptance.

---

This paper addresses to reduce test-time computational load of DNNs. Another factorization approach is proposed and shows good results. The comparison to the other methods is not comprehensive, the paper provides good insights.

---

I do need to see the results in a clear table. Original results and results when compression is applied for all the tasks. In any case, i would like to see the results when the compression is applied to state of the art nets where the float representation is important. For instance a network with 0.5% - 0.8% in MNIST. A Imagenet lower that 5% - 10%. Some of this results are feasible with float representation but probably imposible for restricted representations.

---

This paper explores a new quantization method for both the weights and the activations that does not need re-training. In VGG-16 the method reaches compression ratios of 20x and experiences a speed-up of 15x. The paper is very well written and clearly exposes the details of the methodology and the results.

My major criticisms are three-fold: for one, the results are not compared to one of the many other pruning methods that are described in section 1.1, and as such the performance of the method is difficult to judge from the paper alone. Second, there have been several other compression schemes involving pruning, re-training and vector-quantization [e.g. 1, 2, 3] that seem to achieve much higher accuracies, compression ratios and speed-ups. Hence, for the practical application of running such networks on low-power, low-memory devices, other methods seem to be much more suited. The advantage of the given method - other then possibly reducing the time it takes to compress the network - is thus unclear. In particular, taking a pre-trained network as a starting point for a quantized model that is subsequently fine-tuned might not take much longer to process then the method given here (but maybe the authors can quantify this?). Finally, much of the speed-up and memory reduction in the VGG-model seems to arise from the three fully-connected layers, in particular the last one. The speed-up in the convolutional layers is comparably small, making me wonder how well the method would work in all-convolutional networks such as the Inception architecture.

[1] Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding,

---

I suggest to refer the following two papers.

- Kyuyeon Hwang and Wonyong Sung. ""Fixed-point feedforward deep neural network design using weights +1, 0, and −1."" 2014 IEEE Workshop on Signal Processing Systems (SiPS). IEEE, 2014.

- Jonghong Kim, Kyuyeon Hwang, and Wonyong Sung. ""X1000 real-time phoneme recognition VLSI using feed-forward deep neural networks."" 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2014.

The retrain-based neural network quantization algorithm was first published in these two papers.

Thanks.

---

This paper explores a new quantization method for both the weights and the activations that does not need re-training. In VGG-16 the method reaches compression ratios of 20x and experiences a speed-up of 15x. The paper is very well written and clearly exposes the details of the methodology and the results.

My major criticisms are three-fold: for one, the results are not compared to one of the many other pruning methods that are described in section 1.1, and as such the performance of the method is difficult to judge from the paper alone. Second, there have been several other compression schemes involving pruning, re-training and vector-quantization [e.g. 1, 2, 3] that seem to achieve much higher accuracies, compression ratios and speed-ups. Hence, for the practical application of running such networks on low-power, low-memory devices, other methods seem to be much more suited. The advantage of the given method - other then possibly reducing the time it takes to compress the network - is thus unclear. In particular, taking a pre-trained network as a starting point for a quantized model that is subsequently fine-tuned might not take much longer to process then the method given here (but maybe the authors can quantify this?). Finally, much of the speed-up and memory reduction in the VGG-model seems to arise from the three fully-connected layers, in particular the last one. The speed-up in the convolutional layers is comparably small, making me wonder how well the method would work in all-convolutional networks such as the Inception architecture.

[1] Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding,

---

The paper presents a method for quantizing neural network weights and activations. The method is not compared to related state-of-the-art quantization techniques, so in the current form the paper is not ready for acceptance.

---

This paper addresses to reduce test-time computational load of DNNs. Another factorization approach is proposed and shows good results. The comparison to the other methods is not comprehensive, the paper provides good insights.

---

I do need to see the results in a clear table. Original results and results when compression is applied for all the tasks. In any case, i would like to see the results when the compression is applied to state of the art nets where the float representation is important. For instance a network with 0.5% - 0.8% in MNIST. A Imagenet lower that 5% - 10%. Some of this results are feasible with float representation but probably imposible for restricted representations.

---

This paper explores a new quantization method for both the weights and the activations that does not need re-training. In VGG-16 the method reaches compression ratios of 20x and experiences a speed-up of 15x. The paper is very well written and clearly exposes the details of the methodology and the results.

My major criticisms are three-fold: for one, the results are not compared to one of the many other pruning methods that are described in section 1.1, and as such the performance of the method is difficult to judge from the paper alone. Second, there have been several other compression schemes involving pruning, re-training and vector-quantization [e.g. 1, 2, 3] that seem to achieve much higher accuracies, compression ratios and speed-ups. Hence, for the practical application of running such networks on low-power, low-memory devices, other methods seem to be much more suited. The advantage of the given method - other then possibly reducing the time it takes to compress the network - is thus unclear. In particular, taking a pre-trained network as a starting point for a quantized model that is subsequently fine-tuned might not take much longer to process then the method given here (but maybe the authors can quantify this?). Finally, much of the speed-up and memory reduction in the VGG-model seems to arise from the three fully-connected layers, in particular the last one. The speed-up in the convolutional layers is comparably small, making me wonder how well the method would work in all-convolutional networks such as the Inception architecture.

[1] Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding,

---

I suggest to refer the following two papers.

- Kyuyeon Hwang and Wonyong Sung. ""Fixed-point feedforward deep neural network design using weights +1, 0, and −1."" 2014 IEEE Workshop on Signal Processing Systems (SiPS). IEEE, 2014.

- Jonghong Kim, Kyuyeon Hwang, and Wonyong Sung. ""X1000 real-time phoneme recognition VLSI using feed-forward deep neural networks."" 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2014.

The retrain-based neural network quantization algorithm was first published in these two papers.

Thanks.",,,,,,5.0,,,3.3333333333333335,,
