You are an expert in natural language processing and technical documentation, with a focus on metrics for evaluating generative models. I am building a metric bank to recommend the best metrics for various generative tasks. Each metric in this bank will have a corresponding Metric Card designed to provide detailed, standardized documentation about the metric. The purpose of these cards is to help users understand the metric’s strengths, weaknesses, and appropriate use cases.

Your Task

Using the provided materials, including the original paper, any reference implementations, the Metric Card template, and the BLEU Metric Card example, your task is to draft a comprehensive Metric Card for the given metric. The documentation must:
	1.	Follow the provided template closely, adhering to its format and required sections.
	2.	Incorporate relevant details from the original paper and reference materials, ensuring technical accuracy.
	3.	Match the style and quality of the BLEU example, which serves as the exemplar for this task.

Guidance
	•	Focus Areas:
	•	Clearly explain the metric’s purpose, how it works, and its mathematical formulation.
	•	Detail its inputs and outputs, as well as whether it is reference-based or input-required.
	•	Include practical considerations like efficiency, scalability, known limitations, and intended use cases.
	•	Use formal language that is precise yet accessible to technical readers.
	•	What to Avoid:
	•	Including extraneous implementation-specific details that are not broadly applicable (e.g., unique quirks of a particular implementation unless widely adopted).
	•	Providing actual code examples, as the metric bank will include a unified API for all metrics.

Materials You Will Be Provided
	1.	The original paper introducing or defining the metric, and any supplementary reference material such as documentation from popular implementations (e.g., SacreBLEU README for BLEU).
	2.	The Metric Card Template, which specifies the required structure and key sections.
	3.	The BLEU Metric Card Example, which serves as an exemplar of a high-quality metric card.

Outputs

Your output must be a complete Metric Card written in Markdown format that adheres to the provided template.

=== TEMPLATE FOR METRIC CARDS ===
---
# Metric Card for {{ metric_name | default("Metric Name", true) }}

{{ metric_summary | default("A brief description of the metric and its purpose.", true) }}

## Metric Details

### Metric Description

{{ metric_description | default("Detailed explanation of the metric, including how it is calculated and what it measures.", true) }}

- **Metric Type:** {{ metric_type | default("[More Information Needed]", true) }}
- **Range:** {{ metric_range | default("[More Information Needed]", true) }}
- **Higher is Better?:** {{ higher_is_better | default("[More Information Needed]", true) }}
- **Reference-Based?:** {{ reference_based | default("[More Information Needed]", true) }}
- **Input-Required?:** {{ input_required | default("[More Information Needed]", true) }}

### Formal Definition

{{ metric_definition | default("Mathematical formula or detailed algorithmic definition.", true) }}

### Inputs and Outputs

- **Inputs:**  
  {{ metric_inputs | default("Description of required inputs (e.g., generated text, reference text, input prompt).", true) }}
  
- **Outputs:**  
  {{ metric_outputs | default("Description of the metric output (e.g., scalar score, distribution).", true) }}

## Intended Use

### Domains and Tasks

- **Domain:** {{ domain | default("[More Information Needed]", true) }}
- **Tasks:** {{ tasks | default("[More Information Needed]", true) }}

### Applicability and Limitations

- **Best Suited For:** {{ best_suited_for | default("[More Information Needed]", true) }}
- **Not Recommended For:** {{ not_recommended_for | default("[More Information Needed]", true) }}

## Metric Implementation

### Reference Implementations

- **Libraries/Packages:** {{ libraries | default("[More Information Needed]", true) }}

### Computational Complexity

- **Efficiency:** {{ efficiency | default("[More Information Needed]", true) }}
- **Scalability:** {{ scalability | default("[More Information Needed]", true) }}

## Known Limitations

{{ known_limitations | default("[More Information Needed]", true) }}

- **Biases:** {{ biases | default("Potential biases inherent in the metric.", true) }}
- **Task Misalignment Risks:** {{ task_misalignment | default("[More Information Needed]", true) }}
- **Failure Cases:** {{ failure_cases | default("[More Information Needed]", true) }}

## Related Metrics

{{ related_metrics | default("[More Information Needed]", true) }}

## Further Reading

- **Papers:** {{ papers | default("[More Information Needed]", true) }}
- **Blogs/Tutorials:** {{ blogs | default("[More Information Needed]", true) }}

## Metric Card Authors

- **Authors:** {{ metric_authors | default("[More Information Needed]", true) }}  
- **Acknowledgment of AI Assistance:**  
  {{ ai_assistance | default("Portions of this metric card were drafted with assistance from generative AI. All content has been reviewed and curated by the author to ensure accuracy.", true) }}  
- **Contact:** {{ metric_contact | default("[More Information Needed]", true) }}
======

=== BLEU METRIC CARD EXAMPLE ===
---
# Metric Card for BLEU

BLEU (Bilingual Evaluation Understudy) is a widely used metric for evaluating the quality of text generated in tasks like machine translation and summarization. It measures the overlap of n-grams between a generated text and one or more reference texts, with a brevity penalty to penalize overly short translations. SacreBLEU, a modern implementation, ensures reproducibility and standardization of BLEU scores across research.

## Metric Details

### Metric Description

BLEU evaluates the quality of text generation by comparing n-grams in the generated output with those in one or more reference texts. It computes modified precision for n-grams and combines scores using a geometric mean, with a brevity penalty to ensure the length of the generated text matches that of the references. Higher BLEU scores indicate closer similarity to the references.

- **Metric Type:** Surface-Level Similarity
- **Range:** 0 to 1
- **Higher is Better?:** Yes
- **Reference-Based?:** Yes
- **Input-Required?:** Yes

### Formal Definition

\[
\text{BLEU} = \text{BP} \cdot \exp \left( \sum_{n=1}^N w_n \log p_n \right)
\]

where:
- \( \text{BP} = \min(1, e^{1 - r/c}) \) is the brevity penalty,
- \( r \) is the effective reference length (based on the closest matching reference length for each sentence),
- \( c \) is the candidate translation length,
- \( p_n \) is the modified precision for n-grams of length \( n \),
- \( w_n \) are weights for each n-gram (commonly uniform, \( w_n = \frac{1}{N} \)).

The standard BLEU metric uses up to 4-grams (\( N = 4 \)) and applies uniform weights.

### Inputs and Outputs

- **Inputs:**  
  - Generated text (candidate translation)  
  - Reference text(s) (gold-standard translations)  
- **Outputs:**  
  - Scalar BLEU score (range: 0 to 1)

## Intended Use

### Domains and Tasks

- **Domain:** Text Generation
- **Tasks:** Machine Translation, Summarization, Data-to-Text Generation

### Applicability and Limitations

- **Best Suited For:**  
  Structured tasks with a clear correspondence between generated and reference texts, such as translation or summarization.
  
- **Not Recommended For:**  
  Open-ended or creative generation tasks where diversity or semantic similarity matters more than lexical overlap (e.g., storytelling, dialogue).

## Metric Implementation

### Reference Implementations

- **Libraries/Packages:**
  - [SacreBLEU](https://github.com/mjpost/sacrebleu) (robust, standard implementation)
  - [NLTK](https://www.nltk.org/api/nltk.translate.html) (basic Python implementation)
  - [Hugging Face `evaluate`](https://huggingface.co/docs/evaluate) (integrated metric framework)

### Computational Complexity

- **Efficiency:**  
  BLEU is computationally efficient, requiring \( O(n \cdot m) \) operations for \( n \)-gram matching where \( n \) is the number of words in the candidate text and \( m \) is the number of reference words. SacreBLEU optimizes tokenization and scoring, making it highly suitable for large-scale evaluations.

- **Scalability:**  
  BLEU scales well across datasets of varying sizes due to its simple design. SacreBLEU further supports evaluation with multiple references, diverse tokenization schemes, and language-specific preprocessing, making it adaptable to diverse evaluation setups.

## Known Limitations

- **Biases:**  
  - BLEU penalizes valid paraphrases or semantically equivalent outputs that do not match reference n-grams exactly.  
  - The brevity penalty can overly penalize valid shorter outputs, particularly for tasks where shorter text may be acceptable or even preferred (e.g., summarization).  

- **Task Misalignment Risks:**  
  - BLEU is not designed for evaluating tasks with high diversity in acceptable outputs (e.g., open-ended dialogue).  
  - Scores depend on the quality and number of references; fewer or inconsistent references can lead to misleading evaluations.

- **Failure Cases:**  
  - BLEU struggles to capture semantic adequacy beyond lexical similarity. For instance, it cannot identify whether a translation preserves the meaning of the original sentence if word choices diverge significantly.

## Related Metrics

- **ROUGE:** Often used for summarization tasks, emphasizing recall over precision.  
- **METEOR:** Incorporates synonym matching for better semantic alignment.  
- **BERTScore:** Uses contextual embeddings for semantic similarity.  

## Further Reading

- **Papers:**  
  - [Original BLEU Paper (Papineni et al., 2002)](https://www.aclweb.org/anthology/P02-1040)  
  - [SacreBLEU: A Call for Clarity in Reporting BLEU Scores (Post, 2018)](https://www.aclweb.org/anthology/W18-6319)
  
- **Blogs/Tutorials:**  
  - [Understanding BLEU](https://machinelearningmastery.com)  
  - [SacreBLEU Documentation](https://github.com/mjpost/sacrebleu)

## Metric Card Authors

- **Authors:** Michael J. Ryan  
- **Acknowledgment of AI Assistance:**  
  Portions of this metric card were drafted with assistance from OpenAI's ChatGPT, based on user-provided inputs and relevant documentation. All content has been reviewed and curated by the author to ensure accuracy.  
- **Contact:** mryan0@stanford.edu
======

Now please write a high quality metric card for {METRIC NAME} given the provided materials!