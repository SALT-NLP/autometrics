import torch
from typing import List, Union
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from autometrics.metrics.reference_free.ReferenceFreeMetric import ReferenceFreeMetric

class GRMRewardModel(ReferenceFreeMetric):
    """---
# Metric Card for GRMRewardModel

The GRMRewardModel is a general-purpose reward model designed to evaluate the quality and safety of LLM-generated outputs. It achieves high generalization performance by applying a novel regularization method on hidden states during supervised fine-tuning. GRMRewardModel is fine-tuned on the decontaminated Skywork/Skywork-Reward-Preference-80K-v0.2 dataset and achieves state-of-the-art results among models of comparable size (3B), even outperforming some 8B reward models and proprietary LLM judges on RewardBench.

## Metric Details

### Metric Description

The GRMRewardModel is a transformer-based reward model that assigns scalar scores to LLM responses based on their alignment with human preferences. Its main innovation lies in *Hidden State Regularization (HSR)*, a method that regularizes the representation space of hidden states across different examples to improve generalization. Unlike most reward models that rely solely on output logits or fine-tune only the final layer, GRM constrains intermediate hidden states using contrastive learning objectives, enabling more robust preference modeling.

This particular instantiation, `Ray2333/GRM-Llama3.2-3B-rewardmodel-ft`, is based on the Llama-3.2-3B-Instruct model and fine-tuned on the Skywork preference dataset using pairwise comparisons of completions. Given a message (prompt + response), the model outputs a scalar reward score indicating the desirability of the response.

- **Metric Type:** Reference-Free  
- **Range:** Unbounded (typically scaled to [âˆ’5, 5] depending on implementation)  
- **Higher is Better?:** Yes  
- **Reference-Based?:** No  
- **Input-Required?:** Yes

### Formal Definition

Let $x$ denote the input prompt and $y$ the response generated by a model. The GRMRewardModel, $f_\theta$, takes the concatenated sequence $(x, y)$ and computes a scalar reward:

$$
\text{Reward} = f_\theta(x, y)
$$

The model is trained on pairwise preferences $(x, y^{+}, y^{-})$ using a pairwise loss function:

$$
\mathcal{L}_{\text{pairwise}} = -\log \sigma(f_\theta(x, y^{+}) - f_\theta(x, y^{-}))
$$

Additionally, during training, a hidden state regularization term is applied across layers:

$$
\mathcal{L}_{\text{HSR}} = \sum _{l=1}^{L} \text{Sim}(h_l^{+}, h_l^{-})
$$

where $h_l^{+}, h_l^{-}$ are the hidden states at layer $l$ for the preferred and dispreferred completions respectively, and $\text{Sim}$ is a similarity or contrastive loss (e.g., cosine similarity or InfoNCE loss).

### Inputs and Outputs

- **Inputs:**  
  - Tokenized message (prompt + response pair), formatted using the chat template from the underlying Llama tokenizer.  
  - Attention mask.  
  - (During training) Preference pairs.

- **Outputs:**  
  - A scalar reward value representing alignment with human preferences.

## Intended Use

### Domains and Tasks

- **Domain:** Text Generation  
- **Tasks:** Dialogue Generation, Response Generation, Safety Evaluation

### Applicability and Limitations

- **Best Suited For:**  
  Evaluating response quality and safety in conversational agents, especially in RLHF or reranking pipelines where pairwise preferences are available.

- **Not Recommended For:**  
  Evaluating creative generation (e.g., poetry or storytelling) or tasks that require ground-truth references (e.g., translation, summarization).

## Metric Implementation

### Reference Implementations

- **Libraries/Packages:**  
  - [Hugging Face Transformers](https://huggingface.co/Ray2333/GRM-Llama3.2-3B-rewardmodel-ft)  
  - Pretrained model available: `Ray2333/GRM-Llama3.2-3B-rewardmodel-ft`

### Computational Complexity

- **Efficiency:**  
  Inference is comparable to a standard forward pass through a 3B-parameter LLM, efficient for scoring single or batched examples.

- **Scalability:**  
  Scales well to large-scale evaluation or online RLHF pipelines. Small model size (3B) enables practical deployment on modest hardware (e.g., single A100).

## Known Limitations

- **Biases:**  
  Depends on the biases of the preference data (Skywork dataset), which may reflect annotator or cultural preferences.  
  Sensitivity to prompt formatting may affect score stability.

- **Task Misalignment Risks:**  
  Since it is trained on generic preference data, it may not align with task-specific criteria or specialized user needs.

- **Failure Cases:**  
  May output high reward scores for fluent but factually incorrect or manipulative responses, especially outside the training distribution.  
  Does not explicitly verify factual correctness or consistency.

## Related Metrics

- **OpenAI's GPT-Judge**: LLM-as-a-judge paradigm with GPT-4.  
- **DPO/RM models from Anthropic and OpenAI**: Trained using preference data, but often larger in scale and less interpretable.  
- **RRHF Reward Models**: Reward models trained on Reddit or StackExchange upvotes.  
- **UltraRM, Starling-RM**: Competing public reward models evaluated on RewardBench.

## Further Reading

- **Papers:**  
  - Yang et al. (2024), *Regularizing Hidden States Enables Learning Generalizable Reward Model for LLMs*, NeurIPS 2024  
    https://openreview.net/forum?id=jwh9MHEfmY

- **Blogs/Tutorials:**  
  - [More Information Needed]

## Citation

```
@inproceedings{yang2024regularizing,  
  title={Regularizing Hidden States Enables Learning Generalizable Reward Model for {LLM}s},  
  author={Rui Yang and Ruomeng Ding and Yong Lin and Huan Zhang and Tong Zhang},  
  booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},  
  year={2024},  
  url={https://openreview.net/forum?id=jwh9MHEfmY}  
}
```

## Metric Card Authors

- **Authors:** Michael J. Ryan  
- **Acknowledgment of AI Assistance:**  
  Portions of this metric card were drafted with assistance from generative AI. All content has been reviewed and curated by the author to ensure accuracy.  
- **Contact:** mryan0@stanford.edu"""
    def __init__(
        self,
        name: str = "GRMRewardModel",
        description: str = "Ray2333/GRM-Llama3.2-3B reward model (reference-free).",
        model_name: str = "Ray2333/GRM-Llama3.2-3B-rewardmodel-ft",
        torch_dtype = torch.float16,
        device_map: Union[str, dict] = "auto",
        batch_size: int = 1,
        persistent: bool = True
    ):
        super().__init__(name, description)
        self.model_name = model_name
        self.torch_dtype = torch_dtype
        self.device_map = device_map
        self.batch_size = batch_size
        self.persistent = persistent
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.tokenizer = None
        self.model = None
        if self.persistent:
            self._load_model()

    def _load_model(self):
        """Load tokenizer and model into memory."""
        if self.model is None:
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            self.model = AutoModelForSequenceClassification.from_pretrained(
                self.model_name,
                torch_dtype=self.torch_dtype,
                device_map=self.device_map,
                trust_remote_code=True
            )
            self.model.eval()

    def _unload_model(self):
        """Unload model and tokenizer from memory to free resources."""
        if self.model is not None:
            del self.model
            del self.tokenizer
            torch.cuda.empty_cache()
            self.model = None
            self.tokenizer = None

    def calculate(self, input: str, output: str, **kwargs) -> float:
        """
        Score a single input-output pair.
        """
        if self.model is None:
            self._load_model()
        # prepare conversation
        conv = [
            {"role": "user", "content": input},
            {"role": "assistant", "content": output}
        ]
        # tokenize chat template
        tok = self.tokenizer.apply_chat_template(conv, tokenize=True, return_tensors="pt").to(self.device)
        with torch.no_grad():
            outputs = self.model(**tok)
            logits = outputs.logits if hasattr(outputs, 'logits') else outputs[0]
            # shape (batch,1)
            score = logits.squeeze(-1).squeeze(0).cpu().item()
        if not self.persistent:
            self._unload_model()
        return score

    def calculate_batched(self, inputs: List[str], outputs: List[str], **kwargs) -> List[float]:
        """
        Score batches of input-output pairs.
        """
        if self.model is None:
            self._load_model()
        all_scores: List[float] = []
        for i in range(0, len(inputs), self.batch_size):
            chunk_in = inputs[i:i+self.batch_size]
            chunk_out = outputs[i:i+self.batch_size]
            convs = [
                [{"role": "user", "content": inp}, {"role": "assistant", "content": out}]
                for inp, out in zip(chunk_in, chunk_out)
            ]
            tok = self.tokenizer.apply_chat_template(convs, tokenize=True, return_tensors="pt").to(self.device)
            with torch.no_grad():
                out_obj = self.model(**tok)
                logits = out_obj.logits if hasattr(out_obj, 'logits') else out_obj[0]
                vals = logits.squeeze(-1).cpu().tolist()
                # ensure list
                if isinstance(vals, float):
                    all_scores.append(vals)
                else:
                    all_scores.extend(vals)
        if not self.persistent:
            self._unload_model()
        return all_scores 